[
  {
    "text1": "Ao longo dos últimos anos, tem aumentado exponencialmente a utilização de Tecnologiasde Informação (TIs) e de ferramentas computacionais em vários setores económicos, incluindoo setor da saúde, por serem defendidas como tecnologias que podem transformare melhorar radicalmente a prestação de cuidados de saúde.O principal objetivo das instituições de saúde é prestar os melhores cuidados de saúdeaos seus utentes, garantindo, assim, a prestação de serviços de qualidade e a consequentesatisfação dos utentes, bem como reduzir os custos e desperdícios desnecessários associados.Portanto, as decisões devem ser tomadas rapidamente mas, igualmente, eficazmente.Acredita-se, assim, que o futuro realmente bem sucedido das TIs no setor da saúdepassa então pelo desenho e pela implementação de sistemas user-friendly, incluindo Sistemasde Apoio à Decisão Clínica (SADCs), personalizados e focados no paciente, bem comoa receptibilidade dos profissionais de saúde aos mesmos. Abrange, igualmente, o uso detecnologias emergentes na sua conceção, incluindo Business Intelligence (BI), de modo a tirarreal partido da informação disponível.Deste modo, no âmbito deste projeto de dissertação, foram desenhadas, desenvolvidase exploradas uma nova geração de ferramentas Web de Business Intelligence para o apoio àdecisão e a prática clínica em unidades hospitalares. Englobou, em particular, o desenvolvimentode uma plataforma de BI versátil, incluindo a sua aplicação a dois casos práticosdiferentes, notadamente no apoio à decisão nas listas de espera de consultas e de cirurgias,assim como nos cuidados de Ginecologia e Obstetrícia (GO), e de uma ferramenta decodificação clínica ICD-9-CM (International Classification of Diseases, Ninth Revision, ClinicalModification).Assim, as ferramentas foram projetadas de modo a auxiliar os profissionais de saúde doCentro Hospitalar do Porto (CHP) no seu trabalho diário, incluindo a lidar com pacientesem condições delicadas e determinadas situações que requerem uma tomada de decisãoeficiente, bem como a codificação clínica de episódios de altas hospitalares.",
    "text2": "A alocação adequada de órgãos para transplantação é crítica e crucial. No entanto, o número de órgãos a ser doados não é suficiente dada a quantidade de pacientes em lista de espera. Assim, a determinação do maior número possível de potenciais dadores, de forma eficiente e eficaz torna-se essencial e pode contribuir para melhorar a taxa de sucesso de transplantação de órgãos.Ao longo dos últimos anos, a utilização de Tecnologias de Informação (TIs) e de ferramentas computacionais em vários setores económicos, incluindo o setor da saúde, cresceu exponencialmente, já que têm potencial para transformar e melhorar a prestação de cuidados de saúde. Assim, e aliando a necessidade da eficiência na descoberta de potenciais dadores com a emergência das TIs na saúde, surge a necessidade de uma plataforma Web de apoio à decisão clínica. O objetivo desta plataforma é automatizar o processo de descoberta de informaçãoútil e acionável, através da utilização de tecnologias como Business Intelligence(BI) e Data Mining (DM), ajudando na tomada de decisão clínica diária. Assim, esta éresponsável pela recolha, gestão, armazenamento e sinalização de potenciais dadores.No âmbito deste projeto de dissertação, foi redesenhada e otimizada a plataforma Web Organite, atualmente implementada no Centro Hospitalar do Porto (CHP). Envolveu transformações tanto no design da interface do utilizador, como no modo como a informação está organizada na plataforma, de forma a melhorar a experiência do utilizador e a interação com os dados clínicos. Foi ainda desenvolvida uma metodologia, com base em técnicas de Data Mining, para construir um modelo preditivo que avalia quais os pacientes que dão entrada no hospital que têm maior probabilidade em serpotenciais dadores de órgãos. O objetivo é tornar mais simples e eficaz o processo de identificação de potenciais dadores, contribuindo positivamente na tomada de decisão do Gabinete de Coordenação de Colheita e Transplantação (GCCT), e impactando na redução da lista de doentes que aguarda um transplante.",
    "similarity": 0.3554952665063469
  },
  {
    "text1": "Os exames endoscópicos são prescritos em grandes quantidades, pois são eficazes no diagnóstico,baratos quando comparados com outros exames e estarem generalizados há muito tempo, pois podemser realizados em quase todos os hospitais. O resultado deste exame é normalmente um relatório queinclui anotações médicas complementadas com algumas imagens retiradas durante o exame.Alguns dos exames realizados são apenas feitos para confirmar informação já recolhida, o que leva a umaduplicação de esforços desnecessária e desperdício de recursos. Os profissionais de saúde podemdescartar informação relevante ao não conseguirem anotar em pormenor uma região de interesse paraposterior análise mais cuidada.O objetivo deste trabalho consiste na criação de um sistema que consiga resolver o problema apresentadoanteriormente, usando tecnologia de reconhecimento de voz. Este sistema deve reconhecer um pequenovocabulário, independentemente do falante, usado para anotar regiões de interesse nos exames.O sistema MyEndoscopy atua como uma cloud privada, que contém vários dispositivos que usam eprovidenciam serviços entre si. O dispositivo central deste sistema é a MIVbox, que se liga ao endoscópioe permite a captura digital do sinal de vídeo que este gera. A principal funcionalidade providenciada poreste sistema é a capacidade de armazenar indefinidamente os vídeos completos que são produzidosdurante exames endoscópicos, bem como disponibilizar estes vídeos e outros dados para outrosprofissionais de saúde que os necessitem de consultar.Nesta dissertação apresenta-se um módulo de reconhecimento de voz para línguas portuguesa e inglesa,denominado MIVcontrol, totalmente integrado no sistema MyEndoscopy. Este módulo reconhece umpequeno vocabulário, que consiste em comandos usado para controlar os outros módulos. O MIVcontrol éapresentado como uma alternativa a sistemas similares baseados na cloud, que resolve certos problemasrelacionados com proteção de dados e segurança.Foi realizado um estudo sobre o módulo desenvolvido para determinar a sua eficácia em comparação aoestado da arte. Na sequência desse estudo conclui-se que o sistema tinha uma taxa de erro comparável asistemas similares para outras línguas, e que como resultado é passível de ser usado em ambientes reais.",
    "text2": "A displasia congénita da anca é uma doença esquelética congénita comum em recém nascidos. O seudiagnóstico é importante para evitar complicações tardias no crescimento e locomoção. Por ser um exametão complexo e de grande responsabilidade, os diagnósticos feitos pelos profissionais são muitas vezesassociados a um grau elevado de incerteza na decisão, provocando receio na realização de exames dogénero. Os atos complementares de diagnóstico, neste caso a construção de ferramentas de apoio, sãosem dúvida o maior passo para reduzir ou eliminar este problema. Desta forma, com profissionais maisinstruídos, consegue-se um diagnóstico mais seguro e fiável.São apresentadas recomendações para a realização do exame, englobando parâmetros como a realizaçãodo exame clínico, do exame de ecografia e da leitura de imagens de ecografia. As imagens de ecografiatêm imenso ruído e para permitir um melhor processamento foram experimentadas operações básicas deprocessamento de imagem. É também proposto um relatório normalizado para este exame. O benefício daimplementação do relatório é a sua ligação ao sistema de machine learning em que informaçõescolocadas nos campos de preenchimento do relatório seriam transformadas em metainformação dasimagens de ecografia guardadas também no relatório, funcionando como a alimentação do sistema. Estesistema permitiria avaliar e classificar imagens de ecografia de um exame às articulações coxo-femorais.Para além destas ferramentas descritas, é proposto uma para otimizar em termos práticos o exame - umsistema de comandos por voz com ligação ao ecógrafo para que o profissional não tenha de desviar aatenção para carregar num simples botão do ecógrafo para assinalar frames essenciais para o diagnóstico.A adoção de ferramentas de apoio ao diagnóstico da displasia congénita da anca que permitam melhorara prestação dos cuidados de saúde é uma necessidade. As ferramentas apresentadas são um contributo erepresentam o início de novas abordagens ao despiste desta anomalia.",
    "similarity": 0.4101756161659209
  },
  {
    "text1": "Os Sistemas de Informação (SI) das Unidades de Saúde recorrem, cadavez mais, a ferramentas informáticas para gerir a grande quantidade de informação, e assim garantir a qualidade e a segurança da mesma.A interoperabilidade semântica é uma urgência nos Sistemas de Informação de Saúde (SIS), pois, através de normas, permite a uniformização dostermos médicos, assegurando registos clínicos com informação fiável, semredundância e ambiguidade, conferindo qualidade e segurança à informação.Sem terminologias médicas a prestação de cuidados de saúde pode tornaruma tarefa complexa e conduzir a erros médicos, pelo que a utilizaçãodas mesmas é fulcral para o registo de diagnósticos, procedimentos e peçasanatómicas no Registo Clínico Eletrónico (RCE) de cada utente.Como tal, o desenvolvimento de uma plataforma de interoperabilidadesemântica vai permitir uniformizar termos médicos e conduzir à diminuiçãode erros. Recorrendo às tecnologias mais avançadas de eHealth e mHealth,pretende-se implementar o Systematized Nomenclature of Medicine ClinicalTerms (SNOMED CT) em contexto hospitalar real, num serviço de AnatomiaPatológica.A solução consiste numa aplicação independente da plataforma de registode relatórios médicos, utilizando Web Services, que proporcionam a interação humana com diferentes interfaces para diferentes tipos de dispositivoseletrónicos.",
    "text2": "O controlo e a prevenção de infeções nosocomiais são essenciais para aredução de custos, bem como para a melhoria dos cuidados prestados numainstituição de saúde. Por outro lado, o tratamento de dados que permitamcompreender, caracterizar e monitorizar as infeções possibilita um controloe uma prevenção mais eficaz das mesmas. Sendo um método automatizadoe eficiente para o tratamento de dados, a tecnologia de Business Intelligencepermite a extração de informação importante para gerar conhecimento quepode auxiliar o processo de tomada de decisão dos profissionais de saúde.O principal objetivo deste trabalho é o desenvolvimento e implementaçãode uma plataforma de Business Intelligence que permita o estudo da incidênciade infeção nosocomial nas Unidades de Medicina do Centro Hospitalardo Porto. Este estudo é feito através da apresentação de um conjunto deindicadores clínicos (informações importantes extraídas dos dados referentesa infeções nosocomiais) que ajudam a analisar e caracterizar estas infeções.Por conseguinte, depois de identificados os indicadores relevantes, torna-sepertinente desenvolver um sistema que permita tratar os dados, extrair osindicadores destes e apresentá-los, de forma atrativa, na plataforma. Porsua vez, a plataforma facilita a análise das informações que disponibiliza,apoiando a tomada de decisões, nomeadamente através da identificação dosprincipais fatores de risco. Assim, o sistema atua como um Sistema de Apoioà Decisão Clínica, podendo auxiliar no controlo e prevenção destas infeções.Pretende-se ainda estudar a aplicabilidade da tecnologia de Data Miningna criação de modelos de classificação capazes de prever a ocorrência deinfeções nosocomiais, na presença de determinados fatores de risco.O conhecimento obtido com a análise dos indicadores e as previsões efetuadaspode possibilitar a diminuição da incidência de infeção nosocomiale, consequentemente, a redução dos custos associados à sua ocorrência, bemcomo o aumento da segurança e do bem-estar dos doentes, ao permitir a tomadade decisões mais fundamentadas. A aplicação de Business Intelligencena área da saúde contribui para melhorar não só o fluxo de trabalho diárionas unidades de saúde, como também a qualidade dos cuidados prestados.",
    "similarity": 0.3627529820792583
  },
  {
    "text1": "Most event data analysis tasks in the ATLAS project require both intensive data access and processing, where some tasks are typically I/O bound while others are compute bound. This dissertation work mainly focus improving the code efficiency of the compute bound stages of the ATLAS detector data analysis, complementing a parallel dissertation work that addresses the I/O bound issues.The main goal of the work was to design, implement, validate and evaluate an improved and more robust data analysis task, originally developed by the LIP research group at the University of Minho. This involved tuning the performance of both Top Quark and Higgs Boson reconstruction of events, within the ATLAS framework, to run on homogeneous systems with multiple CPUs and on heterogeneous computing platforms. The latter are based on multicore CPU devices coupled to PCI-E boards with many-core devices, such as the Intel Xeon Phi or the NVidia Fermi GPU devices.Once the critical areas of the event analysis were identified and restructured, two parallelization approaches for homogeneous systems and two for heterogeneous systems were developed and evaluated to identify their limitations and the restrictions imposed by the LipMiniAnalysis library, an integral part of every application developed at LIP. To efficiently use multiple CPU resources, an application scheduler was also developed to extract parallelism from simultaneously execution of both sequential and parallel applications when processing large sets of input data files.A key achieved outcome of this work is a set of guidelines for LIP researchers to efficiently use the available computing resources in current and future complex parallel environments, taking advantage of the acquired expertise during this dissertation work. Further improvements on LIP libraries can be achieved by developing a tool to automatically extract parallelism of LIP applications, complemented by the application scheduler and additional suggested approaches.",
    "text2": "Recent evolution of high performance computing moved towards heterogeneous platforms:multiple devices with different architectures, characteristics and programming models, shareapplication workloads. To aid the programmer to efficiently explore these heterogeneousplatforms several frameworks have been under development. These dynamically manage theavailable computing resources through workload scheduling and data distribution, dealingwith the inherent difficulties of different programming models and memory accesses. Amongother frameworks, these include GAMA and StarPU.The GAMA framework aims to unify the multiple execution and memory models ofeach different device in a computer system, into a single, hardware agnostic model. It wasdesigned to efficiently manage resources with both regular and irregular applications, andcurrently only supports conventional CPU devices and CUDA-enabled accelerators. StarPUhas similar goals and features with a wider user based community, but it lacks a singleprogramming model.The main goal of this dissertation was an in-depth evaluation of a heterogeneous frameworkusing a complex application as a case study. GAMA provided the starting vehiclefor training, while StarPU was the selected framework for a thorough evaluation. The progressivephoton mapping irregular algorithm was the selected case study. The evaluationgoal was to assert the StarPU effectiveness with a robust irregular application, and make ahigh-level comparison with the still under development GAMA, to provide some guidelinesfor GAMA improvement.Results show that two main factors contribute to the performance of applications writtenwith StarPU: the consideration of data transfers in the performance model, and chosenscheduler. The study also allowed some caveats to be found within the StarPU API. Althoughthis have no effect on performance, they present a challenge for new coming developers.Both these analysis resulted in a better understanding of the framework, and a comparativeanalysis with GAMA could be made, pointing out the aspects where GAMA could be furtherimproved upon.",
    "similarity": 0.3597614786850685
  },
  {
    "text1": "Since the dawn of times, curiosity and necessity to improve the quality of theirlife, led humans to find means to understand everything surrounding them, aimingat improving it. Whereas the creating abilities of some was growing, the capacityto comprehend of others follow their steps. Disassembling physical objects to comprehendthe connections between the pieces in order to understand how they worktogether is a common human behavior. With the computers arrival, humans feltthe necessity of applying the same techniques (disassemble to comprehend) to theirprograms.Traditionally, these programs are written resorting to general-purpose programminglanguages. Hence, techniques and artifacts, used to aid on program comprehension,were built to facilitate the work of software programmers on maintainingand improving programs that were developed by others. Generally, these genericlanguages deal with concepts at a level that the human brain can hardly understand.So understanding programs written in this languages is an hard task, because thedistance between the concepts at the program level and the concepts at the problemlevel is too big.Thus, as in politics, justice, medicine, etc. groups of words are regularly usedfacilitating the comprehension between people, also in programming, languages thataddress a specific domain were created. These programming languages raise theabstraction of the program domain, shortening the gap to the concepts of the problemdomain.Tools and techniques for program comprehension commonly address the programdomain and they took little advantage of the problem domain. In this master’s thesis,the hypothesis that it is easier to comprehend a program when the underlying problemand program domains are known and a bridge between them is established, isassumed. Then, a program comprehension technique for domain specific languages,is conceived, proposed and discussed. The main objective is to take advantage fromthe large knowledge about the problem domain inherent to the domain specific language,and to improve traditional program comprehension tools that only dealt, untilthen, with the program domain. This will create connections between both programand problem domains. The final result will show, visually, what happens internallyat the program domain level, synchronized with what happens externally, at problemlevel.",
    "text2": "Software applications evolve over the years at a cost: their architecture modularity tends to be degraded. This happens mainly because software application maintenance often leads to architectural degradation. In this context, software architects need to elaborate strategies for detecting architectural degradation symptoms and thus maintaining the software architectural quality. The elaborations of these strategies often rely on tools with domain-specific languages (DSLs), which help them to specify software architecture rules. These tools also enforce the adherence of these rules in the evolving program. However, their adoption in mainstream software development is largely dependent on the usability of the language. Unfortunately, it is also often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify before a DSL is actually used by its stakeholders. There is even less support and experience on how to quantitatively evaluate the usability of DSLs used in software maintenance tasks. To this end in this dissertation, a usability measurement framework was developed based on the Cognitive Dimensions of Notations (CDN). The framework was evaluated both qualitatively and quantitatively using two textual DSLs for architecture rules in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify the DSL usability limitations to be addressed, (2) to reveal specific features of the DSLs favoring software maintenance tasks, and (3) to successfully analyze eight usability dimensions that are critical in many DSLs. However, along with these results this evaluation also revealed that this kind of tools lack support for communication among the stakeholders, creating a gap in the software development. To solve this problem we proposed heuristics for tools that use DSLs for detecting architecture degradation symptoms. These heuristics will permit the exchange of information between the stakeholders, thereby, also increasing the tool usability. Finally, we chose TamDera as the tool to implement these heuristics in our study domain. Therefore, we implemented in the new version of TamDera the communication support for the stakeholders by using a new architecture and a new environment with the developed heuristics.",
    "similarity": 0.5537685721480735
  },
  {
    "text1": "Since the dawn of times, curiosity and necessity to improve the quality of theirlife, led humans to find means to understand everything surrounding them, aimingat improving it. Whereas the creating abilities of some was growing, the capacityto comprehend of others follow their steps. Disassembling physical objects to comprehendthe connections between the pieces in order to understand how they worktogether is a common human behavior. With the computers arrival, humans feltthe necessity of applying the same techniques (disassemble to comprehend) to theirprograms.Traditionally, these programs are written resorting to general-purpose programminglanguages. Hence, techniques and artifacts, used to aid on program comprehension,were built to facilitate the work of software programmers on maintainingand improving programs that were developed by others. Generally, these genericlanguages deal with concepts at a level that the human brain can hardly understand.So understanding programs written in this languages is an hard task, because thedistance between the concepts at the program level and the concepts at the problemlevel is too big.Thus, as in politics, justice, medicine, etc. groups of words are regularly usedfacilitating the comprehension between people, also in programming, languages thataddress a specific domain were created. These programming languages raise theabstraction of the program domain, shortening the gap to the concepts of the problemdomain.Tools and techniques for program comprehension commonly address the programdomain and they took little advantage of the problem domain. In this master’s thesis,the hypothesis that it is easier to comprehend a program when the underlying problemand program domains are known and a bridge between them is established, isassumed. Then, a program comprehension technique for domain specific languages,is conceived, proposed and discussed. The main objective is to take advantage fromthe large knowledge about the problem domain inherent to the domain specific language,and to improve traditional program comprehension tools that only dealt, untilthen, with the program domain. This will create connections between both programand problem domains. The final result will show, visually, what happens internallyat the program domain level, synchronized with what happens externally, at problemlevel.",
    "text2": "O recurso a ferramentas de geração automática de código permite economizar tempo quando se desenvolvem soluções de software, factor importante em questões de produtividade.Existe um conjunto de padrões de conceção [Gamma et al., 1995] que representam soluções genéricas para problemas relativos ao desenvolvimento de aplicações de software, numa perspetiva orientada aos objetos. Para cada um deles pode ser vista a sua estrutura de classes, métodos e relacionamentos, bem como as situações mais adequadas para a sua utilização. Bastará consultar o catálogo de padrões de conceção [Gamma et al., 1995] e utilizar aquele que mais se adequar à resolução de determinado problema que surja no desenvolvimento de um novo programa.A existência de uma aplicação de software capaz de fazer a geração automática do código associado aos padrões de conceção, agiliza o desenvolvimento de novas aplicações, porque fornece de imediato o respetivo código.O que se propõe com o desenvolvimento desta dissertação é uma solução de software, capaz de efetuar a geração automática de código para os padrões de conceção catalogados em [Gamma et al., 1995]. Juntamente com o programa desenvolvido, é também apresentado um levantamento do estado da arte sobre os padrões de conceção, considerando também situações atuaisda sua aplicabilidade. Em seguida, é descrita a especificação da aplicação elaborada, bem como o seu processo de desenvolvimento, acompanhado de um exemplo de utilização. Por fim, encontram-se dois casos de estudo, servindo para provar que o programa elaborado pode ser utilizado em contextos reais.",
    "similarity": 0.4573648631477293
  },
  {
    "text1": "Automation developments are enabling industrial restructuring through the incorporationof more efficient and accurate processes with less associated cost. Consequently, robots arebeing increasingly used in the most various scenarios, including in Safety Critical domains.In such cases, the use of suitable methods to attest both the system’s quality and their safetyis absolutely essential.Following the current increase of complexity of cyber-physical systems, safety guardswhich used to be fully hardware dependent, are constantly migrating to software. Here upon, middleware software to abstract systems hardware are constantly evolving and arebeing increasingly adopted. The common feature of these systems is usually associated withits modular architectures based on message-passing communication patterns. A notoriouscase is the ROS middleware, where highly configurable robots are usually built by composingthird-party modules. The verification of such systems is usually very hard, and its implemen tation in real industrial environments is, in most cases, impracticable. To promote adoption,this work advocates the use of lightweight formal methods associated with semi-automatictechniques that require minimal user input and provide valuable intuitive feedback.This work explores and proposes a technique to automatically verify system-wide safetyproperties of ROS-based applications in continuous integration environments. It is basedon the formalization of ROS architectural models and nodes behaviours in Electrum, aspecification language of first-order temporal logic supported by a model-finder over which,system-wide properties are subsequently model-checked. In order to automate the analysis,the technique is deployed as an HAROS plug-in, a framework for quality assessment of ROSsoftware, specially aimed to its community.The technique proposal and its implementation under the HAROS framework are eval uated with positive results on a real agricultural robot, AgRobV16, whose dimension andcomplexity are industrially representative.",
    "text2": "Industrial manufacturing is becoming highly reliant on automation developments, as they bring moreefficient and accurate processes, with lower associated costs. Consequently, robots are increasingly beingdeployed in a wide range of scenarios, especially where safety is demanded. In such cases, it is criticalto employ appropriate procedures to verify both the system’s quality and safety.Following the current growth of cyber-physical systems, as well as their usage in various technologydomains, the development of software applications is becoming more demanding due to the complexitybehind the integration of complementing services, beyond those provided by the operating system.One of the most popular open-source software platforms for building robotic systems is the Robot Operating System (ROS) [53] middleware, where highly configurable robots are usually built by composingthird-party modules. Robot Operating System 2 (ROS2) is implemented using the Data DistributionService (DDS) [49] communication protocol. ROS2 implicitly makes use of the DDS-Security artefactsthrough the Secure Robot Operating System 2 (SROS2) security toolset.The present study focus on detecting security problems in ROS2 networks, in which it is intended toverify, through formal techniques, security properties. However, security is a very broad subject, so thisstudy focuses on a particular security property to show the viability of the proposed technique, namelyObservational Determinism (OD).This dissertation introduces a software tool, named Security Verification in ROS (svROS), thatprovides multiple functionalities to support this type of security analysis using Alloy [32], a formal specification language and analysis tool.",
    "similarity": 0.3704325395816921
  },
  {
    "text1": "Emergency departments have a higher number of visits compared to other hospital de partments. Technology has played a crucial role in promoting improvements in hospitalmanagement and clinical performance. The number of visits to emergency departments hasincreased considerably, giving rise to crowding situations that cause several adverse effects.This situation negatively affects the provision of emergency services, impairs the quality ofhealth care and increases the time patients wait for medical check-up. One of the leadingcauses contributing to the crowding is the high number of patients with low severity clinicalcondition. These are referred to as non-urgent or inappropriate patients, whose clinicalsituation should be taken care through self-care or primary health care.It is the responsibility of the institutions to analyse and quantify the possible causes ofcrowding to find the best solution to mitigate the adverse effects caused. It is believed thatnon-urgent patients can use the time spent in the waiting room more productively, namelyby using a self-service kiosk to which they can provide valuable information to facilitate andaccelerate the clinical processing.This work proposes a solution to be used in the waiting room of emergency departments,which aims to reduce the period of medical check-up. The solution uses a self-service kioskfor the patient to provide relevant clinical data that would otherwise have to be collectedby the physician during the clinical observation process. In particular, the kiosk will collectvital signs, past medical history, main complaint and usual medication. This data willbe processed and provided to the physician in a structured and uniform way before eachmedical check-up. The primary purpose of this solution is to reduce the period of patients’medical check-up and thus improve the response capacity of the emergency departmentswith the same resources.During the Master’s work period, an Android application was implemented for patientsto enter the clinical data mentioned above, and a Web application for physicians to access it.Additionally, a data warehouse was implemented to store the data in a consolidated wayto discover hidden relationships and patterns in the data. The first moment of evaluation,undertaken in a non-hospital facility, shows positive acceptability by participants, with alarge majority considering the system user-friendly. Due to the pandemic, it was impossibleto perform the second planned evaluation moment in a real emergency environment.",
    "text2": "One of the reasons for the increased number of visits to emergency departments is theprimary health care inability to handle urgent needs and provide all the health servicesneeded to assess complex conditions. A significant amount of these visits are due to theabnormal flow of patients whose clinical condition is of low severity and could ideally beresolved with self-care and primary health care.The crowding in emergency departments causes operational and logistical problems andhas undesirable consequences for patients, health professionals and hospitals. Delays intreatment interventions and increased mortality, medical errors and waiting times are just aphew examples of critical consequences that can occur, resulting in a significant barrier tothe quality of health care delivery.With the advances in technology, several institutions have found in self-service an alternativefor the patient’s collection of health information autonomously. These devices can be usedby low clinical severity patients (with the blue, green or yellow bracelets from Manchestertriage) to reduce waiting time in the emergency departments.This dissertation proposes a technological solution to improve both the time and qualityof the anamnesis procedure performed by medical staff in the emergency department. Theintroduction of a self-service kiosk in the emergency department waiting room will make itpossible to quickly and intuitively collect the patient’s past medical history, usual medication,main complaint symptoms and vital signs. Subsequently, this data will be made availableto the physician before each clinical observation. The hypothesis considered is that byproviding a selective, structured and uniform anamnesis information’s presentation of eachpatient, medical staff observation can proceed much faster and accurately, focusing on theconfirmation of the most relevant aspects. The primary purpose of this solution is to reducethe period of clinical observation and thus improve the response capacity of the emergencydepartment with the same resources.",
    "similarity": 0.3563808262073949
  },
  {
    "text1": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.Historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.The work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.At last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
    "text2": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.Historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.The work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.At last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
    "similarity": 0.39885402866433817
  },
  {
    "text1": "Na literatura do domínio do processamento analítico de dados facilmente se podem encontrar métodos e soluções que respondem ao problema de seleção de vistas multidimensionais no processo de implementação de um cubo OLAP. Uma forma que se evidencia como sendo extremamente vantajosa, é a de fazer a seleção baseada em critérios que se apoiem essencialmente nos conteúdos que são consultados sobre o cubo de dados ao longo das sessões de consulta OLAP. As principais vantagens que advêm desta monitorização, está relacionada com a possibilidade de efetuar correspondências rigorosas com a informação em que os agentes de decisão mais se apoiam para efetuar as suas tomadas de decisão. Ao ser feita a identificação da informação que se evidencia como sendo a mais relevante, ou pelo menos a mais frequentemente consultada, várias ilações se podem retirar, como, por exemplo, a definição de perfis de utilização, a expressão de preferências, a identificação de metodologias de trabalho, ou então a definição de processos que procurem construir cubos iceberg com forte probabilidade de explorações futuras sobre o cubo. Este último aspeto constitui, basicamente, o trabalho desta dissertação. Ao se efetuar a materialização dos conteúdos mais pesquisados no servidor OLAP, obtém-se um melhor desempenho ao nível do servidor, uma vez que o preparamos antecipadamente com os dados que mais vezes são solicitados, reduzindo assim o número de vezes que seria necessário recorrer ao data warehouse para retornar os resultados pretendidos por uma dada query multidimensional. Em termos gerais, neste trabalho de dissertação, desenvolveu-se um estudo detalhado acerca das ideias e práticas que levam ao desenvolvimento de um dado método de seleção, que seja capaz de indicar de forma precisa as partes de um cubo que são mais utilizadas, sugerindo com base nessa informação uma nova estrutura para o cubo em questão que utilize menos recursos computacionais, nomeadamente espaço em disco e tempo de processamento.",
    "text2": "Com o emergir da era da informação foram muitas as empresas que recorreram a data warehouses para armazenar a crescente quantidade de dados que dispõem sobre os seus negócios. Com essa evolução dos volumes de dados surge também a necessidade da sua melhor exploração para que sejam úteis de alguma forma nas avaliações e decisões sobre o negócio. Os sistemas de processamento analítico (ou OLAP – On-Line Analytical Processing) vêm dar resposta a essas necessidades de auxiliar o analista de negócio na exploração e avaliação dos dados, dotando-o de autonomia de exploração, disponibilizando-lhe uma estrutura multiperspetiva e de rápida resposta. Contudo para que o acesso a essa informação seja rápido existe a necessidade de fazer a materialização de estruturas multidimensionais com esses dados já pré-calculados, reduzindo o tempo de interrogação ao tempo de leitura da resposta e evitando o tempo de processamento de cada query. A materialização completa dos dados necessários torna-se na prática impraticável dada a volumetria de dados a que os sistemas estão sujeitos e ao tempo de processamento necessário para calcular todas as combinações possíveis. Dado que o analista do negócio é o elemento diferenciador na utilização efetiva das estruturas, ou pelo menos aquele que seleciona os dados que são consultados nessas estruturas, este trabalho propõe um conjunto de técnicas que estudam o comportamento do utilizador, de forma a perceber o seu comportamento sazonal e as vistas alvo das suas explorações, para que seja possível fazer a definição de novas estruturas contendo as vistas mais apropriadas à materialização e assim melhor satisfaçam as necessidades de exploração dos seus utilizadores.Nesta dissertação são definidas estruturas que acolhem os registos de consultas dos utilizadores e com esses dados são aplicadas técnicas de identificação de perfis de utilização e padrões de utilização, nomeadamente a definição de sessões OLAP, a aplicação de cadeias de Markov e a determinação de classes de equivalência de atributos consultados. No final deste estudo propomos a definição de uma assinatura OLAP capaz de definir o comportamento OLAP do utilizador com os elementos identificados nas técnicas estudadas e, assim, possibilitar ao administrador de sistema uma definição de reestruturação das estruturas multidimensionais “à medida” da utilização feita pelos analistas.",
    "similarity": 0.37087830738370425
  },
  {
    "text1": "A presente dissertação baseia-se na expectativa de diminuir o trabalho e tempo necessário para o desenvolvimento de uma loja online por programadores. Com a evolução do comércio, começaram a surgir cada vez mais negócios com lojas online, sendo por isso cada vez maiores as exigências e condições para que estas se tornassem um sucesso. Todas as lojas online costumam ter aspetos em comum como o carrinho de compras, a autenticação, entre outros componentes básicos sobre os quais se poderia salvar tempo de desenvolvimento se não se tivesse de reescrever sempre o mesmo código. Esta dissertação teve início com a investigação dos parâmetros necessários para que uma loja on-line funcionasse, pelo que em seguida foi desenvolvido uma loja online de venda de jogos e produtos relacionados. Após a investigação foi possível identificar vários aspetos e parâmetros necessários para odesenvolvimento web tanto no frontend como no backend, conseguindo-se assim fazer a distinção entre componentes e as suas relações.De seguida foi abordado o principal objetivo da dissertação que consistia em desenvolver uma ferramenta de criação de lojas online automática seguindo as configurações de diferentes programadores. Esta ferramenta pretende diminuir o tempo despendido pelos programadores em cada novo projeto, ao lhes dar a oportunidade de obter o código de lojas online com diferentes layouts e diferentes parâmetros definidos por eles. Após a criação deste projeto template usando a configuração dos programadores, o objetivo é que eles o adaptem aos pedidos dos seus clientes de uma forma simples, tendo sido usada uma programação modular ao longo deste projeto para facilitar a sua utilização. O projeto desenvolvido nesta dissertação focou-se na autenticação, algumas páginas relacionadas com o produto, na organização do código, na programação modular e na capacidade de os inputs dos programadores alterarem o resultado da ferramenta. No final deste trabalho, foi possível obter uma framework funcional com os pontos anteriormente mencionados, pelo que se atingiu o principal objetivo de desenvolver uma ferramenta que simplificasse o trabalho dos programadores.",
    "text2": "O desenvolvimento de todo o código necessário para uma loja online é um trabalho demorado e complexo. Todas as lojas online têm diferentes exigências e condições, no entanto, existem pontos comuns entre vários tipos de loja. Supondo a possibilidade de extrair esses aspetos comuns, seria possível a criação de um esqueleto de código com todos os componentes básicos, sobre os quais um programador poderia acrescentar e moldar a loja online, reduzindo assim o tempo necessário para a desenvolver. A elaboração desta dissertação começou pela criação de uma loja online que suportasse um negócio fictício de venda de bicicletas e produtos relacionados. Este passo serviu para identificar e avaliar quais seriam os parâmetros necessários especificar, e em que sentido fariam diferença na construção de um website de vendas bem como, distinguir os componentes comuns e as suas relações.A presente dissertação tem como propósito principal o desenvolvimento de uma ferramenta para a criação de lojas online. Esta ferramenta pretende ser usada por programadores para produzir o código padrão que é comum a qualquer implementação de uma solução de loja online. Através da recolha de parâmetros sobre a loja online a ser criada, são construídos os ficheiros necessários para a implementação do site.A aplicação concebida focou-se na utilização de apenas alguns dos componentes básicos, nomeadamente, “Utilizadores” e “Categorias” no back-end, “Registo e Login” e “Perfil” no front-end, mostrando assim ser possível usar a framework desenvolvida para uma implementação parcial de uma loja online. A abordagem modular permite expandir a aplicação, criando e adicionando outros componentes à estrutura parcial existente.",
    "similarity": 0.38041000567366423
  },
  {
    "text1": "The recent advances in metabolomics experimental techniques have provided novel approachesfor many research issues in the biological fields. Indeed, the ability to identifyand quantify numerous compounds in biological samples provides significant advances infunctional genomics, biomarker identification, sample characterization or drug discoveryand development. To take full advantage of these data advanced bioinformatics methodsfor data analysis and mining have been required.A number of methods and tools for metabolomics data analysis have been put forwardrecently, being one of the major limitations still faced the lack of integrated frameworks forextracting relevant knowledge from these data and being able to integrate these data withprevious biochemical knowledge. Also, the lack of reproducibility in many data analysesor data mining processes is a strong obstacle for biological discovery.In recent work from the host group, specmine, a metabolomics and spectral data analysis/mining framework, in the form of a package for the R system, has been developed toaddress some of these issues.In this thesis, an integrated web-based platform for metabolomics data analysis and mining,named WebSpecmine, was designed and developed, based on the specmine package, thusproviding an easier and friendly user interface. This website provides means for analysingmetabolomics data from different formats, including tasks such as pre-processing, univariateand multivariate analysis and metabolite identification. This web-based platform wasdeveloped collaboratively and, therefore, this work focused mainly in data from nuclearmagnetic ressonance and mass spectrometry.Also, the package faced some limitations regarding types of analysis not yet provided,such as metabolite identification for other data formats besides Mass Spectrometry coupledto Liquid Chromatography. Therefore, the extension of the metabolite identification featurewas addressed, by implementing such analysis for Nuclear Magnetic Ressonance data inthe specmine package, as well as making it available in the website.The website was validated by applying it to reproduce the pipelines from previous studiesthat made use of the specmine package. Furthermore, a case study involving bananapeels and the analysis of their characteristics and potential made use of the newly createdwebsite to further validate its functionality. All the analyses here executed were stored andare available in the web application, as public projects.",
    "text2": "The recent advances in different analytical techniques able to produce spectral data, includingRaman, Infrared (IR) or Ultraviolet-Visible (UV-vis) spectroscopies, have provided novelapproaches for many research issues in the biological and chemical fields. Indeed, they haveallowed to address tasks in functional genomics, sample characterization and classification,or drug discovery. To take full advantage of these data, advanced bioinformatics methodsare required for data analysis and mining.A number of methods and tools for spectral data analysis have been put forward recently,being one of the major limitations still faced the lack of integrated frameworks for extractingrelevant knowledge from these data and being able to integrate these data with previousbiochemical knowledge. Also, the lack of reproducibility in many data analysis or datamining processes is a strong obstacle for biological discovery, being common the lack ofdata and data analysis pipelines in the published work.In recent work from the host group, specmine, a metabolomics and spectral data analysis/mining framework, in the form of a package for the R system, has been developed toaddress some of these issues. In this thesis, the main aim was to design and develop anintegrated web-based platform for spectral data analysis and mining, based on the specminepackage, providing an easier and more user friendly interface, but also addressing some ofthe package’s current limitations.The developed platform contains features that cover the main steps of the metabolomicsdata analysis workflow, with modules for data reading and dataset creation, data preprocessingand a variety of analysis types. It includes an authentication system, allowingthe user to have his own personal workspace where projects can be stored and accessedlater, with the option to share projects with other users. The different modules were validatedusing real data from previously published studies in the host group, related to theanalysis of the characteristics and potential of natural products, addressing as well theexploration and integration of data from distinct experimental techniques, attesting theplatform’s robustness and utility.",
    "similarity": 0.44568833073395575
  },
  {
    "text1": "Software applications evolve over the years at a cost: their architecture modularity tends to be degraded. This happens mainly because software application maintenance often leads to architectural degradation. In this context, software architects need to elaborate strategies for detecting architectural degradation symptoms and thus maintaining the software architectural quality. The elaborations of these strategies often rely on tools with domain-specific languages (DSLs), which help them to specify software architecture rules. These tools also enforce the adherence of these rules in the evolving program. However, their adoption in mainstream software development is largely dependent on the usability of the language. Unfortunately, it is also often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify before a DSL is actually used by its stakeholders. There is even less support and experience on how to quantitatively evaluate the usability of DSLs used in software maintenance tasks. To this end in this dissertation, a usability measurement framework was developed based on the Cognitive Dimensions of Notations (CDN). The framework was evaluated both qualitatively and quantitatively using two textual DSLs for architecture rules in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify the DSL usability limitations to be addressed, (2) to reveal specific features of the DSLs favoring software maintenance tasks, and (3) to successfully analyze eight usability dimensions that are critical in many DSLs. However, along with these results this evaluation also revealed that this kind of tools lack support for communication among the stakeholders, creating a gap in the software development. To solve this problem we proposed heuristics for tools that use DSLs for detecting architecture degradation symptoms. These heuristics will permit the exchange of information between the stakeholders, thereby, also increasing the tool usability. Finally, we chose TamDera as the tool to implement these heuristics in our study domain. Therefore, we implemented in the new version of TamDera the communication support for the stakeholders by using a new architecture and a new environment with the developed heuristics.",
    "text2": "O recurso a ferramentas de geração automática de código permite economizar tempo quando se desenvolvem soluções de software, factor importante em questões de produtividade.Existe um conjunto de padrões de conceção [Gamma et al., 1995] que representam soluções genéricas para problemas relativos ao desenvolvimento de aplicações de software, numa perspetiva orientada aos objetos. Para cada um deles pode ser vista a sua estrutura de classes, métodos e relacionamentos, bem como as situações mais adequadas para a sua utilização. Bastará consultar o catálogo de padrões de conceção [Gamma et al., 1995] e utilizar aquele que mais se adequar à resolução de determinado problema que surja no desenvolvimento de um novo programa.A existência de uma aplicação de software capaz de fazer a geração automática do código associado aos padrões de conceção, agiliza o desenvolvimento de novas aplicações, porque fornece de imediato o respetivo código.O que se propõe com o desenvolvimento desta dissertação é uma solução de software, capaz de efetuar a geração automática de código para os padrões de conceção catalogados em [Gamma et al., 1995]. Juntamente com o programa desenvolvido, é também apresentado um levantamento do estado da arte sobre os padrões de conceção, considerando também situações atuaisda sua aplicabilidade. Em seguida, é descrita a especificação da aplicação elaborada, bem como o seu processo de desenvolvimento, acompanhado de um exemplo de utilização. Por fim, encontram-se dois casos de estudo, servindo para provar que o programa elaborado pode ser utilizado em contextos reais.",
    "similarity": 0.4572979098463863
  }
]