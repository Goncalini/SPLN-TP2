[
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/85547",
    "title": "Quantum reinforcement learning: a heuristic approach to solve deterministic MDPs",
    "abstract": "This thesis works on Reinforcement Learning tree search and attempts to find the bestpossible sequence of actions the agent needs to execute to get the most reward while usingless computational effort than by just applying a quantum maximum finding algorithm.To achieve this we will use the property that makes it possible to limit our search space tothe elements that were marked by the oracle in Grover’s Algorithm, by marking a fourth ofthe search space and following it with a quantum maximum finding subroutine. From this,one of the marked elements is obtained and the information encoded in it is used to updatea probabilistic distribution stored in a classical memory.The goal is to encounter the minimum amount of iterations of this process and compare theresults, i.e., percentage of success which is measured as the number of times the algorithmproduces a solution (element with maximum reward) and the number of queries used - witha traditional quantum maximum finding procedure. If this is observed, it is also hypothe sized that the algorithm could be used to observe a step further into the future compared tothe traditional procedure, i.e., use the same or fewer queries to evaluate a larger numberof sequences fruit of increasing the horizon of the episodes. The last hypothesis tests thedepth of the circuits, more specifically the number of gates used. If the algorithm evaluatesshallower circuits than the quantum maximum finding, the approach can be applied on thecurrent quantum machines (NISQ) because the shallower circuits produces more error-proofmeasurements.The results show that the proposed algorithm has no advantages compared to a traditionalquantum maximum finding procedure due to using more queries to achieve the same rateof success which, consequently, invalidates the first and second hypothesis. For the thirdhypothesis, the gate complexity was not directly measured. Instead, was opted to measurethe number of queries used by circuit which might not be sufficient to conclude that thealgorithm uses shallower circuits.",
    "authors": [
      "Brito, Renato Alberto Soares de"
    ],
    "keywords": [
      "Amplitude amplification",
      "Heuristic",
      "Model-free",
      "Maximum finding",
      "Query complexity",
      "Amplificação de amplitude",
      "Complexidade de query",
      "Heurística",
      "Modelo livre",
      "Procura pelo máximo",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79936",
    "title": "O impacto da aplicação de modelos de maturidade nas áreas clínicas do Sistema Nacional de Saúde",
    "abstract": "The effects of the rapid technological revolution occurring in our society are undeniable.In the health area, the quick growth of Information Technologies has had a particularand striking impact as it has led to an urgent need to improve the health care provided tothe population. It is imperative that care delivery becomes an increasingly computerizedprocess in order to facilitate not only the work of all health professionals, but the livesof all users.However, it is necessary that this phenomenon of clinical informatization is evaluated,in order to make it possible to determine the current state of health institutions, asmonitored, so that a path of gradual progression can be defined and followed, andinternal flows, processes and systems can be improved.In Portugal, many initiatives have been implemented, such as the National Strategyfor the Health Information Ecosystem 2020, in particular the SNS Sem Papel. Theobjective is to improve access to the National Health System and to expedite the sharingof clinical information by eliminating paper in hospital institutions. Obstacles andresistance to change can naturally occur as these initiatives are implemented.Thus, the emergence of entities such as HIMSS Analytics, capable of creating maturitymodels that provide a clear and concise method, capable of helping institutions toachieve their goals, becomes crucial. Within the scope of this dissertation, two maturitymodels created by this entity, EMRAM and AMAM, were studied in order to understandtheir dynamics and scrutinize how they possibilitate the gradual improvement of theanalytics of the institutions and the progressive dematerialization of their systems, flowsand processes.",
    "authors": [
      "Silva, Lara Correia e"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23408",
    "title": "Extracção de conhecimento a partir do software Open-Source de Business Intelligence Pentaho em Unidade de Cuidados Intensivos",
    "abstract": "As organizações de saúde têm como principal objectivo a prestação deserviços de qualidade à população, e a tomada de decisões de forma rápidae e caz é essencial para que tais objectivos sejam atingidos. Deste modo,neste sector, a adopção de ferramentas tecnológicas automatizadas que facilitameste processo tem vindo a aumentar ao longo dos anos. Neste contexto,surge o conceito de Business Intelligence (BI) que auxilia a tomada de decisãopor parte dos pro ssionais de saúde, uma vez que estes sistemas se baseiamna Extracção de Conhecimento (EC) gerado pelos sistemas de informaçãotransaccionais, sendo capazes de integrar uma enorme quantidade de dadosprovenientes de diversas fontes, normalmente de bases de dados que se encontramem diferentes tecnologias, plataformas e totalmente desintegradas.Assim, ultrapassando-se a heterogeneidade das bases de dados, através daestruturação dos dados, extrai-se informação que permitirá atingir conhecimentoimportante para as decisões clínicas.Especi camente, a Unidade de Cuidados Intensivos (UCI) de um hospitalé a unidade mais cara e que mais recursos exige, de tal forma que os sistemasde BI podem desempenhar um papel preponderante não só na racionalizaçãodos custos, mas também na melhoria da qualidade dos cuidados prestados,através da monitorização dos dados clínicos dos pacientes. Deste modo, esteprojecto pioneiro incidiu na análise da aplicação do Pentaho, um softwareOpen-Source (OS) de BI, nos processos de EC a estas unidades hospitalares,tendo como fonte os dados dos pacientes de um hospital localizado no Nortede Portugal, avaliando o conhecimento obtido e o seu impacto na tomada dedecisão.Este software disponibiliza ferramentas que analisam, sintetizam, assimilame dão sentido às enormes quantidades de informação, sendo capazde estabelecer ligações so sticadas e discernir padrões, dando oportunidadepara tirar conclusões e agir de forma preventiva.",
    "authors": [
      "Viana, Marta Alexandra Rolo Neiva"
    ],
    "keywords": [
      "614:681.3.06",
      "681.3.06:614"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "614:681.3.06",
      "681.3.06:614"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86512",
    "title": "High performance fourier transforms on GPUs with GLSL",
    "abstract": "The Fast Fourier Transform is a family of algorithms indispensable for the computation of the Discrete FourierTransform. As a result, these transforms are the core of many applications in several areas and are required to becomputed efficiently in many scenarios.The continuous evolution of GPUs has increased the popularity of parallelizable algorithm implementations onthis type of hardware. Traditionally GPUs were associated to graphics background, however, with the popularizationof the compute functionality of this hardware, most modern GPUs now have this capability, hence, algorithmsnow are more likely to be implemented in the general-purpose compute pipeline of GPUs. As a result, manyapplications take advantage of compute programming in GPGPU-capable frameworks such as GLSL, a high-levelshading language frequently used in the context of computer graphics.In this dissertation we provide, refine and compare GPU-driven implementations of the family of FFT algorithmsin GLSL, with the goal to provide programmers with efficient and simplified compute kernels for this transform,from the classic Cooley-Tukey algorithm to more suitable algorithms for the GPU such as the Stockham algorithmwith higher radix.Accordingly, we also use the cuFFT NVIDIA framework for reference in the comparisons of the GLSL algorithmsimplementations with the goal to analyse their significance on the tradeoff of using specialized implementations ofthe FFT algorithms or integrating dedicated software tools for any case of application.Finally, we demonstrate how all improvements discussed in this dissertation culminate in performance improvement in a real-time rendering technique that heavily depends on multiple of these transforms in the Nau3D engineas a case of study.",
    "authors": [
      "Mota, Jorge Francisco Teixeira Bastos da"
    ],
    "keywords": [
      "FFT",
      "GPGPU",
      "GLSL",
      "cuFFT",
      "Performance",
      "Compute",
      "Cooley-Tukey",
      "Stockham",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93162",
    "title": "Multilanguage chatbot with artificial intelligence for client support",
    "abstract": "A inteligência artificial surgiu nos anos 50, com a criação de um espaço de estudo com o objetivo principalde desenvolver máquinas inteligentes. A sua evolução foi abrupta e hoje existem mesmo programas que,ao escreverem um conjunto de palavras, conseguem gerar imagens surpreendentes com base no quefoi escrito em segundos, algo que um pintor famoso faria, mas que demoraria horas ou mesmo dias aconcluir.Em vários setores empresariais, desde a educação aos cuidados de saúde, entre outros, tem havidoum aumento notável do interesse e da utilização de chatbots. Esta tendência crescente abriu caminhopara a implementação de chatbots em diversas áreas. Especificamente, estes chatbots servem comointermediários informativos em empresas como a Retail Consult, uma empresa de tecnologia especializadaem desenvolvimento de software na área do retalho e na utilização de processamento em lote, que envolveo tratamento de grandes volumes de dados.Além disso, o sistema de chatbot foi concebido para reconhecer e compreender uma multiplicidade delínguas, incluindo o inglês, que será a língua principal do utilizador, bem como o português e o espanhol,assegurando uma comunicação na língua preferida do utilizador. Esta capacidade multilingue é particu larmente crucial para a Retail Consult, dada a sua presença global com escritórios espalhados por váriospaíses. Sem dúvida, esta versatilidade linguística acrescenta um valor imenso ao produto. Para conseguireste reconhecimento linguístico, foram utilizadas técnicas de inteligência artificial e de processamento dalinguagem natural, que foram implementadas através de um quadro bem estruturado concebido para acriação de chatbots e de software similar, incluindo assistentes virtuais.O desenvolvimento de um serviço com estas características está preparado para aumentar significa tivamente a produtividade interna da organização. Por exemplo, um analista de sistemas pode perguntardiretamente ao chatbot o que pretende, poupando tempo que, de outra forma, teria sido gasto na procurae consulta de informações à base de dados. Quer se trate de determinar o número de trabalhos emexecução ou de identificar os que apresentam falhas, este sistema procura simplificar as operações emelhorar a eficiência dentro da empresa. Com base no trabalho apresentado na introdução, onde discutimos os papéis cruciais do processa mento em lote e da integração do chatbot no ambiente em específico, agora aprofundamos a dinâmica dosetor de retalho nesta análise minuciosa. O objetivo está centrado na criação de um chatbot versátil capazde comunicar com os utilizadores em várias línguas, fornecer respostas precisas e, acima de tudo, auxiliarna tradução de idiomas. Além disso, os resultados confirmam a competência do chatbot na identificaçãoe tradução precisa de idiomas, com base em nossa análise avançada, onde investigamos a importânciado processamento em lote e introduzimos a estrutura de IA conversacional RASA.Esses elementos, contribuem para uma experiência mais envolvente e satisfatória para o utilizador.Ou seja, este trabalho permitiu e ampliou o nosso conhecimento sobre a interação entre processamentoem lote, tecnologia de chatbot e comunicação multilíngue no contexto do retalho.",
    "authors": [
      "Sá, João Miguel Santos"
    ],
    "keywords": [
      "Processamento de Linguagem Natural (PNL)",
      "Linguagem natural (LN)",
      "Aprendizagem Automática (AA)",
      "Aprendizagem Profunda (AP)",
      "Inteligência Artificial (IA)",
      "Processamento em Lote (PL)",
      "Natural Language Processing (NLP)",
      "Natural Language (NL)",
      "Machine Learning (ML)",
      "Deep Learning (DL)",
      "Artificial Intelligence (AI)",
      "Batch Processing (BP)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80121",
    "title": "Evaluating the impact of traffic sampling in network analysis",
    "abstract": "The sampling of network traffic is a very effective method in order to comprehend thebehaviour and flow of a network, essential to build network management tools to controlService Level Agreements (SLAs), Quality of Service (QoS), traffic engineering, and theplanning of both the capacity and the safety of the network.With the exponential rise of the amount traffic caused by the number of devices connectedto the Internet growing, it gets increasingly harder and more expensive to understand thebehaviour of a network through the analysis of the total volume of traffic. The use ofsampling techniques, or selective analysis, which consists in the election of small number ofpackets in order to estimate the expected behaviour of a network, then becomes essential.Even though these techniques drastically reduce the amount of data to be analyzed, the factthat the sampling analysis tasks have to be performed in the network equipment can cause asignificant impact in the performance of these equipment devices, and a reduction in theaccuracy of the estimation of network state.In this dissertation project, an evaluation of the impact of selective analysis of networktraffic will be explored, at a level of performance in estimating network state, and statisticalproperties such as self-similarity and Long-Range Dependence (LRD) that exist in originalnetwork traffic, allowing a better understanding of the behaviour of sampled network traffic.",
    "authors": [
      "Mendes, João Emanuel da Silva"
    ],
    "keywords": [
      "Sampling",
      "Quality of service",
      "Long-range dependence",
      "Análise seletiva",
      "Qualidade de serviço",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79986",
    "title": "Transformação de especificações ETL em YAWL para processos Kettle",
    "abstract": "YAWL é uma linguagem gráfica para a especificação de processos, com uma semântica bem definida, que permite o desenho, especificação, simulação e validação de sistemas, cujos processos a modelar exijam características específicas de comunicação, concorrência e sincronização entre si. A YAWL é baseada, por um lado, em padrões bem definidos de fluxo de trabalho e, por outro, nas conhecidas Redes de Petri Coloridas. Devido às suas características, vários estudos utilizaram esta linguagem na modelação de sistemas de ETL (Extract-Transformation-Load), tentando facilitar e agilizar todo este processo. Este trabalho de dissertação teve como objetivo desenvolver e implementar um sistema de geração de “esqueletos” para sistemas de ETL a partir de um conjunto de especificações YAWL. Nesse sentido, foi idealizado e desenvolvido um sistema capaz de representar e produzir de forma semi-automática a configuração de processos ETL para várias ferramentas de implementação de processos ETL, nomeadamente Kettle, Informatica e Talend, todas elas ferramentas conceituadas no mercado dos sistemas de povoamento de data warehouses. Nesta dissertação apresentamos e descrevemos tal sistema, desde as suas fases de fundamentação e conceptualização até às suas fases de teste e exploração.",
    "authors": [
      "Freitas, Janine Marlene Duarte Silva"
    ],
    "keywords": [
      "Sistemas de data warehousing",
      "Sistemas de povoamento de data warehouses (ETL)",
      "Modelação de sistemas ETL",
      "Geração de esqueletos ETL",
      "YAWL",
      "Kettle",
      "Informática",
      "Talend",
      "Data warehousing systems",
      "Systems to populate data warehouses (ETL)",
      "ETL modeling systems",
      "ETL skeleton generation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84570",
    "title": "Automatic driving: 2D detection and tracking using artificial intelligence techniques",
    "abstract": "Road accidents are estimated to be the cause of millions of deaths and tens of millions of injuries every year. For this reason, any measure that reduces accidents' probability or severity will save lives.Speeding, driving under the influence of psychotropic substances and distraction are leading causes of road accidents. Causes that can be classified as human since they all come from driver errors. Autonomous driving is a potential solution to this problem as it can reduce road accidents by removing human error from the task of driving.This dissertation aims to study Artificial Intelligence techniques and Edge Computing networks to explore solutions for autonomous driving. To this end, Artificial Intelligence models for detecting and tracking objects based on Machine Learning and Computer Vision, and Edge Computing networks for vehicles were explored.The YOLOv5 model was studied for object detection, in which different training parameters and data pre-processing techniques were applied. For object tracking, the StrongSORT model was chosen, for which its performance was evaluated for different combinations of its components. Finally, the Simu5G simulation tool was studied in order to simulate an edge computing network, and the viability of this type of network to aid autonomous driving was analysed.",
    "authors": [
      "Pinto, José Miguel Fernandes Madeira"
    ],
    "keywords": [
      "Autonomous driving",
      "Artificial intelligence",
      "Machine learning",
      "Computer vision",
      "Edge computing",
      "Condução autónoma",
      "Inteligência artificial",
      "Aprendizagem de máquina",
      "Visão por computador",
      "Computação de borda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47441",
    "title": "Development of text mining tools for information retrieval and extraction from patents",
    "abstract": "Biomedical literature is composed of a large and ever increasing number of publications, written in natural language. Patents are a relevant fraction of these publications, considered important sources of information due to all the curated information available in the documents, from the granting process. Although being real technological libraries, their unstructured data turns the search of information within these documents a challenging task. Biomedical text mining is a scientific field that explores this task, creating methodologies to search and structure the information in the biomedical literature.Information retrieval is one of the biomedical text mining tasks, in which the relevant information is obtained from an extensive collection of documents using several text retrieval methodologies. Getting all the information available on a patent document requires the download of the respective PDF document, that is then converted into a machine-readable text by technologies as Optical Character Recognition (OCR).In this project, an information retrieval, and a PDF to text conversion system were developed building a “patent pipeline” which was integrated into @note2, an open-source computational framework for biomedical text mining. The patent pipeline can be disintegrated into four different tasks: the patent search, the retrieval of patent metadata, the retrieval of their PDF files, and the extraction of all the information from these documents.A set of patents from the BioCreative V CHEMDNER task was used to test the developed pipeline, evaluating the framework performance and the real capacity to retrieve the requested patents and extract their unstructured information. The results were promising, bringing to the scientific community the published patent information and allowing the posterior implementation of other biomedical text mining processes over these documents.",
    "authors": [
      "Alves, Tiago Alexandre Pinto"
    ],
    "keywords": [
      "Biomedical text mining",
      "Patents",
      "Information retrieval task",
      "Optical character recognition",
      "@note2",
      "Mineração de textos biomédicos",
      "Patentes",
      "Obtenção de informação",
      "Reconhecimento ótico de caracteres",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27877",
    "title": "Application of Formal Methods in the ITASAT Project",
    "abstract": "Critical software can be potentially dangerous if not well verified, leading to serious failures. Accordingly, there is a need for improved validation and verification methods in order to have guarantees about the software final product. The aim of this project is to define a more linear and organized verification and validation plan to, formally, verify the most critical parts of the OBDH (On-Board Data Handling) subsystem of ITASAT, supported by the Alloy formal language.Alloy supports the description of systems whose state involves complex relational structure. The application of Alloy and Alloy Analyzer was motivated by the need for a formal specification that is more closely tailored to state-machines, and more amenable to automatic analysis. Structural and behavioural properties are described declaratively, by conjoining relations and constrains, making it possible to develop and analyze a model incrementally. Due to the high cost of using these methods, they are mainly used in the development of high-critical software where safety and security are crucial. This dissertation presents a set of guidelines for analysis and modelling of software systems which support the creation of a formal model and allow some extra behaviours such as synchronization, interruptions and flags. A new tool, ModelMaker, was developed in order to create models using these guidelines in a more interactive way.",
    "authors": [
      "Quinta, Daniel Ribeiro"
    ],
    "keywords": [
      "681.3.06"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27853",
    "title": "Reforço da privacidade através do controlo da pegada digital",
    "abstract": "Atualmente existe ainda uma relação assimétrica entre os utilizadores e os fornecedoresde serviços disponibilizados pela internet. É prática comum, aquando da apresentação de umserviço, que o utilizador seja questionado sobre a aceitação, ou não, de um conjunto de políticasreferentes ao uso de informação privada facultada ao fornecedor (por exemplo, a morada,o número de telefone, preferências, etc...). Geralmente os utilizadores aceitam a política combase na confiança que têm no fornecedor e/ou no contrato formal que lhes é apresentado. Oscasos de violação de privacidade por parte de alguns fornecedores de serviços, vendendo oufacultando informação privada sobre os seus clientes a outros, são amplamente conhecidos e resultamem grande medida da falta de controlo que os utilizadores finais têm sobre a informaçãoque entregam aos fornecedores.Este problema também tem grande impacto no ambiente empresarial. Quase toda a informaçãode uma organização é guardada em claro. Mesmo que esta seja guardada num local seguro,aqueles que conhecerem bem o sistema poderão ter indevidamente acesso a informação privadada organização. Além disto, se a organização for alvo de um ataque informático e o atacanteconseguir aceder aos dados poderá consulta-los livremente.Neste trabalho propomos a implementação de um mecanismo que possibilite o envio de informaçõessem que o utilizador tenha necessidade de confiar no local onde as mesmas serãoarmazenadas, através da utilização do conceito de “sticky policies”. Através da utilização detécnicas criptográficas, é estabelecido um vínculo entre a informação cifrada e as políticas deacesso à informação. O sistema desenvolvido garante que, para um terceiro aceder às informaçõespessoais de um utilizador, terá que cumprir o conjunto de regras definidas pelo dono dainformação.Visto que um utilizador autorizado a aceder às informações pode ter um comportamentoincorreto, partilhando indevidamente as informações, propomos também adicionar mecanismosde auditoria dos acessos à informação gerida pelo sistema.",
    "authors": [
      "Macedo, Ricardo Joaquim Pereira de"
    ],
    "keywords": [
      "“Sticky policies”",
      "Sistema criptográfico",
      "PKE",
      "IBE",
      "ABE",
      "PRE",
      "Controlo de acessos",
      "DAC",
      "MAC",
      "ABAC",
      "RBAC",
      "Linguagem de políticas",
      "XACML",
      "XrML",
      "EPAL",
      "Cryptographic system",
      "Access control",
      "Policy language",
      "681.3:658.0",
      "658.0:681.3",
      "681.188",
      "681.3-7"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:658.0",
      "658.0:681.3",
      "681.188",
      "681.3-7"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47718",
    "title": "Análise de segurança para soluções de software para a cloud",
    "abstract": "A gestão do ciclo de vida de produtos informáticos é um processo contínuo que, noscasos de maior sucesso, se prolonga por períodos que ultrapassam duas décadas e se transformanum aspeto crítico à atividade das empresas que desenvolvem e comercializam essesprodutos. A evolução natural de um produto deste tipo resulta de estímulos previsíveisassociados à inovação tecnológica nas plataformas computacionais, como a cada vez maiorutilização de plataformas móveis, à melhoria de funcionalidades existentes e à introduçãode características novas. Incorpora também estímulos menos previsíveis, tais como alteraçõesnos modelos de negócio resultantes de mudanças nos paradigmas de prestação deserviços, como a migração para o modelo Cloud.Neste contexto, as implicações para a gestão da segurança da informação das alteraçõesgradualmente introduzidas nem sempre são claras, e é comum as equipas de desenvolvimentonão estarem preparadas para lidar com a diversidade de novos riscos que surgemcom estas mudanças.Deste modo, e na perspetiva de combater as possíveis falhas supra referidas, pretende-secom este projeto efetuar uma análise de segurança de produtos de software para a Cloudda Primavera bss , identificando potenciais riscos e ameaças de segurança e apresentandosoluções viáveis para os mitigar.",
    "authors": [
      "Rocha, João Diogo Pereira da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79984",
    "title": "Gerador de protótipos de interfaces gráficas para o IVY Workbench",
    "abstract": "A interface de um programa é um elemento importante na experiência que o utilizador temcom o software, pois constitui o principal método de interação com a lógica do programa.A existência de métodos fiáveis de verificação de sistemas de software permite o a conceçãodestes de acordo com a especificação e, em casos mais críticos, evitar erros com consequências graves. Estes métodos rigorosos, no entanto, contrastam com a prática mais comum nodesenho de interfaces. Um dos métodos mais utilizados para o desenho e avaliação de interfaces é a prototipagem. Os protótipos permitem transmitir aspetos do design da interfacee até avaliar a sua usabilidade, mas não oferecem as garantias sobre o seu funcionamentoque os métodos de verificação oferecem.O IVY Workbench é uma ferramenta que suporta a modelação do comportamento de sistemas interativos e a verificação formal dos mesmos. A ferramenta contém um conjunto deplugins que suportam o processo de modelação e análise, incluindo um editor de modelos,um verificador de propriedades e um animador. Este último permite visualizar e interagircom os modelos, mas não suporta associá-los a mockups representativos das interfaces.A interação com os modelos facilita a sua validação por parte de quem os está a desenvolver. Não facilita, no entanto, a comunicação com os potenciais clientes do sistema modelado,para quem um protótipo será um meio mais eficaz de comunicação.Neste documento propõe-se uma solução para o problema acima, assente no desenvolvimento de um novo plugin capaz de suportar a construção e animação de protótipos desistemas interativos modelados no IVY. É descrito todo o processo de desenvolvimento,desde o levantamento de requisitos, até exemplos de aplicação que permitem demonstraras novas funcionalidades existentes.",
    "authors": [
      "Araújo, João Miguel Matela Aidos Manso de"
    ],
    "keywords": [
      "Interfaces com o utilizador",
      "Prototipagem",
      "Modelação e análise formais",
      "User interfaces",
      "Prototyping",
      "Formal analysis and modeling",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84141",
    "title": "GSD: a web application for teacher timetable management - dissertation report",
    "abstract": "This document presents a Masters Thesis in Software Engineering, in the area of AcademicManagement, Support Tools and Web Software.In a given higher education institution, the teaching service is assigned by it’s departmenttwice a year, following a model of their own self-re-creation.Initially, it is intended to standardize the information in a transversal way to all thedepartments and create a model that will serve as the basis for the implementation of a webapplication.This application will allow it’s users to enter and verify information, generating databaseentries in a DSL of their choice (such as SQL), feeding a timetable construction platform.",
    "authors": [
      "Reis, João"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81087",
    "title": "Data analysis and recommender system architecture for e-commerce platforms",
    "abstract": "E-commerce is constantly expanding, leading to greater market competitiveness. The number ofonline platforms offering products or services is increasing; so there is a growing need for companiesto stand out from the competition, which leads to the application of various marketing strategies.However, not all are adequate and mismanagement, as well as a bad investment of these strategies,may prejudice companies.Hence the implementation of recommendation systems in e-commerce platforms, as a safe andeconomical strategy. By investing in a good recommendation mechanism, one can provide betteruser experience, taking his interests into account. As a result, more traffic on the platforms isensured, which may result in a higher sales rate and, consequently, a higher number of revenues.However, to develop a recommendation system, the first step must consist in obtaining informationabout the sales platform, where data about its users and products/services form the basis of recom mendations. But not all information is useful, which can influence the accuracy of the forecastingmodels used by the system to produce results.Following this perspective, a data analysis methodology is proposed, as well as an architecture ofa recommendation system, which allows to extract and treat relevant data, in order to integrate arecommendation engine for most e-commerce platforms.",
    "authors": [
      "Cunha, Gil Fernando Ferreira da"
    ],
    "keywords": [
      "Recommender system",
      "Data analysis",
      "Software architecture",
      "e-commerce",
      "Business intelligence",
      "Sistema de recomendação",
      "Análise de dados",
      "Arquitetura de software",
      "Comércio eletrônico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88562",
    "title": "Decoding human movement intentions and postural reactions through brain signals",
    "abstract": "Falls are one of the most common causes of injuries in the elderly population. As a result, treatment costs havealso increased. Recent efforts to restore lower limb function in these populations have seen an increase in theuse of wearable robotic systems, however, fall prevention measures in these systems require early detection ofloss of balance to be effective. In short, the development of technologies, such as a brain-computer interface, thatis capable of recognizing situations at risk of falling based on the loss of balance caused by several factors, isessential. Previous studies have investigated whether kinematic variables contain information about an impendingfall, but few have examined the potential of using electroencephalography (EEG) as a predictor of falling and howthe brain responds to prevent a fall. Perceived disturbances of balance are always accompanied by a specificcortical activation, called disturbance-evoked potential (PEP).In this study, the recognition of daily activities (walking, lifting, crouching, going up and down stairs) was alsopart of the initial objective, however, due to the challenges encountered, the object of study of the present workwas focused on the recognition and binary classification of the presence of loss of balance (PEPs) in brain signals.Thus, this dissertation intends to take the first steps toward the decoding of brain activity in response to imbalancedevents. Initially, to acquire the data, an experimental protocol was designed, so that the participants, using EEG,were submitted to gliding-like perturbations while walking on the treadmill. Two healthy subjects were exposed toa glide-like perturbation, and these perturbations occurred interspersed over a period lasting 30 to 60 seconds.Each subject performed 2 experiments, that is, perturbations provoked while the individual walked on the treadmill:i) at a speed of 1.6 km/h and ii) at a speed of 2.5 km/h.Based on the approached methods, the perturbation evoked potential (PEP) components were found between70-155 ms after the onset of the external perturbation. To decode pre-processed EGG data, four (4) artificial neuralnetworks were tested and different network architecture parameters and electrode layouts were compared. Overall,the convolutional neural network trained to predict EEG balance disturbances had a far superior classificationperformance than the other architectures, whose mean accuracy was 91.51 ˘ 2.91%, using a short windowlength of 200 ms. The electrode layout composed of 5 channels (Fz, C3, Cz, C4, and Pz) presented the shortestexecution time to train the model, whose average value was 196 ˘ 44.24ms. In addition, it was possible to verifythat the use of a single electrode (Cz) obtained satisfactory precision results (86.47 +/- 0.03%). These discoveriesmay contribute to the development of a system capable of detecting equilibrium disturbances in real-time.",
    "authors": [
      "Neto, Raimundo Nonato Barros"
    ],
    "keywords": [
      "Brain-computer interface",
      "Electroencephalogram",
      "Falls recognition",
      "Pertubation-evoked potential",
      "Deep learning",
      "Interface cérebro-computador",
      "Eletroencefalograma",
      "Reconhecimento de queda",
      "Potencial evocado de perturbação",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/40218",
    "title": "Aplicação móvel android de apoio a gestão de empréstimos e reservas das bibliotecas da Universidade do Minho",
    "abstract": "Os dispositivos móveis, em particular os tablets e smartphones, alcançaram uma enormepopularidade ao longo dos últimos anos devido à sua grande versatilidade e multifuncionalidade,conquistando deste modo, meritoriamente um espaço de destaque no nosso dia-a-dia, tanto anível pessoal como profissional. Neste contexto, os utentes das bibliotecas da Universidade doMinho não são uma exceção, e os SDUM (Serviços de Documentação da Universidade do Minho)no cumprimento da sua missão, definiu como uma das linhas gerais proporcionar aos utentesuma melhor qualidade de assistência, com o desenvolvimento de uma aplicação móvel de gestãode empréstimos e reservas de publicações em posse.Contudo, existe um ainda um grande entrave no mercado do desenvolvimento de aplicações paradispositivos móveis, devido à sua fragmentação em termos de plataformas móveis utilizadas (iOs,Android, Windows Phone, etc.). Esta diversificação exige um maior esforço no desenvolvimentodas aplicações, de modo que obriga o desenvolvimento das mesmas para cada plataforma móvelem particular. É neste sentido que as abordagens de desenvolvimento multiplataforma ganharamrelevância, permitindo o desenvolvimento de aplicações para várias plataformas a partir de umúnico código fonte.O principal objetivo desta dissertação é desenvolver uma aplicação de apoio a gestão deempréstimos e reservas nas bibliotecas da Universidade do Minho. Os objetivos intercalados são:realização de estudos sobre as abordagens e ferramentas de desenvolvimento multiplataforma,adotar métodos de engenharia de requisitos e conceção, implementação e teste da solução final(fundamental no processo de correção de falhas, de modo que a permitir uma solução final commaior qualidade). A primeira fase do modelo de processo de engenharia de requisitos consiste nolevantamento/definição e priorização de requisitos, que tem como objetivo conhecer as técnicasde levantamento, assim como identificar e aplicar as que melhor se adequam a este projeto. Apósa execução da fase de Analise e negociação, efetuou-se a documentação dos requisitos a um nívelde detalhe apropriado, como consta no Anexo B – Documento de Especificação de Requisitos.",
    "authors": [
      "Barros, Júlio Dinis Lopes de"
    ],
    "keywords": [
      "Dispositivos móveis",
      "Smartphones",
      "Tablets",
      "Aplicações móveis",
      "HTML5",
      "Plataformas móveis",
      "Desenvolvimento multiplataforma",
      "Mobile devices",
      "Mobile applications",
      "Mobile operating system",
      "Cross-platform development",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/87252",
    "title": "Development of MOSGUITO: a user-friendly graphical interface for meta-omics data analyses",
    "abstract": "Complex microbial communities are essential to all ecosystems, and by linking microbialidentity to function, meta-omics technologies facilitate the interpretation of the processes cat alyzed by microorganisms. MOSCA is a command-line pipeline that performs bioinformaticsanalyses of metagenomics, metatranscriptomics, and metaproteomics. MOSGUITO is a web based tool developed in React, which allows the configuration of MOSCA’s workflow andthe visualization of MOSCA outputs. Although the metadata and the configuration optionsof MOSCA could be easily customized and downloaded through MOSGUITO, MOSGUITOwas unable to interact with MOSCA automatically. In this thesis, a third-tier client-serverarchitecture was developed containing the Client MOSGUITO, the Server MOSCA, and aDatabase. MOSGUITO as a client-side can retrieve, store and delete data from the Databaseand start running analysis on MOSCA as a server. MOSCA as a server can receive files fromthe client-side and start an analysis run. The database can store results from MOSCA, inputfiles from users, and respective user information from their login session. A full guide to howto utilize this new version of MOSGUITO is provided. MOSGUITO client-side can interactwith MOSCA as a server using Flask APIs, end users don’t need to have knowledge oncommand-line pipelines to use MOSCA, nor the computer resources to download it. There fore users using MOSGUITO can optimize the usage and configuration of MOSCA, beingable to analyze the data from omics experiments with a simple interaction with MOSGUITO.",
    "authors": [
      "Pereira, José Henrique Lopes"
    ],
    "keywords": [
      "Meta-omics",
      "MOSCA",
      "Flask",
      "MOSGUITO",
      "API",
      "Metaómica",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93037",
    "title": "Natural language processing applied to human resources",
    "abstract": "As technology continues to advance, many companies are seeking ways to integrate Artificial Intel ligence (AI) into their operations in order to optimize their workflow and improve efficiency. One areathat could greatly benefit from AI solutions is the Human Resources (HR) department. This disserta tion explores the use of AI and Natural Language Processing (NLP) to extract emotions from text-basedcommunications, such as emails and instant messages between HR representatives and employees. Byproviding insight into the states of mind being expressed in text, this technology has the potential to im prove communication and understanding between the two parties, which in turn may lead to better conflictresolution, increased employee engagement, and improved productivity. The study will examine the useof various machine learning algorithms ranging from Convolutional Neural Network (CNN) and RecurrentNeural Network (RNN) to current state-of-the-art architectures such as transformers like BERT and XLNetto extract better insights from text and to identify the emotions contained in messages. The results of thisthesis contribute to the development of an AI solution capable of identifying one out of seven emotions inboth general and specialized conversations with an accuracy of 75.38%, having the capacity of enhancingthe efficiency of HR departments by facilitating communication and understanding with employees.",
    "authors": [
      "Santos, António Manuel Almeida Pinto Bandeira"
    ],
    "keywords": [
      "Natural language processing (nlp)",
      "Emotion recognition",
      "Learning",
      "Neural network",
      "Human resources",
      "Employee retention",
      "Processamento de linguagem natural (pln)",
      "Extração de emoções",
      "Aprendizagem profunda",
      "Redes neuronais",
      "Recursos humanos",
      "Retenção de colaboradores",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27867",
    "title": "Network inference: extension of linear programming model for time-series data",
    "abstract": "With the widespread availability of high-throughput technologies, it is now possibleto study the behavior of dozens or even hundreds of gene/proteins through asingle experiment. Still, these experiments provide only the gene/protein expressionvalues, telling nothing about their interactions with each other. To understandthese interactions, network inference methods need to be applied. By understandingsuch interactions, new light can be shed into biological processes and, in particular,into disease’s mechanisms of action, providing new insights for drug design: whichgenes/proteins should be targeted in order to cure/prevent a specific disease.In this thesis, we developed and tested two alternative extensions for a previouslydeveloped model based on linear programming. Such model infers signal transductionnetworks from perturbation steady-state data. The extensions now developed takeadvantage of perturbation time-series data, which further improves the resolution ofcausal relationships between genes/proteins.In a first phase, we use artificial networks with simulated data to test the performanceof both extensions in different conditions. Additionally, we compare their performanceto the original model and to a state-of-the-art model for perturbation timeseriesdata, DDEPN. Overall, our second extension exhibits a better performance,and significantly higher sensitivity. This extension assumes a given gene/protein canonly influence its targets if it is in an active form.In a second phase, we use two experimental datasets related to ERBB signalingand evaluate the resulting networks: 1) by finding literature support for the inferrededges, and 2) by using a network assembled with Ingenuity IPA as true network todo a quantitative assessment. Our results are further compared to DDEPN and theoriginal model in a quantitative way. Quantitatively, our second model extension isshown to perform better than both the original model and DDEPN. Qualitatively,we find literature support for most of the inferred edges in both datasets, while alsoinferring a few plausible edges for which no literature evidence was found.",
    "authors": [
      "Matos, Marta R. A."
    ],
    "keywords": [
      "681.3:57",
      "57:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:57",
      "57:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93042",
    "title": "Otimizações de armazenamento distribuído para aprendizagem profunda",
    "abstract": "In today’s world the utilization of Deep Learning (DL) is intrinsically integrated in the activity of severalenterprises and industries. It allows us to extract knowledge from data, detect patterns and make pre dictions, increasing the competitivity and quality of the services provided. However, the DL frameworks(e.g., TensorFlow, PyTorch, Apache MxNet) require not only considerable of computational power, but alsoefficient data storage, since they need to deal with large amounts of data. In particular, in each iteration ofthe DL model train different batches of the training dataset are accessed to be processed and incorporatedin the model. The retrieval of this data can be a bottleneck to the performance of the system, since thedatasets are getting increasingly bigger, reaching sizes in the order of TBs.In the case of multi-node DL this becomes increasingly critical since there are many compute nodestraining models, possibly with the same dataset, resulting in more requests directed to the shared filesystem competing with each other. If data could be stored nearer to the computational nodes and thosenodes shared the data with one another, it would reduce the I/O pressure in the shared storage systemand potentially reduce the time taken by these accesses and, consequently, the training time.This thesis presents DistMonarch, a DL framework agnostic system that takes advantage of the storagesystem hierarchy by copying data to levels closer to each compute node and allows the nodes to sharedata with each other, in a transparent manner. Results show that using this system reduces accesses tothe shared file system by up to 90% and training time of some models and configurations by up to 48%.",
    "authors": [
      "Moreira, Maria Beatriz Cardoso Gonçalves Barbosa e"
    ],
    "keywords": [
      "I/O Optimization",
      "Multi-node deep learning",
      "Otimização de E/S",
      "Aprendizagem profunda multi-nodo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64267",
    "title": "Influence of near-infrared images in the object detection task",
    "abstract": "With the rise of data, the creation of algorithms capable of using that data is an evolution that appearsnaturally. Taking advantage of those algorithms, impressive advances have been made in the abilityfor a computer to recognize objects. Nevertheless, even after all those advances, further ones can stillbe achieved.With the reduction of infrared cameras prices and at the same time the increase in the picture qualityof those same cameras, they are becoming reliable solutions for commercial applications. Theseimages provide an all new kind of information that is not available with the use of only the traditionalvisible light images. As such, in this work, it is tested if the additional usage of infrared images, incomplement with the visible image, has any kind of influence in the results for object detection fordifferent levels of illumination, in the interior of a vehicle.In order to test this influence, several tests are done in equivalent conditions and the results betweenusing infrared images and visible light images compared. In addition to that, there were also experimentsdone in the usage of both types of images at the same time as a way to improve detection.It was also documented the influence of some more traditional modifications over the images of thetraining set, such as data augmentation and changes in the number of classes.To keep the results of the experiments as comparable as possible, a training methodology wasplanned and used in all of the training processes of the algorithms.",
    "authors": [
      "Silva, Pedro Nuno Rodrigues da"
    ],
    "keywords": [
      "Deep learning",
      "Infrared",
      "Computer vision",
      "CNN",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92674",
    "title": "Processamento de imagens histológicas de cérebros marcados com IHC para estudos neurofisiológicos",
    "abstract": "A curiosidade do ser humano é algo que estará sempre presente no caminho da humanidade, sendo este aspeto que nos faz querer perceber como nós e o mundo que nos rodeia funciona. Com os diferentes estudos e ensaios realizados ao longo dos tempos, foi possível observar que os seres humanos não são tão diferentes de outros animais em diversos aspetos, desta maneira, ao estudar os comportamentos e anatomia destes animais foi possível descobrir e estudar de forma análoga novos aspetos dos seres humanos. Nesta dissertação o foco principal é o cérebro de rato da estripe Wistar e tem como objetivo auxiliar o estudo desta estrutura. Mais especificamente foram usadas dois tipos de imagens: uma de microscopia de secções do cérebro do rato e outra de um atlas que representam esquematicamente as primeiras. De modo a facilitar o processo de análise das imagens de microscopia, esta dissertação explica o desenvolvimento de uma linha de processos capaz de auxiliar esta etapa essencial na realização de qualquer tipo de estudo com as estruturas. O resultado obtido ao executar o método, publicamente disponível num repositório,consiste na sobreposição da representação esquemática de uma parte do cérebro do rato com a imagem de microscopia do tecido que lhe corresponde, o que permite a visualização mais clara dos componentes presentes na imagem original da secção coronal do cérebro de rato, auxiliando no seu estudo.",
    "authors": [
      "Vieira, Rui Emanuel Gomes"
    ],
    "keywords": [
      "Processamento de imagens",
      "Atlas",
      "Imagens histológicas de cérebros de rato",
      "Registo de imagens",
      "Image processing",
      "Brain rat histological images",
      "Image registration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92714",
    "title": "Consola de atendimento para gestão de chamadas telefónicas",
    "abstract": "Esta dissertação apresenta um estudo compreensivo e extensivo de todo o processo de planeamento, desenvolvimento e implementação de um protótipo de uma consola de operador com o objetivo de auxiliaros seus utilizadores a gerir um elevado número de chamadas, de forma a ir de encontro às necessidades de várias organizações, face aos dias de hoje. Devido ao ritmo acelerado associado à evolução dosoftware, é do interesse destas organizações melhorarem a eficiência dos locais de trabalho, algo que oprotótipo desenvolvido pretende alcançar, agilizando e otimizando as várias operações de rececionistas eoperadores.Este documento inicia com um estudo aprofundado de várias soluções de consolas de operadorpresentes no mercado, com o intuito de analisar o que estas oferecem, incluindo a solução da AlticeLabs, com o objetivo final de estabelecer uma comparação entre estas, e analisar possíveis aspetos amelhorar desta última, assim como a sua posição e papel no mercado atual.A fase de planeamento inicia com um processo de tomada de decisão relativo às diferentes abordagens possíveis face ao trabalho a desenvolver. Após uma longa e rigorosa ponderação, que inclui umafase de testes englobando outras soluções oferecidas por outras organizações, a abordagem relativa aodesenvolvimento de um protótipo de raiz, focado num cliente web, foi selecionada, uma vez que foi aúnica, na altura, viável.Por este motivo, a fase de planeamento foi bastante extensa, envolvendo uma seleção de funcionalidades e requisitos com uma abordagem centrada ao utilizador, assegurando uma compreensão das suasnecessidades e preferências, tendo sempre em mente as operações suportadas pelos serviços a integrar,que se revelaram como um quanto restritivos, passando também pelo desenho de uma interface modulare intuitiva, com foco nas várias funcionalidades planeadas.A fase de desenvolvimento focou-se principalmente na implementação daquilo planeado na fase anterior, esclarecendo os motivos que levaram à escolha das tecnologias escolhidas e tendo sido tomado ocuidado de abordar a arquitetura e o funcionamento da aplicação, explicando os motivos que levaram àsdecisões tomadas. Para terminar esta fase, foi também exposto o resultado final e as várias alterações sofridas face ao planeamento, na tentativa de melhorar o produto final.Para terminar o documento, a eficácia da plataforma desenvolvida é avaliada através de um teste deusabilidade, com o objetivo de analisar o comportamento dos utilizadores na utilização da aplicação ecoletar a opinião dos participantes, contribuindo para o levantamento de aspetos a melhorar e para tirarconclusões relativas ao produto final.Os resultados obtidos neste teste de usabilidade foram positivos, indicando que a plataforma desenvolvida desempenhou um bom papel ao auxiliar os utilizadores a lidarem com um elevado fluxo dechamadas, aumentando a sua produtividade e eficiência, mas também servindo como fundação paratrabalhos futuros na área.",
    "authors": [
      "Mendes, André Filipe da Rocha"
    ],
    "keywords": [
      "Consola de operador",
      "Gestão de chamadas",
      "Desenvolvimento front-end",
      "Desenvolvimento web",
      "Interface do utilizador",
      "Arquitetura do cliente",
      "Attendant console",
      "Call handling",
      "Ffront-end development",
      "Web development",
      "User interface",
      "Client architecture",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47744",
    "title": "Implementing an integrated syntax directed editor for LISS",
    "abstract": "The aim of this master work is to implement LISS language in ANTLR compiler generatorsystem using an attribute grammar which create an abstract syntax tree (AST) and generateMIPS assembly code for MARS (MIPS Assembler and Runtime Simulator) . Using that AST,it is possible to create a Syntax Directed Editor (SDE) in order to provide the typical helpof a structured editor which controls the writing according to language syntax as definedby the underlying context free grammar.",
    "authors": [
      "Vaz, Damien da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92800",
    "title": "Regressão não linear em curvas de crescimento para parâmetros placentares em R",
    "abstract": "Nos últimos anos, tem existido um crescente interesse na avaliação dos parâmetros biométricos da pla centa e a sua relação com resultados obstétricos. Evidências têm sido publicadas sugerindo que asmedidas da placenta e a sua evolução são capazes de refletir alterações no desenvolvimento do feto eaté mesmo doenças do recém-nascido e do adulto. Tendo em conta, que os gráficos de crescimentodesempenham um papel crucial na avaliação e vigilância da população pediátrica, surgiu o tema destadissertação. O principal objetivo passa por estudar a aplicabilidade de modelos de regressão não linearem curvas de crescimento de parâmetros como o Diâmetro 1 (D1) e 2 (D2), Espessura (EP) e Peso pla centar (PP) e Peso fetal (PF). Para isso, utilizou-se um conjunto de dados de parturientes portuguesasrecolhidos no CGC (Centro de Genética Clínica), Porto.Neste estudo foi utilizada uma abordagem de regressão semiparamétrica para a construção de cur vas de crescimento de referência. Esta metodologia utiliza os modelos aditivos generalizados para lo calização, escala e forma (GAMLSS), Lamda-Mu-Sigma (LMS), LMS com Box-Cox t (BCT) e LMS comBox-Cox-powe-exponential (BCPE), oferecendo vantagens distintas sobre os métodos tradicionais comoa regressão quantílica. Uma das principais vantagens do GAMLSS é a sua flexibilidade para acomodarqualquer distribuição estatística, permitindo a modelação de vários parâmetros biométricos.Através da aplicação da metodologia proposta, foi demonstrado que com a utilização do métodoGAMLSS com BCT e P-splines para os parâmetros D1 e D2 e do método LMST para os parâmetros EP, PPe PF, podemos alcançar curvas de crescimento representativas. Além disso, foi desenvolvida uma apli cação web, disponível em https://placentalgrowth.shinyapps.io/uminho_pt/, utilizandoo ambiente R. Permite que profissionais de saúde e investigadores analisem e interpretem as curvas decrescimento desenvolvidas com facilidade. Os resultados deste estudo fornecem informações importantessobre o desenvolvimento da placenta e têm implicações significativas para a prática clínica em obstetrícia,permitindo o seu avanço e acompanhamento da saúde materno-fetal.",
    "authors": [
      "Lemos, Daniela Filipa Machado"
    ],
    "keywords": [
      "Placenta",
      "Curvas de crescimento",
      "Regressão não linear",
      "GAMLSS",
      "LMST",
      "Growth curves",
      "Nonlinear regression",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79702",
    "title": "Tecnicas de Deep Learning para a determinação da idade óssea",
    "abstract": "A avaliação da idade óssea (a maturação esquelética) é uma prática clínica comum para investigar doenças endocrinológicas, genéticas e de crescimento em crianças. Geralmente é realizada por exame radiológico da mão esquerda usando o método Greulich e Pyle (G & P) ou o Tanner Whitehouse (TW). No entanto, ambos procedimentos clínicos demonstraram várias limitações, desde o esforço do exame que tem que ser feito pelos radiologistas até a significativa variabilidade intra e inter-operador. Para resolver este problema, várias abordagens com recurso a sistemas de apoio ao diagnóstico médico (especialmente tomando como base o método TW) foram propostas. Nenhum deles demonstrou capacidades de generalização para diferentes raças, faixas etárias e géneros. A avaliação de exames radiológicos requer a análise de um profissional com a máxima atenção. No caso do método de Greulich e Pyle a radiografia da mão do paciente é comparada com um atlas padrão sendo possível observar deficiências no crescimento dos pacientes. Este é um trabalho exaustivo e sujeito a erros devido ao nível de atenção que é necessário durante o diagnóstico. Os métodos de deep learning têm sido aplicados a diversas tarefas de análise de imagem médica como, por exemplo, classificação de lesões e segmentação de tecidos. O principal objectivo deste trabalho e desenvolver um modelo capaz de automaticamente determinar a idade óssea. Neste trabalho foram primeiramente testadas várias arquitecturas de redes neuronais convolucionais na determinação da idade óssea que mostraram bons resultados em tarefas comuns de visão por computador. Baseado nos resultados obtidos foi desenvolvido/optimizado um novo modelo que é apresentado neste documento. Foi usado transfer learning e o treino de raiz nas redes neuronais seleccionadas obtendo uma taxa de erro de 7.89 meses na determinação da idade óssea em pacientes do sexo feminino e uma taxa de erro de 8,28 meses ao executar esta tarefa em homens.",
    "authors": [
      "Pinheiro, Gonçalo Manuel Barbosa Teles"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83632",
    "title": "Mobile ID como um serviço: generalização da arquitetura do standard ISO/IEC DIS 18013-5",
    "abstract": "O processo de identificação geralmente é usado para facilitar transações comerciais e governamentais. Apesarda existência de formas de identificação, maioritariamente presenciais, estas são pouco úteis para realizaçãode negócios online.De forma a encarar este problema, vários governos de diferentes países estão a criar sistemas nacionaisde identificação eletrónica (eID), isto é, uma coleção de tecnologias e políticas que permitem aos cidadãosprovarem eletronicamente a sua identidade, ou um atributo da sua identidade para um sistema de informação.Com o aparecimento da eID, um dos principais problemas que surgiu foi a insuficiente interoperabilidade entreos sistemas de identificação dos diferentes países adotantes, especialmente devido à falta de uma base jurídicacomum.Ao longo dos últimos anos surgiram soluções que, direta ou indiretamente, solucionam o problema de interoperabilidade,como é o caso do eIDAS. No entanto, o eIDAS é um regulamento seguido apenas pela UniãoEuropeia, existindo também a necessidade de readaptar o sistema de identificação eletrónica dos diferentesestados membros para criar uma ligação entre os diferentes sistemas.Como alternativa às soluções já existentes, o comité ISO/IEC JTC 1 estabeleceu uma norma, aplicada especificamentepara a Carta (ou licença) de Condução, mas que pode ser adaptada para outros documentosde identificação. Esta norma tem sido bem recebida pela maioria dos países e promete ser uma alternativabastante viável para a implementação de um sistema de identificação eletrónica.Assim, o principal foco desta dissertação é a definição de uma arquitetura aplicacional genérica, baseada nanorma técnica ISO/IEC DIS 18013-5. Com base na arquitetura definida, como prova de conceito pretende-secriar de um sistema de identificação eletrónica configurável, que permita ao utilizador final implementar o seusistema de identificação eletrónica, conforme as suas necessidades.",
    "authors": [
      "Parente, Filipa Correia"
    ],
    "keywords": [
      "Identificação eletrónica",
      "eIDAS",
      "ISO-IEC DIS 18013-5",
      "Sistema de identificação eletrónica configurável",
      "Aplicação",
      "Electronic identification",
      "Configurable electronic identification system",
      "Application",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81065",
    "title": "Exploring paraconsistent logics for quantum programs",
    "abstract": "Superconducting quantum circuits are a promising model for quantum computation, al though their physical implementation faces some adversities due to the hardly unavoidabledecoherence of superconducting quantum bits. This problem may be approached from aformal perspective, using logical reasoning to perform software correctness of programsexecuted in the non-ideal available hardware. This is the motivation for the work devel oped in this dissertation, which is ultimately an attempt to use the formalism of transitionsystems to design logical tools for the engineering of quantum software.A transition system to capture the possibly unexpected behaviors of quantum circuitsneeds to consider the phenomena of decoherence as a possible error factor. In this way, wepropose a new family of transition systems, the Paraconsistent Labelled Transition Systems(PLTS), to describe processes that may behave differently from what is expected when facingspecific contexts. System states are connected through transitions which simultaneouslycharacterize the possibility and impossibility of that being the system’s evolution. Thiskind of formalism may be used to represent processes whose evolution is impossible tobe sharply described and, thus, should be able to cope with inconsistencies, as well aswith vagueness or missing information. Besides giving the formal definition of PLTS, weestablish how they are related under the notions of morphism, simulation, bisimulationand trace equivalence.It is a common practice to combine transition systems through universal constructions,in a suitable category, which forms a basis for a process description language. In this dis sertation, we define a category of PLTS and propose a number of constructions to combinethem, providing a basis for such a language.Transition systems are usually associated with modal logics which provide a formal set ting to express and prove their properties. We also propose a modal logic, more specifically,a modal intuitionistic paraconsistent logic (MIPL), to talk about PLTS and express theirproperties, studying how the equivalence relations defined for PLTS extend to relations onMIPL models and how the satisfaction of formulas is preserved along related models.Finally, we illustrate how superconducting quantum circuits may be represented by aPLTS and propose the use of PLTS equivalence relations, namely that of trace equivalence,to compare circuit effectiveness.",
    "authors": [
      "Cruz, Ana Luzia"
    ],
    "keywords": [
      "Paraconsistency",
      "Transition systems",
      "Modal logic",
      "Quantum computation",
      "Paraconsistência",
      "Sistemas de transição",
      "Lógica modal",
      "Computação quântica",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47639",
    "title": "Real-time healthcare intelligence in organ transplantation",
    "abstract": "Organ transplantation is the best and often the only treatment for patients with end-stage organ failure. However,the universal shortage of deceased donors and the international variation in donation and transplantationactivities result in a worrying situation that must be addressed. As in most countries, Portugal has implementeddonation programs to answer the increasing need for transplants with the objective to identify all thepossible and potential donors admitted to hospitals. These donors constitute the largest share of organdonors in Portugal, but identifying a patient that may progress to brain death could be a complex task andcadaveric organs must be transplanted in a short period of time in order to achieve satisfactory results.Therefore, the urgent need of intelligent solutions that are able to support the decision-making process iscrucial in critical areas as the organ transplantation is.The aim of this dissertation is firstly the knowledge acquisition on the potential organ donor criteria for furtherdetection and secondly the design and implementation of a software platform to assist the inefficient processof identification of potential organ donors. This will result in an increase of control of the screening methodand consequently optimize the workflow of the pre-transplantation process.Accordingly, and after several meetings with the transplant team, a prior identification pattern was structuredand used to characterize the development of the proposed solution, named Organite. Organite is defined as asystem to support the transplantation process, based on Business Intelligence technologies. It is responsiblefor the collection, management, storage, and signaling of potential organ donors using information from thedisparate Health Information Systems to provide real-time tracking of patients and optimize the transplantteam’s workflow.The developed platform is currently implemented at Centro Hospitalar do Porto, Hospital de Santo António,EPE and displays a steady and competent behavior providing consequently a way to have more control of the information needed for the decision-making process. As a result, the number of transplantation records atCentro Hospitalar do Porto, Hospital de Santo António, EPE are expected to show more profitable outcomes.",
    "authors": [
      "Fernandes, Bruno Daniel Pereira"
    ],
    "keywords": [
      "Decision-making process",
      "Organ transplantation",
      "Potential organ donors detection",
      "Deteção de potenciais dadores de órgãos",
      "Processo de tomada de decisão",
      "Transplantação de órgãos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27855",
    "title": "Parallel interactive ray tracing and exploiting spatial coherence",
    "abstract": "Ray tracing is a rendering technique that allows simulating a wide range of light transport phenomena, resulting on highly realistic computer generated imaging. Ray tracing is, however, computationally very demanding, compared to other techniques such as rasterization that achieves shorter rendering times by greatly simplifying the physics of light propagation, at the cost of less realistic images.The complexity of the ray tracing algorithm makes it unusable for interactive applications on machines without dedicated hardware, such as GPUs. The extreme task independent nature of the algorithm offers great potential for parallel processing, increasing the available computational power by using additional resources. This thesis studies different approaches and enhancements on the decomposition of workload and load balancing in a distributed shared memory cluster in order to achieve interactive frame rates.This thesis also studies approaches to enhance the ray tracing algorithm, by reducing the computational demand without decreasing the quality of the results. To achieve this goal, optimizations that depend on the rays’ processing order were implemented. An alternative to the traditional image plan traversal order, scan line, is studied, using space-filling curves.Results have shown linear speed-ups of the used ray tracer in a distributed shared memory cluster. They have also shown that spatial coherence can be used to increase the performance of the ray tracing algorithm and that the improvement depends of the traversal order of the image plane.",
    "authors": [
      "Cruz, Eduardo José Tanque de Pádua"
    ],
    "keywords": [
      "Ray tracing",
      "Parallel computing",
      "Spatial coherence",
      "Computação paralela",
      "Coerência espacial",
      "519.674"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "519.674"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/40863",
    "title": "Plataforma de apoio à decisão nos cuidados de ginecologia e obstetrícia",
    "abstract": "A grande quantidade de dados gerada todos os dias na indústria, e nomeadamente,na área da saúde, impulsiona a utilização de Tecnologias deInformação (TIs) para o seu registo, tratamento e exploração, com o intuitode adquirir conhecimento com valor assim como instrumentos para o apoiona tomada decisões.Na unidade de cuidados materno-infantis do Centro Hospitalar do Porto(CHP), o Centro Materno Infantil do Norte (CMIN), os pro ssionais de saúdelidam com utentes em condições delicadas e situações que requerem a tomadade medidas rápida e e ciente. Um vasto conhecimento dos processos de Ginecologiae Obstetrícia (GO) pode ser crucial para promover as boas práticasmédicas e evitar eventos adversos na mãe e no recém-nascido. A aplicaçãobem sucedida de Sistema de Apoio à Decisão Clínica (SADC) em ambienteclínico e a recetibilidade dos pro ssionais de saúde e de TI do CMIN à introdução de novos conceitos e tecnologias, motivam o desenvolvimento de novosartefactos.Neste sentido, este projeto aplica os conceitos de Business Intelligence(BI) e Descoberta de Conhecimento sobre informação disponível nos Sistemasde Informação (SIs) da instituição, através de uma grande variedadede metodologias, métodos e tecnologias para construir soluções e apoiar osserviços prestados na unidade de cuidados materno-infantis.Um dos artefactos criados é o desenvolvimento de indicadores clínicos ede desempenho a partir da tecnologia de BI Pentaho Community Edition(CE). Este processo inicia-se com a extração, transformação e carregamentoda informação numa estrutura multidimensional, o Data Warehouse (DW),permitindo a posterior representação da informação através da aplicação de BI. Esta plataforma integra indicadores dos módulos de GO, de triagem e daadmissão hospitalar do CMIN.Na componente da descoberta de conhecimento, vários estudos são efetuados,tendo em conta as preocupações dos pro ssionais de saúde e as necessidadesda instituição, provando a viabilidade de utilizar técnicas de DataMining (DM) na construção de modelos de previsão no sector da saúde. Osnascimentos pré-termo, a seleção do tipo de parto mais adequado, a categorização das utentes e o seu percurso pela unidade de GO e os tempos deespera de pré e pós-triagem são alguns dos problemas analisados. Os estudosalcançam resultados promissores e clinicamente relevantes, permitindo aidenti cação de fatores de risco clínico. A título de exemplo, adquiriam-semodelos de previsão para os nascimentos pré-termo com valores de sensibilidadee especi cidade de 89% e 93%.De forma a presentear os pro ssionais de saúde com as soluções desenvolvidas,foi criada uma plataforma de alto-nível que integra os produtos deBI e os modelos de previsão de DM, culminando o objetivo do projeto numartefacto nal, que visa o apoio às práticas clínicas, a qualidade dos cuidadosprestados e a consequente satisfação dos utentes.",
    "authors": [
      "Pereira, Sónia Patrícia Pinto"
    ],
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83520",
    "title": "Diagnóstico da doença de Alzheimer com redes neuronais profundas",
    "abstract": "A doença de Alzheimer é o tipo mais predominante de demência e, apesar de não existircura para a mesma, o seu diagnóstico prematuro é fundamental para um tratamento efetivoe que permita retardar o progresso dos sintomas. Desta forma, nos últimos anos, temsurgido um grande interesse em estudar e desenvolver sistemas automáticos de diagnósticoque usam como fonte de dados os exames médicos realizados pelos pacientes.Esta dissertação enquadra-se na temática da utilização de aprendizagem profunda paradiagnosticar a doença de Alzheimer. Pretende-se que seja avaliado o desempenho deredes neuronais profundas existentes da forma mais real possível, e que seja proposta umaarquitetura com bom desempenho, que possa ser usada num sistema de diagnóstico assistidopor computador. A capacidade da aprendizagem profunda em encontrar padrões ocultos emimagens médicas permite reduzir o erro de diagnóstico humano e auxiliar num diagnósticomuito mais preciso.É pretendido que, com base numa única imagem por paciente a rede neuronal propostaseja capaz de fazer um diagnóstico sobre a doença de Alzheimer. Para isso, serão utilizadasimagens do cérebro obtidas pela técnica de Ressonância Magnética Estrutural, adquiridas apartir do conjunto de dados da Iniciativa de Neuroimagem da Doença de Alzheimer.Para a concretização do objetivo proposto, foram estudados e implementados, num cenárioexperimental recorrendo a ferramentas simples, os métodos mais indicados para a realizaçãodesta tarefa.Os resultados mostram que as Redes Neuronais Convolucionais (CNNs) permitem construirmodelos com enorme potencial, contudo, a sua utilização em ambientes reais aindanão é muito viável nos dias de hoje.",
    "authors": [
      "Silva, Mateus Ferreira da"
    ],
    "keywords": [
      "Doença de Alzheimer",
      "Diagnóstico assistido por computador",
      "Défice cognitivo ligeiro",
      "Ressonância magnética estrutural",
      "Aprendizagem profunda",
      "Redes neuronais convolucionais",
      "Alzheimer’s disease",
      "Computer aided diagnosis",
      "Mild cognitive impairment",
      "Structural MRI",
      "Deep learning",
      "Convolutional neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79735",
    "title": "Development of a self-diagnosis tests system for integration in a cyber-physical system",
    "abstract": "Hoje, a CONTROLAR fornece para a Bosch a Intelligent Functional Test System Machine, um sistemaciber-físico desenvolvido para realizar diferentes níveis de testes funcionais em dispositivos e componenteselectrónicos. A Bosch utiliza-a para testar o correto funcionamento dos auto-rádios produzidos. Duranteeste processo, os auto-rádios são submetidos a vários testes e o problema surge quando a máquinadetecta erros em vários auto-rádios consecutivos e não é possível saber se a própria máquina está comproblemas, pois não possui nenhum módulo que permita saber se está a funcionar corretamente ou não.A origem deste trabalho surge da necessidade de encontrar uma solução que resolva o problemaenunciado, mas também, inovadora e com contribuições para o mundo da investigação em sistemasciber-físicos e sistemas de testes de autodiagnóstico. A solução é integrar um sistema de autodiagnósticona máquina que possa testar o seu funcionamento para que a Bosch possa ter certeza se o problema estána máquina ou nos auto-rádios. Como a máquina é um sistema ciber-físico, permite a integração de umsistema de software que possa gerir a execução de testes, sendo capaz de detectar falhas nas máquinas.O trabalho aqui apresentado aborda o problema criando um novo sistema de testes de autodiagnósticoque garantirá a confiabilidade e integridade do sistema ciber-físico. Em detalhe, esta dissertação começapor expôr um estudo sobre o estado da arte atual de sistemas ciber-físicos, automação de testes, metodo logia de teste keyword-driven e mais alguns conceitos relacionados a linguagens específicas de domínioque serão relevantes para a solução final. São apresentadas a especificação e análise do sistema, a fimde definir bem os seus componentes. Uma nova arquitetura modular e extensível é proposta para siste mas de testes de autodiagnóstico, bem como uma arquitetura para estendê-lo e integrá-lo num sistemaciber-físico. Foi proposto um novo sistema de testes de autodiagnóstico que aplica a arquitetura propostaprovando que é possível realizar o autodiagnóstico em tempo real do sistema ciber-físico e permitindo aintegração de qualquer tipo de teste. Para validar o sistema, foram realizados 28 casos de teste, abran gendo todas as suas funcionalidades. Os resultados mostram que todos os casos de teste passaram e,portanto, o sistema cumpre todos os objetivos propostos.",
    "authors": [
      "Pereira, Ricardo Barros"
    ],
    "keywords": [
      "Sistemas ciber-físicos",
      "Auto-diagnóstico",
      "Automação de testes",
      "Aplicação web",
      "Cyber-physical systems",
      "Self-diagnosis",
      "Test automation",
      "Web Application",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92713",
    "title": "MulletBench: Multi-layer Edge Time Series Database Benchmark",
    "abstract": "Internet of Things (IoT) systems generate massive amounts of time series data that need to be storedfor historical analysis. As a result, Database Management Systems (DBMSs) for these scenarioshave particular requirements in their ability to ingest large amounts of data and to optimise aggregation,filtering and time-ranged queries over this data, which are essential for historical analysis.Through the use of Fog Computing, combining both Edge and Cloud layers, it is possible to achievereduced latency and increased scalability, privacy and connectivity through the Edge, while still benefitingfrom the enhanced computing and storage power of the Cloud. This has led to the development of FogDBMSs. Database benchmarking allows standardising performance assessment and comparison of different solutions. However, current time series database benchmarking tools are not designed for multi-layer architectures, such as the ones used by Edge-Cloud hybrid DBMSs. This thesis proposes MulletBench, a benchmarking tool that is able to evaluate the internal load balancing capabilities of a multi-layer Time Series Database Management System (TSDBMS). This is achieved by integrating automated deployment features, per-node and per-layer performance and system resource metrics, allowing for a more detailed analysis of the SUTs’ performance than previously possible. The performance of InfluxDB and IoTDB is evaluated using the developed tool, comparing their performance in multiple workloads and deployment scenarios. Results show that the Edge layer can be used to improve performance by distributing the workload over multiple layers and performing downsampling at the Edge layer, increasing overall throughput and reducing latency at the Cloud. These conclusions are enabled by MulletBench’s novel features, and would not have been possible with previously existing solutions.",
    "authors": [
      "Pereira, Pedro Miguel de Leal Meireles"
    ],
    "keywords": [
      "Benchmark",
      "Internet of things",
      "Fog",
      "Time series",
      "Databases",
      "Internet das coisas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27775",
    "title": "Modelação de padrões de conectividade cerebral funcional",
    "abstract": "O estudo da conectividade cerebral, da integração e segregação das funções das diferentespartes do cérebro será no futuro próximo uma das ferramentas mais importantes nacompreensão do cérebro humano. Contudo, a variedade de processos e dinâmicas lápresentes tornam esta tarefa extremamente complexa.Assim, o objectivo principal deste trabalho passa por desenvolver e testar um modelo capaz derepresentar redes de conectividade cerebral. A teoria de grafos, em conjugação commodalidades de neuroimagiologia como a ressonância magnética, tem-se mostrado umaferramenta extremamente valiosa neste sentido.O trabalho aqui apresentado foca-se em três pontos: o pré-processamento das imagens deressonância magnética; a definição dos elementos que constituem a rede, comparandodiferentes estratégias, e construção das redes; a utilização de métricas e conhecimentos deteoria de grafos para caracterizar e comparar as redes.Utilizando dados reais foi possível construir redes esparsas, eficientes, resilientes, com fortedivisão em comunidades e arquitetura small-world. Foi observado o efeito das diferentesestratégias nas características das redes, e mesmo na falta de fortes conclusões sobre qual amais adequada, foi possível compreender a dificuldade inerente á comparação de redescomplexas e dados passos importantes no sentido de melhorar essa comparação.",
    "authors": [
      "Magalhães, Ricardo José Silva"
    ],
    "keywords": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/39320",
    "title": "Monitorização e controlo em ambientes industriais através de rede de sensores sem fios",
    "abstract": "Ao longo dos últimos anos tem existido um elevado crescimento na utilização dedispositivos de comunicação sem fios, em que se destacam as redes de sensores semfios, com elevado número de possíveis aplicações, nas mais diversas áreas, sendo cadavez mais uma realidade no quotidiano das pessoas e da indústria. As redes de sensoressem fios têm como principal objetivo o encaminhamento dos dados sensoriaisrecolhidos por determinados nós da rede para outros nós da rede (e.g., dadosrecolhidos pelos sensores de um dispositivo terminal e enviados para uma estaçãobase).Nesta dissertação, foi realizado um projeto que consiste no estudo,desenvolvimento e testes de uma rede sem fios ZigBee, para controlar e monitorizarequipamentos em ambiente industrial. A implementação desta rede surge como umprocesso de substituição da rede cablada existente e dos dispositivos que controlamos equipamentos. Esta implementação foi feita utilizando plataformas de hardware esoftware da Texas Instruments e o IDE NetBeans. Numa fase inicial foram realizadostestes para avaliação da taxa de erros na transmissão num ambiente industrial, deforma a poder viabilizar o projeto. Foi desenvolvido código para dispositivos do tiporouter ZigBee com a capacidade de controlar equipamentos de refrigeração, recolheros dados dos sensores que lhe estão associados, e enviá-los pela rede até ao seudestino, diretamente ou por intermédio de outros routers. Foi desenvolvido tambémum coordenador ZigBee ligado a um PC que, através de uma interface gráfica com outilizador, tem a capacidade de monitorizar e controlar todos os routers existentes narede.",
    "authors": [
      "Castro, Pedro Filipe Mendes de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80033",
    "title": "Verification of distributed algorithms with the Why3 tool",
    "abstract": "Nowadays, there currently exist many working program verification tools however, the developed tools are mostly limited to the verification of sequential code, or else of multi-threaded shared-memory programs. Due to the importance that distributed systems and protocols play in many systems, they have been targeted by the program verification community since the beginning of this area. In this sense, they recently tried to create tools capable of deductive verification in the distributed setting (deductive verification techniques offer the highest degree of assurance) and claim to have achieved impressive results. Thus, this dissertation will explore the use of the Why3 deductive verification tool for the verification of dis tributed algorithms. It will comprise the definition of a dedicated Why3library, together with a representative set of case studies. The goal is to provide evidence that Why3 is a privileged tool for such a task, standing at a sweet spot regarding expressive power and practicality.",
    "authors": [
      "Cruz, Carla Isabel Novais"
    ],
    "keywords": [
      "Deductive verification",
      "Distributed systems and protocols",
      "Why3",
      "Why3do",
      "Verificação dedutiva",
      "Sistemas e protocolos distribuídos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27841",
    "title": "Fault injection for the evaluation of critical systems",
    "abstract": "Atualmente, os sistemas críticos estão cada vez mais presentes no nosso dia-a-dia, fazendoaumentar a necessidade de os assegurar cada vez mais e reduzindo o risco de acidente oufalha. A industria espacial e automóvel são exemplos de indústrias que usam esses sistemase que necessitam de os ver assegurados. Consequentemente, têm de ser tomadas medidaspara garantir a segurança de um sistema ao nível de software e hardware.A injeção de falhas é uma das respostas a esse problema, fazendo uso das suas diferentestécnicas para poder avaliar e validar sistemas críticos. A injeção de falhas pode ser consideradauma técnica de teste ao software, onde as falhas podem ser injetadas ao nível do softwareou hardware e cujos resultados podem ser monitorizados de forma a avaliar como é que osistema reagiu a tais falhas. Scan-Chain Implemented Fault Injection é a técnica de injeçãode falhas que proporciona uma maior acessibilidade, observabilidade e controlabilidade. Comesta técnica, os níveis de hardware e de integração de sistemas podem ser validados.O csXception® é um ambiente de injeção de falhas automatizado desenvolvido pela CriticalSoftware S.A para avaliar e validar sistemas críticos. A sua arquitetura é dinâmica e baseadaem plug-ins de injeção de falhas. Devido à crescente presença dos microcontroladores ARM®Cortex-M3 na industria automóvel, surgiu a necessidade de criar um novo plug-in de injeçãode falhas para o csXception®.Assim, o objectivo principal desta dissertação de mestrado é o desenvolvimento de umnovo plug-in de injeção de falhas para o csXception®, que permita injetar falhas em microcontroladoresARM® Cortex-M3, contextualizar o novo plug-in com a norma ISO-26262 e utilizarum caso de estudo para mostrar alguns dos resultados obtidos.",
    "authors": [
      "Cunha, João Mário Quintas"
    ],
    "keywords": [
      "681.3.06"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80270",
    "title": "Eficiência energética em redes de sensores sem fios: medição adaptativa num sistema de rega inteligente usando o CupCarbon",
    "abstract": "Today, there are many cities that offer to citizens smart solutions to make their dailylives easier so that the available resources can be better managed and the global qualityof life improved. These solutions generally rely on a variety of Wireless Sensor Networks(WSN), which are applied in a wide range of scenarios. Most of these solutions workwithout human intervention, therefore, there has been a lot of interest in increasing thelongevity of these sensor networks.In this context, the main purpose of this work is to study and optimize an adaptive,energy-aware sensing algorithm for WSNs, e-LiteSense [11], wich is an algorithmcapable of auto-regulate how data is sensed, adjusting it to each applicational scenario.This work, resorts to a simulation scenario representing a case in real life, namely, anIntelligent Irrigation system. In this study, CupCarbon is used as a simulation toolto implement WSN-based system and the e-LiteSense algorithm. The aim is to adaptthe number of measurement events of environmental parameters so that the energyconsumption of the different nodes of the network can be reduced while maintainingthe correct evaluation of the measurement data and increasing the lifetime of the sensornetwork. The versatility of the algorithm in relation to its effectiveness and ability toself-configure in different types of sensing scenarios is also evaluated.",
    "authors": [
      "Fortunato, Filipe Vieira"
    ],
    "keywords": [
      "Wireless Sensor Networks",
      "Adaptive sensing",
      "Energy-aware sensing",
      "Intelligent Irrigation System",
      "CupCarbon",
      "Energy Consumption",
      "Redes de Sensores Sem Fios",
      "Sensing adaptativo",
      "Eficiência energética",
      "Sistema de Rega Inteligente",
      "Consumo energético",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83361",
    "title": "Análise de sentimentos para a geração de índices de bem-estar",
    "abstract": "In recent years, due to constant social and economic crises, there has been some concern regarding the quality of life and satisfaction of the population. Then, a need to create measures or criteria that would allow assessing the population's well-being, arose. Usually, these criteria are quite complex, because any one of them can be analyzed through different perspectives or dimensions, since the quality of life of a human being depends on several factors, such as health and education. The analysis of such criteria can be done through indexes. To deal with the complexity of these indexes and to create conditions that facilitate decision making, multidimensional systems can be used. These allow for a broader analysis of well-being indexes and their underlying dimensions. In this dissertation work, we will explore this area by creating a well-being analysis system based on indexes that will be calculated through the analysis of feelings expressed in texts.",
    "authors": [
      "Veloso, Ricardo Milhazes"
    ],
    "keywords": [
      "Well-being Indexes",
      "Sentiment analysis",
      "Text Mining",
      "Online Analytical Processing (OLAF)",
      "Índices de Bem-estar",
      "Análise de sentimentos",
      "Processamento de textos",
      "Sistemas multidimensionais de dados (OLAP)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92606",
    "title": "Simulação de sistemas distribuídos de gestão de bases de dados",
    "abstract": "Hoje em dia, graças à existência de várias aplicações em grande escala com acesso a grandes quantidades de informação, bases de dados monolíticas não são capazes de satisfazer as suas necessidades,quer a nível de disponibilidade, de escalabilidade ou de performance. Deste modo, necessitamos desistemas distribuídos de gestão de bases de dados para conseguir satisfazer estas aplicações. Destessistemas, são particularmente interessantes aqueles que se destinam a um grande número de servidoresespalhados por diferentes zonas geográficas, devido à urgência de os aproximar das populações para obter uma melhor escalabilidade do sistema e uma melhor performance. Estes sistemas estão geralmentedivididos em duas famílias: uma que dá prioridade à coerência dos dados e uma que dá prioridade àdisponibilidade do serviço.Apesar do interesse que estes sistemas despertam, existe um grande custo associado ao seu teste nomundo real, sendo necessário recorrer a modelos de simulação para reproduzir o seu comportamento.Além disso, estes sistemas contém bastantes diferenças entre eles, sendo muitas vezes difícil de compararas suas vantagens e desvantagens em contexto real.Nesta tese desenvolvemos o SAGeo, um simulador de bases de dados geo-replicadas configurável,capaz de avaliar e comparar o desempenho relativo de diversas bases de dados distribuídas. Para alémdisso, configuramos este simulador para três algoritmos de bases de dados diferentes e apresentamoscomparações de resultados de diversas simulações realizadas.",
    "authors": [
      "Sousa, Paulo Silva"
    ],
    "keywords": [
      "Base de dados",
      "Sistemas distribuídos",
      "Base de dados distribuída geo-replicada",
      "Simulação",
      "Simulação de eventos discreto",
      "Database",
      "Distributed systems",
      "Distributed geo-replicated database",
      "Simulation",
      "Discrete event simulation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82127",
    "title": "Navegação segura - análise do uso de HTTPS na perspectiva do utilizador final",
    "abstract": "The Internet emerged in the late sixties in a scenario marked by the race of world hegemonybetween USA and USSR. Besides military applications, it was also initially used by researchers,academics, and college students, enabling file transfer between hosts. After the nineties theInternet reached the general public. It was then focused on other purposes, such as access tohypermedia, social networks, advertising and even products sale.Given the diversification of these accesses, the adoption of protocols for safe browsing has be come essential to protect user’s information. Combined with the classification of encrypted traffic,using appropriate techniques for this purpose, this paper aims to analyze the use of HTTPS pro tocol in various browsing scenarios once considered safe. Through testing scenarios, this researchintends to verify changes and impacts that this protocol promotes regarding the data collectionfrom the users during the Internet access experience.",
    "authors": [
      "Machado, Carlos Eduardo Ribeiro"
    ],
    "keywords": [
      "HTTPS",
      "User",
      "Web security",
      "Utilizador",
      "Segurança na Web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84357",
    "title": "Automatic quantification of microglial cells from brain images",
    "abstract": "Microglia are a type of glial cell residing in the central nervous system and represent about 10 to15% of the brain cell population. These cells don’t produce electrical impulses and are responsible forfundamental physiological and pathological processes, as they represent the first line of immune defencewithin the central nervous system. Thus, the quantification of these cells is essential in a clinical context,as it allows better monitoring and planning of treatments for different pathologies.Conventional cell counting involves a specific set of tools and devices developed for this purpose.This process is time-consuming and imprecise due to being heavily dependent on the operator. Currently,most processes are performed manually. However, other approaches have been studied and developedto improve the counting process, making it less time-consuming, more efficient and reduce the errorassociated with factors external to the counting. That said, the objective of this dissertation is to study thebest approach to automate the quantification of microglial cells, ranging from classical to deep learningmethodologies. Combined with the appropriate image processing and analysis techniques, the classicalapproach proves to be an adequate solution. However, in recent years, approaches based on deep learninghave shown promising performance in various image analysis tasks, such as classification, detection andsegmentation.The approaches developed to automate the quantification process were tested on a set of images builtin partnership with researchers from the School of Medicine of the University of Minho. As for the classicalmethodology approach, a protocol was developed within ImageJ, which was combined with image processingtechniques that allowed the automation of the counting process. Based on Convolutional NeuralNetworks, the classification problem referring to a deep learning methodology obtained an accuracy of0.9021 and managed to classify the 661 images in 5 minutes and 44 seconds. The two approaches, consideredoptimal within each methodology, are competitive with the state-of-the-art methods, as they allowedfor the automation of the quantification process, and showed a significant improvement in reproducibility,efficiency and reduced error associated with human factors.",
    "authors": [
      "Lopes, Diogo Alexandre Rodrigues"
    ],
    "keywords": [
      "Automatic quantification of cells",
      "Central nervous system",
      "Deep learning",
      "Image processing",
      "Image segmentation",
      "Microglial cells",
      "Células microgliais",
      "Processamento de imagem",
      "Quantificação automática de células",
      "Segmentação de imagem",
      "Sistema nervoso central",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80054",
    "title": "A quantum algorithm for ray casting using an orthographic camera",
    "abstract": "Quantum computing has the potential to provide better time complexities (than those achieved with classical computers) for challenging problems solved by classical comput ers or even provide a solution for problems out of reach of classical computers in terms oftime complexity (where the time consumed for the resolution of the problem is not prac tical, e.g. thousands of years). Here we’re not solving an unsolvable problem but tryingto improve its time complexity. There are several problems in rendering that are good can didates to being solved in a quantum fashion. Although previous research has proposedof theoretical ways of doing this, here we present a practical solution. This work takes afirst step in applying quantum computing to one of the most fundamental operations inrendering: ray casting. This technique allows computing visibility between two points ina 3D model of the world which is described by a collection of geometric primitives. Thealgorithm returns, for a given ray, which primitive it intersects closest to its origin. Withouta spatial acceleration structure, the complexity for this operation is O(N). The main goalof this work is to use the Grover’s Algorithm, a quantum search algorithm based on ampli tude amplification, to improve the complexity of this problem. This algorithm provides aquadratic speed up allowing for visibility evaluation for unstructured primitives in O(√N)steps. Due to technological limitations associated with current quantum computers we hadto simplify our problem’s structure and in this work the geometrical setup is limited torectangles and parallel rays (orthographic projection).",
    "authors": [
      "Alves, Carolina Isabel Monteiro"
    ],
    "keywords": [
      "Quantum computing",
      "Grover’s algorithm",
      "Complexity",
      "Ray casting",
      "Computação quântica",
      "Algoritmo de Grover",
      "Complexidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46413",
    "title": "Simulação de protocolos de encaminhamento em redes oportunistas",
    "abstract": "Nos dias que correm existe um crescimento tecnológico que se traduz não só nonúmero de dispositivos, mas também nas suas capacidades de processamento. Cada vezmais é possível encontrar pedestres e veículos equipados com dispositivos capazes decomunicar sem fios. E graças a esses desenvolvimentos, é necessário criar aplicações quepermitam otimizar a utilização destes dispositivos.De forma a ser possível prever o comportamento destas redes móveis, é necessáriosimular a utilização destes dispositivos em ambientes urbanos. No entanto, ossimuladores existentes continuam a ser muito genéricos e incapazes de simular em grandeescala ou simular protocolos específicos de encaminhamento. Foi com o objetivo deresponder a estes problemas que foi criado este simulador.Esta dissertação descreve o desenvolvimento de um simulador de comunicaçõesem ambientes urbanos. Este simulador permite simular as movimentações de vários tiposde atores e permite simular a interação entre atores através de vários protocolos deencaminhamento. Estes protocolos terão que realizar a comunicação da forma maisrealista possível. Foram então implementados vários protocolos de encaminhamento, demaneira a que seja possível existir essa interação. Para ter a certeza que os protocolosimplementados foram implementados da forma mais realista, estes protocolos foramtestados a nível de performance e de resultados, e os resultados obtidos foram comparadoscom resultados obtidos por um dos simuladores mais usados para a simulação deste tipode redes. Assim, com esta comparação, é possível concluir que os objetivos propostospara este projeto foram atingidos.",
    "authors": [
      "Pires, João Paulo Vilaça"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86733",
    "title": "Dynamic application to identify percentiles related to placenta parameters according to gestational age",
    "abstract": "Durante anos, o crescimento do feto e o seu desenvolvimento eram as medidas que os médicos usavam para estimar o desenvolvimento e crescimento do mesmo. No entanto em estudos recentes tem-se vindo a provar que o desenvolvimento e crescimento da placenta é de grande importância para estudar a saúde e crescimento do feto.Para a correta análise do desenvolvimento da placenta, curvas de crescimento precisam de ser criadas para comparação com diferentes percentis. Métodos de regressão linear foram usados para a criação de curvas de crescimento para a população Portuguesa (Nogueira et al., 2019).Neste trabalho, usando os mesmos dados usados por Nogueira et al. foram criadas curvas de crescimento usando um método estatístico conhecido por regressão de quantis que permite criar curvas usando um método mais robusto que o anteriormente usado.Foi também objetivo deste trabalho a criação de uma aplicação que permita ao utilizador colocar os valores de crescimento da placenta e poder comparar com as curvas de crescimento. Para isto, duas aplicações foram criadas. A primeira denominada APP1 tenta simular as ligações a bases de dados existentes em hospitais, tentando ao máximo recriar as condições presentes num hospital. A segunda aplicação, APP2, é mais simples e só precisa de documentos .csv com os valores da placenta, e permite a sua utilização sem necessidade de criação de algum tipo de ligação(https://samuelalves.shinyapps.io/APP2/).Ambos os objetivos foram conseguidos com sucesso, permitindo um avanço na área que cada vez mais se mostra de grande utilidade.",
    "authors": [
      "Alves, Samuel Gustavo Correia Nogueira"
    ],
    "keywords": [
      "Placenta",
      "Curvas de crescimento",
      "Desenvolvimento",
      "Shiny",
      "Regressão de Quantis",
      "Growth curves",
      "Development",
      "Shiny",
      "Quantile regression",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59725",
    "title": "Middleware de acesso coerente a serviços de bases de dados na nuvem",
    "abstract": "O aumento da quantidade de dados a processar, em diversos domínios, levou à necessidadede escalar os sistemas de armazenamento. Além dos sistemas de bases de dados tradicionais,que têm suporte a transações com propriedades ACID (Atomicidade, Coerência, Isolamento,Durabilidade), surgiram sistemas com base em outros paradigmas, que oferecemoperações mais simples, baseadas no modelo chave-valor. Nestes sistemas, abdicou-se dosuporte a transações com propriedades ACID para atingir a escalabilidade necessária.Por outro lado, apareceram os serviços de armazenamento na nuvem, seguindo o modelochave-valor, em que o tarifário de utilização é baseado no número de operações aprovisionadase o nível de coerência a que estas são executadas.No entanto, continuam a haver aplicações que necessitam de aceder a dados com garantiasde coerência. Para tal, surgiram camadas transacionais de interface com sistemas dearmazenamento chave-valor que medeiam todos os acessos das aplicações ao serviço dearmazenamento.Esta dissertação analisa os compromissos dos modelos de coerência, oferecidos por serviçosde armazenamento na nuvem, e propõe uma arquitetura que tira partido da mediaçãodos acessos à nuvem para otimizar o custo e o desempenho. Esta proposta é avaliada comum modelo de simulação, que permite demonstrar a sua validade.",
    "authors": [
      "Ribeiro, Cláudia Fernandes"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46396",
    "title": "Encaminhamento probabilístico de dados nomeados em redes tolerantes a atraso",
    "abstract": "Atualmente a arquitetura na qual a Internet se baseia assenta na ideia de comunicação fima-fimsempre disponível. É esperado que uma ligação entre a origem e o destino da informaçãoesteja sempre disponível. Contudo, existem situações onde tal não é possível, ou não égarantido. Nessas situações é necessário ter uma abordagem diferente.Foi com esse intuito que nasceram as Redes Tolerantes a Atrasos (Delay Tolerant Networks,DTN). Neste tipo de redes, as ligações ponto a ponto são esporádicas, podendo acontecer emdeterminado momento, e no momento seguinte deixarem de estar disponíveis. Por exemplo,numa rede em que os nós são móveis, quando dois nós se cruzam estabelecem ligação. Nomomento seguinte, afastam-se e a ligação perde-se. As DTNs têm a capacidade de operar nestetipo de cenários, por exemplo.Para além desta assunção de que existe sempre uma ligação fim-a-fim disponível, a Internet,atualmente baseia-se também no conceito de IPs, ou seja, endereço de origem e endereço dedestino. A informação em si não é tida em conta. Ultimamente tem sido tentada umaabordagem diferente. Uma abordagem na qual é tido em conta o conteúdo das mensagens. Umexemplo deste tipo de redes são as Redes de Dados Nomeados (Named Data Network, NDN). Éaplicado o paradigma de produtor/consumidor, no qual existem dois tipos distintos de nós. Oprimeiro cria a informação e o segundo consome-a. Um nó que produza determinado tipo deinformação coloca-a na rede, os nós que tiverem interessados na mesma, devem entãosubscrevê-la.O principal objetivo desta dissertação é acoplar estes dois tipos de redes. Maisprecisamente tentar colocar uma abordagem semelhante às NDNs em cenários onde se utilizamDTNs. Nomeadamente, é sugerida uma abordagem no que diz respeito à distribuição de dadosnomeados em cenários onde as DTNs operam.Pretende-se melhorar um protocolo de encaminhamento probabilístico que encaminhamensagens tendo em conta encontros prévios. Transportou-se para as redes de dadosnomeados o paradigma das redes oportunistas de que se dois nós se encontram a probabilidadede se voltarem a encontrar aumenta. Havia-se aplicado esse conceito no contacto comconteúdos, agora esse conceito é também aplicado no contacto de interesses. Ou seja, osconteúdos são agora encaminhados consoante a probabilidade do nó recetor encontrarinteressados no mesmo.",
    "authors": [
      "Lima, Fábio Joel Sá"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82799",
    "title": "HyLake: atualização de lagos de dados com granularidade fina",
    "abstract": "Os lagos de dados, também conhecidos por data lakes, suportam a recolha de grandes quantidades deinformação em ficheiros imutáveis para processamento analítico. No entanto, tem surgido a necessidadede modificar e atualizar esta informação de forma fiável, seja porque os dados são recebidos de formaincremental (por exemplo, de sensores e outras fontes de eventos) ou para eliminar os mesmos (porexemplo, devido ao RGPD (Regulamento Geral sobre a Proteção de Dados)). As soluções atuais para ofazer não são no entanto ideais: o armazenamento em SGBD (Sistema de Gestão de Bases de Dados)NoSQL (Not only SQL) tem um grande impacto no desempenho analítico, enquanto que sistemas baseadosem ficheiros, como o Delta Lake, permitem apenas atualizações de granularidade grossa.Neste trabalho aborda-se este problema propondo uma solução híbrida que combina o armazena mento de longo prazo em ficheiros com um armazenamento transitório num SGBD NoSQL de forma aobter as vantagens de ambos os sistemas. Para o efeito, é implementado uma prova de conceito usandoSpark, com ficheiros Parquet, e MongoDB. Assim, com a introdução deste sistema pretende-se possibi litar a execução de transações frequentes e de granularidade fina para suportar uma carga de trabalhoOLTP (Online Transaction Processing). Os resultados experimentais obtidos confirmam que esta propostaobtém desempenho analítico e transacional comparável a cada um dos sistemas isolados.",
    "authors": [
      "Teixeira, Nelson José Dias"
    ],
    "keywords": [
      "Lagos de dados",
      "Transações",
      "Processamento híbrido transacional-analítico",
      "Bases de dados",
      "Sistemas distribuídos",
      "Data lakes",
      "Transactions",
      "HTAP (Hybrid Transactional Analytical Processing)",
      "Databases",
      "Distributed systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79711",
    "title": "Towards model checking electrum specifications with LTSmin",
    "abstract": "Model checking é uma técnica comum de verificação; garante a consistência e integridade dequalquer sistema fazendo uma exploração exaustiva de todos os possíveis estados. Devido àgrande quantidade de intercalações possíveis entre eventos, modelos de sistemas distribuídosmuitas vezes acabam por gerar um número de estados muito grande. Nesta dissertaçãovamos explorar os efeitos de partial order reduction — uma técnica para mitigar os efeitosda explosão de estados — implementando uma linguagem semelhante ao Electrum comLTSmin. Vamos também propor um event layer por cima do Electrum e uma análise sintáticapara extrair informação necessária para que esta técnica possa ser implementada.",
    "authors": [
      "Cancelinha, Bruno Miguel Sousa"
    ],
    "keywords": [
      "Alloy",
      "Electrum",
      "Model checking",
      "LTSmin",
      "Partial order reduction",
      "TLA+",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92675",
    "title": "Properties that better describe a programming language",
    "abstract": "This document reports the development of a Master’s Thesis, included in the second yearof the Master’s Degree in Informatics Engineering at Universidade do Minho in Braga,Portugal. The main goal for this project was to identify which characteristics influence the recognition and identification of a programming language, considering both its typical source codeelements and its linguistic style. In other words, which elements contribute the most tothe characterization of a language? How many structural elements of a language may bemodified without losing its identity? In order to achieve these goals, a comprehensive bibliographic research was made, ranging from basic concepts such as programming languages and how they work, to several state-of-the-art studies that have been conducted in the same context of this project. Complementary to this research, a set of programming languages was also chosen as a study subject, which resulted in a detailed review and categorization of their characteristics.After the definition of a general approach, a survey was developed and conducted togather programmers’ answers on how they identify and recognize programming languages.In addition to the survey, a machine learning model was also used to evaluate how thesetwo facets (human versus machine) compared to each other. This dual approach providedinsights into which syntactic and semantic elements have a greater influence on the identityof a programming language. This Master’s project resulted in an overall picture of programming languages’ characteristics and the relative influence they have on both programmers’ and AI-driven recognition. This result may serve as support for language engineers and project managers who wish to reduce attrition when defining or designing new languages for a project, domain, or context.",
    "authors": [
      "Alves, Júlio Miguel de Sá Lima Magalhães"
    ],
    "keywords": [
      "Programming languages",
      "Programming language characterization",
      "Programming anguage design",
      "Programming language identification",
      "Linguagens de programação",
      "Caracterização de linguagens de programação",
      "Design de linguagens de programação",
      "Identificação de linguagens de programação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84136",
    "title": "A machine learning approach to boredom detection in smartphones",
    "abstract": "Desde tempos antigos que o ser humano tenta combater sentimentos negativos, como a tristezae a solidão. Uma outra emoção que sempre perturbou a humanidade é o aborrecimento. Destacausa nascem vários tipos de arte e também diversos desportos, sendo que estes continuam aser observados e/ou praticados até hoje. Nos dias de hoje, dado o facto que os smartphones setornaram dispositivos utilizados a nível global, faz com que as pessoas sejam submetidas a cadavez mais estímulos. Assim sendo, quando as mesmas não estão ocupadas, sentem a necessidadede fazer algo que mantenha o seu cérebro activo. Por esta razão, detectar aborrecimento quandose usa um smartphone, abre caminho para melhorar os índices de sucesso dos sinais de estímulo,com um sistema menos intrusivo e mais inteligente. Numa fase inicial, este trabalho de investigaçãofocou-se na recolha de dados. Desenvolveu-se uma aplicação inicial, para cumprir este objectivo.O principal propósito desta aplicação protótipo é a recolha da gama de valores dos sensores físicose virtuais de um dispositivo móvel, e dados que possam ilustrar o comportamento digital duranteo seu uso. Posteriormente à construção do conjunto de dados, foi realizado uma série de técnicase processos relacionados com Machine Learning para eleger o melhor modelo possível. Por fim, aúltima etapa foi a elaboração da aplicação final, já com o modelo ideal incorporado, que é capazde indicar quando o utilizador de um smartphone está aborrecido, com base nos valores indicadospelos sensores e pelo estado do próprio dispositivo móvel. O modelo incorporado trata-se de umaRede Neuronal Artificial que tem a capacidade de prever o nível de aborrecimento da pessoa queestá a interagir com o telemóvel. Este modelo consegue prever o sentimento em causa com umaprecisão de 70%.",
    "authors": [
      "Campos, Carlos José Gomes"
    ],
    "keywords": [
      "Aborrecimento",
      "Aprendizagem automática",
      "Computação afectiva",
      "Inteligência artificial",
      "Sensores",
      "Affective computing",
      "Artificial intelligence",
      "Boredom",
      "Machine learning",
      "Smartphone",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34116",
    "title": "Avaliação da qualidade de informação no registo da cirurgia segura",
    "abstract": "Os sistemas de informação representam uma parte importante dentro das organizações e oseu papel, para o fornecimento de informação a tempo e onde é necessária, é crítico. Nos últimosanos, o recurso a estes sistemas tem aumentado exponencialmente, sendo possível encontra-los empraticamente todas as áreas da ação humana.A área da saúde não é alheia a esta tendência e tem-se observado um aumento bastanteacentuado na quantidade de dados processados e armazenados.Assegurar que a informação essencial para o correto funcionamento da instituição estádisponível e tem um elevado grau de qualidade são missões da Informática Médica.As ferramentas de Business Intelligence entram nesta área com uma elevada relevância, poispermitem a análise de dados e da informação que estes potencialmente representam, através dorecurso a diversas metodologias, aplicações e tecnologias.O registo clínico electrónico, para os dados do projeto “Cirurgia Segura Salva Vidas”, pode serpotencialmente vital para a qualidade e segurança do paciente no Bloco Operatório, uma vez que estedefine etapas e passos a serem realizados pelos profissionais envolvidos com o objetivo de reduzir onúmero de eventos adversos.Avaliar a qualidade da informação deste registo torna-se, por isso, particularmenteinteressante, principalmente quando se ponderam as potenciais consequências que a informaçãoextraída deste registo pode ter na instituição.Os indicadores utilizados permitiram retirar conclusões sobre a qualidade da informaçãoencontrada nos registo da cirurgia segura no CHAA e considera-se que a ferramenta de BusinessIntelligence, Pentaho Community, foi aplicada com sucesso no sector da saúde. As necessidades efalhas de informação presentes no CHAA foram identificadas.",
    "authors": [
      "Lopes, Jorge Miguel Lobão"
    ],
    "keywords": [
      "681.3:61",
      "61:681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84384",
    "title": "Your travel: plataforma de agendamento de viagens personalizadas",
    "abstract": "Hoje, no turismo, a tendência é hoje cada vez mais voltada para a personalização, para as experiências locais, para a qualidade da viagem e não para a quantidade de cidades visitadas. Para o sucesso de uma viagem personalizada é muito importante dois fatores, a agência, que planeia a viagem e o guia. A agência, para além de ter de conhecer o destino a ser visitado, tem de ser organizada, seja relativamente á equipa que a constitui como com os parceiros com que trabalha para construir as Viagens. Para além, disso esta também precisa de ter uma comunicação fácil com o cliente pois sem isso não tem como saber quais são as preferências e motivos da viagem que este quer fazer. O principal objetivo desta dissertação é fazer uma plataforma web que possa fornecer as ferramentas necessárias á agência para que esta possa construir viagens personalizadas com sucesso. Para isso, o desenvolvimento desta foicontou com a orientação da equipa de BackOffice da Your Tours e graças a isso foi possível detetar quais as maiores dificuldades que era necessário a plataforma resolver, relativamente a organização, comunicação e planejamento e perceber quais as features que faltavam nas plataformas utilizadas por estes. Graças a isso, esta plataforma que possibilita fácil organização de trabalhos /tarefas ecomunicação entre a equipa do BackOffice, uma comunicação direta com o cliente para que este possa ter uma participação ativa na construção da sua viagem e possa acompanhar todo o processo em tempo real e um flow eficiente e otimizado de construção de viagens personalizadas com orçamentação automática, cronogramas dinâmicos e construção de itinerários digitais fácil e eficiente.",
    "authors": [
      "Ribeiro, Miguel Mateus"
    ],
    "keywords": [
      "Personalização",
      "Comunicação",
      "Otimização",
      "Eficiência",
      "Personalization",
      "Communication",
      "Optimization",
      "Efficiency",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79854",
    "title": "Modelling an intelligent interaction system for increasing the level of attentiveness and engagement",
    "abstract": "One of the main factors for achieving school success is related to the level of attention andinterest that students manifest in the classroom. When in the classroom students performtasks on electronic devices, and if the classes have a high number of students, the teacherdoes not have the idea if a student is really attentive and focused on the tasks defined(Pimenta et al. (2015)). Usually the teacher only realizes this situation when he evaluatesthe students, which is in most of cases too late.However, if the teacher receives information about the attention and interest of each user(student) of a class in real time, he/she can adopt a set of teaching strategies in order tomaximize the results of his/her students. Hence it is possible to avoid and prevent somenegative behaviors in the classroom and increase the level of attention and consequently,school success. In terms of school success, it is common sense that a high level of attention,allows to acquire better results.This thesis intends to develop a framework which allows the teacher to visualize, in realtime, the attention level of each student, allowing him to adopt strategies for the studentswith abnormal behaviors (Carneiro et al. (2015); Duraes et al. (2016b)).",
    "authors": [
      "Castro, David Barbosa"
    ],
    "keywords": [
      "Ambient intelligent system",
      "Attention",
      "Decision support system",
      "Teacher",
      "Students",
      "Sistema inteligente ambiente",
      "Atenção",
      "Sistema de suporte de decisão",
      "Professor",
      "Alunos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27868",
    "title": "Simulation of large scale pervasive displays networks",
    "abstract": "As redes de ecrãs públicos de área alargada estão-se a tornar um paradigmaemergente e representam uma transformação radical em relação à maneiracomo encaramos a disseminação da informação em locais públicos.Estas redes com o sua natureza ubíqua levantam alguns desafios paraquem tem que as desenhar, instalar e usar. É bastante importante perceberquais são as principais compromissos a assumir quanto ao desenho das redesde ecrãs, principalmente em relação aos seus componentes e respectivosprotocolos, para desta forma podermos oferecer uma rede aberta, global esobretudo escalável.A partir destas ideias o trabalho de caracterizar os componentes de redeé um dos pontos essenciais para alcançar um desenvolvimento fundamentadodeste sistema. Também é fundamental ter uma avaliação dos desenvolvimentosrespeitantes à desempenho do sistema e à forma como o aumento donumero de intervenientes no mesmo afecta o seu comportamento.Assim este trabalho, complementando essa caracterização e classificaçãoinicial, pretende desenvolver uma ferramenta que permita às demais equipasmultidisciplinares criar cenários e modelos de simulação para confirmar seas suas decisões quanto aos padrões a implementar são os que melhor seadequam aos requisitos destas redes.",
    "authors": [
      "Ferreira, Luís António Araújo"
    ],
    "keywords": [],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23104",
    "title": "Simulation of the nucleation of the precipitate Al3Sc in an Aluminum Scandium alloy using the kinetic Monte Carlo method",
    "abstract": "As estruturas de precipitados desempenham uma função fundamental nas ciências dos materiais devido à capacidade de obstruir o movimento de deslocamentos dentro do material.Esta tese de mestrado debruça-se sobre uma aplicação baseada na mecânica estatística, nomeadamente o método Monte Carlo, no estudo e previsão do fenómeno de precipitação em ligas de alumínio. A liga de alumínio em estudo é a liga alumínio escândio.Esta tese aborda temas como a mecânica computacional, mecânica estatística, ciências dos materiais, difusão, e ainda métodos de mineração de dados (data mining). A tese descreve as condições que influenciam a precipitação e como controlar este fenómeno.O resultado desta tese é um conjunto de aplicações de software que permitem (i) efetuar a simulação de Monte Carlo, (ii) analisar os resultados usando a técnica de mineração DBSCAN e (iii) comparar os resultados da simulação com a teoria clássica da nucleação. Os resultados práticos obtidos com estas aplicações são:- Relatórios da simulação, da análise dos clusters/precipitados com o algoritmo DBSCAN e da aplicação da teoria clássica da nucleação;- Ficheiros para visualização 3D da simulação (em vários pontos ao longo do tempo);- Ficheiros para visualização 3D dos precipitados. Larry Gonick é um cartoonista que desenhou cartoons para a revista Discover. É autor de um vasto conjunto de livros das quais quero mencionar: “The Cartoon Guide to Statistics” da qual esta figura foi retirada e que me parece adequado na forma como ilustra vários temas que esta tese aborda: estatística, aleatoriedade, “salto” e “barreira”.",
    "authors": [
      "Moura, Alfredo de"
    ],
    "keywords": [
      "519.245",
      "669.715"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "519.245",
      "669.715"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92610",
    "title": "Integração de uma aplicação de reporting para testes de software no confluence cloud",
    "abstract": "A crescente evolução tecnológica verificada nas últimas décadas e a necessidade de criação e implementação de novas soluções de software, levam a que o papel dos testes de software assuma, cada vez mais,uma maior relevância no respetivo ambiente de desenvolvimento. Boas ferramentas de gestão e monitorização de testes, assim como, de geração de relatórios de estados e resultados associados aos mesmostestes, melhoram o processo de desenvolvimento tornando-o melhor e mais completo a todos os níveis.O principal objetivo da presente dissertação de mestrado é criar uma nova aplicação de geração de relatórios em páginas Confluence, utilizando como fonte de dados, os testes gerados a partir do Xray. O Xrayé uma aplicação de gestão e monitorização de testes de software em instâncias JIRA. O desenvolvimentoda aplicação implica o estudo das diferentes estratégias de desenvolvimento de software disponíveis noecossistema Atlassian.A aplicação Xray foi analisada em detalhe, com especial foco nos relatórios de testes de softwaredisponibilizados, o que permitiu a formulação de uma proposta de solução para o problema em questão. De seguida, foram analisadas diferentes frameworks de desenvolvimento de software para produtosAtlassian, culminando na escolha da ferramenta Forge face a todas as vantagens e desvantagens a elaassociadas.De igual modo, é apresentado todo o processo de desenvolvimento da aplicação, assim como asestratégias adotadas para a correta implementação de cada um dos relatórios propostos.",
    "authors": [
      "Sá, João Pedro Coimbra Martins de"
    ],
    "keywords": [
      "Confluence",
      "Desenvolvimento de software",
      "Documentação",
      "Forge",
      "Macro",
      "Relatório",
      "Testes de software",
      "Xray",
      "Documentation",
      "Report",
      "Software development",
      "Software testing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92667",
    "title": "Fabrication of PEDOT:PSS/silver nanowire based films for the development of transparent heating systems",
    "abstract": "Transparent and flexible heating systems have become increasingly important for applications suchas in defrosting windows, sensors, or heating displays. PEDOT:PSS and silver nanowire (AgNW) films offerthe possibility of high transparency, flexibility, and conductivity, making them promising substitutes for ITO,the most used material in these systems. In this work, these films were studied with the aim of utilizingscreen-printing as a reproducible method to produce effective transparent and flexible heating systemswith low input voltages. With that purpose, AgNWs were synthesized and the effect of different factorson the films’ resistances was studied: the films’ thickness; PEDOT:PSS modifications with water, PEG,glycerol, methanol, or DMSO; post-treatments with methanol, DMSO, CTAB or sodium borohydride, andthe utilization of sintered AgNWs, an ionic liquid, and PVA.The results showed that depositing a layer of AgNWs on PEDOT:PSS decreased its resistance slightly,while another layer of PEDOT:PSS decreased it considerably. Modifying PEDOT:PSS and post-treating itwith methanol had an insignificant effect on the resistances, while post-treatment with DMSO generallylowered them. Utilizing sintered AgNWs, an ionic liquid, PVA, or post-treating the films in CTAB or sodiumborohydride was ineffective. As such, it was concluded that the resistance depended mainly on the thick ness of the films. When characterized thermally with an input voltage of 12 V, most films showed aninsignificant increase in their temperature. Nevertheless, films with two manually deposited layers of PE DOT:PSS post-treated with methanol with a layer of AgNWs, and films with screen-printed PEDOT:PSS witha big mesh size and a layer of AgNWs were able to increase their temperature values by almost 5 °C.In conclusion, different methods were studied to improve the conductivity of films for transparent andflexible heating systems, which showed that the films’ thickness is a key factor in their resistance and,consequently, in their heating abilities.",
    "authors": [
      "Ribeiro, Sara Coelho"
    ],
    "keywords": [
      "Transparent flexible heating systems",
      "PEDOT:PSS",
      "Silver nanowires",
      "Solvents",
      "Post-treatments",
      "Films deposition",
      "Screen-printing",
      "Resistance",
      "Sistemas de aquecimento transparentes e flexíveis",
      "Nanofios de prata",
      "Solventes",
      "Pós-tratamentos",
      "Deposição de filmes",
      "Serigrafia",
      "Resistência",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92808",
    "title": "Evaluating constrained users ability to interact with virtual reality applications",
    "abstract": "This Master’s Project presents a comprehensive exploration of a novel Virtual Reality (VR)application designed to evaluate and enhance user performance within the context ofconstraints experienced by individuals, including those confined to an Intensive Care Unit(ICU). The work unfolds through a detailed examination of the proposal, development, andassessment phases.The proposal lays the foundation for the project, emphasizing the need for an immersivetechnology-based solution to assess ICU patients’ abilities. It includes a well-structuredsystem architecture, deployment architecture, and data architecture. This framework guidesthe subsequent phases, offering insights into the development and assessment processes.In the development phase, the practical realization of the VR application is explored.It highlights adjustments tailored to the specific needs of ICU patients, offering valuableinsights into user progress, reducing dependency on external assistance. Eight distinct tasksare detailed, categorized based on complexity and fundamental functionalities.The assessment phase evaluates the real-world impact of the VR application through threeinterventions. While limited to non-ICU environments, these interventions capture data fromusers who share critical constraints with ICU patients. The assessment involves correlationanalysis of numerous variables, including age, cognitive function (assessed through theMini-Mental Status Examination), and prior VR experience. The results unearth significantcorrelations, shedding light on age-related differences, the influence of cognitive ability, andthe impact of prior VR exposure on task performance.This comprehensive exploration represents an essential contribution to the burgeoningfield of VR applications for healthcare, specifically targeting constrained user groups likeICU patients. The findings underscore the significance of considering user characteristics,prior experience, and cognitive function when designing VR interventions. Further researchis warranted to refine assessment methodologies and expand the scope of real ICU patienttesting, ultimately paving the way for improved patient care and enhanced rehabilitationpractices.",
    "authors": [
      "Ribeiro, Tiago Ramos"
    ],
    "keywords": [
      "Virtual reality",
      "Intensive care",
      "Cognitive resilience",
      "Non-pharmacological therapies",
      "Realidade virtual",
      "Cuidados intensivos",
      "Resiliência cognitiva",
      "Terapias não-farmacológicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27852",
    "title": "Botnet detection: a numerical and heuristic analysis",
    "abstract": "Internet security has been targeted in innumerous ways throughout the ages and Internet cyber criminality has been changing its ways since the old days where attacks were greatly motivated by recognition and glory. A new era of cyber criminals are on the move. Real armies of robots (bots) swarm the internet perpetrating precise, objective and coordinated attacks on individuals and organizations. Many of these bots are now coordinated by real cybercrime organizations in an almost open-source driven development resulting in the fast proliferation of many bot variants with refined capabilities and increased detection complexity.One example of such open-source development could be found during the year 2011 in the Russian criminal underground. The release of the Zeus botnet framework source-code led to the development of, at least, a new and improved botnet framework: Ice IX.Concerning attack tools, the combination of many well-known techniques has been making botnets an untraceable, effective, dynamic and powerful mean to perpetrate all kinds of malicious activities such as Distributed Denial of Service (DDoS) attacks, espionage, email spam, malware spreading, data theft, click and identity frauds, among others.Economical and reputation damages are difficult to quantify but the scale is widening. It’s up to one’s own imagination to figure out how much was lost in April of 2007 when Estonia suffered a well-known distributed attack on its internet country-wide infrastructure. Among the techniques available to mitigate the botnet threat, detection plays an important role. Despite recent year’s evolution in botnet detection technology, a definitive solution is far from being found. New constantly appearing bot and worm developments in areas such as host infection, deployment, maintenance, control and dissimulation of bots are permanently changing the detection vectors thought and developed.In that way, research and implementation of anomaly-based botnet detection systems are fundamental to pinpoint and track all the continuously changing polymorphic botnets variants, which are impossible to identify by simple signature-based systems.",
    "authors": [
      "Mendonça, Luís Miguel Ferreira Costa"
    ],
    "keywords": [
      "681.324",
      "681.3-7"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.324",
      "681.3-7"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80088",
    "title": "Monitorização de saúde física e psicológica utilizando dispositivos móveis",
    "abstract": "Na última década, tem sido cada vez mais frequente a utilização de testes de personalidade, tais como o OCEAN/BIG FIVE, desenvolvido por Goldenberg em 1992, para avaliar aquilo que os psicólogos consideram ser as cinco principais dimensões da personalidade (Extroversão, Agradabilidade, Abertura, Consciencioso e Estabilidade). Assim sendo, o propósito fundamental desta dissertação é o de conceber e desenvolver uma aplicação móvel focada no tratamento de vários dados biométricos do utilizador assim como na sua personalidade. Para isso, será implementado na aplicação o teste de personalidade, baseado no modelo de Saucier, que permite ao utilizador escolher de entre quarenta adjetivos, os que melhor se adequam a si. Aquando da realização do teste, que se repetirá ao longo do tempo, será tida em consideração a informação biométrica do utilizador e o seu histórico. O objetivo é o de oferecer, ao utilizador, a possibilidade de conhecer os traços que compõe a sua personalidade ao mesmo tempo que lhe é fornecido informação biométrica, sendo possível analisar relações e correlações entre as cinco dimensões fundamentais da personalidade e os dados biométricos recolhidos, assim como a evolução de cada um destes atributos ao longo do tempo.",
    "authors": [
      "Pereira, Adriana"
    ],
    "keywords": [
      "Aplicações de saúde",
      "Big Five Mini-Markers",
      "Dados biométricos",
      "Machine learning",
      "Teste psicológico",
      "Biometric data",
      "Heath apps",
      "Psychological tests",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83932",
    "title": "Traceability and safety tradeoffs in modern vehicles",
    "abstract": "In this dissertation, the efficiency of privacy protecting mechanisms in short-range vehicular communications,namely Pseudonym Change Strategies, is investigated. To evaluate these strategies, a set of simulation tools is used, that allow for the assessment of several metrics, such as the privacy level obtained and the real pseudonym consumption, resulting from the use of a representative set of pseudonym change strategies. Most importantly,hybrid strategies were considered, which combine schemes that were previously analysed separately. The results show that combining mix-zones with another scheme provides better privacy in most cases. Lastly, we showcase and analyse the problems found in the process of trying to make the simulated scenarios more realistic, which easily comes into conflict with tool limitations and/or subtle and hard to anticipate interactions between different components.",
    "authors": [
      "Fernandes, Mariana Pereira"
    ],
    "keywords": [
      "V2X",
      "Privacy",
      "Simulation",
      "Vehicles",
      "Privacidade",
      "Simulação",
      "Veículos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79952",
    "title": "Combining data and text mining techniques for automatic analysis of financial reports",
    "abstract": "The application of Data Science techniques, specifically Natural Language Process ing (NLP) and Machine Learning, in financial markets is of immense interest to in vestors, as these techniques can have a potential economic impact. In particular, stockmarkets represent an opportunity that has been exploited in several ways, such as us ing market opinions (e.g., news, blogs) to predict the direction of price movement or even volatility.This study analyses the 10-K documents of the S&P 100 index for 10 years (2008-2017), which contains the 102 largest companies in the United States of America. The10-K is an annual financial report required by the United States Securities and Ex change Commission (SEC), which describes the financial performance of a company.Recent research suggests that the readability of a company’s 10-K text document mayinfluence its future financial performance, since the way the market perceives textualinformation also depends on the readability of that text. In this sense, this work aimsto understand the relationship between 48 readability metrics applied to these reportsand the corresponding future financial performance of these companies. A clusteringapproach was applied over these readability metrics, aiming to identify distinct andvaluable readability clusters. As an external evaluation, we assessed the informationvalue of the clusters by analyzing 3 future crash risk metrics, that are often used toassess the companies’ financial performance.",
    "authors": [
      "Pinto, Marcelo Queirós"
    ],
    "keywords": [
      "Data science",
      "Financial performance prediction",
      "Natural language processing",
      "Readability evaluation",
      "Stock markets",
      "Avaliação da legibilidade",
      "Ciência de Dados",
      "Mercado de ações",
      "Previsão do desempenho financeiro",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84493",
    "title": "Artificial intelligence-based software for recognizing parkinsonian gait patterns based on wearable miniaturized sensors",
    "abstract": "A Doença de Parkinson (DP) é uma doença degenerativa do sistema nervoso central, geralmente caracterizada por prejudicar vários aspetos da marcha dos pacientes, como bradicinesia, comprimento do passoencurtado e congelamento da marcha. As escalas de avaliação clínica são tipicamente usadas com baseem exames para monitorizar esses sintomas motores associados à marcha. Além disso, estas avaliaçõessão baseadas na memória dos pacientes e pesquisas subjetivas, fornecendo dados tendenciosos. Assim,são necessários dados de longo prazo sobre as atividades motoras diárias do paciente.Avanços tecnológicos forneceram dispositivos sensores pequenos e vestíveis capazes de capturar dados de longo prazo, podendo ser utilizados em ambientes domiciliares permitindo a captura de dadosprecisos. A combinação desses sensores com inteligência artificial (IA) produz modelos capazes de biomarcar os níveis de doença, condições motoras e bem-estar dos pacientes, e de fornecer dados nãotendenciosos sobre os padrões de marcha dos pacientes. A integração destes modelos num aplicativopara médicos facilitará gerir o estado de DP e tratamentos mais personalizados serão alcançados.Tendo isto em conta, esta tese tem como objetivo usar dados de pacientes que apresentam deficiências de marcha para treinar modelos baseados em IA que sejam capazes de classificar níveis de doença,condições motoras e qualidade de vida desses pacientes.Para isso, foram adquiridos dados de 40 pacientes com DP, com o objetivo de desenvolver 3 modelosde IA diferentes, um usado para classificar o nível de doença de um paciente na escala UPDRS-III, outropara classificar as condições motoras escala H&Y e outro usado para classificar a qualidade de vida.Esses modelos foram implementados numa APP para auxiliar os médicos durante as suas consultas.Os resultados obtidos foram positivos. O modelo UPDRS-III conseguiu uma acurácia de 91,67%,uma sensibilidade de 90,43% e uma especificidade de 93,98%, enquanto o modelo H&Y alcançou umaacurácia de 88,98%, uma sensibilidade de 88,71%, e especificidade de 92,79%, sendo que o modeloPDQ-39 obteve acurácia de 84,19%, sensibilidade de 82,13% e especificidade de 90,24%.",
    "authors": [
      "Pinheiro, Pedro Gonçalo Santos Pires"
    ],
    "keywords": [
      "Doença de Parkinson",
      "Marcha",
      "Biomarcar",
      "Padrões",
      "Sensores vestíveis",
      "Inteligência Artificial",
      "Parkinson’s Disease",
      "Gait",
      "Biomark",
      "Patterns",
      "Wearable sensors",
      "Artificial Intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84069",
    "title": "Twitter Observatory: developing tools to recover and classify information for the social network Twitter",
    "abstract": "As redes sociais tornaram-se na nova forma de comunicar e, consequentemente, uma importantefonte de informação. Mais concretamente, o Twitter, desde a sua criação, tornou-se numa das redessociais mais utilizadas. Esta popularidade permitiu um aumento do número de investigações na área deText Mining usando o Twitter para diferentes aplicações, como saúde e política. Nesta área, a classificação de documentos tem sido aplicada a vários dados, nomeadamente tweets, para analisar tendências,entender o comportamento humano e prever determinados eventos. No entanto, nem sempre é possívelter os datasets desejados para efectuar essa classificação e análise.Para resolver o problema encontrado, esta dissertação, proposta pela OmniumAI, pretende exploraras abordagens já existentes para a extração e classificação de dados do Twitter, focando-se principalmente na língua portuguesa. Para isso, foi desenvolvida uma API capaz de extrair tweets de acordocom um determinado tópico de interesse, e criar datasets classificados automaticamente com labels derelevância. Foi ainda desenvolvida uma pipeline de classificação de tweets com base nas abordagensde Deep Learning encontradas no Estado de Arte para a classificação de documentos. O produto finalconsiste numa framework, Twitter Observatory, que permite aos utilizadores criar datasets de acordo comum determinado tópico de interesse e analisar esses mesmos datasets.Para testar a framework desenvolvida, foram selecionados dois casos de estudo: COVID-19 e a Invasão Russa da Ucrânia em 2022. Relativamente a estes dois tópicos, dois datasets foram extraídos eclassificados de acordo com a relevância dos tweets, contendo, respetivamente, 2,268,575 e 219,887tweets em português. Foi feita uma análise exploratória destes dados e os resultados de classificaçãousando modelos de Deep Learning foram apresentados. Para validar esses resultados, foi utilizado odataset existente CrisisLex, traduzido para português.",
    "authors": [
      "Elias, Constança Machado Aires Lobo"
    ],
    "keywords": [
      "Twitter",
      "Classificação de documentos",
      "Deep Learning",
      "Língua portuguesa",
      "Document classification",
      "Portuguese language"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64745",
    "title": "Assistive autonomous smart walker in a hospital environment as a rehabilitation tool",
    "abstract": "Ageing brings a lot of problems in the human body. One really common issue is the lost ofsome mobility be it simply by losing control or by contracting some sort of illness. However,some times these problems can be tackled with some physical therapy and a recovery canbe achieved, even if not full recovery.This master dissertation tries to enable a medical device with the ability of adapting todifferent types of disabilities, developing a system capable of tackling a range of mobilitydisorders through therapy.This work adds an autonomous mode where more severe problems can be treated leavingall the coordination to the walker letting the user focus on training their gate.It also adds a shared control mode where the task of guiding the robot can be offloadedto the user, targeting patients in a more advanced recovery state forcing them to focuson training their coordination and ambient perception besides from the treating their gateproblems.The directed tests showed a system capable of adapting to any situation it was putthrough, with a robust set of emergency mechanism in case anything malfunctions.Based on the analysed results and the answers to the survey, we can indeed say that thesystem would serve as a suitable integration in the ASBGO*, walker enabling the walkerwith more versatility.",
    "authors": [
      "Soares, Rui Jorge Cerqueira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92798",
    "title": "Exploring visualization methods for CNNs in X-ray image classification",
    "abstract": "The chest radiography is one of the most frequently requested hospital exams, playing a pivotal role inthe diagnosis, management, and monitoring of respiratory diseases in patients presenting with respiratorysymptoms. Harnessing the power of deep learning methodologies has revolutionized the field of medicalimaging analysis, enabling the development of robust models for accurate disease prediction and clas sification, encompassing an extensive array of conditions. The inherent strength of deep learning lies inits ability to automatically extract intricate features from complex datasets, thereby facilitating enhancedaccuracy without the need for extensive human intervention.This study delves into the intricate mechanisms of a chest X-ray classification model, highlightinga noteworthy inclination toward seemingly inconsequential regions of the X-rays, particularly fixating onnon-diagnostic elements such as letters and symbols. While the elimination of these artifacts did notsubstantially alter the model’s classification outcomes, it raises pertinent inquiries about the potentialbenefits of a preprocessed dataset devoid of such distractions.This research also scrutinizes the visualization methods employed, unraveling an intriguing tendencyof the model to focus on regions comprised entirely of black pixels in certain cases. This observation insti gated a comprehensive examination of the efficacy of the visualization methods, uncovering a substantialmisalignment between the generated hotspots and the annotations provided by radiologists. Notably, thestudy identifies that the size of the bounding box and the model’s classification success rate share a strongcorrelation, indicating that larger bounding boxes generally demonstrate better alignment. Additionally,we establish a direct relationship between the model’s classification accuracy and the alignment withvisualization techniques.This analysis not only deepens our understanding of the dynamics at play within chest X-ray analysisbut also underscores the essential considerations necessary for refining the training and visualizationprocesses in machine learning-based diagnostic tools.",
    "authors": [
      "Apostolyuk, Valeriy"
    ],
    "keywords": [
      "Chest radiography",
      "Deep learning",
      "Convolutional neural networks",
      "Medical imaging analysis",
      "Visualization methods",
      "Dataset pre-processing",
      "Data augmentation techniques",
      "Diagnostic tool refinement",
      "Radiografia torácica",
      "Aprendizagem profunda",
      "Redes neurais convolucionais",
      "Análise de imagens médicas",
      "Métodos de visualização",
      "Pré-processamento de conjunto de dados",
      "Técnicas de aumento de dados",
      "Refinamento de ferramentas de diagnóstico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28552",
    "title": "Um modelo para previsão de churn na área do retalho",
    "abstract": "O ambiente de grande competitividade característico do sector do retalho e crescente dificuldade na captação de novos clientes leva as empresas a apostar na implementação de estratégias adequadas para promover a satisfação dos clientes adquiridos para motivar a sua lealdade. É neste contexto que se começa a reconhecer a importância de combater o fenómeno de churn, ou seja, a perda de clientes. É necessário identificar os clientes que estão em risco de churn e, para isso, é necessário criar um método que o permita fazer com antecedência para que possam recair sobre eles as campanhas de retenção proactivas. Quanto mais eficaz for o método a identificar os clientes em riscos, maior será o retorno da aplicação da campanha. Muitos trabalhos têm sido desenvolvidos na área de previsão de churn nos mais diversos sectores. Contudo, na área do retalho a pesquisa têm sido muito limitada. Assim, com este trabalho de dissertação pretendeu-se estudar o fenómeno da perda de clientes com o objectivo de definir e implementar um modelo de churning para o sector do retalho recorrendo a técnicas de mineração de dados. Pretendeu-se fazer um levantamento das principais questões envolvidas na previsão de churn no retalho, na construção do conjunto de dados (assinaturas dos clientes) e na aplicação de técnicas de mineração de dados no processo de previsão. Nesse sentido, foram construídos alguns modelos para fazer a previsão de casos de churn baseados em cinco das técnicas de classificação mais utilizadas em trabalhos de previsão de churn: Árvores de Decisão, Regressão Logística, Redes Neuronais, Random Forests e SVM. A avaliação e comparação da performance dos modelos elaborados foi feita de acordo com várias medidas como accuracy, precision, sensitivity, specificity, f-measure e AUC e, para além disso, foi testado o impacto, na precisão do modelo, da alteração dadensidade de eventos de churn no conjunto de treino.",
    "authors": [
      "Veloso, Fernando Jorge Machado"
    ],
    "keywords": [
      "Data mining",
      "Árvores de decisão",
      "Regressão logística",
      "Redes neuronais",
      "Random forests",
      "Support vector machines",
      "Previsão de churn",
      "Sistemas de retalho",
      "Decision trees",
      "Logistic regression",
      "Neural networks",
      "Churn prediction",
      "Retail systems",
      "681.3",
      "658.8"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3",
      "658.8"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27831",
    "title": "Técnicas de ponto de controlo e adaptação em grelhas computacionais",
    "abstract": "A recente popularidade dos ambientes de grelhas introduziu a necessidade de suportar a execução robusta de aplicações numa gama alargada de recursos computacionais. Em contextos de grelhas computacionais, onde a fiabilidade e disponibilidade dos recursos não é garantida, as aplicações deverão ser capazes de suportar dois requisitos fundamentais: 1) tolerância a faltas; 2) adaptação aos recursos disponíveis. As técnicas tradicionais utilizam uma abordagem \"caixa-negra\", onde a camada intermédia de software (mediador) é a única responsável por assegurar estes dois requisitos. Estes tipos de abordagens possibilitam o suporte a estes serviços com uma intervenção mínima do programador, mas limitam a utilização de conhecimento sobre as características da aplicação, visando a otimização destes serviços. Nesta tese são apresentadas abordagens orientadas aos aspetos para suportar tolerância a faltas e adaptação dinâmica aos recursos em grelhas computacionais.Nas abordagens propostas, as aplicações são aprimoradas com capacidades de tolerância a faltas e de adaptação dinâmica através da ativação de módulos adicionais. A abordagem de tolerância a faltas utiliza a estratégia de ponto de controlo e restauro, enquanto a adaptação dinâmica utiliza uma variação da técnica de sobre-decomposição. Ambas são portáveis entre sistemas operativos e restringem a quantidade de alterações necessárias no código base das aplicações. Além disso, as aplicações poderão adaptar de uma execução sequencial para uma configuração multi-cluster. A adaptação pode ser realizada efetuando o ponto de controlo da aplicação e restaurando-a em diferentes máquinas, ou então, realizada em plena execução da aplicação.",
    "authors": [
      "Medeiros, Bruno Silvestre"
    ],
    "keywords": [
      "Ponto de controlo e restauro",
      "Adaptação dinâmica aos recursos",
      "Grelhas computacionais",
      "Programação orientada aos aspectos",
      "Checkpointing and restart",
      "Run-time adaptation",
      "Grid computing",
      "Aspect oriented programming",
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83937",
    "title": "Deteção de anomalias em estações de tratamento de águas residuais",
    "abstract": "A iminente escassez de recursos naturais e o constante aumento populacional tem assolado o presente século. Tal crescente habitacional contribui para uma concentração nos grandes centros urbanos e, consequentemente, um maior nível de poluição quer em contextos habitacionais como industriais. Nesta vertente, as Estações de Tratamento de Águas Residuais, desempenham um papel crucial no controlo do nível de qualidade da água que é reutilizada ou descarregada para o exterior. Estas instalações recebem ininterruptamente cargas de afluentes extremamente poluentes que são provenientes da rede pública de esgotos e que carecem de um tratamento faseado para a purificação das mesmas. Porém, para garantir a qualidade da água que é reaproveitada ou devolvida ao meio ambiente, é necessária monitorização contínua destas estações de forma a permitir o processo de tomada de decisão. Posto isto, esta dissertação visa implementar modelos de Machine Learning com o intuito de detetar possíveis anomalias nas substâncias presentes no efluente destas infraestruturas. Assim sendo, são aplicados modelos como Isolation Forest (IF), One Class Support Vector Machine (OCSVM) e Long Short-Term Memory Autoencoder (LSTM-AE) para identificar os registos do Azoto Total, Nitratos e pH que possam ser anómalos. No caso em específico das LSTM-AE, são considerados três thresholds para classificar os registos, dos quais, dois utilizam valores estáticos e um consiste em valores dinâmicos. De entre os melhores modelos candidatos, no global, os modelos de IF e OCSVM alcançaram resultados superiores aos modelos baseados em LSTM-AE. No que diz respeito aos thresholds, as abordagens com valores estáticas de forma geral, atingiram resultados ligeiramente superiores. Em suma, os vários cenários aplicados permitiram concluir que os modelos concebidos conseguiram detetar as várias anomalias presentes nas substâncias referidas.",
    "authors": [
      "Silva, Diogo Filipe Gigante da"
    ],
    "keywords": [
      "Controlo analítico",
      "Deteção de anomalias",
      "Estações de tratamento de águas residuais",
      "Machine Learning",
      "Analytical control",
      "Anomaly detection",
      "Wastewater treatment plants",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27881",
    "title": "Interoperabilidade e portabilidade em ambientes PaaS",
    "abstract": "O Cloud Computing tem emergido como sendo um novo paradigma para entrega de serviçoes através da Internet. Neste mercado em expansão, o serviço de PaaS (Platform-as-a-Service) tem sido objeto de grande interesse por parte das mais variadas organizações permitindo o fácil deployment de aplicações sem necessidade de uma infraestrutura dedicada, instalação de dependências ou configuração de servidores. No entanto, cada fornecedor de solução PaaS acaba por gerar um lock-in do utilizador às suas características proprietárias, tecnologias ou APIs (Application Programming Interfaces). Além disso, dando como garantida a conectividade até aos clientes, a rede de operadores como seja o caso da Portugal Telecom (PT) acaba por servir apenas de dumb-pipe entre o fornecedor e os seus clientes.Este projeto foca-se na especificação, desenvolvimento e avaliação de uma camada de abstração que visa unificar os processos de gestão e aquisição de informação sobre aplicações e bases de dados criadas atraves de diversos PaaS, de modo a combater o lock-in existente no mercado. Neste sentido, um utilizador de PaaS pode selecionar a plataforma mais adequada para uma aplicação interagindo de forma idêntica com qualquer fornecedor suportado,tendo também a possibilidade de migrar aplicações entre fornecedores distintos. Assim sendo, um operador como a PT tem agora a possibilidade de agir como um mediador entre os utilizadores e os fornecedores de PaaS.",
    "authors": [
      "Cunha, David"
    ],
    "keywords": [
      "681.324"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80967",
    "title": "Análise e optimização de protocolos de Acordo Distribuído Aproximado",
    "abstract": "Esta dissertação aborda o acordo distribuído aproximado, no qual é pretendido que um grupo de processos decida um valor dentro de um intervalo com uma amplitude limitada. Na primeira fase procede-se ao levantamento dos algoritmos existentes na literatura,onde se incluem algoritmos que propõem a resolução no modelo assíncrono, no qual sãoconsideradas faltas bizantinas. Através de uma análise comparativa, são evidenciadas as principais diferenças entre os algoritmos. Posteriormente, após a seleção criteriosa de um dos algoritmos, e feita a sua análise detalhada sobre os fatores de escalabilidade em resultado da sua implementação. Com base nos resultados, são propostas alterações que promovem aperformance do algoritmo face ao aumento do número de processos no sistema.",
    "authors": [
      "Oliveira, Joaquim Manuel Gonçalves"
    ],
    "keywords": [
      "Acordo Distribuído Aproximado",
      "Escalabilidade",
      "Sistemas distribuídos",
      "Approximate distribution agreement",
      "Distributed system",
      "Scalability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92593",
    "title": "Development of a bot like entity to emulate an user in a tridimensional virtual environment",
    "abstract": "This dissertation explores a novel approach for the development of a virtual entity or artificial intelligencecapable of simulating user behavior within the immersive and expansive virtual realm of the World ofWarcraft video game. Classified as a three-dimensional massively multiplayer online role-playing game,World of Warcraft serves as an exemplary context for studying and refining techniques that can be readilyadapted to other applications. The research methodology employed in this study involves a systematicanalysis of the application’s process memory space, with a focus on identifying crucial memory datalocations. Furthermore, the investigation entails the identification and preservation of pathways leading tothe aforementioned memory data points, ensuring their efficient and viable accessibility. To enable thecreation of the virtual entity, the Neuroevolution of Augmenting Topologies technique is employed, whichfacilitates the generation and intricate development of an artificial neural network—the entity’s simulatedbrain. By utilizing the previously acquired memory data points as sensory inputs, and emulating the entity’sresponses as inputs within the running process, a comprehensive framework for emulating user behavioris established. The findings presented in this dissertation contribute to the advancement of knowledgein the field of virtual entity creation and artificial intelligence, offering practical implications for a range ofapplications beyond World of Warcraft.",
    "authors": [
      "Neto, Luís Paulo Ferreira Gomes"
    ],
    "keywords": [
      "Machine Learning",
      "Artificial Intelligence",
      "Neural network",
      "Neuroevolution",
      "Neuroevolution of augmenting topologies",
      "Genetic algorithm",
      "Reverse engineering",
      "Input emulation",
      "Process memory scanning",
      "Process memory reading",
      "Bot",
      "3D",
      "MMORPG",
      "PVE",
      "Video game",
      "Player emulation",
      "Exploit",
      "Scripting",
      "Automation",
      "Hooking",
      "DLL Injection",
      "Aprendizagem de máquina",
      "Inteligência Artificial",
      "Rede neuronal",
      "Neuroevolução",
      "NEAT",
      "Algoritmo genético",
      "Engenharia reversa",
      "Emulação de input",
      "Pesquisa de memoria",
      "Leitura de memória",
      "Video jogo",
      "Emulação de jogador",
      "Exploit",
      "Automação",
      "Hooking",
      "Injecção DLL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92661",
    "title": "Automation of machine learning pipelines for anomaly detection challenges",
    "abstract": "Machine Learning (ML) and Data Science can solve different real-world problems. Businesses arebecoming increasingly interested in these approaches, and as technology evolves, new challenges canbe identified, mostly regarding the ML models development, deployment cycle and data cleansing, whichcan significantly decrease the accuracy and viability of ML software systems. Development and Operations(DevOps) practices have become popular in operating software systems at scale successfully, but theyneed to be adapted to deliver the best results when applied to ML systems. This led to the emergenceof Machine Learning and Operations (MLOps), a development culture specific for ML systems, derivedfrom DevOps principles. What MLOps attempts to address is the unification of the development cycle ofML based software systems while striving for automation and monitoring, in order to allow continuousintegration and delivery. With this thesis, the goal is to study different available frameworks and methodsfor ML systems, in order to develop an automated ML pipeline to ingest and manipulate high volumesof data. A sensorial system, which simulates the interior of a vehicle, gathers enough data to feed thepipeline. Alongside the development of the ML system, a visual interface which allows control over theoverall system and its data is created.",
    "authors": [
      "Martins, Ricardo Rodrigues"
    ],
    "keywords": [
      "Software engineering",
      "Machine learning",
      "MLOps",
      "Model",
      "Automation",
      "Engenharia de software",
      "Aprendizagem automática",
      "Modelo",
      "Automação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79710",
    "title": "GPM: generic predictive machine an automated machine learning framework for the prediction of a business’ financial evolution and other generic supervised learning problems",
    "abstract": "One of the biggest problems for business owners, of both big and small companies, is thecorrect forecast of its financial evolution. This means that the owners in question need tospend a lot of time studying the economic issues that involve these situations and/or payspecialized workers to do a thorough study on the area. In addition, it becomes a constant,chronic and seasonal problem that demands to be solved. If there was a prediction systemcapable of forecasting and advising the owner of the company with enough precision to bereliable, one could save time, money and, above all, test or simulate the future with differentparameters. This project then aims to complete a framework capable of encapsulating all thedata-science work and knowledge needed to create good enough machine learning modelsthat provide these predictions even to those without data-science knowledge and to anygeneric business. This framework goes by the name Generic Predictive Machine (GPM). Itsperformance, both in terms of accuracy of results and execution time in various differentsituations, as well as the process leading to its development and the development itself arethoroughly documented. The problem of having to create a generic system arises as everybusiness’ data is inherently different and assumptions cannot be made on the developer’send. Therefore the final application allows for the solving of a vast array of problemseven if not related to economics and requires little knowledge about Machine Learning.The problem as a whole serves too as a vehicle for a deeper study on machine learningitself. Touching upon important aspects like, how far can such a system learn beyond thecapabilities of the human who programmed it, how faster can it do it and what can it teachus about the inherent patterns in data that sometimes remain unnoticed to the human eye.In the process of answering these questions many works on the area are referenced thattouch specific points of the theory behind machine learning, a final novel conclusion is thenderived from the knowledge found among all related works as to take a step forward in thetheory of machine learning as a whole.",
    "authors": [
      "Malhadas, Daniel"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47822",
    "title": "Monitorização da performance do aluno no processo de aprendizagem",
    "abstract": "Nos dias de hoje verifica-se cada vez mais que vivemos num mundo onde a tecnologia tem grande relevância em todas as áreas. Com este constante avanço, pode salvar-se milhares de vidas todos os dias devido à sua união com a medicina, melhora-se a capacidade de prever o tempo na área da meteorologia, apoia-se a química com o desenvolvimento de software para o estudo molecular, entre outros. A tecnologia está em todo o lado, como se pode notar, e requer uma constante adaptação por parte do Ser Humano para melhorar o seu desenvolvimento. Uma área que já utiliza este avanço é a educação, e é onde este projeto se irá focar. Com a ajuda de um sistema de e-learning (Moodle) pretende-se criar um sistema inteligente para reduzir a distância entre os professores e os alunos durante a sua vida académica. Pretende-se, também, dar a possibilidade aos professores de perceber mais facilmente e mais objetivamente as dificuldades dos seus alunos através do desempenho que os mesmos apresentam durante os momentos de avaliação.",
    "authors": [
      "Oliveira, Ruben Eliseu da Cunha"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84104",
    "title": "Chatbot development to assist patients in health care services",
    "abstract": "Dados de alta qualidade sobre tratamentos médicos e de informação técnica tornaram-seacessíveis, criando novas oportunidades de E-Saúde para a recuperação de um paciente.A implementação da aprendizagem automática nestas soluções provou ser essencial eeficaz na elaboração de aplicações para o utilizador para aliviar a sobrecarga do sectorde saúde. Atualmente, muitas interações com os utentes são realizadas via telefonemase mensagens de texto. Os agentes de conversação podem responder a estas questões,fomentando uma rápida interação com os pacientes.O objetivo fundamental desta dissertação é prestar apoio aos pacientes, fornecendouma fonte de informação fidedigna que lhes permita instruir-se e esclarecer dúvidassobre os procedimentos e repercussões dos seus problemas de saúde. Este propósito foiconcretizado não apenas através de uma plataforma Web intuitiva e acessível, compostapor perguntas frequentes, mas também integrando um agente de conversação inteligentepara responder a questões.Para este fim, cientificamente, foi necessário conduzir a investigação, implementaçãoe viabilidade dos agentes de conversação no domínio fechado para os cuidados desaúde. Constitui um importante contributo para a comunidade de desenvolvimento dechatbots, na qual se reúnem as últimas inovações e descobertas, bem os desafios actuaisda aprendizagem automática, contribuindo para a consciencialização desta área.",
    "authors": [
      "Barbosa, António Pedro Mesquita"
    ],
    "keywords": [
      "Cuidados de saúde",
      "Processamento de linguagem natural",
      "Aprendizagem automática",
      "Chatbot",
      "Perguntas e respostas",
      "Healthcare",
      "Natural language processing",
      "Machine learning",
      "Question answering",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79994",
    "title": "Mecanismos de profiling para sistemas de avaliação de conhecimento",
    "abstract": "Nos dias que correm, é cada vez mais frequente o recurso à tecnologia para a resolução de problemas nas mais diversas atividades, nos mais variados setores. O setor da educação não é, pois, uma exceção. Nesse setor, um dos aspetos que tem vindo a obter alguma relevância aborda a temática dos ITS — Intelligent Tutoring Systems, apesar da timidez das diversas aproximações realizadas, em especial ao nível do ensino de nível académico. Atualmente, está em curso na Universidade do Minho o desenvolvimento de um ITS, denominado Leonardo, cujo objetivo principal é o auxílio dos alunos no seu processo de aprendizagem e formação. Este sistema pretende fornecer um acompanhamento personalizado ao utilizador, tanto em termos do seu processo de formação, como, posteriormente, em termos do processo de avaliação correspondente. Nesta dissertação apresenta-se o desenvolvimento de um sistema de profiling para apoio ao processo de avaliação, que possibilita a construção do perfil do utilizador (do estudante) à medida que este interage com o sistema ao longo do tempo. Este sistema de profiling representa um dos módulos do sistema Leonardo e, através das devidas interações com os restantes módulos, serve de base de apoio a todas as decisões que são tomadas no sistema, uma vez que estabelece o perfil do utilizador pelo qual o sistema se deve reger ao adaptar os conteúdos a disponibilizar.",
    "authors": [
      "Fernandes, Luís Miguel Moreira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80721",
    "title": "Development of a tool for partial automated annotation of metabolic reactions",
    "abstract": "Metabolic network reconstructions provide the mathematical in-silico frame work for the study of metabolism through the simulation of generic or specificmetabolic pathways.Polyphenols are niche dietary compounds with a growing field of interestin the scientific community. Modelling with these compounds is a step for ward in the completion of metabolic reconstructions.As polyphenols are metabolized by both the gut microbiome and the humanhost, it is essential to understand the organism-specific metabolic mechanismbehind their degradation. Since information is spread out through severalinformation sources and their metabolism is highly complex, it is extremelychallenging and time-consuming to manually annotate their metabolism.The focus of the work here developed was thus the creation of a tool thatcould speed up the data collection process for posterior manual curation, witha focus on the addition of polyphenol metabolism into the largest and mostcomprehensive reconstruction of human metabolism, ”Recon”. This resultedin the creation of the Database Reaction Automatic eXtraction (DRAX)tool, a biological database web scraper. DRAX was initially targeted atpolyphenols but it also allows the collection of reactions for other metabolitesand drugs.DRAX allows the comprehensive extraction of metabolite reactions throughmetabolic pathway-based iterative web scraping. It will provide researcherswith a starting point for metabolism reconstruction, allowing a more efficientaddition of novel metabolic pathways.",
    "authors": [
      "Queirós, Pedro Miguel Teixeira"
    ],
    "keywords": [
      "Ciências Naturais::Ciências Biológicas"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências Biológicas"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86514",
    "title": "Avaliação automática de testes de atenção e acuidade visual",
    "abstract": "In recent years, our research group on Language Processing, GEPL, has been collaboratingwith Centro Neurosensorial de Braga, led by Dr. Ana Paula Azevedo.In this context, some serious games were developed and installed for recognizing shapes,emotions and training central and peripheral vision. They are used in memory therapy,deconcentration, dyslexia, and other problems that affect the acquisition of knowledge inlearning processes.The ideas that rose up along the literature review done on those areas,will be exposed along the state-of-the-art chapter in this report.This thesis proposes a system that will implement an error detection algorithm based onspeech-to-text analysis to check whether the spoken sequence contains errors or not. As thesystem is intended to be installed in the Neurosensory Center, the results will be presentedvisually to help the therapist in their day-to-day work and monitor the actual use of thesystem.",
    "authors": [
      "Pereira, Mariana de Oliveira"
    ],
    "keywords": [
      "Serious games",
      "Speech recognition",
      "Jogos sérios",
      "Reconhecimento de fala",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83225",
    "title": "Deep Learning for activity recognition in real-time video streams",
    "abstract": "In an ever more connected world, smart cities are becoming ever more present in our society. In these smartcities, use cases in which innovations that will benefit its inhabitants are also growing, improving their quality of life.One of these areas is safety, in which Machine Learning (ML) models reveal potential in real-time video-streamanalysis in order to determine if violence exists in them.These ML approaches concern the field of Computer Vision, a field responsible for traducing digital imagesand videos, and be able to extract knowledge and understandable information from them, in order to be usedin diverse contexts. Some of the available alternatives to recognise actions in video streams are based on MLapproaches, such as Deep Learning (DL), that grew in popularity in the last years, as it was realised that it hadmassive potential in several applications that could benefit from having a machine recognising diverse humanactions.In this project, the creation of a ML model that can determine if violence exists in a video-stream is proposed.This model will leverage technology being used in State of the Art methods, such as video classifiers, but alsoaudio classifiers, and Early/Late Fusion (EF / LF) schemes that allow the merging different modalities, in the caseof the present work: audio and video. Conclusions will also be drawn as to the accuracy rates of the differenttypes of classifiers, to determine if any other type of classifiers should have more prominence in the State of theArt.This document begins with an introduction to the work being conducted, in which both the its context, mo tivation and objectives are explained. Afterwards, the methodology used in order to more efficiently conductthe research in this Thesis is clarified. Following that, the State of the Art concerning ML based approaches toAction Recognition and Violence Detection is explored. After being brought to date in what are the State of theArt approaches, one is able to move forward to the following chapter, in which the Training method that will beemployed to train the models that were seen as the best candidates to detect violence is detailed. Subsequently,the selected models are scrutinized in an effort to better understand their architecture, and why they are suitedto detect violence. Afterwards, the results achieved by these models are explored, in order to better comprehendhow well these performed. Lastly, the conclusions that were reached after conducting this research are stated,and possibilities for expanding this work further are also presented.The obtained results prove the success and prevalence of video classifiers, and also show the efficacy ofmodels that make use of some kind of fusion.",
    "authors": [
      "Reinolds, Francisco André Vieira"
    ],
    "keywords": [
      "Machine Learning",
      "Deep Learning",
      "Action recognition",
      "Violence detection",
      "Early fusion",
      "Late fusion",
      "Aprendizagem Máquina",
      "Aprendizagem Profunda",
      "Reconhecimento de ações",
      "Deteção de violência",
      "Fusão antecipada",
      "Fusão tardia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94076",
    "title": "Detection and classification of anonymous traffic using machine learning",
    "abstract": "Traffic classification is a fundamental task for various aspects of network management and monitoring.Knowing traffic characteristics allows a better design of the network and services, contributing for animproved performance and quality of service. Regarding anonymized traffic, this becomes an intricateprocedure due to packet encryption.The presence of anonymous traffic in networks is noticeable, demonstrating user’s concern aboutprivacy and anonymity at different levels. The growing use of anonymity tools creates the need of a secureand efficient experience. Therefore, traffic classification is also relevant to developers of these tools (toimprove user service robustness) and Internet Service Providers (to understand what type of traffic iscirculating in the network).This dissertation’s primary goal is to study the application of machine learning algorithms in multilevelanonymous traffic classification. This work proposes an adaptation of an intrusion detection system asproof of concept, using datasets containing mixed traffic (benign and anonymized). Furthermore, it ispresented a comparison to empirical studies carried out previously.",
    "authors": [
      "Regueiras, Maria Miguel Albuquerque"
    ],
    "keywords": [
      "Anonymized traffic",
      "Traffic classification",
      "IDS",
      "Machine learning",
      "Tor",
      "Classificação de tráfego",
      "Tráfego anonimizado",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91570",
    "title": "Multi-people tracking using a distributed camera network: application to a university campus",
    "abstract": "Nesta dissertação é explorado o tema da monitorização de múltiplos objetos no contexto de um ”smartcampus”, com foco no contexto específico num campus universitário, sendo este o tema principal do projeto deinvestigação Lab4USpace. A monitorização de múltiplos objetos, especialmente de pessoas, é relevante paradiversas aplicações, incluindo aplicações de vigilância, mobilidade e inteligência ambiental. No entanto, torna-separticularmente desafiante no contexto de espaços abertos, às quais exigem soluções com múltiplas câmarascom problemas inerentes, tais como a reidentificação.O objetivo desta dissertação é desenvolver um framework capaz de fornecer informações sobre o percurso devárias pessoas ao longo do campus universitário usando um cenário com múltiplas câmaras. A solução visa não sóa monitorização de uma pessoa num único cenário, mas também em todo o campus, coberto por diversas câmarascom ou sem sobreposição.Esta dissertação discute os diversos desafios enfrentados durante o desenvolvimento deste projeto, incluindopreocupações com a privacidade e segurança dos utilizadores do campus. Com isso, optou-se por não enviarimagens para nenhuma aplicação, tratando apenas das informações estritamente retiradas da monitorização daspessoas. Um dos principais desafios foi desenvolver um framework que rastreie vários objetos num ambiente de um”smart campus”, abordando desafios de espaços abertos e problemas de reidentificação. Além disso, devido aosrecursos computacionais limitados, foi usado um computador de bordo para lidar com processamento de imagense operações relacionadas às técnicas de visão computacional de maneira mais eficaz.O framework proposto utiliza modelos de deteção de objetos e algoritmos de monitorização em tempo real queforam comparados neste contexto específico. Depois de pesquisar outras alternativas, a estrutura usa o modeloYOLOv7-tiny para deteção de objetos, BoT-Sort para a monitorização dos vários objetos e Deep Person Reid paraa reidentificação. O programa foi desenvolvido em Python e juntamente a ele foi também criado um website paraalterar as configurações do sistema de monitorização utilizando o framework Flask. Um message broker tambémfoi utilizado para a comunicação entre os diversos componentes do sistema.Os testes de validação demonstram a eficácia da framework proposta na monitorização das várias pessoas emtodo o campus. O sistema proposto contribui significativamente para o desenvolvimento de soluções de múltiplascâmaras mais eficientes e eficazes para aplicações de ”smart campus”, com benefícios potenciais para a segurança,proteção e gestão do campus.No geral, esta dissertação apresenta uma estrutura que rastreia de maneira eficaz várias pessoas num ambientede ”smart campus”. A framework é uma contribuição importante para o desenvolvimento na área do ”smartcampus” e tem potencial para desenvolvimento futuro e aplicações para além do campus universitário.",
    "authors": [
      "Matos, Henrique Miguel Cardoso"
    ],
    "keywords": [
      "Campus inteligente",
      "Detecção de objetos",
      "Monitorização de múltiplos objetos",
      "Reidentificação",
      "Monitorização de pessoas",
      "Smart campus",
      "Object detection",
      "Multiple object tracking",
      "Re-identification",
      "People tracking",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47791",
    "title": "Desenvolvimento de um sistema de software para gerir ocorrências em municípios",
    "abstract": "Reportar ocorrências relativas a espaços ou equipamentos públicos é uma tarefa essencial, pois a identificação e a resolução dos problemas devem ser breves, de modo a evitar novos como consequência da demora deste processo.Atualmente, os cidadãos quando são confrontados com alguma ocorrência têm dificuldade em saber como e onde participar a mesma para que esta seja resolvida.Pretende-se, com o desenvolvimento deste projeto, criar um meio de comunicação facilitador entre os cidadãos e as autarquias, que vise não só auxiliar os cidadãos a reportarem as mais variadas ocorrências relativas aos espaços ou equipamentos públicos, mas também permitir aos municípios gerir e monitorizar ocorrências reportadas nas diversas áreas de intervenção dos serviços municipais.O resultado do desenvolvimento deste projeto enquadra-se na criação de uma aplicação móvel e de uma aplicação web.",
    "authors": [
      "Costa, Ricardo Filipe da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59724",
    "title": "Sensing, coordination and actuation in office spaces",
    "abstract": "Productivity in an office space is directly affected by atmospheric conditions. With thecapabilities of Internet of Things appliances, it’s possible to automate the surrounding environmentand maintain optimal work conditions while, at the same time, integrating withthe team’s workflow. The direct control of the environment then becomes part of any officemanagement application used by the teams. This dissertation addresses the creation of aprototype capable of doing so. Starting with a Single Board Computer packed with atmosphericsensors, it describes the building blocks for office automation, creating a new architectureand communication protocol. Instead of implementing the code to interact withphysical appliances and their own intrinsic behaviour and interfaces, this protocol is easilyextensible, allowing consumers to add custom nodes responsible for bridging that gap.These custom nodes can themselves produce readings and automate physical appliancesregardless of the nodes that are already taking part in the protocol. Having a protocol withthose properties, the prototype to be developed can produce relevant information about thesurrounding environment while leaving complex computations to a third party, which canbe technology enthusiasts or even appliance vendors.",
    "authors": [
      "Mendes, Fernando José Ribeiro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82123",
    "title": "TrustZone based attestation in secure runtime verification for embedded systems",
    "abstract": "ARM TrustZone é um “Ambiente de Execução Confiável” disponibilizado em processadores da ARM, queequipam grande parte dos sistemas embebidos. Este mecanismo permite assegurar que componentescríticos de uma aplicação executem num ambiente que garante a confidencialidade dos dados e integridadedo código, mesmo que componentes maliciosos estejam instalados no mesmo dispositivo. Neste projectopretende-se tirar partido do TrustZone no contexto de uma framework segura de monitorização em temporeal de sistemas embebidos. Especificamente, pretende-se recorrer a components como o ARM TrustedFirmware, responsável pelo processo de secure boot em sistemas ARM, para desenvolver um mecanismode atestação que providencie garantias de computação segura a entidades remotas.",
    "authors": [
      "Quaresma, Miguel Miranda"
    ],
    "keywords": [
      "Atestação",
      "Sistemas embedidos",
      "TEE",
      "TrustZone",
      "Attestation",
      "Embedded systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27827",
    "title": "Uma experiência educativa com robótica inteligente",
    "abstract": "Este trabalho, ou este estudo, ou esta experiência tenta demonstrar que existem ferramentas diferentes das já utilizadas para ajudar os alunos a compreender o funcionamento exato da disciplina de Matemática. Assim sendo, todo este projeto concentra-se na obtenção de formas diferentes de ensinar matemática aos alunos. A robótica, como instrumento de ensino e demonstração, vai permitir aos alunos terem um contacto prático com as matérias que lhes são ensinadas.Este projeto decorre numa escola básica com alunos do 5o ano. Todos estes alunos são voluntários e eram alunos com notas medianas a esta disciplina no ano anterior.Os robôs utilizados, são robôs da Lego Mindstorms NXT, com o respectivo software de programação. Os alunos tinham de os programar consoante a matéria que iam dando nas aulas. Nenhum aluno tinha tido contacto com estes robôs e muitos deles nem sequer nunca tinham mexido em computadores. Esta articulação programa-aluno-professor foi feita com a Diretora de Turma e a professora de Matemática, que iam revelando a planificação das aulas para que os alunos pudessem experimentar na prática o que tinham aprendido.",
    "authors": [
      "Borges, André Paulo Renato Pereira"
    ],
    "keywords": [
      "681.5",
      "51:372.851",
      "372.851:51"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.5",
      "51:372.851",
      "372.851:51"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55840",
    "title": "Distinguishing kinships beyond identity and paternity",
    "abstract": "In kinship testing powerful statistical results are usually obtained when genetic informationis expected to be shared between a pair of samples, which happens in paternity andidentification testing. However, there are other pedigrees where genetic information sharingis not required, such as when a pair of full-siblings or avuncular, is analyzed. Studyingthese pedigrees, where the sharing of genetic information is not mandatory, will be thefocus of this work. We will consider several kinship problems where two (exhaustive andmutually exclusive) hypotheses will be compared, through a statistical evaluation based onthe calculation of a likelihood ratio (LR) where the probabilities of genotypic configurations,assuming one or another kinship hypothesis, are compared. This analysis will allow theidentification of the proportion of cases where the statistical evaluation had weak results,and those where LR favored the false hypothesis of kinship for a widely used commercialkit of genetic markers, considering simulated profiles assuming the pedigrees in question.In addition, we will compare the statistical gain of increasing the battery of analyzed markersand infer the impact of considering the genetic information given by the knowledge ofthe genetic profile of a relative, as the undoubted mother in the case where the hypotheses”individuals A and B are related as full-siblings“ and ”the individuals A and B areunrelated“ are asked to be compared. Furthermore, a validation of the Familias softwarefor two individuals will be performed for the simplest assumptions - absence of mutationand absence of silent allele - through the implementation of the algebraic formulas alreadyestablished.",
    "authors": [
      "Simões, Raquel"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94075",
    "title": "Methodology for validation and performance analysis of compilation toolchains",
    "abstract": "A Toolchain is a fundamental element of software development and provides developerswith the resources necessary for the development, testing, and maintenance of applications.Unreliable test environments can cause major problems for the successful completion oftests. resulting in a decrease in performance and quality of the application. The disparity be tween tests executed locally and remotely by different development teams can exacerbatethis issue, leading to inconsistency in results. In order to guarantee reliable and comparableresults it is essential that all developers use the same environment and configuration whenconducting tests. This consistency is particularly important to obtain an accurate depictionof the application’s capabilities, revealing any potential issues that may not be prevalent inone environment but exist in another.This research aims to create a streamlined and efficient testing methodology for Toolchaindevelopers, developed in collaboration with Synopsys, a leader in the semiconductor indus try, provides a range of ARC RISC processors supported by various commercial and open source operating systems and middleware. This work proposes a tool designed to simplifythe testing process for different tools in the Toolchain, allowing developers to easily execute,compare, profile, explore, and report tests with confidence. The tool will enable direct localcomparisons, reducing the time and effort required for manual testing and reporting. By us ing this tool, developers can save valuable time while ensuring their tests are accurate andreliable. An improved testing methodology will be beneficial to all Toolchain developers, as itwill allow them to focus less on manual testing processes and more on developing innovativesolutions with confidence. Tests are an essential component of any Toolchain developmentprocess, so having an efficient testing methodology in place is key to ensuring the higheststandards of quality code. With the proposed tool, developers can rest assured that theirtests are accurate, reliable and replicable.",
    "authors": [
      "Silva, Luís Manuel Gonçalves da"
    ],
    "keywords": [
      "Continuous integration",
      "Compare past executed iterations",
      "Web API",
      "Integração contínua",
      "Comparar iterações anteriores executadas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86548",
    "title": "Structural cognitive training with immersive virtual reality",
    "abstract": "In this thesis, a systematic review was conducted on the study of the use of VRSs.VR is an immersive technology capable of simulating real life events through image,sound and headed mounted devices or technologies such as windows kinnect. Thesetechnologies can be used to evaluate the performance and evolution of IADLs in olderadults. An electronic data search was conducted, during January 2022. The finalanalysis includes 12 studies with 285 participants in total. The use of VRSs is aninnovative and feasible technique to support and improve the functional autonomy ofolder adults living in the community compared to conventional treatment. Between20% to 25% of community-dwelling people over 75 years old have limitations in theability to perform ADLs. The ability to perform ADLs is extremely important as itenables individuals to have a good quality of life by creating a sense of competence,self-esteem, confidence, identify and realisation. In this thesis we present the conceptof structural cognitive training, in which cognitive training tasks (executive functionsand cognitive abilities) are combined with training of instrumental activities of dailyliving. The methodology adequacy is assessed by the design of a digital game to trainolder adults to conduct IADLs.",
    "authors": [
      "Pereira, Nuno Afonso Araújo"
    ],
    "keywords": [
      "Virtual reality",
      "Functional autonomy",
      "Rehabilitation",
      "Exergame",
      "Head-mounted devices",
      "Activities of daily living",
      "Realidade virtual",
      "Autonomia funcional",
      "Reabilitação",
      "Dispositivos montados na cabeça",
      "Actividades da vida quotidiana",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84569",
    "title": "Q-Learning applied to games: a reward focused study",
    "abstract": "Q-Learning is one of the most popular reinforcement learning algorithms. It can solve different complex problemswith interesting tasks where decisions have to be made, all the while using the same algorithm with no interfer ence from the developer about specific strategies. This is achieved by processing a reward received after eachdecision is made.In order to evaluate the performance of Q-Learning on different problems, video games prove to be a greatasset for testing purposes, as each game has its own unique mechanics and some kind of objective that needsto be learned. Furthermore, the results from testing different algorithms on the same conditions can be easilycompared.This thesis presents a study on Q-Learning, from its origins and how it operates, showcasing various state ofthe art techniques used to improve the algorithm and detailing the procedures that have become standard whentraining Q-Learning agents to play video games for the Atari 2600.Our implementation of the algorithm following the same techniques and procedures is ran on different videogames. The training performance is compared to the one obtained in articles that trained on the same gamesand attained state of the art performance.Additionally, we explored crafting new reward schemes modifying game default rewards. Various customrewards were created and combined to evaluate how they affect performance.During these tests, we found that the use of rewards that inform about both good and bad behaviour led tobetter performance, as opposed to rewards that only inform about good behaviour, which is done by default insome games.It was also found that the use of more game specific rewards could attain better results, but these also requireda more careful analysis of each game, not being easily transferable into other games.As a more general approach, we tested reward changes that could incentivize exploration for games that wereharder to navigate, and thus harder to learn from. We found that not only did these changes improve exploration,but they also improved the performance obtained after some parameter tuning.These algorithms are designed to teach the agent to accumulate rewards. But how does this relate to gamescore? To assess this question, we present some preliminary experiments showing the relationship between theevolution of reward accumulation and game score.",
    "authors": [
      "Ferreira, Pedro Henrique de Passos"
    ],
    "keywords": [
      "Q-Learning",
      "Policy",
      "Markov processes",
      "Neural networks",
      "Política",
      "Cadeias de Markov",
      "Redes neuronais",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46713",
    "title": "Desenvolvimento e exploração de uma nova geração de ferramentas de Business Intelligence para o apoio à decisão e a prática clínica em unidades hospitalares",
    "abstract": "Ao longo dos últimos anos, tem aumentado exponencialmente a utilização de Tecnologiasde Informação (TIs) e de ferramentas computacionais em vários setores económicos, incluindoo setor da saúde, por serem defendidas como tecnologias que podem transformare melhorar radicalmente a prestação de cuidados de saúde.O principal objetivo das instituições de saúde é prestar os melhores cuidados de saúdeaos seus utentes, garantindo, assim, a prestação de serviços de qualidade e a consequentesatisfação dos utentes, bem como reduzir os custos e desperdícios desnecessários associados.Portanto, as decisões devem ser tomadas rapidamente mas, igualmente, eficazmente.Acredita-se, assim, que o futuro realmente bem sucedido das TIs no setor da saúdepassa então pelo desenho e pela implementação de sistemas user-friendly, incluindo Sistemasde Apoio à Decisão Clínica (SADCs), personalizados e focados no paciente, bem comoa receptibilidade dos profissionais de saúde aos mesmos. Abrange, igualmente, o uso detecnologias emergentes na sua conceção, incluindo Business Intelligence (BI), de modo a tirarreal partido da informação disponível.Deste modo, no âmbito deste projeto de dissertação, foram desenhadas, desenvolvidase exploradas uma nova geração de ferramentas Web de Business Intelligence para o apoio àdecisão e a prática clínica em unidades hospitalares. Englobou, em particular, o desenvolvimentode uma plataforma de BI versátil, incluindo a sua aplicação a dois casos práticosdiferentes, notadamente no apoio à decisão nas listas de espera de consultas e de cirurgias,assim como nos cuidados de Ginecologia e Obstetrícia (GO), e de uma ferramenta decodificação clínica ICD-9-CM (International Classification of Diseases, Ninth Revision, ClinicalModification).Assim, as ferramentas foram projetadas de modo a auxiliar os profissionais de saúde doCentro Hospitalar do Porto (CHP) no seu trabalho diário, incluindo a lidar com pacientesem condições delicadas e determinadas situações que requerem uma tomada de decisãoeficiente, bem como a codificação clínica de episódios de altas hospitalares.",
    "authors": [
      "Esteves, Marisa Araújo"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80123",
    "title": "Encoding and analysis of variational ROS computation graphs",
    "abstract": "In robotic applications, it is common to develop several variants of the same system (also known as a softwareproduct line), for example, to support different configurations of a robot. ROS is the most popular framework fordeveloping robotic applications, where each application is implemented as a distributed system of computationnodes that communicate through message passing. HAROS is a framework for static analysis of ROS-basedcode. It can extract an abstract model of a ROS system’s architecture (called the computation graph) and performan analysis on that model. However, it can only analyse one configuration at a time.In this thesis, we present three different approaches for encoding various ROS computation graphs in a singlevariational data structure, which contains the information related to the whole system and not just a configura tion. Additionally, we also define a variational execution algorithm for each approach, along with a small querylanguage, so that we can query and perform some analysis on said data structures. Lastly, we evaluate thesealgorithms and data structures so that we can reach some conclusions on which approaches work best, and inwhat conditions.",
    "authors": [
      "Moura, Pedro Rafael Paiva"
    ],
    "keywords": [
      "Variability",
      "Variational software",
      "Variational data structures",
      "Variational query languages",
      "Robotics",
      "Software product lines"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55471",
    "title": "Exploring heterogeneous computing with advanced path tracing algorithms",
    "abstract": "Currently, most computing systems have access to more than one type of processing unit,typically a multicore CPU device and a computing accelerator, such as a GPU. However,the vast majority of the existing implementations of advanced path tracing algorithms onlytake advantage of one of these processing units. The implementation of these algorithms insuch heterogeneous platforms while efficiently using both types of computing units alreadyproved to provide improved performance results.This dissertation examines four path tracing algorithms (Path Tracing aka PT, BidirectionalPath Tracing aka BPT, Bidirectional Photon Mapping aka BPM and Vertex Connectionand Merging aka VCM) and extends previous work by exploring a richer heterogeneousenvironment with more GPU accelerators and with manycore x86 devices (i.e., Xeon PhiKnights Corner), complemented with an insight into the challenges introduced by eachcomputing architecture and their programming environment. It also shows how these arecombined together to perform heterogeneous computation managed by a simple schedulingalgorithm, created to take advantage of each device’s features.This work proved that a fully heterogeneous approach to these four path tracing algorithmsis feasible and the performance results are significantly improved.",
    "authors": [
      "Oliveira, André David Gomes Monteiro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47830",
    "title": "Scheduling scientific workloads on an heterogeneous server",
    "abstract": "The goal of this dissertation is to explore techniques to improve the efficiency and performancelevel of scientific applications on computing platforms that are equipped with multiplemulti-core devices and at least one many-core device, such as Intel MIC and/or NVidia GPUdevices. These platforms are known as heterogeneous servers, which are becoming increasinglypopular both in research environments as in our daily gadgets.To fully exploit the performance capabilities of the heterogeneous servers, it is crucial tohave an efficient workload distribution among the available devices; however the heterogeneityof the server and the workload irregularity dramatically increases the challenge.Most state of the art schedulers efficiently balance regular workloads among heterogeneousdevices, although some lack adequate mechanisms for irregular workloads. Scheduling thesetype of workloads is particularly complex due to their unpredictability, namely on their executiontime. To overcome this issue, this dissertation presents an efficient dynamic adaptivescheduler that efficiently balances irregular workloads among multiple devices in a heterogeneousenvironment.To validate the scheduling mechanism, the case study used in this thesis is an irregularscientific application that has a set of independent embarrassingly parallel tasks applied to avery large number of input datasets, whose tasks durations have an unpredictable range largerthan 1:100. By dynamically adapting the size of the workloads that were distributed amongthe multiple devices in run-time, the scheduler featured in this dissertation had an occupancyrate of every computing resources over 97% of the application’s run-time while generating anoverhead well below 0.001%.",
    "authors": [
      "Maia, John Camilo Ferreira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83510",
    "title": "Data Science para o controlo analítico e eficiência energética numa ETAR",
    "abstract": "Atualmente, principalmente nos países mais desenvolvidos, as águas residuais são tratadas através dasEstação de Tratamento de Águas Residuais (ETARs) de forma a tentar atenuar os efeitos da atividadehumana na poluição da água potável. As ETARs são infraestruturas que desempenham um papel funda mental e imprescindível para a sociedade, pois estas permitem que a água potável já utilizada em diversasatividades e no uso doméstico possa retornar ao seu habitat natural nas melhores condições, podendoser reaproveitada.Contudo, uma ETAR para realizar a sua função requer elevados consumos energéticos, devido a todoo processo de tratamento que é auxiliado com um número elevado de equipamentos, desde das águasresiduais afluentes até ao seu destino final. Esses consumos oscilam consoante o tipo de afluente, sendoimportante analisar através de um controlo analítico, quais as substâncias que, quando presentes noafluente, requerem maior tratamento, e consequentemente maior consumo de energia.Com isto, o objetivo desta dissertação é, através do uso de Data Science, elaborar um forte controloanalítico às substâncias do afluente e relacionar o mesmo com a oscilação do consumo energético,tornando as ETARs mais eficientes e sustentáveis.",
    "authors": [
      "Faria, Carlos Daniel Coutinho"
    ],
    "keywords": [
      "Controlo analítico",
      "Data Science",
      "Eficiência energética",
      "Estação de Tratamentos de Águas Residuais",
      "Analytical control",
      "Energy efficiency",
      "Wastewater Treatment Plant",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79423",
    "title": "Desenvolvimento de uma plataforma de serviços para facilitar a integração de aplicações empresariais",
    "abstract": "O número de soluções utilizadas pelas empresas para fornecer serviços aos clientes e para fazer a gestão dos processos internos é cada vez maior. Este aumento provocou o aparecimento de novos problemas no que diz respeito à manutenção e à integração de novos serviços, uma vez que grande parte das aplicações não conseguem viver isoladamente. Com o objetivo de conseguir a integração de diferentes aplicações, surgiu o conceito de Enterprise Service Bus (ESB) — uma infraestrutura de conectividade que permite a comunicação entre aplicações, que podem ter diferenças a nível das plataformas em que são executadas, das linguagens de programação em que são escritas e dos modelos de dados que utilizam. A Eurotux Informática, S.A., é uma empresa especialista no planeamento, integração e implementação de sistemas informáticos, onde, devido à existência de diferentes interdependências entre aplicações de apoio ao negócio, surgiu a necessidade de implementar uma solução de integração utilizando um Barramento de Serviços. Assim, nesta Dissertação de Mestrado, para além do estudo dos padrões de integração de aplicações, é apresentado o processo de planeamento e implementação de uma solução de integração de aplicações num contexto empresarial.",
    "authors": [
      "Leite, Rui Filipe Castro"
    ],
    "keywords": [
      "Aplicações",
      "EAI",
      "ESB",
      "Integração",
      "SOA",
      "Applications",
      "Integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80053",
    "title": "Tool for semi autonomous building of traffic sign images repositories",
    "abstract": "Either to improve the drivers knowledge about the road, or focusing on the current development of autonomous vehicles, most car manufacturers began offering driving assistancesystems in their vehicles. A crucial part of the monitoring task performed by those systems isthe detection and reaction over found traffic signs. Since the decision flow of those solutionsis reliant on the information gathered from the found signs, these systems deeply rely on theirrecognition stage.To achieve high-accuracy classification rates at nearly real-time, recognition is usually implemented using machine learning techniques, such as state of the art Convolution NeuralNetworks (CNNs). However, these methods demand a large amount of data for their learningprocess. Due to the lack of large traffic signs repositories, these systems are restricted to oneof the few available datasets. A significant decrease in accuracy was observed when using arecognition model trained with samples from a given country and latter used to classify signsfrom another country, thus supporting the need for country-specific repositories and trainedmodels. Traffic signs reveal several differences when compared to the same functional signover different countries. Those changes, although similar for the human perception, causea significant disturbance in the classification abilities learned by a given machine learningmodel.Aiming to overcome this issue, this dissertation proposes a semi autonomous tool to createtraffic sign repositories, for almost any given country. The created dataset is intended to laterbe applied to train country-specific models, with traffic sign images from the country wherethe recognition model is going to be used. To achieve this, a pipeline based on an Ensemblearchitecture joining several computer vision techniques is proposed. The combination ofvarious methods allows to improve the recognition rates and, more importantly, decrease thenumber of false positives gathered in the produced repositories.Finally, the pipeline was used to create the first Portuguese traffic sign repository, havingcurrently around 33000 labelled signs.",
    "authors": [
      "Miranda, Miguel Dias"
    ],
    "keywords": [
      "Traffic signs detection and recognition",
      "Key-point features",
      "Machine learning",
      "Convolutional neural networks",
      "Sinais de trânsito",
      "Deteção e reconhecimento de objectos",
      "Key-point features",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66043",
    "title": "Integration and customization of a management system for indoor autonomous vehicles",
    "abstract": "This document reflects the work developed for a master’s dissertation in scope of Smart AutonomousMobile Units (SAMU) project. This project is inserted in the iFactory programmewhere exists a partnership between Bosch Car Multimedia (CM) and University of Minho(UM).The SAMU project main objective is the creation and development of a system that supportsthe internal movement of materials along the supply chain using autonomous vehicles(AVs). With this type of system is possible to increase the efficiency and productivity of thelogistics processes. In this context, the present document makes a brief description aboutthe project area of intervention and a summary about the current state of material flows inBosch BrgP.The work developed in this dissertation is integrated in SAMU project and is fundamentalto achieve SAMU system goals. Briefly, it consists of a solution responsible for managingand tracking a fleet of indoor autonomous vehicles. In addition, the integration process ofSAMU system in Bosch infrastructure was necessary to validate the management system.This document has a brief contextualization about SAMU project and the detailed descriptionof all the work made in this dissertation. It is also presented the analysis of thestate of art for this type of systems, the problems that the project intends to address andthe challenges encountered throughout the work. Finally, it is described the solution foundwith the mechanisms and strategies adopted, the results analysis and the final conclusions.",
    "authors": [
      "Monteiro, João Carlos Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27908",
    "title": "Revisão e reestruturação da arquitetura da plataforma Vortal Next : Vortal Software Development Kit",
    "abstract": "A evolução tecnológica das últimas décadas generalizou o uso de software para a substituição ou suporte de múltiplos processos das empresas e, evidenciou novas perspectivas para o desenvolvimento de soluções com altos níveis de performance, disponibilidade, escalabilidade e flexibilidade. No contexto Vortal (empresa líder no mercado de contratação electrónica português com a plataforma VortalNext>), esta generalização levou à necessidade da existência mecanismos que permitam aos seus clientes a personalização/criação de áreas de trabalho dedicadas.Tendo esta necessidade como foco, são avaliados os diferentes componentes da plataforma Next>, a metodologia de desenvolvimento atualmente utilizada (Model Driven Architecture) e quais as melhores aproximações para o desenvolvimento de aplicações no âmbito de uma plataforma web, focando as suas vantagens e desvantagens a nível arquitetural e aplicacional.Concluiu-se que todas as soluções estudadas são adequados ao desenvolvimento de aplicações web, sendo o seu grau de adequação variável com o contexto de utilização. São soluções diferentes relativamente à complexidade de implementação, aos recursos necessários, aos riscos envolvidos e à simplicidade de utilização por parte do grupo de utilizadores finais.Por fim, é apresentada a arquitetura de um Software Development Kit (são estudadas outras opções, sendo esta a que oferece mais estabilidade aplicacional e mais vantagens competitivas) e a sua integração no ecossistema aplicacional e arquitetural da plataforma maximizando, não apenas a flexibilidade e funcionalidade para o cliente final, como também a segurança, robustez e fiabilidade do ecossistema da plataforma. A arquitetura definida em conjunto com o modelo de negócio apresentado formam a linha de ação indicada para garantir a existência de aplicações personalizadas a serem executadas no ecossistema VortalNext>.",
    "authors": [
      "Santos, Bruno Miguel Almeida"
    ],
    "keywords": [
      "Arquitetura de Software",
      "Revisão",
      "Reestruturação",
      "Modelação",
      "Software Development Kit",
      "Aplicações de Terceiros",
      "Model Driven Architecture",
      "Software Architecture",
      "Revision",
      "Restructuring",
      "Modeling",
      "Third Party Applications",
      "681.3.06"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91559",
    "title": "Armazenamento confiável e em larga escala para aplicações compatíveis com POSIX",
    "abstract": "A Internet of Things (IoT) é uma das áreas tecnológicas que necessita de sistemas distribuídos quesuportem o armazenamento e acesso à enorme quantidade de dados constantemente produzidos porcentenas a milhares de dispositivos. Até agora, a maioria dos sistemas desenvolvidos encontravam-seadaptados a instalações em centro de dados, impulsionados pela adoção de serviços de computação emnuvem, porém, o sistema distribuído Large Scale File System (LSFS), veio mudar o paradigma atual emover o armazenamento distribuído para infraestruturas totalmente descentralizadas. Este é um sistemade ficheiros peer-to-peer não estruturado, parcialmente compatível com a interface POSIX, que permite arealização de leituras por parte de múltiplos utilizadores, mas escritas por parte de um só utilizador. Foiconstruído para atingir alta disponibilidade e resiliência e encontra-se preparado para escalar para infraes truturas do futuro. Todavia, o LSFS não apresenta operações essenciais de um sistema de ficheiros, comoa eliminação ou modificação de dados, a grande carga que é exercida sobre a rede tem consequênciasnegativas no sistema como um todo e a forma como este foi avaliado levanta várias questões.Com o propósito de resolver estes desafios, desenvolveu-se o improved Large Scale File System(iLSFS), um sistema de ficheiros que estende o sistema LSFS, dotando-o de uma melhor usabilidade, maspreservando todas as suas características fundamentais como a escalabilidade, a resiliência e a disponi bilidade. Para isso, o sistema adota soluções, como Tombstones, que viabilizam a eliminação de dadose a disponibilização de uma interface com maior compatibilidade com o standard POSIX, implementamétodos, como Version Vectors, que permitem o controlo de concorrência entre múltiplos utilizadores, emecanismos, como caches, que ajudam a mitigar o problema de saturação da rede.Os resultados obtidos, demonstram que o iLSFS, com todas as funcionalidades introduzidas, apre senta uma melhor usabilidade sem, no entanto, comprometer significativamente o desempenho. Quandointroduzido num caso de estudo real, demonstra-se que o sistema é capaz de escalar para ambientes delarga escala, com centenas de nodos, e mesmo quando submetido a cenários de instabilidade, onde aocorrência de falhas aleatórias é a norma, o iLSFS mostra-se capaz de tolerar a falha de uma grandequantidade de nodos de armazenamento, sem que esta provoque uma disrupção do seu funcionamento.",
    "authors": [
      "Ferreira, Alexandre José Branco"
    ],
    "keywords": [
      "Sistema de ficheiros",
      "POSIX",
      "LSFS",
      "Gossip",
      "Escalabilidade",
      "Tolerância a faltas",
      "File system",
      "Scalability",
      "Fault tolerance",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47730",
    "title": "Building the Museum of the Person based on a combined CIDOC-CRM - FOAF - DBpedia Ontology",
    "abstract": "This document presents the work developed to fulfill the requirements for a Master Thesis in Software Engineering, in the areas of Virtual Museums, and Ontologies for knowledge representation and exploration.The first objective of this thesis work was the creation of a specific ontology for thedocument repository of the Museum of the Person (Museu da Pessoa), using a standard formuseums, CIDOC-CRM (Comit´e Internacional pour la Documentation - Conceptual ReferenceModel), complemented with FOAF (Friend-of-a-Friend) and DBpedia that provides specificconcepts and relations to deal with persons. This abstract ontology was then populatedwith life stories collected previously through of interviews of common people.Two different approaches have be proposed to create the web pages for the Virtual Museum(VM), but only the approach 1 was implemented. A TripleStore was used as database tostore all the information that constitutes the Museum assets; the VM was created consultingthe datastore through SPARQL (SPARQL Protocol and RDF Query Language) queries.In the dissertation will be discussed the design decisions, and provided the technicaldetails; the project outcomes will be illustrated.The npMP site created can be accessed at http://npmp.epl.di.uminho.pt/ and complementsthis reading.",
    "authors": [
      "Araújo, Cristiana"
    ],
    "keywords": [
      "Cultural heritage",
      "Virtual museums",
      "Museum of the Person",
      "Ontologies",
      "CIDOC-CRM",
      "FOAF",
      "DBpedia",
      "Herança cutural",
      "Museus virtuais",
      "Museu da Pessoa",
      "Ontologias",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27869",
    "title": "Data warehouses espaciais: projeto e implementação",
    "abstract": "Segundo um estudo realizado pela empresa International Data Corporation (IDC) (Adrian Bridgwater, 2009), o mercado dos data warehouses tem tido um grande crescimento. Cada vez mais as empresas procuram guardar todos os dados relacionados com o seu negócio, de forma a obter o máximo de conhecimento possível, podendo, assim, tomar melhor decisões relacionadas com o seu negócio. Os data warehouses aparecem como uma ferramenta útil para suporte a processos de tomada de decisão. A capacidade dos data warehouses guardarem grandes quantidades de dados relativos ao negócio da empresa e permitirem aos agentes de decisão acederem de forma simples e fácil a esses dados, fazem deles uma ferramenta de eleição para o processo de tomada de decisão. A partir dos dados presentes num data warehouse pode-se efetuar relatórios e análises detalhadas. Apesar de serem ferramentas muito poderosas, os data warehouses ditos convencionais ainda contêm limitações relativamente à capacidade de guardar e analisar dados com características geográficas. Estas ferramentas capazes de lidar com este tipo de características são largamente utilizadas pelas empresas em muitos domínios de aplicação, como as telecomunicações ou a segurança, que com auxilio desta ferramenta conseguem descobrir qual o melhor local onde instalar antenas ou, então, por entidades Governamentais, de forma a descobrir as zonas do seu país, com a maior criminalidade. Ao longo desta dissertação, pretende-se entender o processo de construção de um data warehouse espacial desde a sua fase de levantamentos e análises de requisitos até à sua fase de implementação, sendo, por fim, transformado um data warehouse convencional num data warehouse espacial recorrendo a toda a informação obtida ao longo do processo.",
    "authors": [
      "Morgado, André Correia"
    ],
    "keywords": [
      "681.3.02:910",
      "910:681.3.02"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.02:910",
      "910:681.3.02"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92694",
    "title": "Deteção de conflitos em programas de home automation",
    "abstract": "O desenvolvimento da internet, evidenciado nos últimos anos, proporcionou o aumento da comunicação entre pessoas e dispositivos. Esta evolução, resultante na internet of things (IoT), permitiu odesenvolvimento de novas tecnologias e o progresso de tecnologias já existentes, aplicadas aos mais diversos contextos, como é o caso das habitações. Desta forma, surge a criação do conceito de casasinteligentes. Casas inteligentes, permitem monitorizar e controlar remotamente os seus dispositivos IoT.Estas características possibilitam que os dispositivos sejam controlados através de mecanismos de automatização. Este tipo de habitações permite a melhoria da qualidade de vida dos seus habitantes, bemcomo a redução dos recursos necessários ao seu funcionamento.Geralmente, a programação de sistemas para casas inteligentes é realizada através da aplicação detécnicas de programação baseada em regras de ativação-condição-ação. Este estilo de programação, emconjunto com as plataformas de automação, torna acessível a todos os utilizadores, automatizarem ecoordenarem os seus dispositivos IoT. Assim, com a definição de um conjunto de regras do tipo “se umacondição se verificar, então executa-se uma ação”, é possível automatizar o conjunto de dispositivos. Estaautomatização permite adicionar inteligência às habitações.O aumento da disponibilidade dos dispositivos, bem como a sua simplicidade de programação, resultano aumenta da adesão a estes sistemas. Porém, a criação de sistemas complexos implica um conjuntode regras também complexo. À criação de um conjunto de regras complexo agrega-se a dificuldade degarantir que não ocorram conflitos entre todas as regras criadas. A ocorrência de conflitos neste tipo desistemas pode resultar na execução de ações erradas que prejudicam a experiência do utilizador ou quecomprometam a sua segurança.Nesta dissertação, estuda-se a viabilidade da aplicação da ferramenta IVY Workbench, na deteção deconflitos que ocorrem neste tipo de sistemas. Esta ferramenta permite modelar um sistema, e verificarum conjunto de propriedades sobre ele expressas.",
    "authors": [
      "Veloso, Pedro Miguel Dias"
    ],
    "keywords": [
      "Internet of things",
      "Casas inteligentes",
      "Deteção de conflitos",
      "Verificação formal",
      "Modal action logic interactors",
      "Smart homes",
      "Conflict detection",
      "Formal verification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92802",
    "title": "Análise de componentes principais para variáveis qualitativas: exploração em R",
    "abstract": "A análise de grandes conjuntos de dados categóricos é um problema recorrente nas ciências sociais, comportamentais e biológicas. Surge por isso a necessidade de diminuir estes dados conseguindo, contudo, que as perdas de informação sejam mínimas. Tendo por base este problema, emergiu o tema desta dissertação, cujo objetivo passa pela análise exploratória do software R em busca de ferramentas para trabalhar com a análise de componentes principais categórica (ACPCAT), que surge como alternativa à tradicional análise de componentes principais (ACP), e permite reduzir a dimensionalidade de variáveis medidas em escalas diferentes. De forma a compreender os princípios fundamentais deste método estatístico, foi feita uma buscade fontes bibliográficas, que permitiu, adicionalmente, destacar o pacote Gifi e pacote Homals como sendo os únicos que possuem funções que permitem aplicar a ACPCAT. Estes pacotes foram explorados utilizando o mesmo dataset de exemplo, sendo feita uma descrição detalhada dos seus argumentos e dos valores e gráficos obtidos como forma de comparação das suas funcionalidades e recursos. O pacote Gifi descende do pacote Homals como sendo uma versão mais fácil de manipular e mais flexível devido a uma diferença na formulação da sua função de perda e ao facto deste utilizar B-splines.De modo a explorar as funcionalidades e limitações da função princals() do pacote Gifi e para estabelecer de que forma dados biológicos podem ser trabalhados, foi também executada a análise de um conjunto de dados sensoriais recolhidos de provas de vinhos. Com o intuito de permitir ao utilizador executar uma ACPCAT de forma simplificada e intuitiva foi criada uma aplicação web, que está disponível no endereço https://andreiagomes.shinyapps.io/Gify/ e pode ser acessada livremente de qualquer dispositivo desde que este tenha acesso à internet. A aplicação, de nome Gify, tem por base a função princals() do pacote Gifi e permite ao utilizador, mesmo que este não tenha nenhum tipo de conhecimento sobre o software R, carregar o seu conjunto dedados, definir os seus parâmetros de análise, executar a ACPCAT e consultar os resultados, sendo tudo isto processado recorrendo a botões de seleção e espaços de preenchimento.",
    "authors": [
      "Gomes, Andreia Filipa Araújo"
    ],
    "keywords": [
      "Variáveis categóricas",
      "ACP",
      "ACPCAT",
      "R",
      "Shiny",
      "Categorical variables",
      "PCA",
      "CATPCA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93146",
    "title": "Software Defined Vehicular Networks (SDVN): traffic routing process management",
    "abstract": "This work focuses on the innovation of vehicular networks through the application of software-defined networks(SDN) to optimize connectivity and decision-making in urban mobility scenarios. Using the Ryu controller, theMininet-WiFi emulation environment and the SUMO urban mobility simulator, this research establishes a completeand realistic experimental environment for the study of vehicular networks.The Ryu controller plays a key role in the dynamic orchestration of network decisions, enabling continuousadaptation to changes in the topology and communication demands of vehicular networks. Mininet-WiFi offers theability to emulate mobility scenarios, making it possible to analyze connectivity and performance in dynamic urbanenvironments. In addition, the SUMO simulator accurately replicates urban roads, providing realistic modeling ofthe vehicle movements.The combination of these tools allows a comprehensive evaluation of the performance of vehicular networks inurban environments, as well as the study of resource management strategies and real-time decision-making. Thisresearch contributes to the advancement of vehicular communication technologies and offers valuable insights forthe development of an efficient and safe urban mobility solutions.This study highlights the importance of integrating SDN, mobility emulation and road simulation to improveconnectivity and quality of service in vehicular networks, providing a solid foundation for further researches in thearea.",
    "authors": [
      "Tomás, Erikson Neves"
    ],
    "keywords": [
      "SDN (Software-Defined Network)",
      "OpenFlow",
      "Mininet-wifi",
      "Ryu-controller",
      "Vanets",
      "SUMO (Simulation of Urban MObility)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84479",
    "title": "Development and implementation of methodologies for integrating omics data with genomic-scaled metabolic models",
    "abstract": "As plantas são organismos fotossintéticos multicelulares essenciais para a vida humana e têm umenorme impacto na nossa economia. Como o crescimento e a sobrevivência das plantas estão intrinse camente relacionados com o seu metabolismo, as suas respostas metabólicas às condições ambientaismerecem ser estudadas. Com esse objetivo nasceu a Biologia de Sistemas, que usa a computação demodelos teóricos e matemáticos de modo a analisar sistemas biológicos como um todo. Estes modelosajudam a perceber processos biológicos e a estudar mecanismos metabólicos em diferentes condições.Vários métodos têm vindo a ser desenvolvidos de modo a criar modelos metabólicos mais precisos queintegram diferentes ómicas. Isto foi possível devido ao avanço das tecnologias de alto rendimento. Taismodelos são capazes de gerar previsões de fluxo mais precisas.Assim, neste estudo, métodos e algoritmos para integrar diferentes dados ómicos foram implementa dos com um modelo metabólico. Três algoritmos foram estudados: GX-FBA, RIPTiDe e EXAMO. O modelometabólico selecionado foi o da Arabidopsis thaliana (AraGEM), com dados ómicos em condições deseca. Os resultados dos algoritmos foram comparados com a literatura e com os modelos gerados peloGIMME. Inicialmente, os resultados do GX-FBA demonstravam-se promissores, com número de reaçõese metabolitos semelhante aos modelos do GIMME. No entanto, após estudar as reações contendo fluxo,GX-FBA aparentava não ser capaz de distinguir as condições de um modo significativo. Em relação aoRIPTiDe, os resultados foram surpreendentes, sendo capaz de diferenciar as duas condições, apesar deinicialmente apresentarem significativamente menos reações e metabolitos, e de partilhar com o GIMMEreações importantes envolvidas na resposta do metabolismo à seca. Por fim, o EXAMO não conseguiugerar modelos viáveis.",
    "authors": [
      "Rocha, Miguel Alexandre Oliveira"
    ],
    "keywords": [
      "A. thaliana",
      "Seca",
      "Modelos metabólicos",
      "Algoritmos",
      "Multi-ómicas",
      "Drought",
      "Metabolic models",
      "Algorithms",
      "Multiomics",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92712",
    "title": "PathIt: computational thinking training resource for visually impaired individuals",
    "abstract": "The contemporary landscape of problem-solving requires individuals to possess robustComputational Thinking (CT) skills. Acquiring these skills is contingent on the availabilityof adequate training resources. This scarcity is particularly pronounced for visually impairedindividuals, as the majority of existing CT training materials are inaccessible due to theirreliance on visual elements.To remedy this situation, PathIt is introduced, offering a non visual CT resource tailored to provide full accessibility to visual impaired individuals.PathIt encompasses a physical component that provides a tactile interface for usersinteraction and a software platform offering a range of CT challenges with both visual andauditory outputs, thereby catering to a diverse audience, including those with and withoutvisual impairments.This Master’s Dissertation encompasses the design, development, assessment, and pre ceding research that led to the creation of the PathIt system. The efficacy of PathIt as aCT training resource is rigorously evaluated across a spectrum of visual abilities and agegroups, showcasing its potential as a versatile tool for nurturing CT skills and underscoringits adaptability and inclusivity.",
    "authors": [
      "Cunha, Angélica Soares"
    ],
    "keywords": [
      "Learning resources",
      "Computational thinking",
      "Visual impaired students education",
      "Game-based learning",
      "Pedagogy",
      "Special education",
      "Recursos de aprendizagem",
      "Pensamento computacional",
      "Educação de alunos com deficiência visual",
      "Aprendizagem baseada no jogo",
      "Pedagogia",
      "Educação especial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82579",
    "title": "An efficient and accurate framework for large-scale sequences of DNA barcodes",
    "abstract": "DNA barcodes are short sequences of pre-defined gene regions that contain a sufficientamount of intra- and inter-species genetic information. High-throughput sequencing techniques are currently used to identify large sequences of DNA barcodes in a species genome, in a relatively short time.Domain experts require adequate self-contained tools to accurately and efficiently processDNA barcode data in a reasonable time, taking advantage of current parallel and heterogeneous computing systems. They also expect to use these tools on different computing platforms, from laptops to high-performance servers, without requiring a broad knowledge in software engineering to develop efficient computational applications.The main goal of this project was to develop a framework and associated user-friendly toolsfor domain experts to efficiently support DNA barcoding studies, providing an abstractionof the performance issues.4SpecID is the key outcome of this work: an application software that integrates asemi-automated auditing and annotation tool for reference libraries, to ensure the qualitystandards of the compiled data, aiming to enable a grounded decision when identifyingspecies from DNA barcodes. Its graphics interface aids the end user to specify the operationsand it also simplifies data filtering and remote file handling.The C++ ported version (from MATLAB) was fully tested and is more robust thanthe original version. Architecture features common to laptop and compute servers wereexploited, namely parallel programming techniques and memory models.The presented validation and performance results show significant improvements onexecution times, not only on the sequential version, but also by using the available parallelcapabilities of the underlying computing platforms.",
    "authors": [
      "Neto, Luís Manuel Pacheco"
    ],
    "keywords": [
      "High-throughput sequencing",
      "High performance computing",
      "DNA barcoding",
      "DNA metabarcodin",
      "Códigos de barras ADN",
      "Sequenciação de ADN",
      "Computação de alto desempenho"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92718",
    "title": "Comparative analysis of microbial communities from full scale anaerobic digesters",
    "abstract": "Nas estações de tratamento de águas residuais (ETAR), a exploração da composição taxonómica e daabundância de comunidades microbianas tornou-se um esforço indispensável. É essencial para otimizar osprocessos de tratamento, monitorizar a saúde do sistema e garantir a conformidade com as normas ambientais.Os microrganismos desempenham um papel crucial na decomposição da matéria orgânica, na remoção decontaminantes e na recuperação de recursos. O conhecimento das comunidades microbianas leva a estações detratamento de águas residuais (ETARs) mais eficientes e resilientes, promovendo a saúde pública e asustentabilidade ambiental. A utilização do sequenciamento do gene 16S rRNA para a análise de comunidades microbianas em digestores anaeróbicos em grande escala em estações de ETARs tem vindo a crescer. Essa metodologia envolve uma série de processos computacionais, abrangendo avaliação da qualidade da sequência, remoção de ruído, classificação taxonômica, alinhamento e construção de árvores filogenéticas.Notavelmente, o conjunto de software Quantitative Insights Into Microbial Ecology versão 2 (QIIME2) emergiu como uma ferramenta valiosa, simplificando a análise de dados do gene marcador 16S rRNA. Facilita a análise ponta a ponta de diversos conjuntos de dados de microbiomas e facilita estudos comparativos com dados disponíveis publicamente. QIIME2 equipa os investigadores com ferramentas para selecionar profundidades de amostragem apropriadas para conduzir análises de diversidade alfa e beta.Nesta tese, apresentamos uma análise comparativa abrangente das comunidades microbianas que habitamdigestores anaeróbios de grande escala em ETARs. O estudo utiliza a sequenciação de nova geração,nomeadamente a sequenciação de alto rendimento do gene 16S rRNA, e depois utiliza o Qiime2 para determinar eanalisar a taxonomia, a diversidade e a abundância relativa das comunidades de digestores em cada ETAR e entreelas. Ion torrent foi a tecnologia utilizada para a sequenciação de alto rendimento de nova geração do gene 16SrRNA. O fluxo de trabalho utilizado no qiime2 consistiu na importação de dados, no controlo de qualidade(denoising), clustering e só depois a análise da diversidade e da taxonomia para determinar a composição, adiversidade e a abundância das comunidades. Os resultados mostraram que as comunidades microbianaspermaneceram bastante estáveis durante diferentes pontos de amostragem, mas distantes quando comparadasentre diferentes digestores de lamas. Porém, alguns dos microrganismos que compõem a comunidademetanogénica presentes nas comunidades foram Methanobacteriales, Methanomicrobiales, Methanosarcinales.Em relação à comunidade bacteriana, os microrganismos mais abundantes em um determinado digestor epresentes em todos os digestores foram atribuídos a Sedimentibacter, Phycicoccus, Thermovirgae, Cloacimonas ePhycicoccus.",
    "authors": [
      "Inácio, Ana Carolina Matos"
    ],
    "keywords": [
      "ETAR",
      "Digestores anaeróbios",
      "16S rRNA",
      "Qiime2",
      "Taxonomia",
      "Diversidade",
      "Abundância relativa",
      "WWTP",
      "Anaerobic digesters",
      "Taxonomy",
      "Diversity",
      "Relative abundance",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79371",
    "title": "Microservices architectures in healthcare with Apache Kafka",
    "abstract": "Over the past few years, we have seen an exponential increase in the amount of data produced. This increase indata is due, in large part, to the massive use of sensors, as well as the immense amount of existing applications.Due to this factor, and in order to obtain relevant information through the data, companies, institutions and thescientific community are constantly looking for new solutions to be able to respond to the challenges.One of the areas where evolution is most needed is the area of healthcare, an area on which we all dependas a society. Every day, traditional healthcare information systems produce a large amount of data, making itcomplex to manage. Much of this data is produced by IoT devices, such as vital signs monitors, and in manycases can be critical to the patient’s health, as in the case of Intensive Care Units.In this sense, the main objective of this dissertation is to expose the advantages and disadvantages of theapplicability of microservices architectures and the use of Apache Kafka in the health area, more specificallyin Intensive Care Units where the information flow is critical. In order to support these objectives, a Proof ofConcept was developed, based on a future real applicability, which will support the carrying out of analyzes andtests.",
    "authors": [
      "Santos, Rui Fernando Carvas dos"
    ],
    "keywords": [
      "Apache Kafka",
      "Microservices Architectures",
      "Intensive Care Units",
      "Big Data",
      "Internet of Things",
      "Health Information Systems",
      "Arquiteturas de microservices",
      "Unidades de Cuidados Intensivos",
      "Sistemas de Informação na saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64531",
    "title": "Gestão inteligente de estacionamento em ambiente urbano",
    "abstract": "Desde as últimas décadas que a população mundial tem vindo a aumentar de uma formaexponencial, originando e potenciando vários problemas de difícil resolução como, porexemplo, problemas de trânsito relacionados com o elevado fluxo de veículos, problemasde poluição, de alojamento, de acesso à saúde, de gestão de recursos, entre outros. Nesteenquadramento surgiu a necessidade de se “virtualizar” as próprias cidades, levando aoconceito de Cidades Inteligentes (Smart Cities) com o principal objetivo de criar condiçõesde sustentabilidade nas próprias cidades e disponibilizar um acesso mais flexível a informaçãoútil para os seus cidadãos. O conceito de Smart Cities é conhecido pelo uso de váriastecnologias para melhorar as infraestruturas urbanas e tornar os centros urbanos mais eficientestendo em conta as necessidades das populações. Estes projetos recorrem geralmentea redes de sensores distribuídas de forma estratégica para a recolha de dados do meio. Geralmente,estes projetos passam por uma avaliação experimental do problema em estudo,recorrendo na maioria das vezes a ferramentas de simulação, de que são exemplos o Cup-Carbon, o InterSCSimulator, o UrbanSim, entre outros.Neste contexto, neste projeto de mestrado pretende-se estudar um problema concreto noâmbito das Smart Cities, sendo este o problema da gestão inteligente de estacionamento emambiente urbano tendo em vista uma maior eficiência da ocupação dos espaços disponíveise uma melhor experiência dos utilizadores no uso e partilha deste recurso muitas vezesescasso. O ambiente de simulação CupCarbon será utilizado para simulação e teste doscenários de gestão de estacionamento propostos.",
    "authors": [
      "Oliveira, Filipe Manuel Gonçalves de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83125",
    "title": "Robi: a visual programming language for educational robotics",
    "abstract": "This document presents a Master’s thesis with researches focused on the teaching ofcomputational thinking and present the development details of Robi, a block-based visualprogramming language that is able to program a robot built with an Arduino Uno. Theseresearches had the purpose of evaluating if the development of Robi, a block-based program ming language that communicates with Arduino, would really be needed. The researcheshave proved that from the popular programming environments that exist in the market,that were investigated, none have the requirements that Robi requires. The platform will beused to teach computational think through a block-based programming environment andeducational robotics. Robi development is motivated by the intersection between the costsof educational robotics kits and the existing block-based programming language, in whichsimplicity and intuitiveness could be improved, so children with learning difficulties or evenyounger children, in the context of educational robotics, can leverage the learning benefitsthat the Robi environment can bring. The educational robotics kit used with the block-basedprogramming environment developed, is the one based on Arduino Uno, a microcontrollerboard that, together with electronic components, can be considered cheaper than some ofthe famous educational robotics kits. The main goal of this project is to provide a simplerand more intuitive visual programming language platform to program a robot based onArduino Uno.",
    "authors": [
      "Galvão, Gustavo Linhares"
    ],
    "keywords": [
      "Computational thinking",
      "Visual programming language",
      "Block-based programming language",
      "Educational robotics",
      "Pensamento computacional",
      "Linguagem de programação visual",
      "Linguagem de programação baseada em blocos",
      "Robótica educacional",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79855",
    "title": "Suporte para refatorização automática de lógica de negócio baseada em modelos",
    "abstract": "Software’s structure profoundly affects its development and maintenance costs. Poorsoftware’s structure may lead to well-known design flaws, such as large modules or longmethods.A possible approach to reduce a module’s complexity is the Extract Method refactoringtechnique. This technique allows the decomposition of a large and complex method intosmaller and simpler ones, while reducing the original method’s size and improving itsreadability and comprehension.Nowadays, it’s almost mandatory that Integrated Development Environments (IDEs) supportthis and other refactoring techniques. Despite the wide availability of the extract methodoperation on IDEs, the identification of portions of code that are worthwhile to refactor stillrelies mostly on developer knowledge and expertise.Thus, the purpose of this dissertation is to empower the OutSystems platform with a system that is able to analyse modules complexity and automatically suggest Extract Methodrefactoring opportunities.",
    "authors": [
      "Fernandes, Tiago Fernando Santos Braga"
    ],
    "keywords": [
      "Refactoring",
      "Program slicing",
      "Code complexity metrics",
      "Low-code",
      "OutSystems",
      "Graph theory",
      "Refatorização",
      "Program slicing",
      "Métricas de complexidade",
      "Low-code",
      "OutSystems",
      "Teoria de Grafos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86579",
    "title": "Data structure centered SPH performance evaluation",
    "abstract": "Smoothed-particle Hydrodynamics (SPH) is a particle-based simulation considered by many to be the maincandidate for fluid simulation. This model was developed by R.A. Gingold and J.J. Monaghan in 1977 and hadthe purpose of solving astrophysical problems. Over the years, Monaghan has revisited SPH (1985, 1988, 1992and so on) and it also gained traction with other researchers who discovered new applications for the model suchas ballistics, volcanology, oceanography, and so on. Among the fields there is one we are particularly interestedin, and that is Fluid Simulation.This work aims to implement SPH using efficient data structures that allow a real-time simulation to run on theGraphics Processing Unit (GPU). According to the literature, the z-order indexing method and the hash map arethe most suitable structures for this purpose. It is intended to see its impact and in which situations one will bebetter suited to use than the other.With said implementation, several tests were performed in order to analyze the robustness and stability of themethod. With these tests it was possible to compare the two data structures used.The implemented SPH showed realistic and robust results in most cases, being able to handle multiple scenesof varying levels of complexity. Despite the good results, it showed some difficulties in maintaining stability insome boundaries (boundaries with great curvature or sharp edges) and also showed some difficulties in sceneswith two fluids with different densities.As for the data structures, it was possible to observe that both are efficient and support real-time simulationswith more than 1 million particles (using a NVIDIA RTX 3080). In the case of z-order, it proved to be the methodwith the best performance when compared to the hash map under the same conditions, that is, scenes with thesame number of particles and the same simulation volume. This is due to the larger data locality that z-order has.On the other hand the hash map was a bit slower (when compared with the z-order under the same conditions)but allowed for greater freedom when creating a scene. When comparing the two methods with the same numberof particles but different simulation volumes we can see that the hash map catches up with the z-order methodas the particles spread across the simulation.With the two data structures analyzed it is possible to draw some conclusions. The z-order method is recom mended when we have a limited and relatively small simulation volume. In case there is no simulation volume,or it is very large, it is recommended to use a hash map since the performance deficit seems to disappear as thesimulation volume gets bigger and the particles spread across the volume.",
    "authors": [
      "Barbosa, Paulo Alexandre Ferreira"
    ],
    "keywords": [
      "Fluid simulation",
      "Smoothed-particle hydrodynamics",
      "SPH",
      "Particle-based method",
      "Hash map",
      "Z-order",
      "Data structures",
      "Simulação de fluidos",
      "Método baseado em partículas",
      "Estruturas de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84102",
    "title": "Development of a tool based on deep learning able to classify biomedical literature",
    "abstract": "In the last decades, the scientific community has produced huge amounts of publications aboutthe most varied biomedical topics, making the search for relevant information a really difficulttask for every researcher. Some approaches have been followed to develop tools that canfacilitate this process. For instance, PubMed implemented in 2017 a Machine Learning model tosort documents by their relevance. Nevertheless, even the authors consider that their systemwould benefit from the implementation of a Deep Learning model, which for now needs morestudies.In this context, a package called BioTMPy1 was developed in this work, to perform documentclassification of biomedical literature using the Python programming language. The packageis divided into different modules to provide to the user functions to read documents in differentformats, perform preprocessing and data analysis and to train, optimize and evaluate Machineand Deep learning models. Our package also provides intuitive pipelines that can be easilyadapted for the user needs, illustrating how to implement complex deep learning models.The developed package was applied to a dataset from a challenge of the BioCreative forum,from 2019, about protein-protein interactions altered by mutations, an important topic for theadvances related to precision medicine. Using this dataset, it was possible to observe a slightlybetter performance of BioWordVec pre-trained embeddings over GloVe, ”pubmed pmc” and”pubmed ncbi” embeddings. Also, with the evaluation of the developed models on the test set,we managed to overcome the challenge’s best submission, by using a model with BioBERT anda bidirectional LSTM on top, resulting in a difference of 7.25% for average precision, 3.22% forprecision, 2.99% for recall and 3.15% for the f1-score.Also, a web server was developed to provide access to the best Deep Learning modeltrained in this work. The overall pipeline here developed can be applied to other case studies indifferent topics, provided there is a set of documents annotated as relevant and non-relevant,allowing to train the models.",
    "authors": [
      "Alves, Nuno Miguel Caetano"
    ],
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Document classification",
      "Text mining",
      "Aprendizagem máquina",
      "Classificação de documentos",
      "Mineração de texto",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84495",
    "title": "Development of a recommendation system for scientific literature based on deep learning",
    "abstract": "The previous few decades have seen an enormous volume of articles from the scientific commu nity on the most diverse biomedical topics, making it extremely challenging for researchers tofind relevant information. Methods like Machine Learning (ML) and Deep Learning (DL) havebeen used to create tools that can speed up this process. In that context, this work focuseson examining the performance of different ML and DL techniques when classifying biomedicaldocuments, mainly regarding their relevance to given topics. To evaluate the different techniques,the dataset from the BioCreative VI Track 4 challenge was used. The objective of the challengewas to identify documents related to protein-protein interactions altered by mutations, a topicextremely important in precision medicine. Protein-protein interactions play a crucial role in thecellular mechanisms of all living organisms, and mutations in these interaction sites could beindicative of diseases.To handle the data to be used in training, some text processing methods were implementedin the Omnia package from OmniumAI, the host company of this work. Several preprocessingand feature extraction methods were implemented, such as removing stopwords and TF-IDF,which may be used in other case studies. They can be used either with generic text or biomedicaltext. These methods, in conjunction with ML pipelines already developed by the Omnia team,allowed the training of several traditional ML models.We were able to achieve a small improvement on performance, compared to the challengebaseline, when applying these traditional ML models on the same dataset. Regarding DL, testingwith a CNN model, it was clear that the BioWordVec pre-trained embedding achieved the bestperformance of all pre-trained embeddings. Additionally, we explored the application of morecomplex DL models. These models achieved a better performance than the best challengesubmission. BioLinkBERT managed an improvement of 0.4 percent points on precision, 4.9percent points on recall, and 2.2 percent points on F1.",
    "authors": [
      "Silva, Tiago Rafael Ferreira Miranda da"
    ],
    "keywords": [
      "Deep learning",
      "Document classification",
      "Machine learning",
      "Biomedical text mining",
      "Text mining",
      "Aprendizagem profunda",
      "Classificação de documentos",
      "Aprendizagem máquina",
      "Mineração de texto biomédico",
      "Mineração de texto",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28291",
    "title": "Protocolo CAP em Smartphones: análise e implementação do protocolo de autenticação Chip Authentication Program",
    "abstract": "sociedade atual tem vindo a adotar a conveniência dos serviços bancários online. Estes, no entanto, tem enfrentado um problema crescente de fraudes. Uma das soluções que emergiu no mercado para tentar contrariar esta tendência é o protocolo Chip Authentication Program (CAP), que fornece ao sistema mecanismos de autenticação forte, baseados em dois fatores de autenticação. Contudo, este implica a utilização de um dispositivo dedicado, que, pelo investimento financeiro avultado envolvido e pela falta de aceitação dos clientes em transportar um dispositivo adicional sempre que pretendem usufruir dos serviços bancários online, tem funcionado como barreira à massificação deste protocolo.Com a presença cada vez mais relevante dos smartphones na sociedade e com o surgimento de smart cards para estes dispositivos, os objetivos desta dissertação focaram-se, fundamentalmente, em chamar a atenção para a potencialidade destas tecnologias para aplicações com requisitos de segurança críticos e em incentivar a utilização do protocolo CAP, ultrapassando os principais entraves à sua adoção e contribuindo com uma solução que permita reduzir as fraudes em serviços bancários online.O resultado desta dissertação é um sistema baseado no protocolo CAP, que reúne as características que melhor caracterizam um smartphone com as propriedades de segurança que os smart cards acrescentam a um sistema. A utilização do smartphone, um dispositivo bem mais ubíquo do que o dispositivo dedicado tipicamente usado no protocolo CAP, permite reduzir o investimento necessário, uma vez que não é necessário fornecer dispositivos aos clientes, e reflecte-se também numa maior aceitação por parte do utilizador final que não tem de transportar um dispositivo adicional para usufruir dos serviços bancários online.",
    "authors": [
      "Martins, Tiago Filipe Maia Campos"
    ],
    "keywords": [
      "681.3-7",
      "336.71:681.324",
      "681.324:336.71"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3-7",
      "336.71:681.324",
      "681.324:336.71"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79827",
    "title": "Applying attribute grammars to teach linguistic rules",
    "abstract": "This document presents the topic “Applying Attribute Grammars to teach LinguisticRules”, at Universidade do Minho in Braga, Portugal. This thesis is focused on using theformalisms of attribute grammars in order to create a tool to help linguistic students learnthe different rules of a natural language. The system developed, named Lyntax, consistsin a processor for a domain specific language which intends to enable the user to specifydifferent kinds of sentence structures, and afterwards, test various phrases against saidstructures. The processor validates and evaluates the input given, generating a grammarwhich is specific to a previously chosen sentence. Lastly, using ANTLR, a parser is generatedfor that specific grammar referred above. The processor built by ANTLR also creates asyntax tree that is presented to the user for analysis purposes.An interface that supports the specification of the language (written in Lyntax DSL) wasbuilt, also allowing the use of the processor and the generation of the specific grammar,exempting the user from knowing the details of the process.Within this document, the focus will be primarly dedicated to the analysis of the systemand how each block was built. Different examples of the processor in action will be shownand explained.",
    "authors": [
      "Sousa, Manuel Gouveia Carneiro de"
    ],
    "keywords": [
      "Linguistic",
      "Natural language processing",
      "Attribute grammar",
      "Linguística",
      "Processamento de língua natural",
      "Gramáticas de atributo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79676",
    "title": "Intelligence on nutrition in healthcare and continuous care",
    "abstract": "In the healthcare industry, the patient’s nutrition is a key factor in their treatment process,as every user has their own specific nutritional needs and requirements. For example, aftera major surgery, a patient should eat products with high fiber while avoiding processedfoods and dairy. An appropriate nutrition policy can therefore complement the patient’srecovery process, alleviating possible symptoms.Food recommender systems are platforms that offer personalised suggestions of recipesto users. These systems are often implemented in food recipe websites, offering similar sug gestions. They are also used for improving the user’s health and recommending healthierrecipes while keeping their preferences in consideration. However, there is an absence ofusage of recipe recommendation systems in the healthcare sector. Multiple challenges inrepresenting the domain of food, coupled with the patient’s needs, make it complicated toimplement these systems in healthcare services and continuous care.In the context of this master’s dissertation, the aim was to design, develop, and explorea new generation platform for the provision, planning, and reservation of food plans, com prised of web and mobile tools. A key feature of this platform is the suggestion of mealplans to each department, taking into account the patient’s nutritional requirements.Data regarding the user’s nutritional requirements were collected and analysed, as wellas feedback from health professionals and users from the social cafeteria. The collectedinformation supported the development of a food recommendation system. These toolswill help nutrition professionals at the Santa Casa da Misericórdia of Vila Verde in their work,namely with the making of meal plans for multiple departments, each with their specificnutritional requirements.",
    "authors": [
      "Miranda, Rui Pedro Mesquita"
    ],
    "keywords": [
      "Machine learning",
      "Recommender systems",
      "Meal planning",
      "Decision support systems",
      "Sistemas de recomendação",
      "Planeamento de ementas",
      "Sistemas de suporte a decisão",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84472",
    "title": "In memory data grid clusters HR employee data fusion",
    "abstract": "Com o aumento constante da quantidade e complexidade dos dados, vem a necessidade de serem estudadasnovas tecnologias de forma a acompanhar este crescimento sem pôr em causa o desempenho.A presente dissertação foca-se numa destas soluções, os In-Memory Data Grid (IMDG). Um IMDG define-secomo sendo uma base de dados com armazenamento primário em memória volátil colocada numa camadaimediatamente a cima de uma base de dados tradicional (armazenamento persistente). Além disso, permite adistribuição de recursos e computações por diversos nodos. Assim, no decorrer da dissertação, foram estudadosos principais conceitos desta solução, bem como algumas das mais prestigiadas tecnologias na área.Após estabelecido que o Apache Ignite seria a tecnologia que poderia trazer mais vantagens, esta foi aplicadaa um caso real. Assim foi executado o cálculo de uma matriz de segurança, que permite atribuir as permissõesde visualização de dados entre membros de uma empresa, bem como a execução de uma simples interrogaçãode forma a efetuar uma análise de desempenho entre o IMDG e a solução previamente existente baseada emSQLServer.No que diz respeito ao cálculo da matriz de segurança, os valores de desempenho foram limitados,maioritariamente devido às limitações de Structured Query Language (SQL) da ferramenta, obtendo umdesempenho três vezes inferior comparativamente com a solução anterior. Por outro lado, com a interrogaçãoselecionada, o Ignite mostrou um melhor desempenho (na utilização de uma grid com 4 nodos, esta interrogaçãoapresentou, para uma thread, uma média de 2266.92ms vs. 8099.64ms no SQLServer).",
    "authors": [
      "Mota, João Diogo Mendes Teixeira da"
    ],
    "keywords": [
      "Apache ignite",
      "Disco",
      "In-memory data grid",
      "Memória",
      "Recursos Humanos",
      "Disk",
      "Human Resources",
      "Memory",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64183",
    "title": "Local analysis strategies for exudate detection in fundus images",
    "abstract": "Diabetic Retinopathy (DR) is a common complication of diabetes, which is among the major causes of vision loss in the world. An early detection of the disease is the key to avoid the patient’s blindness. However, at the initial phase of the disease, the vision impairment is not easily percieved by the patient. Therefore, regular follow-up exams are recommended in order to detect anomalous patterns in the patient’s retina. Exudates are one of the most prevalent signs during the early stage of DR and, therefore, its early detection is vital to prevent the patient’s blindness. However, the manual detection of exudates by experts is laborious and time-consuming. Thus, automated screening techniques for exudate detection have great significance in saving cost, time and labor, allowing the ophthalmologists to make the treatment decision timely. In this sense, one of the main objectives of this thesis is to develop and compare different strategies to locally extract information of fundus images for detecting exudates.Several methods related to the automatic detection of exudates have been proposed in the literature however, these methods focus their efforts in the segmentation of exudates or require the extraction of features from a lesion candidate map. On the other hand, in the methodologies proposed in this thesis, the characterization of healthy and damaged retinal areas is performed by applying image descriptors in a local way, avoiding the segmentation step and the generation of candidate maps.A system based on local feature extraction and Support Vector Machine classification is used to develop and compare different strategies for automated detection of exudates. The main novelty of this work is allowing the detection of exudates using non-regular regions to perform the local feature extraction. To accomplish this objective, different methods for generating superpixels are applied to the fundus images of E-OPHTA database and texture and morphological features are extracted for each of the resulting regions. Finally, each region is classified according to healthy and pathological classes, during the classification stage.The strategies proposed in order to generate superpixels rely on applying the marker-controlled watershed transformation to a spatially regularized gradient. From these strategies, two different types of superpixels are created: c-Waterpixels and m-Waterpixels. In the end, an elaborated comparison between the proposed methods for generating m and c-waterpixels and the state-of-the-art method for generating SLIC superpixels is performed.Additionally, a system based on Convolutional Neural Networks (CNN) is explored to discriminate between healthy and pathological regions in fundus images. Transfer learning is applied to fine-tune some of the most important state-of-the-art CNN architectures. Exudates usually represent less than one percent of the total number of pixels that compose the retinal image. This is the reason why, in both the systems presented in this thesis, the fundus images are divided in superpixels and the classification is performed for each of the regions.Lastly, an exhaustive comparison between the two created systems to automatically detect exudates is performed. In other words, the classification results obtained through the system involving CNNs are compared with the ones obtained by applying the approach based on feature extraction and subsequent classification using machine learning algorithms.",
    "authors": [
      "Pereira, Joana Daniela da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82583",
    "title": "Smart irrigation system: otimização do sistema de rega em espaços verdes",
    "abstract": "An optimization of irrigation systems and better management of green spaces is essential nowadays, as oneof our main resources, water, is often wasted and the soil does not contain the necessary nutrients, which canlead to death of vegetation. This work presents a solution where, using a set of public APIs through which theenvironment data is collected, it is possible to intelligently and autonomously activate or deactivate the irrigationsystem, taking into account a group of previously defined metrics. The system is also prepared to receivereal data from sensors implemented in the field. A web application is also developed so that these data arepresented in a clear and intuitive way, in order to support decision-making by the owner of a particular land.Finally, a machine learning algorithm was created that, based on the history of rain occurrence, tries to predictthe occurrence of precipitation for a particular day, thus contributing for a more efficient solution.",
    "authors": [
      "Barros, Paulo Filipe Moreira"
    ],
    "keywords": [
      "APIs",
      "Web application",
      "Irrigation system",
      "Water",
      "Machine Learning",
      "Aplicação Web",
      "Sistema de rega",
      "Água",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83250",
    "title": "Using Machine Learning to automatically infer an approximation of a physical system",
    "abstract": "The development of Cyber-physical Systems (CPSs) models is a complex process which requires deep multi-disciplinary knowledge of the intended topic to model. Added to this complexity is the difficulty of combining multiple models, sometimes without access to their source code, and make them communicate in a harmonious and integrated way in order to represent the vicissitudes of the environment where the physical system is inserted into. Functional Mockup Interface is a set of C headers that define a protocol that allows the interoperability of different models, independently of the programming languages and tools that generated them. A model that implements this interface is called Functional Mockup Unit (FMU). This dissertation explores the usage of Machine Learning to generate automatically a FMU from parsing a dataset containing the inputs and outputs obtained during the observation of a physical system. A Command-line Interface (CLI) tool named AutoFMU is also presented here, and it accepts as parameters a set of CSV tables and the names of the column that correspond to the inputs and outputs, using several supervised learning algorithms to infer the relationships between these variables. Its invocation results in a file containing a valid FMU ready to be used. In order to assess its feasibility in a real context, the tool AutoFMU was used to generate approximations of a controller of a line follower robot. The generated models were then simulated in the INTO-CPS program and the robot movements under the purview of the new controller were observed. The values generated by the new models were also compared with the datasets of the original physical unit.",
    "authors": [
      "Silva, Afonso João Borges Cabral Cerejeira da"
    ],
    "keywords": [
      "Cyber-physical System",
      "Functional Mockup Interface",
      "Machine Learning",
      "Python",
      "Sistemas ciber-físicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92561",
    "title": "Analysis of I/O patterns for data management systems",
    "abstract": "The exponential growth of digital information that has been witnessed in recent years requires a continuousevolution and optimization of data management systems, such as databases and storage solutions.In order to provide efficient processing and storage capabilities for large amounts of data, data man agement systems must adopt different optimizations (e.g., caching, replication, data reduction) that in crease their complexity. As a result, developing, configuring and maintaining a data management systembecomes increasingly difficult and costly.Tracing and analyzing the interactions and exchanges between components of these systems is funda mental to uncover performance, correctness and dependability issues almost unavoidable in any complexsolution. On the other hand, this presents several challenges, such as minimizing the impact on applica tions’ performance and storage space, improving tracing accuracy and achieving real-time analysis, thatmust be explored.With this thesis, we present a tracing and analysis pipeline capable of capturing and analyzing the I/Opatterns of these data-centric systems in order to better understand their behavior, using LTTng as tracingtool.In particular, the proposed solution includes a tracing component that efficiently collects disk andnetwork I/O metrics originated by the target application. This component is the major focus of this thesisand allows for the capture of system calls that the application executes, as well as their arguments, in anon-intrusive and almost real-time way. The rest of the pipeline facilitates the analysis and visualizationof captured events through search queries and diagrams, allowing the user to find potential performanceand optimization problems.In the end, we demonstrate that the proposed solution allows for the identification of inefficient andredundant I/O patterns in production applications without causing significant impacts on the runtimeperformance of the application and allowing for near real-time analysis.",
    "authors": [
      "Rodrigues, Pedro Miguel Borges"
    ],
    "keywords": [
      "Analysis",
      "Data management",
      "I/O patterns",
      "LTTng",
      "Performance",
      "Tracing",
      "Análise",
      "Captura",
      "Desempenho",
      "Gestão de dados",
      "Padrões E/S",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84137",
    "title": "Identification of microservices from monolithic applications through topic modelling",
    "abstract": "Microservices emerged as one of the most popular architectural patterns in the recentyears given the increased need to scale, grow and flexibilize software projects accompaniedby the growth in cloud computing and DevOps. Many software applications are beingsubmitted to a process of migration from its monolithic architecture to a more modular,scalable and flexible architecture of microservices. This process is slow and, depending onthe project’s complexity, it may take months or even years to complete.This dissertation proposes a new approach on microservices identification by resortingto topic modelling in order to identify services according to domain terms. This approachin combination with clustering techniques produces a set of services based on the originalsoftware. The proposed methodology is implemented as an open-source tool for explorationof monolithic architectures and identification of microservices. An extensive quantitativeanalysis using the state of the art metrics on independence of functionality and modularityof services was conducted on 200 open-source projects collected from GitHub. Cohesion atmessage and domain level metrics showed medians of roughly 0.6. Interfaces per serviceexhibited a median of 1.5 with a compact interquartile range. Structural and conceptualmodularity revealed medians of 0.2 and 0.4 respectively. Further analysis to understand ifthe methodology works better for smaller/larger projects revealed an overall stability andsimilar performance across metrics.Our first results are positive demonstrating beneficial identification of services due tooverall metrics’ results.",
    "authors": [
      "Brito, Miguel António Ferrão"
    ],
    "keywords": [
      "Microservice architecture",
      "Monolithic decomposition",
      "Topic modelling",
      "Software clustering",
      "Arquitetura de microserviços",
      "Decomposição de monólitos",
      "Modelação de tópicos",
      "Clustering de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47635",
    "title": "Database preservation toolkit: a relational database conversion and normalization tool",
    "abstract": "Databases are one of the main technologies supporting organizations’ information assets,and very often these databases contain information that is irreplaceable or prohibitivelyexpensive to reacquire. The digital preservation field attempts to maintain this kind ofinformation accessible and authentic for multiple decades, but the complexity commonlyfound in databases and the incompatibilities between database systems make it difficult topreserve this kind of digital object.The Database Preservation Toolkit is a software that automates the migration of relationaldatabases to the second version of the Software Independent Archiving of RelationalDatabases format. Furthermore, this flexible tool that supports the current most popularRelational Database Management Systems can also convert a preserved database back toa Database Management System, allowing for some special usage scenarios in an archivalcontext. The conversion of databases between different formats, whilst retaining the databases’significant properties, poses a number of interesting issues, which are described inthis document, along with their current solutions.To complement the conversion software, the Database Visualization Toolkit is introduced,a software tool that provides access to preserved databases, enabling a consumer to quicklysearch and explore a database without knowing any query language. The viewer is capableof handling big databases as well, promptly presenting results of searching and filteringoperations on millions of records.This work covers the challenges of relational database preservation, and the developmentof a format and tools that play an important role in successfully preserving this kind ofinformation.",
    "authors": [
      "Ferreira, Bruno Alexandre Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81325",
    "title": "Bacteriophage-host determinants: identification of bacteriophage receptors through machine learning techniques",
    "abstract": "Bacterial resistance to antibiotics is nowadays becoming a major concern. Several reports indicatethat bacteria are developing resistance mechanisms to various antibiotics. Moreover, the processes involvedin the development of new antibiotics are lengthy and expensive. Therefore, an alternative to antibioticsis needed. One promising alternative are bacteriophages, viruses that specifically infect bacteria,causing their lysis. Hence, it would be interesting to discover which bacteria a specific phage recognizes.The bacterial receptors determine phage specificity, using tail spikes/fibres as receptor binding proteinsto detect carbohydrates or proteins, in bacterial surface. Studying interactions between phage tail spikes/-fibres and bacterial receptors can allow the identification of interaction pairs. Machine learning algorithmscan be used to find patterns in these interactions and build models to make predictions.In this work, PhageHost, a tool that predicts hosts at a strain level, for three species, E. coli, K.pneumoniae and A. baumannii was developed. Several data was extracted from GenBank, retrievinggeneral, protein and coding information, for both phages and bacteria. The protein data was used tobuild an important phage protein function database, that allowed the classification of protein functions,namely, phage tail spikes/fibres. In the end, several machine learning models with relevant protein featureswere created to predict phage-host strain interactions. Compared with previously performed works, thesemodels show better predictive power and the ability to perform strain-level predictions. For the best model,a Matthews correlation coefficient (MCC) of 96.6% and an F-score of 98.3% were obtained. These bestpredictive models were implemented online, in a server under the name PhageHost (https://galaxy.bio.di.uminho.pt).",
    "authors": [
      "Araújo, Pedro Henrique Matela Aidos Manso de"
    ],
    "keywords": [
      "Bacteriophages",
      "Phages",
      "Host prediction",
      "Bacterial strain",
      "Machine learning",
      "Bacteriófago",
      "Fago",
      "Previsão de hospedeiro",
      "Estirpe bacteriana",
      "Aprendizagem máquina",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82514",
    "title": "Development of film production software for a clinical application of advanced MRI technologies",
    "abstract": "A Imagem de ressonância magnética (IRM) é uma técnica que visualiza as estruturas internas do corpo atravésde campos magnéticos poderosos e ondas de rádio, e ao contrário dos Raios-X, um exame de ressonância magnéticanão usa radiação. Normalmente esta técnica é usada para detecção de doenças, monitoramento de tratamento ediagnóstico. Tem um grande impacto na Neurociência desde que começou a ser desenvolvida nos anos 70 e 80,especialmente porque a Ressonância Magnética funcional (IRMf) e a Ressonância Magnética de difusão (IRMd)aumentam a eficiência da ressonância magnética, embora as técnicas de IRMf e IRMd tenham tido muito poucoimpacto nas aplicações clínicas.Os estudos e resultados recentes da IRMf têm sido progressivamente aprovados por médicos e pesquisadores,pois são capazes de fornecer informações únicas sobre as funções cerebrais, no entanto, quando se trata demelhorar o atendimento clínico, os resultados têm sido muito limitados e a utilização de estímulos audiovisuaisavançados de IRMf foram quase nulos. Uma das abordagens recentes que é adequada para criar um estímuloaudiovisual avançado de IRMf, e é útil em aplicações clínicas (por exemplo, Transtorno de Déficit de Atenção eHiperatividade (TDAH)) é a IRMf cinematográfica. Um dos problemas principais das técnicas cinematográficas deIRMf é identificar uma determinada patologia que varia muito entre todos os filmes e patologias, ou seja, encontrare fazer um filme adequado para cada patologia é muito caro e demora muito tempo.Neste projeto, iremos explicar o desenvolvimento de programas de software que estão interligados a um cérebro-máquina para que os scripts possam ser transformados em animação por computador.",
    "authors": [
      "Santos, Pedro Afonso Rodrigues"
    ],
    "keywords": [
      "Produção de filme",
      "Edição de guião",
      "Reconhecimento de palavras",
      "Realidade virtual",
      "Multimedia",
      "Movie making",
      "Screenplay editing",
      "Word recognition",
      "Virtual reality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81340",
    "title": "ONTODL+: an ontology description language and its compiler",
    "abstract": "Ontologies are very powerful tools when it comes to handling knowledge. They offer a goodsolution to exchange, store, search and infer large volumes of information. Throughout theyears various solutions for knowledge-based systems use ontologies at their core.OntoDL has been developed as a Domain Specific Language using ANTLR4, to allow forthe specification of ontologies. This language has already been used by experts of variousfields has a way to use computer-based solutions to solve their problems.In this thesis, included on the second year of the Master degree in Informatics Engineering,OntoDL+ was created as an expansion of the original OntoDL. Both the language andits compiler have been improved. The language was extended to improve usability andproductivity for its users, while ensuring an easy to learn and understand language. Thecompiler was expanded to translate the language specifications to a vaster array of languages,increasing the potential uses of the DSL with the features provided by the languages.The compiler and some examples of the DSL can be downloaded at the website https://epl.di.uminho.pt/∼gepl/GEPL DS/OntoDL/ created for the application and presented inthe final chapters of the thesis.",
    "authors": [
      "Dias, Alexandre Costa"
    ],
    "keywords": [
      "Ontology",
      "Domain specific language",
      "Automatic code generation",
      "Ontologia",
      "Linguagem de domínio específico",
      "Geração automática de código",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27820",
    "title": "An evaluation of key-agreement protocols based on weak-secrets",
    "abstract": "Two agents want to securely communicate on a insecure channel in the presenceof an adversary. For that they agree in a strong cryptographic key based ona weak-source of randomness stemming from the physical network characteristicswhere these agents communicate. In this dissertation we evaluate the tradeo s betweentwo protocols: an information theoretically-secure Authenticated Key Agreement(AKA) [7] that was speci cally designed for this scenario; and a Password-Authenticated Key Exchange (PAKE) protocol [11] whose security guarantees arebased on computational arguments. To this end, we carry out an analysis of theconcrete security of both protocols, considering in both cases that the goal is toagree on a fresh 128-bit secret key.",
    "authors": [
      "Fernandes, Tiago Miguel Soares"
    ],
    "keywords": [
      "681.3-7"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3-7"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92626",
    "title": "Desenvolvimento de uma plataforma de gestão de franchisings",
    "abstract": "Com a constante evolução da economia e da sociedade, surge a crescente preocupação emestabelecer e manter modelos de negócios que não sejam apenas lucrativos, mas tambémsustentáveis a longo prazo. Entre esses modelos, destaca-se o sistema de franchising, que,como qualquer empreendimento, enfrenta desafios complexos e singulares no quotidiano dasua gestão.Ora, na presente dissertação, apresenta-se o desenvolvimento de uma plataforma de gestãode franchisings, criada em colaboração com a empresa Wintouch. Este sistema representaráum valioso complemento à oferta de produtos já disponibilizados pela referida empresasobre o mercado de software de gestão comercial.Dessa forma, foi concebida uma aplicação web, e todos os componentes associados, quevisam abordar os diversos domínios que envolvem a gestão de franchisings. Essa solução foiprojetada para atender às necessidades de dois tipos distintos de utilizadores intervenientesnum negócio de franchising, nomeadamente, os franchisadores e os franchisados. Ainda, aplataforma desenvolvida abrange uma ampla gama de funcionalidades, desde a manutençãoda rede de franchising até a disponibilização de sistemas para controlar as encomendas e demonitorização do desempenho dos franchisados.Por fim, considerando a ambição e complexidade inerentes a este projeto de desenvol vimento de software, foram adotados um planeamento e estratégia bem definidos, quecompreenderam três fases distintas: uma investigação inicial sobre o tema em questão, amodelação cuidadosa e robusta da solução a ser desenvolvida e, por fim, o desenvolvimentoefetivo da plataforma. Todas essas fases estão minuciosamente documentadas ao longodeste relatório.",
    "authors": [
      "Duarte, Luís Pedro Martins"
    ],
    "keywords": [
      "Aplicação web",
      "Gestão",
      "Franchising",
      "Rede de franchising",
      "Franchisador",
      "Franchisado",
      "Encomenda",
      "Auditoria",
      "Web application",
      "Management",
      "Franchising network",
      "Franchisor",
      "Franchisee",
      "Order",
      "Audit",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84082",
    "title": "Atomius: an application to support Bosch internal processes",
    "abstract": "Nowadays, customer satisfaction is highly relevant in the business area, as it is only possible to design, produce and supply a product if a customer or partner is interested and invests in it. And so, it is extremelyimportant to manage with as much rigor and care the product to be placed on the market as the buyer,costumer or group that will use it.Atomius is an application used by Bosch’s chemical team to manage products which need to beanalyzed at a microscopic level and in detail in order to verify if a product has any defects, if it is damagedor if it has any other type of imperfection.This analysis aims to confirm any problem with equipment, whether notified by the factory, whetherpartner or customer, will have a cost for the company depending on its origin.This dissertation, written in a business context, describes the process of defining and monitoring thedevelopment of an interactive platform to support the analysis of products uploaded due to the need ofverifying their composition. It allows chemical components to be associated with each product, as well asorganize them into groups by state of their analysis and draw conclusions on these results.This allows drawing conclusions about the origin of the defect or imperfection, leading to the correctionand verification of the remaining products at the production level or verifying if it is a unique case to beresolved.",
    "authors": [
      "Rocha, Tânia Filipa Amorim da"
    ],
    "keywords": [
      "Atomius",
      "Analysis",
      "Production",
      "Security",
      "Application",
      "Software development",
      "Análise",
      "Produção",
      "Segurança",
      "Aplicação",
      "Desenvolvimento de software",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84381",
    "title": "Where@UM – Where is the classroom for my next lecture? – The problem of the space’s geometry",
    "abstract": "In recent years more and more complex structures have been built. Buildings and locations whichusers must navigate efficiently so they can reach their appointments in a timely fashion, such as hospitals,universities and airports. Unfortunately technologies such as GPS are not well adapted to indoor locationsand therefore do not provide a solution to this problem. Indoor mapping has been subject to increasedamounts of research in the past few years and a plethora of different solutions have started to arisealthough none completely fulfill every requirement this problem presents. This thesis is done in conjunctionwith others with the final objective of creating the prototype of a mobile application and system that will beable to precisely locate where a user is inside of an indoor location through showing them their location ona floorplan. It will more specifically focus on the modeling aspect of the space geometry in an efficient waythat can be used by this application. The purpose of this dissertation is to document the research doneto choose the most appropriate data format, the development of a conversion method of the availabledata to the chosen format, and the development of web services and mobile application components thatwill provide this information to the end user. Additionally the development of a web application that, withthe results obtained throughout this investigation process, helps keep track of the progress of the radiomapping will also be documented.",
    "authors": [
      "Araújo, Ulisses Tiago Simões"
    ],
    "keywords": [
      "Indoor positioning",
      "Floorplans service",
      "GeoJSON",
      "Mobile application",
      "Webservices",
      "Posicionamento indoor",
      "Serviço de plantas",
      "Aplicação mobile",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83610",
    "title": "Detection and classification of small impacts on vehicles based on deep learning algorithms",
    "abstract": "This thesis explores the detection of impacts that cause damage based on data retrieved by an accelerometerplaced inside a vehicle and subsequently classified by deep learning algorithms. The realworld application of this work inserts itself in the car sharing market, by providing an automated servicethat allows constant monitoring on the vehicle status.The proposed solution was set as an alternative to the current machine learning algorithms in use.Previous research showed that deep learning algorithms are achieving better performance results whencompared to non deep learning algorithms.We use data retrieved from two types of events: Normal driving and damage causing situations to testif the models are capable of generalising damage events. The approach to achieve this objective consistedin exploring and testing different algorithms: Multi Layer Perceptron (MLP), Convolutional Neural Network(CNN) and Recurrent Neural Network (RNN).Results revealed promising performance, with the MLP reaching a 82% true positive rate. Despite notmatching the result obtained by the current non deep learning algorithm allows us to assess that deeplearning is a strong alternative in the long term as more data is collected.",
    "authors": [
      "Nascimento, Bruno Manuel Macedo"
    ],
    "keywords": [
      "Impact detection",
      "Artificial intelligence",
      "Deep learning",
      "Neural network",
      "Signal processing",
      "Detecção de impactos",
      "Inteligência artificial",
      "Redes neuronais",
      "Processamento síntese de sinal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34151",
    "title": "Reconhecimento de voz multilingue para controlo de procedimentos endoscópicos",
    "abstract": "Os exames endoscópicos são prescritos em grandes quantidades, pois são eficazes no diagnóstico,baratos quando comparados com outros exames e estarem generalizados há muito tempo, pois podemser realizados em quase todos os hospitais. O resultado deste exame é normalmente um relatório queinclui anotações médicas complementadas com algumas imagens retiradas durante o exame.Alguns dos exames realizados são apenas feitos para confirmar informação já recolhida, o que leva a umaduplicação de esforços desnecessária e desperdício de recursos. Os profissionais de saúde podemdescartar informação relevante ao não conseguirem anotar em pormenor uma região de interesse paraposterior análise mais cuidada.O objetivo deste trabalho consiste na criação de um sistema que consiga resolver o problema apresentadoanteriormente, usando tecnologia de reconhecimento de voz. Este sistema deve reconhecer um pequenovocabulário, independentemente do falante, usado para anotar regiões de interesse nos exames.O sistema MyEndoscopy atua como uma cloud privada, que contém vários dispositivos que usam eprovidenciam serviços entre si. O dispositivo central deste sistema é a MIVbox, que se liga ao endoscópioe permite a captura digital do sinal de vídeo que este gera. A principal funcionalidade providenciada poreste sistema é a capacidade de armazenar indefinidamente os vídeos completos que são produzidosdurante exames endoscópicos, bem como disponibilizar estes vídeos e outros dados para outrosprofissionais de saúde que os necessitem de consultar.Nesta dissertação apresenta-se um módulo de reconhecimento de voz para línguas portuguesa e inglesa,denominado MIVcontrol, totalmente integrado no sistema MyEndoscopy. Este módulo reconhece umpequeno vocabulário, que consiste em comandos usado para controlar os outros módulos. O MIVcontrol éapresentado como uma alternativa a sistemas similares baseados na cloud, que resolve certos problemasrelacionados com proteção de dados e segurança.Foi realizado um estudo sobre o módulo desenvolvido para determinar a sua eficácia em comparação aoestado da arte. Na sequência desse estudo conclui-se que o sistema tinha uma taxa de erro comparável asistemas similares para outras línguas, e que como resultado é passível de ser usado em ambientes reais.",
    "authors": [
      "Afonso, Simão Pedro Oliveira"
    ],
    "keywords": [
      "616-07:681.3",
      "681.3:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616-07:681.3",
      "681.3:616-07"
    ],
    "subjects_fos": [
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83124",
    "title": "Orchestration and distribution of services in hybrid cloud/edge environments",
    "abstract": "The Edge Computing paradigm aims at leveraging the computational and storage capabilities of Internet of Things (IoT) devices, while resorting to Cloud Computing services for more demanding processingtasks that cannot be done at commodity devices. However, deploying distributed services across Edgeand Cloud nodes raises new challenges that must be addressed. Namely, the choice of what nodes runeach service component may be critical for ensuring an efficient service for users. For example, if twocritical components, that must frequently exchange data, are placed in different geographic locations, thewhole performance of the service will be affected. Therefore, these geographically dispersed environmentsdemand new orchestration and distribution systems for hybrid Cloud and Edge environments, based ongeographic location, service demand, business objectives, laws, and regulations.This thesis proposes Geolocate, a generic scheduler for workload orchestration and distribution acrossheterogeneous and geographically distant nodes. In more detail, it provides the design and implementationof a scheduling and placement algorithm based on nodes’ geographic location and resource availabilityand a fully functional prototype, integrating Geolocale with KubeEdge, an edge computing orchestrationplatform based on Kubernetes.The experimental results show that as the network latency and amount of data being transmittedbetween nodes increases, so does the response time for applications resorting to these distributed deployments. Our evaluation of an e-commerce application shows that the use of Geolocate can reduce, relativeto KubeEdge’s default-scheduler, the average response time for requests by about 85%.",
    "authors": [
      "Vilaça, João Pedro Machado"
    ],
    "keywords": [
      "Scheduling",
      "Edge computing",
      "Containers",
      "Kubeedge",
      "Escalonamento",
      "Computação em borda",
      "Containers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34146",
    "title": "Apoio ao diagnóstico das articulações coxofemorais para despiste de displasia congénita da anca",
    "abstract": "A displasia congénita da anca é uma doença esquelética congénita comum em recém nascidos. O seudiagnóstico é importante para evitar complicações tardias no crescimento e locomoção. Por ser um exametão complexo e de grande responsabilidade, os diagnósticos feitos pelos profissionais são muitas vezesassociados a um grau elevado de incerteza na decisão, provocando receio na realização de exames dogénero. Os atos complementares de diagnóstico, neste caso a construção de ferramentas de apoio, sãosem dúvida o maior passo para reduzir ou eliminar este problema. Desta forma, com profissionais maisinstruídos, consegue-se um diagnóstico mais seguro e fiável.São apresentadas recomendações para a realização do exame, englobando parâmetros como a realizaçãodo exame clínico, do exame de ecografia e da leitura de imagens de ecografia. As imagens de ecografiatêm imenso ruído e para permitir um melhor processamento foram experimentadas operações básicas deprocessamento de imagem. É também proposto um relatório normalizado para este exame. O benefício daimplementação do relatório é a sua ligação ao sistema de machine learning em que informaçõescolocadas nos campos de preenchimento do relatório seriam transformadas em metainformação dasimagens de ecografia guardadas também no relatório, funcionando como a alimentação do sistema. Estesistema permitiria avaliar e classificar imagens de ecografia de um exame às articulações coxo-femorais.Para além destas ferramentas descritas, é proposto uma para otimizar em termos práticos o exame - umsistema de comandos por voz com ligação ao ecógrafo para que o profissional não tenha de desviar aatenção para carregar num simples botão do ecógrafo para assinalar frames essenciais para o diagnóstico.A adoção de ferramentas de apoio ao diagnóstico da displasia congénita da anca que permitam melhorara prestação dos cuidados de saúde é uma necessidade. As ferramentas apresentadas são um contributo erepresentam o início de novas abordagens ao despiste desta anomalia.",
    "authors": [
      "Gomes, Hugo Manuel Peixoto"
    ],
    "keywords": [
      "616.718.1",
      "616-07:681.3",
      "681.3:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616.718.1",
      "616-07:681.3",
      "681.3:616-07"
    ],
    "subjects_fos": [
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/40864",
    "title": "Deteção e classificação de regiões de interesse em vídeos de endoscopia",
    "abstract": "As técnicas de Endoscopia Digestiva Alta e de Colonoscopia são fundamentais na prestação de Cuidados deSaúde Primários, pois permitem ao profissional de saúde validar o diagnóstico e prescrever o tratamentomais adequando. Estas técnicas geram vários tipos de resultados, dos quais se destacam os vídeosendoscópicos, uma vez que desempenham um papel preponderante no rastreio de patologias ou de lesõesque possam estar presentes no Trato Digestivo.As tarefas de visualização e análise dos vídeos endoscópicos, subsequentes à realização dos exames, podemvariar entre os 2 a 32 minutos para a Endoscopia Digestiva Alta e entre os 20 minutos a 1 hora para aColonoscopia, para cada vídeo. Tal implica uma demora significativa na interpretação dos dados, comconsequências ao nível da fadiga e de diagnósticos erróneos por parte do profissional de saúde. Para alémdeste problema, identifica-se um outro, relacionado com falta qualidade da imagem captada durante osexames. Esta pode muitas vezes encontrar-se desfocada, podendo obstar a presença de uma dada patologiaou lesão.É com base no panorama descrito previamente que se justifica o desenvolvimento de soluções inovadorasque permitam colmatar os problemas acima identificados, particularmente, o processamento e a análise devídeos endoscópicos de longa duração e identificação de informação não relevante para o diagnóstico.Uma das soluções engloba a eliminação de frames capturados fora do Trato Digestivo e permitiu obtervídeos endoscópicos reduzidos e, consequentemente, uma poupança de tempo utilizado nas tarefas devisualização e análise dos mesmos, na ordem dos 45,6 %, para o caso das Endoscopias Digestivas Altas ede 56 %, para as Colonoscopias. A solução referente à eliminação de frames desfocados permitiu não só terganhos de tempo, 4,6 %, para Endoscopias Digestivas Altas, e 4,8 %, para Colonoscopias, como de tamanhode armazenamento dos vídeos endoscópicos reduzidos, de 4,1 %, para Endoscopias Digestivas Altas, e de4 %, para Colonoscopias. Em ambas as soluções foi identificado um fator limitativo, o aumento do bit rate,no entanto os valores obtidos não vão influenciar o diagnóstico por parte do profissional de saúde.",
    "authors": [
      "Gomes, Margarida Maria Freitas de Oliveira"
    ],
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82787",
    "title": "Gestão de segurança de informação para sistemas de Confiança Seguros",
    "abstract": "Com o aumento da nossa dependência nos sistemas de informação também aumenta anecessidade de sistemas mais seguros e resilientes.A pandemia que vivemos, há mais de um ano, veio agravar a situação e mostrou quetemos de preparar os sistemas que suportam o nosso dia-a-dia para situações inesperadas eque podem comprometer o seu bom funcionamento.Para proteger os sistemas é importante aplicar medidas preventivas. Existem standards quedefinem as melhores práticas para a segurança dos sistemas, que podem ser implementadospelas organizações para melhor se prepararem contra situações adversas. Destacam-se osstandards desenvolvidos pelo International Organization for Standardization (ISO), na área degestão de segurança de informação, e pelo National Institute of Standards and Technology(NIST), na área de sistemas de confiança seguros.Cada vez mais a preocupação com a segurança da informação tem-se reflectido nalegislação e regulamentação Europeia e Portuguesa.Esta dissertação pretende analisar as melhores práticas na área da segurança de informação,através dessa análise, propor uma abordagem para a sua implementação e utilizá-la numcaso prático, sendo este a infraestrutura de chave pública do Cartão de Cidadão.Desta forma, ao longo desta dissertação são analisados os standards relevantes desen volvidos pelo ISO e NIST. Além disso, com o objectivo de contextualizar o caso prático éanalisada a regulamentação e legislação aplicável às infraestruturas de chave pública naEuropa e em Portugal bem como as componentes da infraestrutura de chave pública doCartão de Cidadão.Com esta análise, foi possível apresentar uma abordagem que reduz a complexidade doprocesso de implementação dos standards e colocá-la em prática num projecto de reestrutura ção e actualização da gestão de segurança da informação da infraestrutura de chave públicado Cartão de Cidadão.",
    "authors": [
      "Cunha, Joana Fernandes"
    ],
    "keywords": [
      "Segurança da informação",
      "Gestão de segurança da informação",
      "Sistemas de Confiança Seguros",
      "Gestão de risco",
      "Gestão de incidentes",
      "Infraestrutura de chave pública",
      "Regulamento eIDAS",
      "Information security",
      "Information security management",
      "Trustworthy secure systems",
      "Risk management",
      "Incident management",
      "Public key infraestructure",
      "eIDAS Regulation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27871",
    "title": "Knowledge extraction from the behaviour of players in a web browser game",
    "abstract": "The analysis of the player’s behaviour is a requirement with growing popularity in the traditionalcomputer games segment and has been proven to aid the developers create better and moreprofitable games. There is now interest in trying to replicate this attainment in a less conventionalgenre of games known as web browser games.The main objective of this work is to analyse and create a technique for the analysis of thebehaviour of the players inside a web browser game. For this analysis a system to automaticallycollect, process and store the relevant data for the referred analysis was developed. The webbrowser game used as a case study for this work is developed by 5DLab and is called Wack-a-Doo. The work developed focused on creating short-term prediction models using the informationcollected during the first days of playing for each player. The objectives of these models are topredict the time played or the conversion state of the players. With the study of the createdmodels it was possible to extract results that provide potentially useful information to increase theprofitability of Wack-a-Doo.",
    "authors": [
      "Alves, João Miguel Pereira"
    ],
    "keywords": [
      "681.324"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79850",
    "title": "Using deep learning for unobtrusive sleep stage classification",
    "abstract": "Sleep represents a fundamental role to our well-being and today, as sleep disorders become more and more common, there is a growing necessity to monitor our sleep quality daily. Unobtrusive automatic sleep stage classification has made a tremendous breakthrough in this subject allowing regular users to monitor their sleep with day-to-day wearables, such as Fitbit Charge 2 tracker, contrary to the traditional manual sleep scoring based on polysomnography (PSG). Usingcardiorespiratory signals to sleep stage has attracted increased attention as these signals can be obtained through unobtrusive techniques and have potential for continuous daily application. Therefore, in this thesis, deep learning frameworks based on Long-short-memory networks (LSTMs) and Convolutional Neural Networks (CNNs) are used to sleep stage classify, either just using respiratory effort signals, for example obtained from respiratory inductance plethysmography (RIP), or using the combination of respiratory and cardiac features, often based on heart rate variability (HRV) calculated from electrocardiogram (ECG). The dataset used was the SIESTA dataset that contains a total of 294 subjects (588 PSG recordings) of which 197 are healthy subjects, 51 suffer from obstructive sleep apnea syndrome (OSA), and the remaining from a variety of sleep or sleep related disorders. The classification problem was divided in a three-class and four-class sleep stage classification problem. As for the results, it was obtained with respiratory data for three stages classification (Wake, rapid eye-movement (REM) and non-REM stages) a Cohen’s kappa (𝜅) of 0.46 for the overall pool of subjects (All), 0.50 for healthy subjects and 0.34 for OSA subjects. For four stages classification (Wake, REM, light sleep (N1/N2) and deep sleep (N3/N4) stages) it was obtained a Cohen’s Kappa (𝜅) of 0.40 for the subject pool containing all subjects (All), 0.44 for healthy subjects and 0.31 for OSA. With cardiorespiratory data, for four stages classification, it was obtained a 𝜅 of 0.40 for the overall subject pool (All), 0.44 for healthy subjects and 0.30 for OSA subjects. With three stages, a 𝜅 of 0.46 for All subjects, 0.51 for healthy and 0.32 for OSA subjects. These results demonstrate that, with the developed frameworks, it is possible to achieve fairly good results as they are similar, in some cases moderately higher, to the current state-of-the-art but fail to generalize well, as significant differences can be found between subject types (All, Healthy and OSA).",
    "authors": [
      "Prata, Marco André Ramos Dias"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94074",
    "title": "Ransomware behaviour analysis in Linux environment",
    "abstract": "n the current spectrum of the world, there’s been a significant increase in cybersecurity threats, one ofeach is ransomware threats from whom Linux environments have become a more recent target comparedto Windows environments and the area of study of ransomware in Linuxsystems have been relatively under studied compared to their Windows counterparts, despite the growing diversity of Linux operating systemsand their significance in various infrastructures. To address this research gap, this dissertation conductsa comprehensive investigation into the behaviour of ransomware in a compromised Linux system. Thestudy employs both Static and Dynamic analyses to gain insights into the behaviours and characteristicsof three distinct ransomware payload families, Avos, RansomExx, and REvil.To test the analysis, a customized sandbox environment has been developed using VirtualBox (Oracle,2023) and the Lubuntu operating system. A set of automated tests have been created with shell andpython scripts to carry out Dynamic analysis tests.The Static Analysis uncovered that these payload families contained visible strings associated withmalicious activities, and some of the executables presented obfuscation to challenge reverse engineeringefforts. This suggests a strong likelihood that the payload’s primary objective is malicious.In the Dynamic Analysis, examining ransomware payloads in action yielded valuable insights. Notably,the analysis uncovered that the overall behaviour is significantly similar between all three families studied.The research also identified the influence of the number of files and file size on the ransomware behaviour,particularly in terms of execution time and the number of executions by the system calls. This effecthighlights the importance of specific system calls, such as write, read, lseek, stat, and newfstat. Thissystem calls plays a crucial role in the behaviour of ransomware payloads across the families studied, andthey also highlight the similarities with file-intensive programs with no maliciousness.Intricate relationships are also observed between lseek,read, and write system calls, indicating efficientfile manipulation by the ransomware.Some patterns found that might help distinguish the payloads from non-malicious behaviour are theintriguing pattern discovered between error behaviour and the system call futex. In the payloads where the system call futex is present, the majority of the errors that occurred during the payloads execution arerelated to this system call this pattern although not as evident is also replicated in the clock_nanosleepsystem call behaviour.A distinguishable pattern found in the rename and chmod system callsis that the number of executionsis the same as the number of files present in the directory encrypted.This research also allowed to detect in the studied families, a process behaviour that showed specificmalicious intentions targeting VMware systems, confirming the findings from (VMware, 2022).In the realm of future research, this study paves the way for further exploration of the significance ofspecific system calls in ransomware payloads and their responses to various environmental factors. Thisunderstanding can significantly enhance ransomware detection methods in Linux systems. Moreover, itsets the stage for future investigations into Linux ransomware across diverse scenarios, including differentLinux operating systems, containerized environments, and a wide array of ransomware payloads, such asLocker Ransomware.",
    "authors": [
      "Dias, Ricardo Cunha"
    ],
    "keywords": [
      "System calls analysis",
      "Ransowmare",
      "Behaviour analysis",
      "Information security",
      "Linux security",
      "Análise de chamadas ao sistema",
      "Ransowmare",
      "Análise comportamental",
      "Segurança da informação",
      "Segurança em Linux",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34149",
    "title": "Nomenclaturas e ontologias : plataformas de eHealth e mHealth",
    "abstract": "Os Sistemas de Informação (SI) das Unidades de Saúde recorrem, cadavez mais, a ferramentas informáticas para gerir a grande quantidade de informação, e assim garantir a qualidade e a segurança da mesma.A interoperabilidade semântica é uma urgência nos Sistemas de Informação de Saúde (SIS), pois, através de normas, permite a uniformização dostermos médicos, assegurando registos clínicos com informação fiável, semredundância e ambiguidade, conferindo qualidade e segurança à informação.Sem terminologias médicas a prestação de cuidados de saúde pode tornaruma tarefa complexa e conduzir a erros médicos, pelo que a utilizaçãodas mesmas é fulcral para o registo de diagnósticos, procedimentos e peçasanatómicas no Registo Clínico Eletrónico (RCE) de cada utente.Como tal, o desenvolvimento de uma plataforma de interoperabilidadesemântica vai permitir uniformizar termos médicos e conduzir à diminuiçãode erros. Recorrendo às tecnologias mais avançadas de eHealth e mHealth,pretende-se implementar o Systematized Nomenclature of Medicine ClinicalTerms (SNOMED CT) em contexto hospitalar real, num serviço de AnatomiaPatológica.A solução consiste numa aplicação independente da plataforma de registode relatórios médicos, utilizando Web Services, que proporcionam a interação humana com diferentes interfaces para diferentes tipos de dispositivoseletrónicos.",
    "authors": [
      "Amorim, Magda Gonçalves"
    ],
    "keywords": [
      "614:681.3",
      "681.3:614",
      "61:001.4",
      "001.4:61",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "614:681.3",
      "681.3:614",
      "61:001.4",
      "001.4:61"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/95162",
    "title": "Fluorination of graphene for controlled wetting properties and improved cell adhesion",
    "abstract": "Estudos de interfaces celulares são fundamentais para a compreensão do comportamento e das interações celulares em sistemas biológicos complexos, particularmente em neurociência, onde as interfaces entre células neuronais e seu microambiente são essenciais para a investigação da função cerebral, desenvolvimento e transtornos neurológicos. Avanços recentes em nanotecnologia facilitaram o desenvolvimento de biomateriais ideais para interagir com células neuronais, com o grafeno emergindo como um candidato promissor. Para ser considerado amigável às células neuronais, uma interface ideal deve possuir alta wettability e condutividade elétrica. Embora o grafeno pristino tenha alta condutividade intrínseca, as aplicações biológicas são limitadas pelo comportamento de baixa hidrofilia. Funcionalização com grupos químicos específicos permite ajustar a wettability do grafeno e ampliar suas aplicações.Este trabalho tem como objetivo desenvolver interfaces inovadoras de grafeno para células neuronais. Para aprimorar as suas propriedades de superfície (i.e., wettability), filmes de grafeno foram funcionalizados com grupos químicos de flúor após serem transferidos para substratos de politereftalato de etileno (PET) flexíveis, eletricamente isolantes e biocompatíveis. Os efeitos de fluorinação na wettability do grafeno ainda são enigmáticos devido a resultados inconsistentes na literatura. Portanto, a fluorinação e a caracterização do grafeno em diferentes substratos (Si e Si/SiO2) foram realizadas para uma melhor compreensão desses efeitos. Além disso, métodos para a geração seletiva de áreas de grafeno funcionalizado foram explorados para aplicação de controle celular aprimorado.Para obter a amostra de substrato ideal, foi necessário testar vários métodos e parâmetros de preparação e identificá-los como promissores em relação às propriedades finais da amostra. O ângulo de contato com a água (WCA) e o método de quatro pontas foram usados para monitorar a wettability e a resistência de folha das amostras, respetivamente. Além disso, a composição química e a estrutura cristalina foram caracterizadas por espectroscopia Raman e fotoeletrônica de raios-X, e os efeitos na topografia foram analisados com perfilômetro de contato e microscopia de força atômica. Testes de células cancerígenas foram realizados por nossos colaboradores no INL para resultados preliminares em adesão celular.",
    "authors": [
      "Moreira, Gabriel Cabral de Freitas Lopes"
    ],
    "keywords": [
      "Wettability",
      "Adesão celular",
      "Grafeno",
      "Fluorinação",
      "Cell adhesion",
      "Graphene",
      "Fluorination",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47636",
    "title": "An elasticity controller for applications orquestrated with Cloudify",
    "abstract": "Cloud computing is a widely adopted model for the management of computing resources.Elasticity, an important characteristic of cloud computing, is the ability to allocate andrelease computing resources according to demand. An elasticity controller has the responsibilityof performing decisions regarding when resources are provisioned or releasedand which types of resource are necessary. Developers have expectations of allocating theleast amount of resources required and reaching optimal states using the least numberof costly actions. Therefore, the task of controlling elasticity is challenging, especially incases where the elastic application is composed of components with complex dependenciesamong themselves.In this documentation, we introduce CEController, an elasticity controller for applicationsorchestrated with Cloudify. CEController introduces a novel elasticity strategy thattakes into account dependencies between components and differences between metric dimensions.CEcontroller is evaluated in an environment created to test the controller, whichincludes the adaptation of a previously used application, and use of a load generation tool.Finally, we discuss the results obtained using CEController in a web application and discussthe results.",
    "authors": [
      "Fernandez Afonso, Carlos Eduardo"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47721",
    "title": "Avaliação do consumo energético de consultas em document stores",
    "abstract": "Neste momento marcante do meu percurso académico quero agradecer a todos aqueles que, de alguma forma, contribuíram para a concretização dos meus objetivos. Assim, dirijo as primeiras palavras ao meu orientador, o Professor Doutor Orlando Belo, pela sua disponibilidade e pelo seu apoio constante. A sua motivação, as suas sugestões e as suas críticas foram fundamentais. A sua experiência recheada de sabedoria foi um elemento-chave ao longo do desenvolvimento da presente dissertação. A minha mãe merece um agradecimento especial, pela compreensão e pelo apoio diário. A conﬁança que esta deposita em mim foi importante nas mais diversas tomadas de decisão ao longo da minha jornada académica. Sem ela, as vitórias alcançadas ao longo de todos estes anos de aprendizagem não seriam possíveis. Devo à minha mãe tudo o que sou e tenho. Ela está sempre ao meu lado, é um exemplo de ser e de estar. Dirijo, ainda, um agradecimento à Bárbara, pela paciência, motivação, apoio e amizade. Pelo seu empenho e pela sua valorização do meu trabalho. Por me apoiar acima de tudo nos momentos mais difíceis. Pelas suas sugestões de melhoria que nunca me deixaram desistir e que me mostraram sempre o melhor lado de todas as coisas. Por ﬁm, um último agradecimento, aos meus colegas e amigos, que me apoiaram e que me ajudaram. Estes estiveram presentes quer nos momentos de partilha de conhecimento, quer nos momentos de lazer.",
    "authors": [
      "Duarte, Duarte Nuno Ferreira"
    ],
    "keywords": [
      "Centros de dados",
      "Sistemas de bases de dados",
      "Document stores",
      "Consumo de energia em sistemas de bases de dados",
      "Green queries",
      "Data centers",
      "Database systems",
      "Energy consumption database systems",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28944",
    "title": "Interpretador SMIL de alta performance para controlo de apresentações multimédia num servidor de streaming de media para dispositivos móveis",
    "abstract": "O Synchronized Multimedia Integration Language (SMIL) é um padrão definido pelo World Wide Web Consortium (W3C), baseado na eXtended Markup Language (XML), usada no controlo de apresentações multimédia. Esta linguagem é usada principalmente nos Serviços de Mensagens Multimédia mas, actualmente, também é usada nos High Definition DVD para interactividade e para vídeos na Internet.Este documento descreve de que forma o Ambulant Player, um animador de SMIL open-source, foi modificado para controlar apresentação multimédia fornecidas por um Media Server para dispositivos móveis. A implementação modular do Ambulant baseada em code factories permitiu que o seu módulo de visualização fosse substituído por um mais simples. Este novo módulo envia mensagens para um Media Server em vez de reproduzir os elementos de média. Usando o Ambulant Player como base para este interpretador tornou o desenvolvimento mais rápido e permitiu obter uma ferramenta que respeite a recomendação do W3C de uma forma simples.Como resultado final obteve-se um interpretador que demonstrou ser robusto, suportando cento e oitenta sessões concorrentes e servindo cerca de sessenta mil sessões sem erros.",
    "authors": [
      "Cadinha, João Pedro Domingues"
    ],
    "keywords": [
      "681.3.062"
    ],
    "date": "2009",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.062"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79867",
    "title": "Efficient computational methods to index crystallographic (S)TEM images and ED patterns",
    "abstract": "Electron Microscopy (EM) of nanomaterials relies on grey-scale images to display the material’s atomic arrangement, and a high resolution EM can simultaneously capture multipleatomic structures into a single image. However, the extraction of useful information fromthese images is still limited to the determination of the material’s orientations, an underutilisation of the powerful features of an EM equipment and less productive EM sessions.This is due to the compute-intense tasks that have not been automated yet.This dissertation aims to significantly reduce the time required to extract useful data fromEM images and to remove the user bias when analysing high resolution (S)TEM images, byautomating most user routine tasks and integrating them into a software tool, Im2Cr.The deployed Im2Cr tool aimed to aid an EM user to find the most probable atomicstructure orientation of a nanomaterial in a single 2D image from a set of pre-definedmaterials, with a minimal user interaction.Im2Cr was designed and built with a simple and intuitive Graphical User Interface (GUI)that runs on a common modern laptop. It takes as input a high resolution (S)TEM imageand multiple CIF files with candidate atomic structures to describe the material underobservation. After performing the Fourier Transform (FT) on selected Regions Of Interest(ROI) in the image, the tool automatically detects periodic information related to the atom’spositions by the brighter spots on the image FT. With a set of geometric computations ittries to match the theoretical values computed with the measured ones by assigning acustom made merit index. This quantitative evaluation avoids possible user bias and/orerrors on image characterisation. Im2Cr outputs at the end a report with the best matchingcrystallographic structure, its orientation and the indexation table.This tool was successfully tested for robustness and execution efficiency in a wide rangeof high resolution (S)TEM images from crystalline nanomaterials, with domain size rangingfrom 4 to 100 nm. The autonomous indexation with preset parameters has a very highsuccess rate and runs in a small fraction of typical (S)TEM images acquisition time bytaking advantage of the inherent hardware parallelism. Alternatively, the user can changesome relevant parameters related to the ROI selection on the (S)TEM image and on the FTpeaks detection.Im2Cr promising results point to the possibility of real-time image analysis with reduceduser interaction, allowing for an increased (S)TEM characterisation yield and also enablingthe interpretation of complex images, such as those from nanocrystalline materials imagedin high-order zone axis orientations.",
    "authors": [
      "Silva, André Sá"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60074",
    "title": "Atendimento a utentes de unidades de saúde: uma abordagem baseada em realidade aumentada",
    "abstract": "As necessidades e expectativas dos clientes acompanham o avanço tecnológico que ocorre de forma exponencial, o que gera um ambiente muito competitivo entre as instituições que precisam de manter os clientes habituais e chamar novos. Deste modo, para que as instituições consigam permanecer e evoluir têm de conseguir um elevado grau de qualidade que só é possível caso se mantenham atentos às constantes mudanças sociais e tecnológicas. Na área da saúde este facto não é exceção e, adicionalmente, o fator qualidade é ainda mais importante, já que se lida com a vida dos pacientes. A saúde do utente não está dependente apenas do topo das tecnologias relacionadas com os cuidados de saúde diretos, portanto todas as tecnologias que permitem uma melhor organização dos dados do paciente, bem como um aumento na eficiência de todos os processos envolventes devem ser consideradas.Diversas instituições de saúde encontraram nos sistemas self-service a solução para tornar o processo de atendimento ao utente mais eficiente. No entanto, nas horas de maior afluência estes sistemas não são suficientes e as filas de espera persistem, causando interferências no bem-estar dos pacientes e no desenrolar de outros processos. Tirando proveito das oportunidades que esta quarta revolução industrial revela, problemas deste tipo podem ser minimizados, a experiência do paciente pode tornar-se mais rica e a instituição de saúde pode tomar um lugar de notoriedade. É neste conceito que surge esta dissertação, onde se conceptualiza uma solução para integrar um sistema de quiosques self-service, já existente em algumas unidades de saúde, não só para otimizar o atendimento dos utentes, mas também para modernizar a instituição. É desenvolvido também um possível protótipo desta solução. Para o seu desenvolvimento foi imperativo a utilização de conceitos de tecnologias promissoras, destacando-se a tecnologia mobile, realidade aumentada e WiFi, de modo a que a solução final agregue todas as funcionalidades dos quiosques com todas as vantagens das tecnologias utilizadas.",
    "authors": [
      "Guerra, Marta Alexandra Serapicos"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92823",
    "title": "Anomaly detection for preventive and predictive maintenance systems",
    "abstract": "Dado o aumento da preocupação relativamente às alterações climáticas, e políticas de redução dosefeitos das alterações climáticas, verifica-se cada vez mais um aumento na procura por veículos elétricos.Esta procura, implica alterações na rede de abastecimento de veículos com a necessidade de incluirestações de carregamento. Estas alterações, criaram condições para que novas empresas pudessemsurgir, como é o caso da We Can Charge. Porém, este aumento, implica também novos desafios namonitorização, manutenção e especificamente na deteção de anomalias nas mesmas.Atualmente, a We Can Charge utiliza o Open Charge Point Protocol (OCPP). Este é um protocolo estandard de código aberto, que permite a comunicação entre os diversos componentes de uma rede decarregamento de veículos elétricos, permitindo obter informação relevante sobre os mesmos pela análiseda informação enviada nos pacotes de comunicação. Através de uma ferramenta dedicada, é possível àWe Can Charge, detetar a ocorrência de anomalias. Apesar disto, a deteção das anomalias não é imediatae usa uma quantidade limitada da informação fornecida pelo protocolo, não utilizando a informação doestado dos conectores e transações de carregamento.Assim, o principal objetivo desta dissertação é analisar e utilizar os dados recolhidos sobre o estadodos seus conectores e transações de carregamento, aplicando-os a algoritmos de machine learning. Paratal, efetuamos a criação de vários modelos, um modelo de deteção em tempo real, onde obtivemos osmelhores resultados utilizando o algoritmo Isolation Forest, e dois modelos de previsão baseados emredes LSTMs, um relativo ao estado e outro ao número de erros reportados nos dados relativos ao estadodos conectores. Combinando estes modelos, foi possível a criação da Connectors Forecasting Network(CNF), que nos permite a previsão de anomalias futuras nas estações de carregamento.",
    "authors": [
      "Eiras, Eduardo Veloso"
    ],
    "keywords": [
      "Deteção de anomalias",
      "Machine learning",
      "Open charge point protocol",
      "Previsão de anomalias",
      "Redes de carregamento",
      "Veículos elétricos",
      "Anomaly detection",
      "Anomaly forecasting",
      "Charging networks",
      "Electric vehicles",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94071",
    "title": "Serviço de emissão de documentos para EUDI Wallet no formato SD-JWT",
    "abstract": "Atualmente vivemos numa sociedade que sofreu uma grande explosão tecnológica ao longo dos anos.Graças a isso, muitos progressos ocorreram e como um dos resultados, surgiram os documentos deidentificação em formato digital. Neste momento, na União Europeia, vários Estados-Membros possuemuma infraestrutura nacional que permite a emissão de documentos digitais e métodos que permitemautenticação digital, como no caso de Portugal com a infraestrutura Autenticacao.gov. Mas as aplicaçõese infraestuturas desenvolvidas por cada Estado-Membro apenas funcionam num contexto nacional. Pararesolver esse problema, foi apresentado um projeto piloto denominado por European Digital Identity Wallet(EUDIW). A EU Digital Identity Wallet, permitirá que os cidadãos e as empresas europeias partilhem dadosde identificação de uma forma segura e conveniente. Um dos formatos propostos para os documentosdigitais é o Selective Disclosure JWT (SD-JWT), uma variante de JSON Web Token (JWT) que possibilita quealguns dos atributos presentes nele podem ser seletivamente divulgáveis, possibilitando ao titular do JWTo controlo de quais atributos deseja partilhar com as diferentes Relying Parties que o cidadão interagir.Nesta dissertação será apresentado o serviço que suportará a emissão dos documentos e atestadosdigitais, EUDI Wallet Provider Backend, nomeadamente a componente que possibilitará a emissão dessesdocumentos em formato SD-JWT em conformidade com as regulamentações e standards que deverãoser cumpridas para certificar a salvaguarda e controlo do dados por parte do utilizador final.",
    "authors": [
      "Sá, Tomás Reis Ferreira de"
    ],
    "keywords": [
      "Identidade móvel",
      "Identidade digital",
      "PID",
      "mDL",
      "(Q)EEA",
      "Carteira digital",
      "Mobile identity",
      "Digital identity",
      "EUDIW",
      "SD-JWT",
      "SD-JWT-VC",
      "Digital Wallet",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36716",
    "title": "HTML5 - análise dos riscos de segurança e testes de penetração a aplicações web",
    "abstract": "A nova versão do HTML traz melhorias significativas relativamente à construcão deaplicacões web mais ricas. Contudo, com as novas funcionalidades vêm acopladosa elas sempre novos riscos de segurança que precisam ser analisados e colmatados.Anteriormente ao HTML5 já existiam determinadas ameaças de segurança queafetavam as aplicacões web (tais como SQLInjection, XSS, CSRF, etc), e queganham um novo potencial devido aos novos recursos do HTML. Este estudofoca precisamente a análise dessas ameaças bem conhecidas, em conjunto com aanálise dos riscos de segurança associados às novas funcionalidades do HTML5,assim como a apresentacão de regras para atenuacão das mesmas.Adicionalmente são apresentados um conjunto de módulos para detecão devulnerabilidades HTML5 em aplicacões web. As quais são originadas devido àmá utilizacão do HTML5 durante a fase de desenvolvimento. Esse conjunto demódulos corresponde a uma extensão adicionada a um Black Box Web ApplicationSecurity Scanner bem conhecido da OWASP designado ZAP.Isso implicou adicionar também algumas funcionalidades HTML5 a uma aplicacãoweb também da OWASP designada Wave ZAP, cujo objetivo é ser utilizada pararealizar testes de penetracão a fim de testar esses novos módulos do ZAP.",
    "authors": [
      "Gonçalves, Manuel Francisco Mendes"
    ],
    "keywords": [
      "HTM5",
      "Segurança",
      "Black Box",
      "681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59739",
    "title": "Validation of quantum simulations: assessing efficiency and reliability in experimental implementations",
    "abstract": "Quantum simulation is one of the most relevant applications of quantum computation for thenear future, due to its scientific impact and also because quantum simulation algorithms aretypically less demanding than generalized quantum computations. Ultimately, the success of aquantum simulation depends on the amount and reliability of information one is able to extractfrom the results. In such a context, this work reviews the theory behind quantum simulation,with a focus on digital quantum simulation. The concepts of efficiency and reliability inquantum simulations are discussed, particularly for implementations of digital simulationalgorithms in state-of-the-art quantum computers. A review of approaches for quantumcharacterization, verification and validation techniques (QCVV) is also presented. A digitalquantum simulation of the Schrödinger equation for a single particle in 1 spatial dimension wasexperimentally implemented and analyzed, along with a quantum state tomography procedurefor characterization of the final quantum state and evaluation of simulation reliability.From the literature, it is shown that digital quantum simulation is theoretically sound andexperimentally feasible, with several applications in a wide range of physics-related fields.Nonetheless, a number of conditions arise that must be observed for a truly efficient implementationof a digital quantum simulation, from theoretical conception to experimentalcircuit design. The review of QCVV techniques highlights the need for characterization andvalidation techniques that could be efficiently implemented for current models of quantumcomputation, particularly in instances where classical verification is not tractable. However,there are proposals for efficient verification procedures when a set of parameters defining thefinal result of the simulation is known.The experimental simulation demonstrated partial success in comparison with an idealquantum simulation. From the results it is apparent that better coherence times, betterreliability and finer control are as decisive for the advancement of quantum computing poweras the more-publicized number of qubits of a given device.",
    "authors": [
      "Rodrigues, Afonso Miguel Fernandes"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81357",
    "title": "Patterns and development strategies used on a microservices architecture",
    "abstract": "Microservices are a modern architecture style that divides a single application into small, indepen dently deployable services, each running in its own process and communicating through lightweightmechanisms. However, there is still a lack of research on the design and development of microservicesapplications.The development of applications using microservice-based architectures requires a variety of es sential factors that must be kept in mind to achieve good and future proof results.Given the growing demand for scaling applications and the growth of cloud infrastructures, mi croservices emerged as one of the most prominent architectural advancements in recent years. Theyare still in their early stages of integration, and for that reason this architecture style has yet to bewidely studied.With that in mind, this dissertation aims to close this gap by providing the key elements that shouldbe considered when designing and building solutions based in microservices. It begins by researchingand studying these architectures and finishes with a implementation of microservices based on a casestudy.",
    "authors": [
      "Oliveira, Hugo Manuel Coelho de"
    ],
    "keywords": [
      "Microservices",
      "Software architectures",
      "Patterns",
      "Strategies",
      "Scalability",
      "Microsserviços",
      "Arquiteturas de software",
      "Padrões",
      "Estratégias",
      "Escalabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79752",
    "title": "Utilização dos templates e modelos do Django para desenvolver aplicações web de elevado desempenho",
    "abstract": "This document describes the development of high performance Web applications usingDjango framework. Initially, the operation and usage mode of Django are introduced, aswell as several Web applications’ latency reduction techniques. The work carried out fo cused on the design, implementation and performance optimization of a Web application,which consists of an article sharing system. The development process followed the Scrummethodology. During development, several technologies were explored, such as Memcached,Celery and Varnish, which enabled the implementation of certain performance optimi zation strategies. The latency of several operations was measured, before and after theapplication of optimization techniques, in order to ensure that one was moving in the rightdirection. The optimization of the application’s performance was performed at various le vels, including the transfer of content across the network and the backend services. HTTPcaching, data compression and minification tecniques, as well as static content replicationusing Content Delivery Networks, were used. Partial update of the application’s pages onthe front-end and asynchronous processing techniques were applied. The database utili zation was optimized by creating indexes and by taking advantage of a NoSQL solution.Memory caching strategies, with distinct granularities, were implemented to store templa tes and application objects. Furthermore, asynchronous task queues were used to performsome costly operations. All of the aforementioned techniques favorably contributed to theWeb application’s latency decrease. Django only supports the application of some of thesetechniques, because it operates on the back-end. Since performance must be optimized atvarious levels, it was necessary to use other tools besides Django.",
    "authors": [
      "Fernandes, João Miguel Gonçalves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92594",
    "title": "Sistema de pagamentos descentralizado para e-commerce na Blockchain",
    "abstract": "A tecnologia blockchain tem evoluído a um ritmo incrível desde da criação da Bitcoin em 2008, por SatoshiNakamoto, fazendo com que esta tecnologia seja um dos assuntos mais falados aquando a escrita destadissertação.São já imensas as aplicações e industrias em que a tecnologia blockchain é usada: NFTs (Non-FungibleTokens), Banking, Secure Data Sharing, Music Royalties, IoT, AML Tracking, Voting, Real Estate, SupplyChain, Insurance, Energy, Cross-Border Payments (Payment Gateways) - funcionalidade em que se focará estadissertação - entre outras funcionalidades.O termo Payment Gateway (PG) é utilizado para descrever um sistema de pagamentos que pode ser encontradono checkout das lojas online dos comerciantes, este tipo de sistemas tem como objetivo facilitar os pagamentospara compradores e comerciantes. Assim sendo, Crypto Payment Gateway (CPG) é o termo utilizado parasistemas que, ao contrário dos anteriores, se focam nos pagamentos em criptomoeda, conseguindo assimremover intermediários no processo.São já várias as soluções de Payment Gateways existentes, no entanto, estes sistemas partilham um grandeproblema, não possuem uma solução que permita aos compradores o uso das suas criptomoedas para efetuarcompras ou proceder a pagamentos. É então que os CPGs entram para resolver essa questão, contudo tambémestes possuem limitações, entre as quais se destaca a centralização. Assim sendo, quer PG quer CPG são dealguma forma controlados por uma entidade centralizada, o que pode ser visto como uma desvantagem para outilizador, pela falta de transparência nos processos. Por forma a resolver os problemas mencionados, foi criadoum sistema de pagamentos descentralizado.Para tal foram desenvolvidos smart contracts e criadas interfaces para facilitar a interação com os mesmos, foiainda implementado um sistema para obter as informações dos eventos dos smart contracts. Posteriormente,foi desenvolvido um protótipo de uma Loja Online de um Comerciante, de forma a demonstrar como o projetofuncionaria se implementado em contexto real.",
    "authors": [
      "Vaz, Ricardo Oliveira"
    ],
    "keywords": [
      "Sistema de pagamentos",
      "Blockchain",
      "Smart contracts",
      "Ethereum",
      "Solidity",
      "Payments system",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27796",
    "title": "Analysis of the myocardial function using tagging MR",
    "abstract": "Heart diseases can often manifest themselves by irregularities in the movement of the heartmuscle. To assess the function of the myocardium, a method based in the Optic FlowConstrain Equation (OFCE) is applied in tagging MR images. The sequence of taggingMR images allows us to detect deviations in deformation and strain through time. However,the application of the OFCE implies the assumption of spatial phase conservation.Therefore, harmonic filters in the Fourier domain were used in each frame of the sequenceto remove the variation of intensity trough time.In order to achieve a model capable of distinguishing a malfunction from normal functionof the cardiac wall it is necessary to acknowledge what is the ground truth and whichfactors can affect the results. This study explores several scenarios using synthetic data thatmimic tagged MR images in order to discover which variables can optimize the OFCE.This work allows us to analyze up to what extension the OFCE can be applied to acardiac motion simulator (CMS) based on Waks et al. [1], capable of reproducing thenormal function of the heart. After a series of tests with simulated data and the respectivecomparison with real volunteers data, it is possible to assess quantitatively the methodused.",
    "authors": [
      "Leitão, Ana Jorge Rodrigues de Moura"
    ],
    "keywords": [
      "616.127",
      "616-079"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "616.127",
      "616-079"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81098",
    "title": "Vulnerabilities fast scan: tackling SAST performance issues with Machine Learning",
    "abstract": "Nowadays, cybernetic attacks are a real threat that can compromise any individual,organization or company’s integrity. Every day new cases are reported, that show thereal damage cyber criminals can cause. Sensitive data exposure, identity theft, servicemalfunctioning or shutdown are just a few of the most common threats, which in manycases might impact companies either with financial loss or by damaging their reputation.Population, in general, is becoming each time more aware of the risks of using electronicdevices connected to the web and so are companies. With the rise of this awareness, over thelast years, cyber security has become a major concern for Software companies.This threat also led to the birth of the Software vulnerability detection market. Companiesstarted commercializing Software and advisory to other companies, in order to keep themless exposed to cybernetic risks. There are many mechanisms and technologies used bythese companies to identify vulnerabilities in applications. The most popular technologyused to detect vulnerabilities is SAST (Static Application Security Testing) as it focus onthe detection of vulnerabilities at the early stages of Software development. However, thisrequires the analysis of the source code, which in many cases, is huge and thus such analysisis too time consuming.Being that the context and motivation for this dissertation, the goal is to investigate thepossibility of performing source code analysis in a faster way, relying on machine learningapproaches. Code embeddings, classification algorithms and clustering algorithms were themain approaches explored in this work.Along the project, it was realized that some approaches performed better than others, in thetask of detecting software vulnerabilities. Clustering algorithms, according to the performedexperiments, are not suitable for the problem. Classification algorithms produced results thatcan be considered worthy of further investigation, but did not meet the established goals.After some failed attempts, this project demonstrated that it is possible to train a predictionmodel, based on code2seq approach, capable of detecting vulnerabilities in source code,with better performance and accuracy than classic SAST solutions (according to a specific setof experiments). Moreover, the used approach allows to easily extend the developed work tofind vulnerabilities in any programming language.",
    "authors": [
      "Ferreira, Samuel Gonçalves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79937",
    "title": "Portal Pedidos: plataforma de gestão de pedidos de meios complementares de diagnóstico e terapêutica",
    "abstract": "Over the last few years, there has been a development of Information Technologies (IT)and its applicability has had repercussions in the most varied domains. The healthsector has been no exception, with significant repercussions in terms of improving thequality and effectiveness of healthcare provided by the various organizations, the security of data maintenance and transmission, and the increased interoperability betweenthe various Hospital Information Systems (HIS).The present dissertation project comes in the context of the need for maintenanceand updating of University Hospital Center of Porto computer services. In this sense,the objective was to develop a new web application, called Portal Pedidos, throughwhich will be managed the whole process of requesting Complementary Diagnosticand Therapeutic Means. With reference to users’ perceptions of the current state ofthe platform and the needs identified by them, the developments to be implementedshould ensure the optimization of the existing essential functionality and the introduction of gains that enable earnings in terms of usability, adaptation to devices andincreased data structuring performance. The application developed also allows theavailability of information, previously monolithic and exclusive of the application inuse, with other services within the hospital unit through the implementation orientedto a web service.For the purpose of application development, the prior exploration and selection ofthe technologies to be adopted was crucial in order to guarantee an appropriate optionto achieve the established objectives. The strategy adopted was the use of innovativeweb technologies, specifically using the ReactJs for frontend and Node.js for backend.The adoption of these technologies enabled the implementation of the pre-establishedrequirements and the incorporation of the contributions that emerged throughout theprocess.",
    "authors": [
      "Silva, Inês da Costa e"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88580",
    "title": "Realistic fault assessment in SPDK-enabled storage stacks",
    "abstract": "A eficiência e desempenho das operações de Entrada/Saída(E/S) são aspetos fundamentais na implementação de sistemas de armazenamento. A maioria das soluções atuais são implementadas emkernel, obrigando a trocas de contexto entre espaço de utilizador e kernel por parte das aplicações. Estas trocas de contexto são custosas e, por isso, limitam o desempenho do sistema de armazenamento.A plataforma Storage Performance Development Kit (SPDK) disponibiliza uma forma de construir estessistemas evitando o acesso a kernel, realizando todas as operações de E/S necessárias diretamente doespaço de utilizador para o disco físico.Contudo, os dados continuam a ter que ser guardados com garantias de persistência. Assim sendo,os sistemas construídos com SPDK devem se tolerantes a faltas para garantir resiliência em cenários defalta. A inexistência de uma ferramenta capaz de testar essa resiliência em sistemas de armazenamentoconstruídos com SPDK é um problema para os programadores que querem testar a resiliência dos seussistemas.De forma a resolver este problema, esta dissertação propõe o Fault Injector in SPDK (FISPDK), umaferramenta que estende o SPDK e fornece injeção de faltas determinística ao nível do block device. Parainjetar faltas deterministicamente, FISPDK utiliza diferenciação de pedidos E/S de forma a identificar quaispedidos devem (ou não) ser injetados com uma falta. Para isso, o FISPDK implementa mecanismos depropagação de contexto, que permitem passar informação da aplicação para os níveis mais baixos daspilhas de E/S, e é baseado numa extensão da block device Application Programming Interface (API)original do SPDK. Para providenciar injeção de faltas, o FISPDK apresenta um block device virtual queinterceta pedidos E/S e injeta corrupção de dados ou atraso neles. O block device virtual pode serconfigurado pelos utilizadores para apontar os tipos de faltas e quais os pedidos que devem ser injetadoscom essas faltas.Uma avaliação compreensiva do FISPDK demonstra que a nossa solução consegue injetar faltas deforma determinística e avaliar a tolerância a faltas de sistemas de armazenamento que usam SPDK, semadicionar uma sobrecarga significativa à pilha de armazenamento.",
    "authors": [
      "Miranda, Alexandre Esteves"
    ],
    "keywords": [
      "Armazenamento",
      "Espaço de utilizador",
      "Tolerância a faltas",
      "SPDK",
      "Propagação de Contexto",
      "Storage",
      "User space",
      "Fault tolerance",
      "Context propagation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84476",
    "title": "Automação de testes de carga a partir da interface gráfica com o utilizador",
    "abstract": "Com a evolução tecnológica, as aplicações WEB têm um papel crucial na comunidade. Nesse sentidoé essencial que estas acompanhem o desenvolvimento tecnológico e sejam cada vez mais plataformasconfiáveis e disponíveis. Um dos componentes imprescindíveis para o sucesso de um sistema interativoé a interface gráfica com o utilizador (GUI, em inglês Graphical User Interface), que, neste caso, sãoacedidas através de web browsers. Com o aumento das capacidades dos browsers, cada vez maisaplicações fazem uso dessas capacidades, existindo uma componente lógica que é executada no própriobrowser. Desse modo, é fundamental analisar o impacto, a nível computacional, resultante da execuçãoda componente lógica no próprio browser. Uma forma de o fazer é através de testes de carga quesão executados a partir da interface com o utilizador, permitindo identificar possíveis falhas, tais comoproblemas de implementação, tempos de resposta elevados ou gargalos de desempenho.No entanto, é indiscutível que as aplicações são cada vez mais complexas e, por sua vez, o processode testes torna-se mais difícil e demorado, existindo uma necessidade crescente da automatização domesmo. Os testes baseados em modelos (MBT, em inglês Model-Based Testing) suportam a geração eexecução automática de testes a partir de um modelo do sistema. O MBT aplicado às interfaces gráficaspermite uma avaliação mais exaustiva da aplicação, dado que permitem uma simulação da interação doutilizador com o sistema.Esta dissertação tem como objetivo desenvolver uma solução que, tendo como componentes principais o processo de testes baseados em modelos, testes a interfaces gráficas e testes de carga, permitacom o menor esforço possível gerar e executar testes de carga a partir da interface gráfica.",
    "authors": [
      "Teixeira, Bruno Miguel Fernandes"
    ],
    "keywords": [
      "Aplicações WEB",
      "Teste de software",
      "Testes baseados em modelos",
      "Interface gráfica com o utilizador",
      "Testes de carga",
      "Web applications",
      "Software testing",
      "Model-based testing",
      "Graphical user interface",
      "Load tests",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82993",
    "title": "Fast scan, an improved approach using machine learning for vulnerability identification",
    "abstract": "This document presents a Master Thesis in the Integrated Master’s in Informatics Engi neering focused on the automatic identification of vulnerabilities, that was accomplished atUniversidade do Minho in Braga, Portugal.This thesis work aims at developing a machine learning based tool for automatic iden tification of vulnerabilities on programs (source, high level code), that uses an abstractsyntax11tree representation. It is based on FastScan, using code2seq approach. Fastscanis a recently developed system aimed capable of detecting vulnerabilities in source codeusing machine learning techniques. Nevertheless, FastScan is not able of identifying thevulnerability type. In the presented work the main goal is to go further and develop amethod to identify specific types of vulnerabilities. As will be shown, the goal will beachieved by changing the method of receiving and processing in a different way the inputdata and developing an architecture that brings together multiple models to predict differentspecific vulnerabilities. The best f1 metric obtained is 93% resulting in a precision of 90% andaccuracy of 85%, according to the performed tests and regarding a trained model to predictvulnerabilities of the injection type. These results were obtained with the contribution givenby the optimization of the model’s hyperparameters and also the use of the Search Clusterfrom University of Minho that greatly diminished the necessary time to perform trainingand testing. It is important to refer that overfitting was detected in the late stages of the tests,so this results do not represent the true value in real context. Also an interface is presented,it allows to better interact with the models and analyse the scan results.",
    "authors": [
      "Baptista, Tiago João Fernandes"
    ],
    "keywords": [
      "Vulnerability",
      "Attention models",
      "Atatic analysis",
      "Security",
      "Vulnerabilidade",
      "Modelos de atenção",
      "Análise estática",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27880",
    "title": "Gestão remota para pontos de acesso de redes sem fios",
    "abstract": "No âmbito de uma rede de um provedor de internet sem fios, faz todo o sentido afirmarque é essencial a existência de um sistema de monitorização com capacidades de acessoremoto e funcionalidades automatizadas. Desta forma, consegue-se reduzir a carga nos administradoresda rede, bem como melhorar o tempo de resposta a vários eventos, tais comoperda de rendimento da rede e aumento de colisões. Procura-se também que este sistema tenhabaixas percentagens de uso da largura de banda. Para atingir esta finalidade, recorre-se atecnologias normalizadas facilmente disponibilizadas como o SNMP ou NETCONF. Depoisde um breve estudo comparativo entre as tecnologias referidas, serão analisadas em detalheas MIBs mais relevantes relativamente a pontos de acesso sem fios. A existência de nodosescondidos, pela sua importância na degradação da largura de banda de redes sem fios, foiestudada em particular. Um dos algoritmos mais relevantes para a mitigação deste problemautiliza dinamicamente o mecanismo RTS/CTS através da monitorização de parâmetros, taiscomo o número de retransmissões e número de tramas com erros, activando-o tendo em contaos valores dos parâmetros monitorizados, evitando a introdução de overhead na rede devidoao seu uso desnecessário. Tal algoritmo foi introduzido na aplicação de gestão implementadae testada, sendo que os resultados obtidos não permitiram concluir da relevante bondadedeste mecanismo quando aplicado somente do lado ponto de acesso.",
    "authors": [
      "Silva, Tiago Fontes Carvalho Duque da"
    ],
    "keywords": [
      "681.324",
      "621.39"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.324",
      "621.39"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79885",
    "title": "Sistematização do desenvolvimento de interfaces web",
    "abstract": "Esta dissertação aborda o processo de implementação de interfaces web, mais concretamente, utilizando a framework React. As interfaces de utilizador são peças fundamentais de qualquer produtocomputacional interativo. Uma boa interface consegue conquistar o utilizador e fazer com que este utilizeo produto, enquanto uma interface de menor qualidade pode ser a causa para a pouca utilização de umsoftware. Por este motivo, existem abordagens e metodologias focadas na criação de interfaces, paraproporcionarem uma boa experiência ao utilizador e fazer com que este utilize o software desenvolvido.Após a conceção da interface, é necessário proceder à sua implementação. Para isso existem diversastecnologias e abordagens. Entre as diferentes tecnologias há ainda múltiplas frameworks de desenvolvimento, cada uma com as suas características específicas, o que dificulta, por exemplo, a transição deuma tecnologia para outra. O ideal seria tornar o processo de desenvolvimento de uma interface o maisindependente possível da tecnologia a ser utilizada.Tendo em vista a resolução deste problema a dissertação apresenta duas contribuições principais.Um processo de interpretação do design e da sua divisão em componentes, com o objetivo de maximizar a reutilização de código e consequentemente a eficácia no processo de implementação. A divisãodo design é feita através de uma abordagem atómica, onde componentes mais atómicos se juntam eformam componentes mais complexos.A criação de uma arquitetura genérica capaz de representar uma aplicação React, com o objetivo defornecer uma visão de mais alto nível, mostrando todas as diferentes entidades que existem na arquiteturade uma aplicação, e também a forma como estas entidades se relacionam. Isto permite uma separaçãode responsabilidades, separando a definição da interface, da sua lógica de negócio, e da interação comserviços externos.Além disso, a arquitetura genérica serviu de ponto de partida para a criação de uma estrutura deorganização do código capaz de suportar o crescimento dos projetos ao longo do tempo. Estrutura quefacilita, e sistematiza, o trabalho dos programadores, dado que estes ficam a saber exatamente onde têmde inserir determinados novos ficheiros, ou onde está um qualquer ficheiro que precisa de ser alteradoquando é necessário atualizar um componente da interface.Por último, para provar que os conceitos descritos anteriormente são aplicáveis, para ajudar os programadores a aplicá-los, e para sistematizar o processo de implementação, criou-se uma ferramenta de geração de código. A ferramenta permite criar diferentes partes da arquitetura genérica automaticamente.É também possível gerar um componente React partindo de um protótipo de uma interface.",
    "authors": [
      "Sousa, Nelson Tiago Silva"
    ],
    "keywords": [
      "Interface",
      "Implementação",
      "Design",
      "Componente",
      "React",
      "Framework",
      "Implementation",
      "Component",
      "Software architecture",
      "Methodology",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92826",
    "title": "Optimizing the location of electric vehicle charging points using machine learning",
    "abstract": "One of the major challenges that today’s society deals with the assurance of a sustainable future for present and coming generations. Therefore, Large cities and urban areas are becoming more committed to adopting new sustainability paradigms and are doing so by employing technological solutions. The increasing advancement of technology has made it possible to implement a number of innovative approaches that have a highly beneficial impact on both the environment and the administration of cities.This aspect enables the transition from conventional metropolises to smart cities. Electric vehicles are among the most relevant and popular examples of technological innovations in recent years. The tran sition to a new electric transportation paradigm offers a viable approach to reducing humanity’s ecological footprint due to the low emissions of polluting gases and the substitution of fossil fuels with rechargeablelithium batteries. In light of this, this dissertation aims to optimize the placement of new charging stations in Portuguese cities using various Machine Learning techniques. To do this, We Can Charge data was used to implement and evaluate multiple Machine Learning models in order to discover which one would determine more accurately the daily hours of usage of a potential charging station. Overall, the Random Forest was the model with the best performance. Additionally, Artificial Intelligence explainabilitytechniques were implemented to identify the key factors influencing the charging stations usage. For the districts of Portugal analysed, the number of times a charging station changes states daily, from being occupied by a vehicle to being free and vice-versa, was the most significant factor affecting the daily hours of usage of charging stations.",
    "authors": [
      "Pereira, Ana Filipa Rodrigues"
    ],
    "keywords": [
      "Electric vehicles",
      "Network of charging stations",
      "Smart cities",
      "Sustainability",
      "Cidades inteligentes",
      "Rede de pontos de carregamentos",
      "Sustentabilidade",
      "Veículos elétricos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47793",
    "title": "Remote management of applications: deployment of applications and configurations using a rule system",
    "abstract": "Users expect access to programs and business information anywhere in the simplest waypossible using a device. With the diversification of devices, the standard is disappearingand we are going towards a more heterogeneous world of mobile devices. With this divergenceincreasing, it gets more difficult to update, support and control applications throughall these new platforms. Therefore it is important to facilitate these tasks.The solution to these problems lies on the Mobile Device Management (MDM) programsthat can control what devices install and configure, providing remote tasks and access. Thisdissertation aims not to compete with the current products on the market, but to proposea different way to distribute content to the devices registered on the platform using a Rulesystem. This system will prioritize the newest rules by the device and its location characteristics.As so, providing a different way of grouping devices and distributing content to them.",
    "authors": [
      "Gomes, Fábio André Araújo"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92805",
    "title": "Sistema para gestão de permissão de acesso a atributos de identificação pessoal",
    "abstract": "With the increased use of applications that substitute physical elements of our daily lives, comes the necessityof sharing personal data with third parties so that the use of such applications is possible and viable. The accessto these personal data from the applications should only be possible with explicit permission from the users, whichhe should always be able to manage, so the privacy of his personal data is respected and the user has controlover the access and use of it. As a response to this situation, arises the idea of developing a system that allowsusers more control over the use of their data, as well as allowing the information of the permission to accessto personal attributes to be stored in a secure and trustworthy manner. This system will use a DLT (DistributedLedger Technology) infrastructure, with which is intended to be possible to keep information of the access topersonal attributes of each user, in a reliable, transparent manner, and assuring the possibility to audit over theinformation stored on the infrastructure.",
    "authors": [
      "Vila, Luís Pedro da Silva"
    ],
    "keywords": [
      "DLT",
      "Infraestructure",
      "Privacy",
      "Security",
      "Personal data",
      "Management",
      "Infraestrutura",
      "Privacidade",
      "Segurança",
      "Dados pessoais",
      "Gestão",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92624",
    "title": "Modernising an e-learning platform with a new user interface",
    "abstract": "Distanced learning has a rich history and its evolution is closely connected to technology. It started withhand-written letters, later through radios, CD-ROMs, and now with the internet. With these advancements,the definition of e-learning appears - learning through the use of electronic devices. This new learningmethodology comes with its own list of advantages (accessibility, lower cost, flexibility, etc.) and disad vantages (low motivation, social isolation, effectiveness, etc.). Larger institutions, such as universities andorganisations, find e-learning very appealing mostly due to its long-term cost-efficiency.Web accessibility and usability are important aspects of e-learning, rooted in the fact that the web isone of the most used means for sharing content. Ensuring that educational materials are accessible to alllearners, regardless of their abilities is paramount to e-learning. Developers need to understand how canthey make websites more accessible and usable, considering aspects such as design and implementation.E-learning is now widely used, therefore a market appeared around it, paving the way for platforms suchas Blackboard and Moodle. These platforms have carved their niche in the realm of digital education, eachwith its own strengths and unique features. By looking at what they offer we can deepen our understandingof what e-learning tools have to offer and what functionalities users value most.Formare is an e-learning web platform created by Altice Labs. The company feels the platform is builtusing obsolete technologies (web forms) and wants to modernize the front-end layer of the application.Connecting Formare’s functionalities to a new web interface (using React) is not a straightforward process,so an API that implements the back office and connects it to the front end was introduced.This dissertation documents the implementation process of modernizing an e-learning platform witha new user interface, going in-depth about the problems and solutions found throughout development.",
    "authors": [
      "Félix, João Pedro Neves"
    ],
    "keywords": [
      "Distance learning",
      "E-learning",
      "Web",
      "Formare",
      "Web forms",
      "React",
      "API",
      "Ensino à distância",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47792",
    "title": "Desenvolvimento ágil de uma aplicação web para filatelistas",
    "abstract": "Ao longo dos últimos anos, a área da filatelia tem vindo a perder interessados, nomeadamentenas gerações mais jovens. Essa questão, aliada ao facto do mercado online estar emplena afirmação, leva a que este desinteresse se torne evidente e cada vez mais acentuado,refletindo-se num decréscimo do negócio das lojas físicas de filatelia.Neste contexto, o objetivo da dissertação era desenvolver uma aplicação Web para filatelistasque permitisse e facilitasse a comunicação entre colecionadores para comprar, venderou trocar artigos filatélicos. Pretendia-se que a aplicação fosse desenvolvida segundo umametodologia ágil. A adoção da metodologia ágil Scrum foi fundamentada numa revisãobibliográfica das metodologias existentes mais representativas. Embora fosse sugerida autilização de uma metodologia ágil, também se analisaram algumas metodologias tradicionais.Os resultados alcançados evidenciam que a falta de experiência na utilização da metodologiaScrum pode criar alguns problemas à equipa de Scrum, especialmente nas fasesiniciais do desenvolvimento. No mesmo sentido, uma vez que a metodologia Scrum sefoca na produção de incrementos de produtos funcionais e menos na documentação, leva aque a documentação final da aplicação seja mais difícil caso não seja feita incrementalmenteao longo dos sprints. Por outro lado, pelo facto da metodologia Scrum funcionar em ciclosrepetidos de desenvolvimento completo, permite que seja obtido feedback regular e valiosodas várias partes interessadas, nomeadamente do product owner. Deste modo, é muito maisgarantido que o produto final agrada às partes interessadas.Pode afirmar-se que a aplicação Web desenvolvida é um produto viável mínimo, poisdisponibiliza as funcionalidades suficientes para ser adotada por filatelistas e por outrosinteressados na área de negócio da filatelia.",
    "authors": [
      "Guterres, Paulo Jorge Patrocínio"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47320",
    "title": "Selective reprogramming of WSNs: energetic study and functionality optimization",
    "abstract": "Wireless sensors networks consist of large numbers of small, battery-powered, self-organizing computing motes. Nowadays, these networks are considered ideal candidates for a wide range of applications such as environmental monitoring, military operations and other application fields where it is hard to maintain a continuous presence of human beings.Online remote reprogramming is usually carried out to update the code running on nodes due to factors such as changes in the environment or application. Remote reprogramming might be applied to the whole network or just to a subset of nodes (selective reprogramming), either way it is crucial to provide reliability for such procedure. Therefore, most of the approaches oriented to remote reprogramming resort to flooding the whole network, leading to a major waste of energy in network nodes.When dealing with selective reprogramming, the waste of energy increases steeply even when just a small number of nodes need to get the update messages. These messages may be received and retransmitted from all nodes in the network resulting in a waste of resources.This research identifies multiple scenarios for selective reprogramming and proposes a different energy-aware approach for each one trying to reduce energy consumption in the network by taking advantage of multiple and complementary solutions such as wise routing, clustering and the ability to manage nodes sleeping time instead of using the typical flooding approach.These approaches were tested and compared with typical flooding and Deluge solutions. The results show a significant reduction of the power consumption, thus, making the selective remote reprogramming more energy-efficient.",
    "authors": [
      "Abdah, Hadeel Mohamad-Ali"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66927",
    "title": "Chatbot for digital marketing and customer support: an artificial intelligence approach",
    "abstract": "Human interaction with machines has never been so frequent as nowadays. In order toreduce the redundant workload of a human being that answers repeated and trivial questionsregarding customer support on a digital marketing website, this work has the purposeof replacing this tedious job with an informatics tool, a dialogue tool.A dialogue tool like a Chatbot that could handle customer support to a digital marketingwebsite, provides the opportunity of placing human resources on ”non mechanical tasks”.Given that Chatbots exchange messages directly with customers, they could collect requiredprotocol information in all the interactions. In spite of the possibility of needing humanassistance, he will not need to ask these standard questions and will improve its efficiency.By automating these required dialogues to answer questions about certain products, thatwould otherwise be responded by a human, the organizations will have the opportunity toplace human resources in another sectors that are not so easily automated.",
    "authors": [
      "Vaz, Humberto João Alves"
    ],
    "keywords": [
      "Chatbot",
      "Artificial intelligence",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59962",
    "title": "Plataforma para análise de tráfego e otimização dos recursos de uma infraestrutura de comunicação",
    "abstract": "Uma infraestrutura de comunicação nem sempre é fácil de gerir e o nível de dificuldadeaumenta com a dimensão e o número de dispositivos presentes na infraestrutura de rede. Amonitorização de uma rede é um processo fundamental na medida em que previne/detetaeventuais problemas e dispõe de diversas ferramentas auxiliadoras do trabalho de um administradorde redes de computadores.Este trabalho tem como objetivo primordial o desenvolvimento de uma plataforma de monitorização de infraestruturas de rede de modo a que seja viável observar os tipos de aplicaçõesque estão a gerar tráfego na rede, analisar o respetivo impacto do tráfego gerado, bem comoapresentar detalhes sobre os dispositivos que estão a utilizar essas mesmas aplicações, entreoutras diversas funcionalidades. Esta informação será exposta num Dashboard intuitivo deforma a que um utilizador, mesmo sem ter conhecimentos técnicos aprofundados na área dasredes de computadores, seja capaz de interpretar com facilidade o estado da infraestrutura derede. O Dashboard será desenvolvido de forma a possibilitar uma rápida perceção dos pontoscríticos que existem na rede (e.g. pontos de congestão) e aplicações e dispositivos que estão naorigem dos mesmos. Assim, será possível detetar em tempo real possíveis anomalias e tomarmedidas que as contenham, tendo por base as aplicações e os dispositivos que as originaram.Também serão emitidos alertas referentes a essas mesmas anomalias.Posteriormente, será estudado e desenvolvido um módulo de otimização da rede que integrará na plataforma desenvolvida, consistindo na determinação de alterações que aproveitemao máximo os recursos da rede e, consequentemente, aumentem o desempenho da infraestrutura,de acordo com o tráfego que nela circula. Estas alterações poderão ser implementadaspela plataforma, nos casos em que haja essa possibilidade, ou assumir a forma derecomendações enviadas ao administrador da rede (e.g. horários em que se devam realizardeterminadas operações na rede, tais como backups entre outras possibilidades).A plataforma será desenvolvida, aplicada e testada tento em conta o contexto específicode uma intranet de uma empresa do sector têxtil. Esta empresa necessita deste tipo deplataforma de forma a melhorar o desempenho da sua infraestrutura de rede.",
    "authors": [
      "Pereira, Marco André Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59906",
    "title": "Intelligent medical image analysis: a Deep Learning approach to breast cancer diagnosis",
    "abstract": "Once medical images were scanned and uploaded to a computer, researchers beganto create automated medical imaging systems. From the 1970’s to the 1990’s,medical imaging was performed with sequential application of low-level pixel processingand mathematical modeling to solve specific tasks such as organ segmentation.At the end of the 1990’s, supervised techniques began to appear, wheredata extracted from the images were used to train models and classification systems.One example is the use of automated classifiers to build support systemsfor cancer detection and diagnosis. This pattern recognition and / or MachineLearning approach is still very popular and represented a shift from systems thatwere completely human-engineered to computer-trained systems with the use ofspecific (manually drawn) features and automatically extracted from the trainingdata (example). The following step would be enabling the algorithms to directlylearn characteristics of the pixels of the images. This is the basic concept of DeepLearning algorithms: multi-layered models that transform input data (images) intooutputs (e.g. the presence or absence of pathological lesions or cancer).This study intends to present ways of using Deep Learning algorithms in the analysisof medical images, like the particular case of pathological lesions representativeof breast cancer phenotypes.",
    "authors": [
      "Fontes, João Pedro Pereira"
    ],
    "keywords": [
      "Medical image analysis",
      "Deep learning",
      "Artificial intelligence",
      "Breast cancer diagnosis",
      "Análise de imagem médica",
      "Inteligência artificial",
      "Diagnóstico de cancro da mama",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84355",
    "title": "Automation of companies’ recruitment process: development of an algorithm capable of ranking CVs according to job offers",
    "abstract": "This document presents a Thesis and describes the underlying work which was developedalong the second year of the Master Degree in Informatics Engineering offered by Departamentode Informática of Universidade do Minho and accomplished at Syone SBS Software –Tecnologia e Serviços de Informática, S.A..In the past few years, some attempts to automatically screening CVs with resource toNatural Language Processing have been made not only to save recruiters’ time, but alsoto spare them the most tedious task of the recruitment process and, consequently, smooththeir job. However, the majority is still very primitive, misclassifies a lot of CVs and needs adeeper study.Therefore, the aim of this Master’s Project is precisely to develop an algorithm that iscapable of automatically ranking candidates’ CVs according to their similarity regarding thejob offer they applied for.Thus, a general architecture was proposed where CVs and job offers are preprocessed, inorder to obtain the respective texts proper to be further processed. That said, two differentapproaches were followed, in order to find the similarity between the documents in question.To do so, the first approach resorted to several Machine Learning algorithms and similaritymeasures, while the second approach structured the initial documents to compare theirrespective information.After that, tests were conducted to evaluate both approaches and enable the comparisonbetween them. Finally, the conclusions were drawn and also reported in this dissertation.",
    "authors": [
      "Rocha, Beatriz de Freitas"
    ],
    "keywords": [
      "Curriculum vitae (CV)",
      "CV screening",
      "Machine learning (ML)",
      "Natural language processing (NLP)",
      "Aprendizagem automática",
      "Curriculum vitae (CV)",
      "Processamento de linguagem natural (PLN)",
      "Triagem de currículos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64111",
    "title": "Especificação e validação de processos ETL em Alloy",
    "abstract": "O desenvolvimento de processos ETL é uma tarefa dispendiosa e complexa. Não admira, pois, ocuidado que os seus implementadores têm, em particular, durante as suas fases de planeamento eanálise. Muito trabalho tem sido desenvolvido em prol do estabelecimento de novos e melhoresmétodos e técnicas de modelação conceptual e lógica destes processos. Todavia, ainda ocorreminúmeros problemas durante as primeiras fases de execução dos processos de ETL, muitos delesprovocados por erros de análise, de desenvolvimento, ou de simples esquecimento. Como tal, é vitalque antes da entrada destes processos em produção, eles sejam submetidos a algum tipo demecanismo que permita validá-los e comprovar a sua correção, relativamente àquilo que se esperaque eles realizem. A utilização da linguagem Alloy na especificação e validação de processos ETLoferece esse tipo de validação. Neste trabalho de dissertação, suportado por um caso de estudoespecífico, Alloy é estudada, utilizada e avaliada quanto à sua aplicação na especificação formal evalidação de processos ETL.",
    "authors": [
      "Capelo, Mariana Almeida Brandão"
    ],
    "keywords": [
      "Data warehousing",
      "Sistemas de ETL",
      "Especificação e verificação de sistemas de ETL",
      "Alloy",
      "ETL systems",
      "Specification and Validation of ETL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84130",
    "title": "Classificador da condição do piso para um sistema de condução autónoma",
    "abstract": "O presente trabalho de dissertação surgiu no contexto do projeto Sensible Car, uma parceria entre a Bosch e a Universidade do Minho (UM), onde se está a desenvolver um sensor da condição do piso (RCS) em que circula um veículo automóvel. Com esta dissertação pretendia-se verificar se a aplicação de dados meteorológicos aliados à informação ótica apresenta vantagens na classificação da condição do piso, para além da utilização da informação ótica. Também se pretendia demonstrar se, com recursos computacionais limitados, é possível implementar um classificador de piso com a fiabilidade e a capacidade de resposta exigidas. Para atingir os objetivos propostos aplicou-se aprendizagem automática com supervisão e utilizaram-se dados de treino que combinam (i) os rácios da intensidade da luz recebida (após reflexão no piso) sobre a intensidade da luz emitida pelos dispositivos óticos do RCS, para quatro comprimentos de onda distintos, com (ii) dados meteorológicos. Os dados óticos são essenciais para a circulação com segurança em veículos com condução autónoma. Isto porque a deteção da condição do piso onde se circula permite ao veículo tomar melhores decisões em tempo real. Para além de se comparar o desempenho de cada modelo treinado só com dados óticos, com o desempenho do mesmo modelo treinado com dados resultantes da fusão entre dados óticos e meteorológicos, testaram-se diversos modelos, para selecionar o que mais se adequa à classificação da condição do piso. Numa fase inicial, selecionaram-se os modelos que apresentaram melhor desempenho, i.e. melhor, precisão e recall, na classificação de amostras dos vários tipos de piso. Os modelos aqui selecionados foram SVM Gaussiano (0.96 de precisão e 0.93 de recall), Regressão Logística (0.91 e 0.88), Árvore de Decisão (0.91 e 0.85) e XGBoost (0.94 e 0.94). Posteriormente, implementaram-se e testaram-se os melhores modelos no dispositivo Nvidia Jetson Nano. Nesta fase, além de se confirmar as percentagens de acerto dos modelos a classificar a condição do piso, verificou-se se eram capazes de classificar as amostras ao ritmo a que o sensor de condição de piso gera os dados óticos. Os resultados obtidos mostraram que os modelos desenvolvidos são capazes de acompanhar o ritmo de geração de dados do sensor da condição do piso, em que o modelo SVM faz 1040 classificações por segundo, a Regressão Logística faz 2080, a Árvore de Decisão efetua 1950 e o XGBoost faz 223. O modelo selecionado no fim foi o SVM Gaussiano, pois apesar de não ser o modelo com maior número de classificações por segundo, é o que possui o melhor desempenho geral na classificação da condição do piso.",
    "authors": [
      "Peixoto, Flávio Joel Martins"
    ],
    "keywords": [
      "Aprendizagem automática",
      "Sensor da condição do piso",
      "Dispositivo edge",
      "Fusão sensorial",
      "Condições meteorológicas",
      "Machine learning",
      "Road condition sensor",
      "Edge device",
      "Sensor fusion",
      "Weather conditions",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27896",
    "title": "Mining quorum sensing in pathogenic P. aeruginosa and C. albicans",
    "abstract": "O estudo do fenómeno de cross-talking entre P. aeruginosa e C. albicans tem sido focadoessencialmente nos processos de quorum sensing e formação de biofilmes, identificando-se osgenes e proteínas envolvidas. Contudo, mesmo já existindo um grande conhecimento dos genese proteínas envolvidas, ainda existe uma lacuna na integração dos mesmos nas redes biológicasdos respectivos microorganismos. O número e qualidade das redes biológicas existentes(Transcriptional Regulatory Network - TRN and Protein-protein Interactions - PPI) para osfenómenos chave tais como, quorum sensing, formação de biofilmes, resistência a antibióticos epatogenecidade, não evidenciam a importância de alguns destes genes no fenómeno de crosstalking.Esta tese apresenta-se como a primeira tentativa de colocar em evidência os genes eproteínas envolvidos em cross-talking, associando-os aos parceiros de interacção nas respectivasredes biológicas de cada microorganismo. Primeiramente, utilizou-se um processo de integraçãode redes para os dois microrganismos, levando ao aumento do conhecimento geral sobre osprocessos de patogenecidade dos dois microorganismos, e por fim os genes envolvidos emcross-talking, identificados na literatura, foram evidenciados nestas redes integradas. Com estatese pretende-se dar algumas pistas sobre como é que estes genes de cross-talking estãoenvolvidos em importantes processos biológicos e também de algum modo apontar novospotenciais drug targets.",
    "authors": [
      "Santos, Nadine Castelhano"
    ],
    "keywords": [
      "Quorum sensing",
      "Formação de biofilms",
      "TRN",
      "PPI",
      "Integração",
      "Cross-talking",
      "Biofilm formation",
      "Integration",
      "681.3:57",
      "57:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:57",
      "57:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83359",
    "title": "Uma ontologia para a descrição de conteúdos de testamentos",
    "abstract": "Cada vez é mais notória a importância que as ontologias têm vindo a ganhar no que tocaao desenvolvimento de sistemas baseados em conhecimento. Para além de ainda haveralguma dificuldade em compreender o seu modo de implementação, a sua construçãomanual é muito dispendiosa tanto a nível de recursos como de tempo e, após a construção,é necessário manter a ontologia atualizada consoante os novos requisitos que poderãosurgir. Nesta dissertação apresentamos, numa primeira parte, a definição de ontologia, a suautilidade e algumas das metodologias que podem ser utilizadas na sua construção manual,analisando a sua evolução ao longo do tempo. Após esta introdução, apresentamos algumastécnicas de construção (semi-)automática de ontologias a partir de textos e abordamoso conceito de ontology learning, bem como tudo aquilo que este processo envolve. Alémdisso, enunciaremos alguns dos sistemas que fazem uso dessas mesmas técnicas. Por fim,apresentamos o trabalho desenvolvido na extração de uma ontologia a partir de um conjuntode textos relativos a testamentos antigos, que foram editados por Barros e Alves (2019) emO Livro dos Testamentos – Picote, 1780-1803, detalhando o processo de extração realizadopara a ontologia pretendida, bem como apresentando as técnicas e ferramentas utlizadas.Neste processo, queremos relevar a importância da utilização de padrões léxico-sintáticos eo dependency parsing, que contribuíram de forma efetiva para a obtenção dos resultados quealcançámos.",
    "authors": [
      "Yusupov, Shahzod"
    ],
    "keywords": [
      "Ontologias",
      "Extração de ontologias",
      "Processamento de linguagem natural",
      "Ontology learning",
      "Padrões léxico-sintáticos",
      "Dependency parsing",
      "Textos não estruturados",
      "Base de dados orientada a grafos",
      "Neo4J",
      "Spacy",
      "Keywords ontology",
      "Ontology extraction",
      "Natural language processing",
      "Lexico-syntactic patterns",
      "Dependency parsing",
      "Unstructured texts",
      "Graph database",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28042",
    "title": "Efficient processing of ATLAS events analysis in homogeneous and heterogeneous platforms with accelerator devices",
    "abstract": "Most event data analysis tasks in the ATLAS project require both intensive data access and processing, where some tasks are typically I/O bound while others are compute bound. This dissertation work mainly focus improving the code efficiency of the compute bound stages of the ATLAS detector data analysis, complementing a parallel dissertation work that addresses the I/O bound issues.The main goal of the work was to design, implement, validate and evaluate an improved and more robust data analysis task, originally developed by the LIP research group at the University of Minho. This involved tuning the performance of both Top Quark and Higgs Boson reconstruction of events, within the ATLAS framework, to run on homogeneous systems with multiple CPUs and on heterogeneous computing platforms. The latter are based on multicore CPU devices coupled to PCI-E boards with many-core devices, such as the Intel Xeon Phi or the NVidia Fermi GPU devices.Once the critical areas of the event analysis were identified and restructured, two parallelization approaches for homogeneous systems and two for heterogeneous systems were developed and evaluated to identify their limitations and the restrictions imposed by the LipMiniAnalysis library, an integral part of every application developed at LIP. To efficiently use multiple CPU resources, an application scheduler was also developed to extract parallelism from simultaneously execution of both sequential and parallel applications when processing large sets of input data files.A key achieved outcome of this work is a set of guidelines for LIP researchers to efficiently use the available computing resources in current and future complex parallel environments, taking advantage of the acquired expertise during this dissertation work. Further improvements on LIP libraries can be achieved by developing a tool to automatically extract parallelism of LIP applications, complemented by the application scheduler and additional suggested approaches.",
    "authors": [
      "Pereira, André Martins"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47724",
    "title": "RODA-in: a generic tool for the mass creation of submission information packages",
    "abstract": "Digital preservation is the sum of activities necessary to ensure the long-term access todigital information. The OAIS standard(ISO, 2012a) was developed in order to ease thecommunication between the various entities involved in the preservation of digital objectsand regulate the long-lasting storage of digital information.The preservation process begins when the producer creates Submission Information Packages(SIP) and uploads them to the archive’s repository. To create these packages, theproducer must choose which files to archive and provide extra information (metadata) todescribe and to allow finding the information. As the production of digital content increasesexponentially, the creation of SIP by current methods can be too onerous and evenunfeasible.This work focuses on creating a semi-automatic way of producing SIPs by employinga simple and well-defined workflow. Using the file system as the source of content, theproducer defines aggregation and metadata association rules and specify how the SIPs arecreated. The application that was developed to support this work, RODA-in, was designedto be able to create thousands of SIPs with gigabytes of data in an easy to use way. Additionally,it has multiple features that ease the work of the producer, such as metadatatemplating and mass edition.",
    "authors": [
      "Pereira, André Diogo Ribeiro Assunção"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59693",
    "title": "Optimization in code generation to reduce energy consumption",
    "abstract": "In recent years we have witnessed a great technological advance accompanied by an equallyimpressive increase in energy consumption, causing problems of both financial and environmentalorder. In order to counteract this tendency, Green Computing emerges with anumber of measures for a more efficient use of computing resources without a great loss ofperformance.This essay is a study of several elements of Information Technology analyzed from thepoint of view of energy efficiency. With special emphasis on microprocessors, moderncompiler design, development tools and optimization of code generation, a wide rangeof information is gathered on very relevant subjects through perspectives still not veryconsidered by the community in general.Also presented are two experimental studies that analyze the optimization of generatedcode for a set of benchmark programs in several programming languages with the aim ofapraise the otimization impact on improving their energy consumption efficiency. A softwaremeasurement framework was also developed that, together with the methodologiespresented in both studies, allows obtaining very precise and pertinent results for analysis.Finally, a ranking was produced for 18 development tools, considering the execution timeand energy consumption of the executables generated through their compilation profiles.This study also intends to contribute to an energy efficient technological advancement.All the work developed here may also serve as motivation so that these and other aspectsof Information Technology may be seen through a greener perspective.",
    "authors": [
      "Branco, David Luis Moniz"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27795",
    "title": "Linguagens de domínio específico para software criptográfico",
    "abstract": "A criptografia desempenha um papel importante na nossa sociedade, visto que é utilizada em sistemas de computação designados como críticos, que têm que funcionar mesmo na presença de erros. Áreas como os sistemas bancários ou de saúde usam software e hardware, que têm que funcionar em todas as circunstâncias. O principal objetivo para o uso de criptografia nesses sistemas, é o de garantir a segurança da informação, que em muitos casos é sensível.Nos últimos anos, foram surgindo linguagens de programação que se focam num domínio específico, chamadas de linguagens de domínio específico (DSLs). No domínio da criptografia, apareceram as linguagens Cryptol e CAO, ambas ambicionando aumentar a produtividade dos programadores, mas também aumentar a comunicação entre estes e os especialistas do domínio.O Cryptol é uma linguagem funcional e tem um conjunto de ferramentas associadas, compostas por um conjunto de ferramentas de verificação e de compilação para linguagens como C ou VHDL, que é uma linguagem descritiva de hardware. O CAO é uma linguagem imperativa, com uma sintaxe idêntica à do C, e tem também um conjunto de ferramentas associado, que permite a introdução de operações de alto nível na linguagem, por exemplo.Neste trabalho, essas duas linguagens foram abordadas, em particular as suas funcionalidades, e como podem ser usadas para implementar um algoritmo através da sua especificação. Além disso, foi desenvolvida uma ferramenta de compilação que pretende transformar código fonte CAO em código Cryptol, de forma a compilá-lo para VHDL posteriormente.Por fim, um caso de estudo que foca curvas elípticas para criptografia, foi utilizado para comparar as duas DSLs e também para testar a ferramenta desenvolvida.",
    "authors": [
      "Miranda, Luís Paulo Ferreira"
    ],
    "keywords": [
      "681.3.062",
      "681.3-7"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.062",
      "681.3-7"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/57384",
    "title": "Plataforma online para caracterizar comunidades microbianas de estações de tratamento de água e efluentes",
    "abstract": "As Estações de Tratamento de Águas Residuais que utilizam o sistema de lamas ativadascaraterizam-se por serem um complexo ecossistema, responsável pelo tratamento de águasresiduais, através de processos biológicos que metabolizam substâncias orgânicas e inorgânicas em produtos mais toleráveis a nível ambiental. A comunidade de microrganismosdesse sistema requer uma apertada monitorização e controlo, recorrendo a inspecçõesregulares por microscopia para evitar perdas económicas e danos ambientais provocadospor populações bacterianas que não estejam balanceadas.A inovação tecnológica das últimas décadas permitiu que a monitorização que era realizadaatravés de um operador humano, passasse a ser desempenhada por computadores,através por exemplo de programas como o ”Flocos e Filamentos” que consegue analisarmorfologicamente as populações bacterianas com o tratamento de uma ou mais imagensdigitais grayscale 8 bits, adquiridas de reatores laboratoriais ou Estações de Tratamento deÁguas Residuais.Nesta dissertação de mestrado, foi construída uma plataforma online, que integrou oprograma ”Flocos e Filamentos” otimizado e compilado. A plataforma permite facilitar amonitorização deste processo biológico, sendo acessível através de dispositivos moveis e computadores.",
    "authors": [
      "Oliveira, Daniel Filipe Braga de"
    ],
    "keywords": [
      "Estações de tratamento de águas residuais",
      "Lamas ativadas",
      "Analise de imagem",
      "Wastewater treatment plants",
      "Activated sludge",
      "Image analysis",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80086",
    "title": "Solução de análise preditiva para gestão de tarefas e avarias",
    "abstract": "This dissertation reports on a Masters’ Project in the field of Computing Engineering.The objective of this project is based on the forecast of possible shortages of goods andpossible damage that the vending machines can have, optimizing the profitability of thedevices, as well as the management of the tasks of the employees. For the execution of thisproject we intend to design and develop a support system of predictive analysis that allowsto expand the functionalities provided by the existing application. The existing applicationmeets all the requirements initially presented by the company’s customer, but does not takeadvantage of all the capabilities that the SAP Hana Cloud Platform (SAP HCP) has available.It is possible and intended in this phase to add new functionalities in order to monetize thedevices including the predictive analytical capability.",
    "authors": [
      "Silva, Flávia Daniela da"
    ],
    "keywords": [
      "Predictive analysis",
      "SAP HCP",
      "R",
      "Análise preditiva"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79962",
    "title": "Early validation of system requirements and design",
    "abstract": "Modern society is relying more and more on electronic devices, most of which are em bedded systems and are sometimes responsible for performing safety-critical tasks. Asthe complexity of such systems increases due to concurrency concerns and real-time con straints, their design is more prone to errors which can lead to catastrophic outcomes.In order to reduce the risk of such outcomes, a model-based methodology is commonlyused. The model describes the behaviour of the system and is subject to verification tech niques such as simulation and model checking in order to verify it behaves according tothe requirements. Common problems that arise with this methodology is the ambiguity ofrequirements written in natural language and the translation of a requirement to a propertythat can be verified along with the model.This thesis proposes a tool that, after the translation of the requirements to temporalformalism, allows the automatic generation of monitors in order to verify the model. Ourtarget platform is Simulink, which is widely used in this domain to model, simulate andanalyze dynamic systems.",
    "authors": [
      "Miranda, Marcelo"
    ],
    "keywords": [
      "Formal methods",
      "Runtime monitoring",
      "Temporal logics",
      "SALT",
      "SpeAR",
      "Métodos formais",
      "Lógicas temporais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64179",
    "title": "Cloud-based analytics for monitoring and classification of arrhythmias",
    "abstract": "Real-time monitoring has become one of the most important and clinically relevant tasksin medical settings, yet one of the most repetitive and tiresome tasks is the analysis of24-hour ECG records. One way to automate this long task is to convert this process intoa real-time process with the automatic classification of the heart rate and with this, theclassification of arrhythmias.Thus, this master thesis focuses on the study of Deep Learning models for the classificationof arrhythmias and data processing tools for the streaming and processing of data inreal time. Consequently, this master’s thesis comprises several phases. In the first place,a more theoretical part is presented which is the ground truth of the use of the tools laterused for the development of the system. The development of the system includes amore practical part of data streaming composed by an IoT middleware, Apache Kafka asan intermediate agent between this middleware and Apache Spark, and ElasticSearch forreal-time data storage for visualization. On the other hand, in the main scope of this thesis,two models of Deep Learning were created, one for the classification of arrhythmias andanother one for their forecast.The results obtained are promising with the arrhythmia classification yielding 98% accuracyin the classification of each beat in one of the four classes used. When the modelwas tested in data obtained directly from the Hospital of Braga, it was not possible to obtainsuch good results, however, the model after new training was able to obtain accuracyvalues of 81% for the testing dataset. This deep learning model was also tested with theintegration of Apache Spark, in order to create data parallelism and increase the speed ofthe deep learning process, which tends to be very time consuming, without neglecting itsperformance.The development of the model for the prediction of arrhythmias was done based on Long-Short Term Memory layers, in order to create a neural network with memory, the resultsobtained in the records with 30 minutes were not high. Despite the less good results in thefirst dataset, when the model was tested in the 24-hour records, the results obtained werequite high which demonstrated that the model can predict if it is based on a longer record.Nevertheless, these results were obtained individually because the Electrocardiograms canbe an object of human identification.Based on the results obtained it was possible to conclude that more tests should be doneincreasing the spectrum of arrhythmias to be classified so that this process becomes fully automatic, without neglecting the precision of the results since human lives may dependon it.",
    "authors": [
      "Brito, Cláudia Vanessa Martins de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27879",
    "title": "Avaliação como momento de aprendizagem : análise de Stress num ambiente de e-Learning",
    "abstract": "A avaliação é um momento determinante na elaboração de estratégias de sucesso da aprendizagem. Em ambientes presenciais o educador pode observar o comportamento dos seus alunos e determinar caminhos que facilitem a avaliação e não induzam o stress e as consequências negativas no resultado da aprendizagem.Nos ambientes de aprendizagem de e-Learning torna-se impossível o contacto direto e, como tal, terão de existir formas facilitadoras de detetar e prevenir as situações de stress nos momentos de avaliação.Urge portanto analisar o stress e determinar estratégias de resolução dos problemas que são causados por ele. Neste trabalho pretende-se desenvolver um módulo de análise de stress em momentos de avaliação em contextos de aprendizagem em linha que possa indicar ao educador os momentos mais propícios para intervir assim como os conteúdos que causam maiores dificuldades. Desta forma o educador poderá intervir de forma mais eficiente junto dos alunos que mais precisem.",
    "authors": [
      "Gonçalves, Sérgio Manuel de Carvalho"
    ],
    "keywords": [
      "37.018.43",
      "681.324",
      "371.26",
      "616.891.6"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "37.018.43",
      "681.324",
      "371.26",
      "616.891.6"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81083",
    "title": "HIODS: hybrid inline and offline deduplication system",
    "abstract": "Deduplication is a technique that allows finding and removing duplicate data at storagesystems. With the current exponential growth of digital information, this mechanism isbecoming more and more desirable for reducing the infrastructural costs of persisting suchdata. Therefore, deduplication is now being widely applied to several storage appliancesserving applications with different requirements (e.g., archival, backup, primary storage).However, deduplication requires additional processing logic for each storage request inorder to detect and eliminate duplicate content. Traditionally, this processing is done inthe I/O critical path (inline), thus introducing a performance penalty on the throughputand latency of requests being served by the storage appliance. An alternative solution is todo this process as a background task, thus outside of the I/O critical path (offline), at thecost of requiring additional storage space as duplicate content is not found and eliminatedimmediately. However, the choice of what type of strategy to use is typically done manuallyand does not take into consideration changes in the applications' workloads.This dissertation proposes HIODS, a hybrid deduplication solution capable of automati cally changing between inline and offline deduplication according to the requirements (e.g.,desired storage I/O throughput goal) of applications and their dynamic workloads. Thegoal is to choose the best strategy that fulfills the targeted I/O performance objectives whileoptimizing deduplication space savings.Finally, a prototype of HIODS is implemented and evaluated extensively with differentstorage workloads. Results show that HIODS is able to change its deduplication mode dy namically, according to the storage workload being served, while balancing I/O performanceand space savings requirements efficiently.",
    "authors": [
      "Pedrosa, Carlos Pinto"
    ],
    "keywords": [
      "Deduplication",
      "Storage",
      "Inline",
      "Offline",
      "Hybrid",
      "Deduplicação",
      "Armazenamento",
      "Híbrido"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81295",
    "title": "Building a hybrid recommender engine for e-commerce",
    "abstract": "A história pode ser resumida em um punhado de eventos que influenciaram a evolução humana: a nossa capacidade de controlar o fogo, a invenção da roda ou a implementação da linha de montagem. Todas estas descobertas tiveram um enorme impacto no futuro da nossa raça e do próprio mundo. Agora encontramo-nos novamente noutra revolução: a revolução dos dados. Facilmente impercetível, esta nova perspetiva está a mudar de todas as formas possíveis como interagimos com a Internet e, pela primeira vez na história, como a internet interage connosco. Este novo tipo de interações é definido por conexões entre utilizadores e bens consumíveis (produtos, artigos, filmes, etc.). Através destas conexões, conhecimento pode ser encontrado. Esta é a definição de mineração de dados. Com o aumento da oferta que as plataformas de comércio virtual oferecem hoje, é necessária uma ferramenta que auxilie a conexão entre clientes e produtos. Os sistemas de recomendação surgem como o fator dominante na criação de novas conexões entre oferta e procura. Esta dissertação segue a investigação e implementação de um motor de recomendações adaptável a várias plataformas de comércio virtual de diversas áreas de negócio. Consequentemente, trata-se de um sistema escalável, genérico e customizável, com o objetivo de aumentar a interação dos clientes, através de uma experiência personalizada com múltiplos tipos de recomendações espalhados pelas plataformas. Este projeto irá seguir uma metodologia CRISP-DM e o motor deverá implementar algoritmos aceites mas adaptados aos dados e à estrutura das plataformas de comércio virtual. As recomendações serão também avaliadas de acordo com métricas apropriadas ao projeto, e os seus resultados discutidos.",
    "authors": [
      "Peixoto, Vítor Emanuel Carvalho"
    ],
    "keywords": [
      "Sistema de recomendações",
      "Comércio virtual",
      "Ciência de Dados",
      "Inteligência de negócio",
      "Aprendizagem automática",
      "Tomada de decisões baseadas em dados",
      "Recommender system",
      "E-commerce",
      "Data science",
      "Business intelligence",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59907",
    "title": "Towards an efficient OLAP engine based on linear algebra",
    "abstract": "Relational database engines associated to the widely used Structured Query Language(SQL) are suffering unsatisfactory performance results in complex business queries, dueto ever increasing volumes of stored data. To retrieve and process data in a more efficientway, Online Analytical Processing (OLAP) models have been proposed with an increasedfocus on attributes (measures and dimensions) over records.OLAP is based on a row-oriented theory, while a columnar-oriented theory could considerablyimprove the performance of analytical systems. The Typed Linear Algebra (TLA)approach is an example of such theory: it encodes each database attribute in a distinct matrix.These matrices are combined in a single Linear Algebra (LA) expression to obtain theresult of a query.This dissertation combines concepts of relational databases, OLAP, TLA and performanceengineering to design, implement and validate an efficient TLA-DB engine: SQL queries areconverted into its equivalent LA expression, using Type Diagrams (TDs), which representeach matrix as an arrow pointing from the number of columns to the number of rows, TDsare converted to a LA expression encoded in Linear Algebra Query language (LAQ) andthe LAQ script of a query is automatically coded in C Plus Plus (C++).An efficient TLA-DB engine required the encoding of the sparse matrices in an adequateformat, namely Compressed Sparse Column (CSC), while the operations specified in LAQexpressions had their performance improved by optimised algorithms and an optimisedquery processor.The functionality of the resulting LAQ engine was validated with several TPC BenchmarkH (TPC-H) queries for various dataset sizes. A comparative evaluation of the TLA-DB withtwo popular Database Management Systems (DBMSs), PostgreSQL and MySQL, showedthat the developed framework outperforms both DBMSs in most TPC-H queries.",
    "authors": [
      "Afonso, João Miguel"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27885",
    "title": "Support Vector Machines na previsão do comportamento de uma ETAR",
    "abstract": "O Data Mining é um processo de exploração de grandes quantidades de dados, com um potencial enorme para ajudar as empresas na extração de conhecimento que está oculto nos mais diversos sistemas de dados. Esta tecnologia é utilizada pelas empresas nos mais variados domínios, com o intuito de as ajudar em atividades de tomada de decisões. Entre os diversos campos de aplicações encontramos o domínio da Biologia e do Ambiente, em particular, as questões relacionadas com as Estações de Tratamento de Águas Residuais (ETAR). As ETAR são infraestruturas essenciais para manter o equilíbrio do meio-ambiente, sendo caracterizadas por terem várias fases de tratamento, nas quais são removidas impurezas como sólidos, matéria orgânica e nutrientes. Todo este processo dinâmico e complexo deve ser processado de forma eficiente, permitindo que o efluente final que nelas é tratado tenha a melhor qualidade possível. A previsão da qualidade da água tratada, com base nos vários fluxos que dão entrada nas ETAR, permite medir a eficácia do tratamento e, assim, obter alguma informação útil para um melhor controle de toda a infraestrutura. A ETAR em estudo neste trabalho de dissertação, localiza-se no Norte de Portugal e serve uma população de cerca de 45 mil habitantes. Os dados fornecidos para alimentação dos processos de interação levados a cabo são referentes a tratamentos realizados nessa ETARdurante o período de um ano. Este estudo pretendeu explorar técnicas de Data Mining preditivas, nomeadamente modelos de regressão, por forma a prever com eficácia os valores dos parâmetros de qualidade da ETAR. As medidas de qualidade do tratamento analisadas neste estudo, basearam-se nos parâmetros de previsão Carência Bioquímica de Oxigénio (CBO) e Sólidos Suspensos Totais (SST). Por sua vez, as técnicas de regressão adotadas neste trabalho são baseadas em Support Vector Machines, mais concretamente nos algoritmos Support Vector Regression e numas das suas variantes: Sequential Minimal Optimization. Este conjunto de técnicas tem sido aplicadas com sucesso em diferentes áreas, inclusive em alguns trabalhos relacionados com as ETAR. Pretendeu-se assim, à custa da utilização destas técnicas de previsão, definir um modelo comportamental para a ETAR em questão, por forma a analisar a sua capacidade preditiva neste tipo de sistemas complexos. Neste problema, as fases de análise e preparação dos dados mostraram-se determinantes na obtenção dos resultados alcançados. Analisaram-se ainda as diversas tarefas de modelação desenvolvidas neste estudo. Os modelosdesenvolvidos demonstraram uma boa capacidade preditiva, especialmente na previsão do parâmetro do efluente final CBO. As técnicas de previsão utilizadas, para além da capacidade de modelação preditiva não linear, permitem ainda uma análise aos atributos mais influentes à qualidade dos parâmetros de previsão.",
    "authors": [
      "Ribeiro, Daniel José Silva"
    ],
    "keywords": [
      "Data Mining",
      "Regressão",
      "Support Vector Machines",
      "Support Vector Regression",
      "Sequential Minimal Optimization",
      "Estações de Tratamento de Águas Residuais",
      "Carência bioquímica de oxigénio",
      "Sólidos suspensos totais",
      "Previsão de comportamento de ETARs",
      "Regression",
      "Wastewater Treatment Plants",
      "Biochemical oxygen demand",
      "Total suspended solids",
      "Prediction of WWTPs performance",
      "628.1:681.3",
      "681.3:628.1"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "628.1:681.3",
      "681.3:628.1"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93048",
    "title": "Injeção de faltas reprodutível em sistemas de armazenamento local",
    "abstract": "A era digital trouxe a incorporação dos computadores em vários setores do nosso quotidiano. Tem-severificado, ao longo dos anos, uma crescente dependência na tecnologia, o que se traduz na necessidadede sistemas cada vez mais resilientes, rápidos, seguros e disponíveis.Consequentemente, a complexidade dos sistemas tem vindo a crescer, o que leva à implementaçãode mecanismos de persistência e atualização de dados mais difíceis de testar. Para elevar o nível dedificuldade, os diversos sistemas de ficheiros possuem particularidades que afetam de forma significativaestes mecanismos e a consistência das aplicações após a ocorrência de falhas de energia e dos sistemasoperativos.Evitar a perda total ou parcial dos dados deve ser um ponto fulcral para sistemas de armazenamentocom fortes garantias de durabilidade e coerência. Na literatura, os sistemas capazes de voltar a um estadocoerente após uma falha de energia são chamados de crash-consistent. Muito do trabalho relacionadofoca-se na crash consistency dos sistemas de ficheiros e, quando se foca nas aplicações, verifica-se afalta de ferramentas para reprodução de bugs. Quando um utilizador reporta problemas encontradosnuma dada aplicação, é útil para as equipas de desenvolvimento terem uma ferramenta que rapidamentereproduz o bug e ajude na sua correção.Esta dissertação propõe o LazyFS+, um sistema de ficheiros que simula a perda total e parcial dedados através da injeção de faltas baseada em software. A nossa solução possui uma cache interna, oque permite uma gestão determinística dos dados das aplicações e a injeção de faltas reprodutível. Umdos seus pontos fortes é o facto de imitar comportamentos que alguns sistemas de ficheiros apresentam,como a reordenação de escritas, sem se prender a uma implementação específica. Para além disso,facilita a análise das operações executadas pela aplicação e permite obter informação sobre os dadosnão sincronizados.O LazyFS+ provou-se útil através da reprodução de bugs já conhecidos e da identificação dessesmesmos bugs em versões antigas das aplicações, nunca antes reportados. Para além disso, foi possívelencontrar novos bugs, quer em versões antigas, quer em versões mais recentes das aplicações. Estesúltimos foram reportados com todos os passos de reprodução realizados com o LazyFS+. Como resultado,atualmente este está a ser integrado nos testes do sistema de armazenamento chave-valor etcd.",
    "authors": [
      "Ramos, Maria José Costa"
    ],
    "keywords": [
      "Crash consistency",
      "Injeção de faltas",
      "Reprodutibilidade",
      "Fault Injection",
      "Reproducibility",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84159",
    "title": "RealROC: a shiny based application for ROC curve study with covariate adjustment",
    "abstract": "A curva ROC (Receiver operating characteristic) é uma ferramenta analítica eficaz para testes clínicos. A análise permite visualizar a variação de sensibilidade e especificidade para uma dada região de corte através de um simples, mas robusto gráfico bidimensional. Num contexto biológico, testes podem ser influenciados por múltiplas variáveis externas e como tal a análise ROC pode não ser a ideal ou gerar resultados incompletos. É então necessário saber que variáveis afetam determinado teste clínico de forma a determinar os melhores parâmetros para determinado teste ou até descartar determinada metodologia mediante a situação. O ajuste da curva ROC a covariáveis permite a normalização do efeito das mesmas ou diretamente ajustar a curva para os seus efeitos. Software direcionado ao ajuste da curva ROC é, infelizmente, escasso e muitas vezes difícil de manusear por utilizadores não especializados. Recentemente o pacote AROC foi lançando para R que disponibiliza vários recursos para estes ajustamentos, no entanto a dificuldade de utilização mantém-se. A combinação deste pacote com a estrutura Shiny, um pacote que permite o desenvolvimento de aplicações interativas, tem por objetivo a criação de um programa grátis e acessível que permita uma análise mais aprofundada disponível para todos os investigadores. RealROC foi capaz de replicar resultados de um caso de estudo que analisou a influência do sexo no sistema de pontuação CRIB e respetiva previsão de mortalidade, demonstrando a usabilidade e acessibilidade do programa que será disponibilizado online e potencialmente contribuir para novos desenvolvimentos na área.",
    "authors": [
      "Costa, Francisco Luís do Amaral Ribeiro Machado e"
    ],
    "keywords": [
      "Curva ROC",
      "AROC",
      "Covariáveis",
      "Shiny",
      "Bioestatística",
      "Informática médica",
      "Classificação estatística",
      "Software",
      "ROC curve",
      "Covariates",
      "Biostatistics",
      "Medical informatics",
      "Statistical classification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64602",
    "title": "Polyglot: sistema poliglota de processamento de dados",
    "abstract": "O aumento exponencial do volume de dados gerados no mundo tecnológico atual é incontestável. A necessidade de armazenar e processar esses grandes volumes de dados levou aindústria a optar por soluções de armazenamento e processamento na nuvem. Além disto,os desenvolvedores optam cada vez mais por sistemas de base de dados que permitemmelhor desempenho e também tirar partido da variedade estrutural dos dados face aossistemas relacionais tradicionais. Estes sistemas que estão a surgir apresentam modelosde dados baseados em estruturas como, p.e., grafos ou índices chave-valor, e oferecem interfacesque podem ser apenas duas operações (PUT/GET) ou, à semelhança dos sistemasrelacionais com o SQL, ter linguagens de interrogação específica.Contudo, a migração de praticamente todos os componentes das infraestruturas dasaplicações para a nuvem implica que os dados sejam processados e armazenados em infraestruturasde terceiros, ficando muitas vezes a privacidade destes comprometida. Poroutro lado, um dado problema pode ter dados com estruturas diferentes ou partes diferentesde uma aplicação podem ter necessidades diferentes quanto aos dados e, por isso, adiversidade entre sistemas de armazenamento leva uma grande complexidade em desenvolversistemas que usem várias fontes de dados diferentes e heterogéneas eficientemente.Assim, esta dissertação pretende dar uma resposta à problemática da gestão de dados deforma privada nas aplicações web, potencializando a utilização de múltiplos sistemas defontes de dados heterogéneas. Em específico, esta dissertação apresenta uma nova arquitetura,à qual se chamou Polyglot, que permite a manutenção da privacidade dos dados, enquantoao mesmo tempo possibilita a utilização de múltiplas fontes de dados heterogénease tira partido da nuvem para grande parte do processamento. Esta arquitetura é tambémimplementada sob a forma de um protótipo direcionado a um sistema de monitorização,que consiste no caso de estudo desta dissertação. Este protótipo permite comprovar a validadeda arquitetura, sendo que a implementação feita demonstra todas as funcionalidadesessenciais ao funcionamento do sistema. Mais ainda, este protótipo é também avaliado anível de desempenho e utilização de recursos, permitindo demonstrar a viabilidade destesistema para uma utilização em cenários reais. Por último exploram-se algumas das funcionalidadesmais relevantes que se poderiam adicionar ao sistema e os ganhos que estastrariam face à implementação atual, demonstrando o potencial do protótipo.",
    "authors": [
      "Gonçalves, Hugo Manuel Ramos Vilas Boas"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64771",
    "title": "A comprehensive phylogenetic analysis of Mycobacterium tuberculosis protein-coding genes: insights into evolution and virulence",
    "abstract": "Understanding the evolutionary relationships between organisms is a complex issue that has gained importance not only in evolution but also in clinical and biological inferences, only possible with technological advances that require new analytical tools. In this work, the main focus is to study the genome of Mycobacterium tuberculosis, the causal agent of tuberculosis, establishing a detailed evolutionary framework.The secondary objective, regardless of the focus on the evolution of Mtb, is that the pipeline created can be applied to any other organism. This dissertation presents some basic facts about tuberculosis, an infectious bacterial disease caused by the Mycobacterium tuberculosis complex, its constitution, evolution, pathology, drug resistance and genetic variation. Our objectives were contextualized considering the most up-to-date tools of alignment and phylogenetics, an area in constant progress due to the growing needs of bioinformatics tools in the area of genomics and evolution. Taxonomic and genetic data were compiled from all organisms with complete genomes in NCBI. This database was subsequently cured by eliminating redundant genomes, i.e., containing only one representative element of each species with the complete proteome. A search of each Mycobacterium tuberculosis protein in this local database using BLAST allowed the detection of probable homologs in a large number of taxonomically informative organisms. The search results were limited to two hundred homologues, which were aligned using MUSCLE. Phylogenetic trees, based on maximum likelihood were constructed for the approximately four thousand Mycobacterium tuberculosis proteins. The phylogenetic relationship and monophyly of Mycobacterium tuberculosis with the remaining bacteria of the same genus(Mycobacterium) and the same family (Corynebacteriaceae) were studied to understand possible processes of acquisition of genes by horizontal transference. Finally, positive selection processes were studied by searching for excess or deficit of non-synonymous mutations in relation to the synonymous (Ka / Ks) using the CODEML software, in order to identify branches with an accelerated evolution in the establishment of the pathogenic species Mycobacterium tuberculosis. These genes may form the basis of the physiological and biochemical characteristics that make this bacterium pathogenic to humans.",
    "authors": [
      "Pereira, Daniela Sofia Gaspar"
    ],
    "keywords": [
      "Evolution of mycobacterium tuberculosis",
      "Bioinformatics",
      "Phylogenetics",
      "Horizontal gene transfer (HGT)",
      "Evolução de Mycobacterium tuberculosis",
      "Bioinformática",
      "Filogenética",
      "Transferência horizontal de genes",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27966",
    "title": "Service and auto-configuration framework for secure Ad-hoc environments in Android and Linux",
    "abstract": "Ad-hoc networks can be useful in many contexts because they can be spontaneously created and do not require any sort of infrastructure. They can be useful for small groups when no other network is accessible. They can also be used in wider areas as a low cost replacement for wireless infrastructure networks with multiple dedicated access points. Despite this, ad-hoc networks are not a very popular option for most users.Unfortunately, ad-hoc networks are not as user friendly as infrastructure networks. The latter ones usually provide standardized mechanisms that perform the essential configurations for the correct functioning of the network. Ad-hoc networks do not have standardized mechanisms adapted to them. Each wireless network manager supports a different set of configuration mechanisms. There is usually no problem when every machine uses the same operating system but when different ones are used, users may need to manually perform the required configurations. Another cause for this low popularity is the lack of useful and easy to use applications. These applications are usually hosted on the Internet, as it provides a larger variety of business models.To tackle these problems, new forms of automatically configuring machines and providing services should be explored. These services must be easy to develop, in order to attract the developers that would develop them. The designed solutions must also be adapted to ad-hoc environments. Another important aspect that must be addressed is security. In some contexts, such as public and corporate environments, security can be essential to provide authentication and even to allow the correct functioning of thenetwork.",
    "authors": [
      "Azevedo, Nuno Filipe Solinho de"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28615",
    "title": "Analysis of the influence of stress on the interaction with the computer",
    "abstract": "The monitoring of different physical and cognitive functions of the human being has been the subject of numerous studies in recent years. However, most of these monitor-ing systems use invasive and very expensive techniques, which complicate its use in research projects and real scenarios alike. Some studies are trying to obtain relevant data from common devices like personal computers or Smartphones, but this area has not been properly explored yet.In this project, it was proposed to perform an analysis of the interaction of the users with a computer using the mouse and the keyboard in order to obtain relevant conclu-sions about the effects of stress on the individual. The hypothesis presented here is in-teresting due to the use of non-invasive techniques to retrieve data as well as the use of common and inexpensive hardware instead of specific and expensive one.",
    "authors": [
      "Catalão, Fábio Alexandre Marques"
    ],
    "keywords": [
      "681.3",
      "616.891.6"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3",
      "616.891.6"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/38671",
    "title": "Estudo de viabilidade de paralelização de códigos de análise de dados em PROOF",
    "abstract": "Esta dissertação surge no contexto das análises de dados gerados pelo LHC (Large Hadron Collider), do esperado crescimento do volume de dados produzidos depois da atualização de 2013-2014 e do atual paradigma pseudo-paralelo destas aplicações no LIP-Minho (Laboratório de Instrumentação e física experimental de Partículas, delegação Minho). O trabalho surgiu como um estudo da utilização do PROOF (Parallel ROOT Facilities) como plataforma para habilitar a extração automática de paralelismo nas aplicações de análises de dados do LIP-Minho. Na consideração que as análises em estudo têm uma estrutura semelhante que é susceptível de ser paralelizada, partimos de um caso de estudo para a familiarização e experimentação do ambiente PROOF. Face às dificuldades de adaptação da aplicação para utilização do sistema PROOF, desenvolvemos e testamos uma nova estrutura de classes, chamada event, que pode eliminar uma série de problemas na fase de desenvolvimento. Esta proposta é suportada por um gerador de código esqueleto de aplicações deste tipo, o makeEvent. Os testes efetuados comprovam a possibilidade de usar a estrutura event como alternativa à API TSelector, sem perda de desempenho e com a possibilidade de alcançar speedups superlineares no ambiente de cluster utilizado. No caso de códigos de análise de dados com alguma dimensão e complexidade, o processo de adaptação para um modelo compatível com o sistema PROOF pode ser uma tarefa morosa e exigente que pode não ser trivial. Por este motivo, propomos como trabalho futuro a criação de uma biblioteca que trate das tarefas habituais no processo de análise dos dados. Prevê-se também que a aplicação makeEvent permita a seleção apenas dos branches utilizados na classe event, reduzindo significativamente o tempo de execução de análises de dados que carregam desnecessariamente todos os branches de uma tree. A conclusão a que chegamos é a da viabilidade da utilização da estrutura event, e consequentemente do makeEvent, como uma alternativa possível para a extração de paralelismo automático das análises de dados em estudo, recorrendo à plataforma PROOF.",
    "authors": [
      "Silva, Rafael Caldeira"
    ],
    "keywords": [
      "Paralelismo",
      "Análise de dados",
      "PROOF",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36805",
    "title": "Improving program comprehension tools for domain specific languages",
    "abstract": "Since the dawn of times, curiosity and necessity to improve the quality of theirlife, led humans to find means to understand everything surrounding them, aimingat improving it. Whereas the creating abilities of some was growing, the capacityto comprehend of others follow their steps. Disassembling physical objects to comprehendthe connections between the pieces in order to understand how they worktogether is a common human behavior. With the computers arrival, humans feltthe necessity of applying the same techniques (disassemble to comprehend) to theirprograms.Traditionally, these programs are written resorting to general-purpose programminglanguages. Hence, techniques and artifacts, used to aid on program comprehension,were built to facilitate the work of software programmers on maintainingand improving programs that were developed by others. Generally, these genericlanguages deal with concepts at a level that the human brain can hardly understand.So understanding programs written in this languages is an hard task, because thedistance between the concepts at the program level and the concepts at the problemlevel is too big.Thus, as in politics, justice, medicine, etc. groups of words are regularly usedfacilitating the comprehension between people, also in programming, languages thataddress a specific domain were created. These programming languages raise theabstraction of the program domain, shortening the gap to the concepts of the problemdomain.Tools and techniques for program comprehension commonly address the programdomain and they took little advantage of the problem domain. In this master’s thesis,the hypothesis that it is easier to comprehend a program when the underlying problemand program domains are known and a bridge between them is established, isassumed. Then, a program comprehension technique for domain specific languages,is conceived, proposed and discussed. The main objective is to take advantage fromthe large knowledge about the problem domain inherent to the domain specific language,and to improve traditional program comprehension tools that only dealt, untilthen, with the program domain. This will create connections between both programand problem domains. The final result will show, visually, what happens internallyat the program domain level, synchronized with what happens externally, at problemlevel.",
    "authors": [
      "Oliveira, Nuno Ernesto Salgado"
    ],
    "keywords": [
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2009",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64164",
    "title": "Retinal image quality assessment using deep convolutional neural networks",
    "abstract": "Diabetic Retinopathy (DR) and diabetic macular edema (DME) are the damages caused to the retina and are complications that can affect the diabetic population. Diabetic retinopathy (DR), is the most common disease due to the presence of exudates and has three levels of severity, such as mild, moderate and severe, depending on the exudates distribution in the retina. For screening of diabetic retinopathy or a population-based clinical study, a large number of digital fundus images are captured and to be possible to recognize the signs of DR and DME, it is necessary that the images have quality, because low-quality images may force the patient to return for a second examination, wasting time and possibly delaying treatment.These images are evaluated by trained human experts, which can be a time-consuming and expensive task due to the number of images that need to be examined. Therefore, this is a field that would be hugely benefited with the development of an automated eye fundus quality assessment and analysis systems. It can potentially facilitate health care in remote regions and in developing countries where reading skills are scarce. Deep Learning is a kind of Machine Learning method that involves learning multi-level representations that begin with raw data entry and gradually moves to more abstract levels through non-linear transformations. With enough training data and sufficiently deep architectures, neural networks, such as Convolutional Neural Networks (CNN), can learn very complex functions and discover complex structures in the data. Thus, Deep Learning emerges as a powerful tool for medical image analysis and evaluation of retinal image quality using computer-aided diagnosis.Therefore, the aim of this study is to automatically assess all the three quality parameters alone (focus, illumination and color), and then an overall quality of fundus images assessment, classifying the images into the classes “accept” or “reject with a Deep Learning approach using convolutional neural networks (CNN). For the overall classification, the following results were obtained: test accuracy=97.89%, SN=97.9%, AUC=0.98 and 𝐹1-score=97.91%.",
    "authors": [
      "Rodrigues, Ana Rita Vieira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80302",
    "title": "PRISMA: a prefetching storage middleware for accelerating deep learning frameworks",
    "abstract": "Deep Learning (DL) is a widely used technique often applied to many domains, from computer vision to natural language processing. To avoid overfitting, DL applications have to access large amounts of data, which affects the training performance. Although significant hardware advances have already been made, current storage systems cannot keep up with the needs required by DL techniques. Considering this, multiple storage solutions have already been developed to improve the Input/Output (I/O) performance of DL training. Nevertheless, they are either specific to certain DL frameworks or present drawbacks, such as loss of accuracy. Most DL frameworks also contain internal I/O optimizations, however they cannot be easily decoupled and applied to other frameworks. Furthermore, most of these optimizations have to be manually configured or comprise greedy provisioning algorithms that waste computational resources. To address these issues, we propose PRISMA, a novel storage middleware that employs data prefetching and parallel I/O to improve DL training performance. PRISMA provides an autotuning mechanism to automatically select the optimal configuration. This mechanism was designed to achieve a good trade-off between performance and resource usage. PRISMA is framework-agnostic, meaning that it can be applied to any DL framework, and does not impact the accuracy of the training model. In addition to PRISMA, we provide a thorough study and evaluation of the TensorFlow Dataset Application Programming Interface (API), demonstrating that local DL can benefit from I/O optimization. PRISMA was integrated and evaluated with two popular DL frameworks, namely Tensor Flow and PyTorch, proving that it is successful under different I/O workloads. Experimental results demonstrate that PRISMA is the most efficient solution for the majority of the scenar ios that were studied, while for the other scenarios exhibits similar performance to built-in optimizations of TensorFlow and PyTorch.",
    "authors": [
      "Correia, Cláudia Sofia Mendonça de Sá"
    ],
    "keywords": [
      "Deep Learning",
      "Storage Systems",
      "I/O",
      "TensorFlow",
      "PyTorch",
      "Prefetching",
      "Parallel I/O",
      "Aprendizagem Profunda",
      "Sistemas de Armazenamento",
      "E/S",
      "Pré-busca",
      "E/S Paralela",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80300",
    "title": "Development of a process for the creation of cross-platform voice applications for Amazon Alexa and Google Assistant",
    "abstract": "Os dispositivos conectados pertencem à área de ”Ambient intelligence” (AmI) e são dispositivos inteligentes que podem fornecer diversos serviços ou através de comandos por voz ou de forma autónoma. Estes dispositivos conseguem ser autónomos, devido ao facto de conseguirem capturar informação do ambiente através dos seus sensores e depois processá-la, de modo a que consigam ativar a ação necessária (”Context-aware Computing”). Os assistentes digitais também pertencem à área de AmI e são programas de software baseados em ”Natural User Interfaces”, o que significa que estes funcionam com recurso a comandos por voz para efetuar uma determinada ação [46]. Os assistentes podem estar presentes em dispositivos conectados e foram desenvolvidos para ajudar as pessoas nas suas tarefas diárias. Devido ao aumento no uso de assistentes digitais, surgiu a necessidade de atender às exigências de uma gama mais ampla de utilizadores, dado que as funcionalidades básicas, para as quais os assistentes haviam sido programados, já não eram suficientes. Esta necessidade levou a uma nova abordagem em relação à expansão das funcionalidades dos assistentes digitais, que consistiu na criação de aplicações por voz. As aplicações por voz ainda são relativamente recentes e como tal ainda não existem muitas ferra mentas, padrões arquiteturais que tenham sido estabelecidos ou uma metodologia ”standard” que possa ser usada no processo de desenvolvimento. Este problema é ainda maior se abordarmos as aplicações por voz ”cross-platform”, dado que hoje em dia existe uma abundância de diferentes assistentes digitais integrados. A inexistência de uma metodologia ”standard” significa que os programadores irão acabar por usar a(s) metodologia(s) que lhes pareçam as mais adequadas tendo em conta o seu objetivo de obter um produto estável. A falta de standardização e de suporte ao desenvolvimento ”cross-platform” de aplicações por voz é a motivação desta dissertação de mestrado. O objetivo desta dissertação é o desenvolvimento de um processo de construção independente de plataforma, que irá promover a criação de aplicações por voz ”cross-platform” e a automatização do mesmo. Este processo vai estar disponível através de uma plataforma, com um editor visual incorporado, que irá permitir a criação de um template de modelo de linguagem que mais tarde irá ser usado para gerar modelos específicos a uma plataforma de modo a que se possa definir o ”frontend” e código ”boilerplate” para o desenvolvimento inicial da funcionalidade do ”backend”. Ao usar esta plataforma, os programadores irão ser capazes de criar e fazer o ”deploy” de aplicações por voz para a Amazon Alexa e para o Google Assistant a partir de uma única fonte de informação, apesar das diferenças que existem entre os seus modelos aplicacionais e, mais importante, recorrendo principalmente aos requisitos pretendidos e não somente aos aspectos tecnológicos.",
    "authors": [
      "Canavarro, Rita de Moura Machado Coelho"
    ],
    "keywords": [
      "Aplicações por voz",
      "Assistentes digitais",
      "Dispositivos conectados",
      "Engenharia de Software",
      "Connected devices",
      "Digital assistants",
      "Software engineering",
      "Voice applications",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93941",
    "title": "Plataforma colaborativa de monitorização, em tempo real, de análises clínicas, nas instituições de saúde",
    "abstract": "Na era atual, marcada pela rápida evolução tecnológica, testemunhamos transformações profundas quepermeiam todos os setores da sociedade. A incessante inovação tecnológica tem redefinido a formacomo vivemos, trabalhamos e interagimos, proporcionando uma revolução que molda a essência da vidamoderna.No âmbito da saúde, essa revolução tecnológica não é apenas evidente, como também tem o potencial de redefinir padrões e expetativas. Os avanços tecnológicos na área da saúde oferecem benefíciossignificativos, desde diagnósticos mais precisos a tratamentos mais personalizados. A integração de tecnologias como inteligência artificial e big data, não apenas amplia o alcance dos serviços de saúde, comotambém melhora a eficiência e a qualidade do atendimento.Os softwares desempenham um papel central nesta transformação, proporcionando ferramentas poderosas para profissionais de saúde. A gestão eficiente de dados, análises avançadas e aprimoramentoda comunicação entre equipas médicas são apenas alguns dos benefícios oferecidos por estas soluções.A relevância crescente destes softwares destaca-se na contribuição das operações clínicas e na melhoriados processos de tomada de decisão, resultando numa prestação de cuidados mais eficaz e centrada nopaciente.A elaboração da presente dissertação considerou o estudo minucioso da seguinte questão de investigação: ”Qual o impacto da utilização de uma aplicação de análise de dados no domínio da medicina e namelhoria da qualidade de vida dos pacientes? ”Para responder a esta questão, foi desenvolvida a AnalyticCare, uma aplicação de monitorização de análises clínicas, com a finalidade de ampliar a capacidade decomparação entre pacientes, proporcionando aos profissionais de saúde uma ferramenta valiosa. Esteartefacto representa uma resposta às demandas crescentes por soluções que otimizem a análise comparativa de casos clínicos. Por fim, foi realizada uma Prova de Conceito - Análise SWOT, para avaliar aeficiência da aplicação, destacando não apenas a sua viabilidade técnica, mas também a sua capacidadede promover avanços na eficácia dos cuidados de saúde, contribuindo para um ambiente clínico maissofisticado e integrado.",
    "authors": [
      "Pinheiro, Quitéria Guimarães"
    ],
    "keywords": [
      "Instituições de saúde",
      "Monitorização",
      "Plataforma web",
      "Sistemas de informação em saúde",
      "Tomada de decisões médicas",
      "Health information systems",
      "Health organizations",
      "Medical decision making",
      "Monitoring",
      "Web platform",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81528",
    "title": "Ansätze for noisy variational quantum Eigensolvers",
    "abstract": "Simulating quantum mechanical systems is one of the main applications envisioned for quantum com puters. In contrast with the first algorithms created for this purpose, that were devised to be implementedin a fault-tolerant quantum computer, the Variational Quantum Eigensolver (VQE) aims to adjust to the con straints of Noisy Intermediate-Scale Quantum (NISQ) devices. There is hope that the class of VariationalQuantum Algorithms (VQAs), to which VQE belongs, will be the first to achieve quantum advantage.The choice of ansatz can dictate the success (or lack thereof) of a VQA: too deep ansätze can hindernear-term viability, or lead to trainability issues that render the algorithm inefficient. In this context, thisdissertation aimed to analyse different ansätze for quantum chemistry, examining their noise-resilienceand viability in state-of-the-art quantum computers. In particular, dynamic ansätze were explored, andtheir performance compared against predetermined ansätze, with a focus on susceptibility to noise.Multiple variants of VQE, namely Unitary Coupled Cluster Singles and Doubles (UCCSD)-VQE (prede termined) and Adaptive Derivative-Assembled Pseudo-Trotter (ADAPT)-VQE (dynamic), were implementedboth in simulators and cloud quantum computers. Using noise models, the impact of several noise sourceson convergence was assessed. Additionally, the importance of the operator pool in ADAPT-VQE was anal ysed, and strategies to manipulate the ansatz beyond the ADAPT-VQE algorithm were explored.Several conclusions could be drawn from this work. Adapting the ansatz to the problem and systemwas concluded to be fundamental in avoiding trainability issues, decreasing the circuit depth requiredfor a given accuracy, and improving noise-resilience (against circuit depth dependent and independentsources alike). Dynamic ansätze were shown to be capable of enduring significantly larger error rates thanpredetermined alternatives, and were thus proved to be better suited for NISQ devices. For 𝐻2, as faras ground state energy calculations are concerned, ADAPT-VQE was shown to tolerate a 20 times lowershot count, 150 times larger error rates in state preparations and measurements, and 850 times lowercoherence times than UCCSD-VQE. The difference is expected to increase with the size of the system.Additionally, it was observed that there is still a margin for improving upon ADAPT-VQE. Further manip ulation of the ansatz was shown to be capable of producing yet shallower circuits for the same accuracy.Using an idea previously proposed in the literature, a more conservative selection criterion was tested.Additionally, removing operators on the fly based on available data was attempted as a new possibility.Both approaches were shown to be capable of improving upon the ADAPT-VQE ansatz, resulting in an upto 35-fold decrease in the error for a similar circuit depth within the first 10 iterations of ADAPT-VQE.",
    "authors": [
      "Alves, Mafalda Francisco Ramôa da Costa"
    ],
    "keywords": [
      "Adaptive ansätze",
      "Quantum chemistry",
      "Quantum computing",
      "Variational quantum algorithms",
      "Variational quantum eigensolver",
      "Algoritmos variacionais quânticos",
      "Ansätze adaptativos",
      "Computação quântica",
      "Eigensolver variacional quântico",
      "Química quântica"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28302",
    "title": "Semantic good morning : news collection, management and presentation",
    "abstract": "Televisions nowadays are shipped with more and more processing power. This allows the development of applications that run in the television. With the evolution of the different Smart TV development frameworks, application development for televisions will become more usual.The aim of the project here reported is to study the viability of the available technologies for Smart TV development, research the means to gather and classify news articles, study techniques of similar document detection and finally to implement a system divided in two parts: the Back-end, where the news aggregation and management will occur; and the Front-end, a Smart TV application that will present to the user the news filtered according to the rating based on standard or customized criteria.As a result of this project, a System for news collection, management, and presentation was implemented.The Back-end collects, classifies, and detects similar news articles, and also obtains news related images. Then, and according to user preferences news are rated and served to the Front-end. The Front-end is a Samsung Smart TV application. Samsung Smart TV was chosen as the best suited Smart TV framework for the project. While news are presented on the Front-end, feedback about each news article is being sent to the Back-end, which will cause changes in the newspresentation order.",
    "authors": [
      "Silva, José Pedro Vieira Costa e"
    ],
    "keywords": [
      "681.3",
      "654.19"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3",
      "654.19"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47823",
    "title": "Arquitetura orientada a serviços para suporte a um sistema de agendamentos online",
    "abstract": "A necessidade das empresas evoluírem as suas aplicações por forma a disponibilizarem mais recursos aos seusutilizadores é uma realidade da atualidade. A disponibilização de informação em tempo real é cada vez mais necessária,mesmo que isso implique a interação entre sistemas distintos, o que exige que essa comunicação seja completamenteagnóstica de tecnologias.Sendo uma das premissas da Q-Better - empresa que permitiu o desenvolvimento desta dissertação em contextoempresarial - proporcionar aos seus clientes uma melhor experiência de utilização aliado ao acompanhamento daevolução tecnológica, tornou-se imperativo a conceção de uma arquitetura que fornecesse suporte ao desenvolvimentode novas aplicações e também às já desenvolvidas, ainda que para tal seja necessária uma reformulação das mesmas.Inicialmente foi feito um estudo sobre a temática das arquiteturas orientadas a serviços, incluindo os vários tiposde web services existentes, e também uma passagem pela temática da sincronização de dados para proporcionar asincronização entre as várias aplicações da Q-Better.A viabilidade da solução final - uma arquitetura orientada a serviços composta por um conjunto de web servicesREST - foi testada com a criação da aplicação Bloom Appointments cujo objetivo passa pela gestão de agendamentosa partir de qualquer dispositivo que tenha ligação à internet ou à rede onde o sistema esteja instalado.Foi possível concluir que a escolha deste tipo de arquitetura se revelou acertada, uma vez que além de permitir ainteroperabilidade entre os vários sistemas existentes na Q-Better, permite uma maior expansão não só da aplicaçãousada como case study, mas também de todo o legacy software e de futuras aplicações.",
    "authors": [
      "Pereira, Nuno Miguel de Lima"
    ],
    "keywords": [
      "Arquitetura orientada a serviços",
      "SOA",
      "Web services",
      "REST",
      "API",
      "Services oriented architecture",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92566",
    "title": "Tradeoff between moving targets, gradient magnitude and performance in quantum variational Q-Learning",
    "abstract": "Reinforcement Learning (RL) consists of designing agents that make intelligent decisions without humansupervision. When used alongside function approximators such as Neural Networks (NNs), RL is capable ofsolving extremely complex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, even achievedsuper-human performance in some specific tasks. Nonetheless, it is also possible to use VariationalQuantum Circuits (VQCs) as function approximators in RL algorithms. This work empirically studies theperformance and trainability of such VQC-based Deep Q-Learning models in OpenAI’s gym CartPole-v0and Acrobot-v1 environments. More specifically, we research how data re-uploading affects both thesemetrics. We show that the magnitude and the variance of the gradients of these models remain substantialthroughout training due to the moving targets of Deep Q-Learning. Moreover, we show that increasing thenumber of qubits does not lead to a decrease in the magnitude and variance of the gradients, unlike whatwas expected due to the Barren Plateau Phenomenon. This hints at the possibility of VQCs being speciallyadequate for being used as function approximators in such a context. We also use the Universal QuantumClassifier as a function approximator in VQC-based Deep Q-Learning and implement VQC-based modelscapable of achieving considerable performance in the Acrobot-v1 environment, a previously untappedenvironment for VQCs.",
    "authors": [
      "Coelho, Rodrigo da Silva Gomes Peres"
    ],
    "keywords": [
      "Reinforcement learning",
      "Quantum computing",
      "Variational quantum circuits",
      "Neural networks",
      "Computação quântica",
      "Circuitos variacionais quânticos",
      "Redes neuronais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27773",
    "title": "Monitorização e prevenção em plataformas de interoperabilidade hospitalar",
    "abstract": "A implementação da interoperabilidade nos Sistemas de Informação Hospitalar(SIH) é cada vez mais um requisito e não uma opção. A Agênciapara a Integração, Difusão e Arquivo de Informação Médica e Clínica (AIDA)consiste numa plataforma de interoperabilidade hospitalar desenvolvida porinvestigadores da Universidade do Minho e que se encontra instalada no CentroHospitalar do Porto (CHP). A AIDA assegura a interoperabilidade entreos SIH e para alémdisto, assegura também a confidencialidade, integridade edisponibilidade dos dados. A AIDA deve possuir um elevado nível de disponibilidadee um funcionamento eficiente 24 horas por dia. Um pequeno períodode paragem poderá trazer graves consequências para a qualidade dos serviçosprestados. Esta plataforma possui mecanismos de recuperação e tolerânciade falhas, contudo devido à sua elevada importância, é preciso agir antes daocorrência das falhas, evitando sérios danos. Os processos de monitorizaçãoe prevenção de falhas devem ser implementados nos “órgãos vitais” da AIDA,que são as base de dados, máquinas e agentes inteligentes.Uma vez que a prevenção de falhas em base de dados da AIDA já tersido alvo de estudo, esta dissertação aborda a monitorização e prevenção defalhas nas máquinas e agentes. Para prever as falhas, foram criados modelosbaseados no Modified Early Warning Score (MEWS). Este modelo através darecolha frequente dos valores dos sinais vitais, calcula um conjunto de scorespara determinar o nível de risco a que o paciente está submetido.Foram desenvolvidos sistemas de monitorização de prevenção para as máquinase agentes que permitem não só prevenir falhas, mas também observare avaliar o comportamento destes componentes através de dashboards de monitorização.A prevenção de falhas nos agentes foi baseada na frequência comque estes registam as suas atividades nos seus ficheiros log, enquanto que paraas máquinas a prevenção foi baseada em indicadores de desempenho como amemória e o CPU. Apurou-se que os componentes, em geral, encontram-secom os seus principais recursos bem balanceados e que os sistemas de prevençãodesenvolvidos detetaram situações críticas com sucesso, contribuindopara um aumento da integridade e disponibilidade da AIDA do CHP.",
    "authors": [
      "Marins, Fernando de Abreu"
    ],
    "keywords": [
      "681.3:61",
      "61:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79474",
    "title": "LPBlocks: a block-based language for linear programming",
    "abstract": "Linear programming is a mathematical optimization technique used in numerous fields including mathematics,economics, and computer science, with numerous industrial contexts, including solving optimizationproblems such as planning routes, allocating resources, and creating schedules. As a result of its widebreadth of applications, a considerable amount of its user base lacks programming knowledge and experienceand thus often resorts to using graphical software such as Microsoft Excel. However, despite itspopularity amongst less technical users, the methodologies used by these tools are often ad-hoc and proneto errors.Block-based languages have been successfully used to aid novice programmers and even children inprogramming. Thus, we created a block-based programming language termed LPBlocks that allows usersto create linear programming models using data contained in spreadsheets. This language guides the usersto write syntactically and semantically correct programs and thus aids them in a way that current languagesdo not. We have also implemented a web application where users can define linear programming models,reactively see their mathematical representation and execute them to obtain the optimization values forthe variables defined by the users.To assess the applicability of LPBlocks we used it to successfully express numerous and varied linearprogramming problems. Furthermore, we designed and ran a qualitative empirical study to understand theexperience our tool and language brings to users from various backgrounds. Although we see differencesamongst the users, most of them were able to model several problems using LPBlocks.",
    "authors": [
      "Gião, Hugo Afonso da"
    ],
    "keywords": [
      "Linear programming",
      "Operations research",
      "Optimization",
      "Block-based languages",
      "Visual languages",
      "Blockly",
      "Programação linear",
      "Investigação operacional",
      "Linguagens de blocos",
      "Linguagens visuais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82459",
    "title": "Metagenomic wastewater and freshwater core resistome analysis",
    "abstract": "A escassez de água é atualmente uma grande preocupação. A reutilização de águas residuais tratadas, seja por descarga em ambientes hídricos (por exemplo, rios) ou pela utilização em irrigação, é apontada como umas das principais soluções. No entanto, é importante monitorizar o possível impacto desta reutilização, sobretudo ao nível da disseminação de contaminantes emergentes como as bactérias resistentes a antibióticos (ARB) e os seus genes de resistência a antibióticos (ARGs). Este estudo teve como objetivo determinar e comparar os resistomas de diferentes amostras de água (afluente, lamas ativadas, efluente e água doce), com base em metagenomas de diferentes geografias, debases de dados públicas. O objetivo final foi identificar padrões e caraterísticas distintas entre amostras. Estes permitirão identificar ARGs como possíveis biomarcadores para monitorizar a contaminação de ambientes aquáticos com agentes biológicos de origem antropogénica.No total, 139 metagenomas (30 afluente, 30 lamas ativadas, 21 efluente, 58 de água doce) de 24 países foram analisados, usando métodos baseados em assembly e em reads. Os resultados mostraram que diferentes tipos de água partilham um grande número de ARGs. Uma nova abordagem foi usada para combinar a anotação de duas das bases de dados de ARGs mais abrangentes (CARD e ResFinder), superando a dificuldade que é lidar com anotações distintas provenientes de bases de dados diferentes. Esta abordagem permitiu determinar o resistoma core dos diferentes tipos de água, com o objetivo de obter genes biomarcadores para rastrear a contaminação em termos de resistência a antibióticos provocado pela descarga de águas residuais em ambientes recetores, como água doce. No final foram obtidos 60 possíveis biomarcadores, para os quais foram desenhadas sequências consenso que poderão ser usadas, por exemplo, para o desenho de primers.Além disso, 7 modelos de deep learning foram desenvolvidos para classificar a transferibilidade de ARGs (genes adquiridos versus intrínsecos), dada a falta de informação sobre transferibilidade. Esta distinção é muito importante quer na monitorização quer na predição do risco, visto que os ARGs adquiridos são mais propensos à disseminação entre bactérias. O modelo de Redes Neurais Convolucionais superou os restantes com destaque(MCC de 0.881 e ROC-AUC de 0.906), o que é considerado um desempenho consistente.",
    "authors": [
      "Cachetas, Diogo Macedo"
    ],
    "keywords": [
      "Biomarcador",
      "Deep learning",
      "Resistência a antibióticos",
      "Resistoma",
      "Antibiotic resistance",
      "Biomarker",
      "Deep learning",
      "Resistome",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64641",
    "title": "A data analysis approach to study events’ influence in social networks",
    "abstract": "Nowadays, the assimilation of web content, by each individual, has a considerable impacton our’ everyday life.With the undeniable success of online social networks and microblogs, such as Facebook,Instagram and Twitter, the phenomenon of influence exerted by users of such platformson other users, and how it propagates in the network, has been attracting, for some yearscomputer scientists, information technicians, and marketing specialists.Increased connectivity, multi-model access and the rise of social media shortened thedistance between almost every person in the world, more and more content is generated.Extracting and analyzing a significant amount of data is not a trivial task, Big Data techniquesare essential.Through the analysis of this interaction, an exchange of information and feelings, it isentirely imaginable its usefulness in understanding complex human behaviours and so,help diverse organization’s decision-making. Influence maximization and viral marketingare among the possibilities.This work is intended to study what is the impact and role that an event’s social influencehas and how does it propagate, particularly on its surrounding territory. This influence isinferred by analysis of the online platform’s data, by applying intelligent techniques, rightafter its extraction. The final step is to validate the results with data from different sources.Helping businesses through actionable and valuable knowledge is the ultimate goal.This document contemplates an introductory section where the study subject and itsState of the Art are addressed. Next, the problem and what direction to take to solve it arediscussed.",
    "authors": [
      "Carvalho, Nuno Miguel Vilela"
    ],
    "keywords": [
      "Social networks",
      "Intelligent techniques",
      "Social influence",
      "Redes sociais",
      "Técnicas inteligentes",
      "Influência social",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84186",
    "title": "Query optimizers based on machine learning techniques",
    "abstract": "Query optimizers are considered one of the most relevant and sophisticated componentsin a database management system. However, despite currently producing nearly optimalresults, optimizers rely on statistical estimates and heuristics to reduce the search spaceof alternative execution plans for a single query. As a result, for more complex queries,errors may grow exponentially, often translating into sub-optimal plans resulting in lessthan ideal performance. Recent advances in machine learning techniques have openednew opportunities for many of the existing problems related to system optimization.This document proposes a solution built on top of PostgreSQL that learns to selectthe most efficient set of optimizer strategy settings for a particular query. Instead ofdepending entirely on the optimizer’s estimates to compare different plans under differentconfigurations, it relies on a greedy selection algorithm that supports several types ofpredictive modeling techniques, from more traditional modeling techniques to a deeplearning approach.The system is evaluated experimentally with the standard TPC-H and Join Order ing Benchmark workloads to measure the cost and benefits of adding machine learningcapabilities to traditional query optimizers.",
    "authors": [
      "Souto, Rui Pedro Sousa Rodrigues do"
    ],
    "keywords": [
      "Database tuning",
      "Machine learning",
      "Query optimization",
      "Aprendizagem automática",
      "Otimização de queries",
      "Tuning de base de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92719",
    "title": "Quantum simulation of spin systems on quantum computers",
    "abstract": "Quantum simulation represents a formidable challenge for classical computers due to the intricate behavior of quantum systems. Digital quantum computers aim for precise approximations of a wide range of quantum systems. Within the realm of quantum simulation, the study of spin systems plays a pivotal role, providing insights into complex properties challenging to model through classical means. This work focuses on investigating chiral spin systems through the development of a dedicated quantum circuit for chirality measurement.In this Masters Dissertation, we present an overview of the current state-of-the-art in quantum simulation of chiral spin systems and introduce our approach to addressing this challenge. Scalar spin chirality,a three-body physical observable, holds a critical position both in classical magnetism, where it characterizes non-coplanar spin textures, and in quantum magnetism, serving as an order parameter for chiral spin liquids. In the context of quantum information, scalar spin chirality serves as a witness to genuine tripartite entanglement.In this study, we delve into various methodologies to tackle the problem at hand, subjecting them to comparison. The objective is to identify the most suitable approach that demands fewer quantum resources. Our best proposed method introduces an indirect measurement scheme based on the Hadamard test, designed to estimate the scalar spin chirality for general quantum states. We apply this innovative approach to measure chirality in two specific types of quantum states: the generic one-magnon states of a ferromagnet and the ground state of a model characterized by competing symmetric and antisymmetric exchange interactions. Our research findings highlight the practicality of achieving a single-shot determination of scalar chirality for chirality eigenstates, leveraging the power of quantum phase estimation with a single auxiliaryqutrit. This novel methodology extends beyond providing a solution to the chirality measurement problem; it also unifies the theory of chirality in both classical and quantum magnetism. The implications of our work offer valuable insights and pave the way for future quantum research endeavors in this domain.",
    "authors": [
      "Reascos Valencia, Irving Leander"
    ],
    "keywords": [
      "Quantum simulation",
      "Magnetism",
      "Quantum entanglement",
      "Magnetic order",
      "Simulação quântica",
      "Magnetismo",
      "Entrelaçamento quântico",
      "Ordem magnética",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27833",
    "title": "Um sistema de controlo de acessos baseado no modelo cargo-organização",
    "abstract": "O paradigma do controlo de acessos, em especial o controlo de acesso à informação, tem vindo a mudar nos últimos anos. Controlo este que inicialmente era efetuado pelas próprias aplicações de forma isolada e autónoma, sem a possibilidade de consultarem ou se integrarem com qualquer sistema centralizado. Todavia, com o crescente uso das tecnologias de informação nas organizações, novas soluções (tais como os serviços de diretoria LDAP) têm vindo a ser adotadas com o intuito de dar resposta à necessidade de uma política de acessos unificada e coesa, transversal aos diversos serviços e aplicações. Estas soluções representam uma mais-valia no desempenho das tarefas organizacionais.Tendo em conta esta necessidade, este trabalho propõe uma nova solução para o controlo de acessos físicos e lógicos através da apresentação e implementação de um novo modelo de controlo de acessos baseado no par Cargo-Organização. É ainda apresentada e implementada neste projeto uma nova abordagem no controlo de acessos lógicos, sendo esta assim capaz de interagir e configurar aplicações que carecem do suporte de protocolos e mecanismos padrão para o controlo de acessos.",
    "authors": [
      "Novais, José Pedro Vilaça"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83608",
    "title": "Ancestors Note Book: uma aplicação para memorabilia e genealogia, baseada em ontologias",
    "abstract": "O tema desta dissertação de mestrado desenrolou-se no âmbito das humanidadesdigitais, pretendendo desenhar e construir um toolkit de apoio ao registo e processamentode histórias de família, micro-história, documentos, fotografias, elementos genealógicos ehistórias de instituição, com especial interesse na inferência de indivíduos. Foi desenvolvidoum protótipo de aplicação web de uso pessoal, sobre Flask/Python, configurável (ereutilizável, salvo ajustes), como prova de conceito a suportar uma ontologia OWL2 sobre odomínio do problema.ANB = Ancestors Note Book =Ontologia AncestorsNB, paraMemorabilia e Genealogia, Relações humanas e Ambiente+ Filosofia Wiki+ Inferência Interativa e Dinâmica+ SPARQL+ Geração de formuláriosO domínio do problema induziu naturalmente uma arquitetura de ontologia, à qual foidada persistência num ficheiro de texto em sintaxe Turtle, carregada pela aplicação numgrafo RDFLib. A ontologia é expansível, mas já inclui à partida relacionamentos de genealogiade pessoas, com diversos graus de parentesco, e ainda relacionamento no âmbito social,institucional e geográfico, para pessoas, organizações e lugares. Ainda no âmbito damemorabilia, prevê-se o registo e referência de eventos, fotografias, documentos,multimédia, histórias de família, artigos sobre qualquer assunto.Na aplicação, acolhe-se uma filosofia Wiki, no sentido em que proporciona umaapresentação gráfica (suportada em mardkdown e html), navegação por hiperligações(internas ou externas), e edição. Em virtualmente qualquer classe ontológica, a apresentaçãoprettyprint de indivíduos e os formulários de ingestão permitem templating e algumaconfiguração. A apresentação prettyprint prevê documentos de variada morfologia, incluindotexto, PDF, markdown e multimédia.Permite-se ao utilizador a ingestão de informação em lote (na sintaxe Turtle, por ediçãodireta ou via ficheiro), e CRUD interativo (via formulários ou SPARQL), num ambienteoperacional rico, com pesquisas de utilizador combinadas com navegação, além de facilitara inferência sobre indivíduos, sejam pessoas ou não.A inferência é interativa (i.e., a pedido interativo do utilizador) e/ou dinâmica (poretiquetagem no conteúdo de um campo configurado como elegível para este efeito),jogando com classes, ID de indivíduo, nomes e datas. A inferência pode ter posicionamentogenealógico: basta que seja encontrado um grau de parentesco. Tanto a inferência como osmotores de pesquisa interativa se baseiam num tratamento rico de nomes e moradas emPortuguês, em que a normalização prevê grafias antigas, além de acentos e partículas deligação (de, da, do, etc.).",
    "authors": [
      "Grenhas, João Manuel Pós de Mina"
    ],
    "keywords": [
      "Humanidades digitais",
      "Ontologias OWL2",
      "Genealogia",
      "Inferência",
      "Prototipagem de aplicações web para ontologias",
      "Geração de formulários html",
      "RDFLib",
      "Processamento de linguagem natural",
      "Digital humanities",
      "OWL2 ontologies",
      "Genealogy",
      "Inference",
      "Web application prototyping for ontologies",
      "HTML form generation",
      "RDFLib",
      "Natural language processing",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92615",
    "title": "e-Saúde - design e desenvolvimento de serviços web de telemonitorização",
    "abstract": "A área da eHealth tem ganho uma enorme importância nos últimos anos. Tal advém não só da progressiva evolução tecnológica que se tem verificado ao longo dos tempos, mas também da necessidade de melhorar a qualidade de vida das pessoas que necessitam de assistência médica permanente ou ocasional. Mais especificamente, a telemonitorização permite aos pacientes estarem mais envolvidos no seu processo de recuperação e agirem mais rapidamente quando surgem novos problemas ou complicações. Neste tipo de serviços, existem geralmente três entidades, o paciente, o cuidador e os dispositivos tecnológicos, que funcionam como facilitadores entre ambos. Atualmente, existem cada vez mais serviçosde telemonitorização, mas, na maioria dos casos, estes serviços seguem uma abordagem genérica, que pode ser demasiado complexa para alguns utilizadores. Tal deve-se ao facto de incluírem por vezes funcionalidades desnecessárias para certos doentes, o que não lhes permite a melhor experiência de navegação e utilização. No âmbito da dissertação, desenvolveu-se uma aplicação web que permite aos utentes registar as suas medições através de dispositivos Bluetooth, e responder a questionários providenciados pelosseus médicos. Quer as medições, quer as respostas aos questionários poderão ser consultadas por parte dos médicos dos pacientes, permitindo assim, tal como pretendido, uma melhor monitorização e controlo do estado de saúde dos pacientes. Esta aplicação pretende ser mais simples que as restantes, utilizando serviços modulares, para que os pacientes possam escolher o que querem utilizar, de acordo com o seu perfil, sempre e quando precisarem. Cada funcionalidade principal da aplicação pode ser vista como um micro-serviço independente. Adicionalmente, o projeto segue as mais recentes abordagens tecnológicas e arquiteturais. Em termos de interface e usabilidade, procurou-se desenvolver uma aplicação atrativa e intuitiva. Por fim, é importante mencionar que o presente projeto se enquadra no âmbito de desenvolvimento da solução de telemonitorização da Altice Labs, denominada SmartAL, que permite recolher dados do paciente a partir de dispositivos inteligentes e questionários.",
    "authors": [
      "Preto, Carlos João Teixeira"
    ],
    "keywords": [
      "Telemonitorização",
      "eHealth",
      "Micro-serviço",
      "SmartAL",
      "Telemonitoring",
      "Microservices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/74179",
    "title": "Uma rede overlay controlada pelo ISP para partilha de conteúdos",
    "abstract": "Atualmente, vivemos numa era extremamente tecnológica e exigente, traduzindo-se noincremento das necessidades dos utilizadores da Internet, face aos serviços que esta lhesdisponibiliza. Neste sentido, existe uma crescente preocupação em desenvolver novos sistemasde rede virtuais, como é o caso das redes overlay, muitas delas ligadas à distribuiçãode conteúdos. Porém, como consequência deste crescimento adoptivo das redes overlay, asinfraestruturas dos fornecedores de rede (Internet Service Provider (ISP)) são sobrecarregadascom tráfego Peer-to-Peer (P2P), produzindo maiores dificuldades em relação à suaadministração. Assim, esta dissertação apresenta como objetivo primário conceber e promovero uso de uma rede overlay controlada pelo ISP, que disponha de mecanismos decontrolo de tráfego P2P, e que, em simultâneo, possibilite a oferta de um serviço de partilhade ficheiros aos seus clientes. Como tal, foi especificada uma arquitetura de redebaseada no paradigma P2P, para suportar funcionalidades colaborativas com o ISP, sendocomposta por quatro entidades principais: peer de acesso, peer de encaminhamento, coordenadore ISP. A partir desta, implementaram-se quatro aplicações correspondentes a cadauma das entidades, tendo sido criadas interfaces gráficas direcionadas para o ISP e paraos clientes (peers de acesso). Foram implementados mecanismos básicos de partilha deconteúdos com suporte de dois modos de transferência (cliente-servidor e P2P), além demecanismos de proteção e priorização de links/routers críticos (com a possibilidade de planeamentopor datas), limitação de circulação de conteúdos na rede overlay e suporte a duasestratégias distintas de encaminhamento aplicacional, como principais medidas de apoioao ISP. Posteriormente, a partir do emulador de rede Common Open Research Emulator(CORE), foram criados cenários distintos de teste, relativos aos mecanismos de colaboraçãocom o ISP implementados. Nestes, além de ser analisado o tráfego transmitido por router,de modo a verificar os impactos dos mecanismos colaborativos com o ISP, foi efetuadaa análise das rotas percorridas na rede física/aplicacional. Por fim, a partir dos cenáriosde teste efetuados, assume-se que os mecanismos de controlo de tráfego P2P, não afetamsignificativamente a qualidade do sistema da rede overlay, facilitando ao mesmo tempo astarefas de administração do ISP.",
    "authors": [
      "Silva, Miguel Gil Pires da"
    ],
    "keywords": [
      "Internet",
      "ISP",
      "Partilha de conteúdos",
      "P2P",
      "Rede overlay",
      "Content sharing",
      "Overlay network",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/76236",
    "title": "Development of a software system for monitoring outbound logistics",
    "abstract": "Currently, Information Technology (IT) is a driving factor in the process of globalization andin the innovative use of resources to promote new products and ideas, creating efficient andeffective channels to exchange information.Products based upon, or enhanced by, technology are used in nearly every aspect of lifein contemporary industrial societies. Business productivity software ensures that organizationshave the tools to overcome the challenges of executing on strategy every day andprosper in an increasingly challenging era. IT has been the catalyst for global integration,thus it is hard to imagine a company that dispenses the support of these services.Therefore, IT allows the return of investment of all the logistics businesses, shippingscheduling, procurement of materials and suppliers. The use of IT tools is considered acompetitive advantage, taking companies to invest in research and development in order toincrease profits, thus influencing their strategy and organizational model.Logistics is the process of planning, implementing, and control the efficient, cost effectiveflow and storage of raw materials, in-process inventory, finished goods and relatedinformation from point of origin to point of consumption, for the purpose of conformingto customer requirements. This project looks at the integration of logistics with IT, sinceinformation is a key element of logistics.The dissertation proposal presented in this document has as main goal, the developmentof a continuous Finished Good (FG) monitoring system, also known as outbound logistics.Given the responsibility and importance of such system different techniques and methodologiesare carefully addressed. Techniques necessary to ensure the reliability and the correctorganization of information present in the system, as well as a projection for expansionin the near future.An intelligent environment is proposed that is able to track and provide real-time informationrelated to FG in transit. The major focus is that the user should not be forcedto actively search for deviations in the supply chain,but be able to pursue other tasks,andstill be notified of any change in the transportation process. To develop this work a prototypewas devised on which the behaviour and external system interactions were thoroughlytested and validated. The result of the project is a system capable of monitoring FG in transit,as well as provide updated information to the users in order to better predict or assessany deviations.",
    "authors": [
      "Loureiro, Beatriz Ribeiro Pires"
    ],
    "keywords": [
      "Software system",
      "Logistics",
      "Systems integration",
      "Track&Trace",
      "Monitoring system",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92792",
    "title": "Explainable AI in fraud detection based on human behaviour",
    "abstract": "In recent years, Artificial Intelligence has prospered to such an extent that it is present in our daily livesthrough the different technologies we use. This fact is evident, for example, in fraud detection in onlinepurchases or games, which has been increasingly precise and has undergone an exponential evolutionas never seen before. However, the use of decision-making oriented by Artificial Intelligence and MachineLearning generates transparency concerns. Moreover, the need for explainability arises when MachineLearning models are manipulated in order to ensure fairness or to deliberate on decisions of greaterimportance. This paper proposes an approach to the general problem of explaining the decisions made by Machine Learning models through the use of Explainable AI methods. The main goal is to develop a decision support solution applied to fraud detection based on human behaviour by exploring techniques that can help explain the results of Machine Learning models and also make them more transparent.",
    "authors": [
      "Barbosa, Pedro Miguel de Soveral Pacheco"
    ],
    "keywords": [
      "Artificial Intelligence",
      "Machine learning",
      "Explainable AI",
      "Fraud detection",
      "Human behaviour",
      "Inteligência Artificial",
      "Deteção de fraude",
      "Comportamento Humano",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36619",
    "title": "Gestão inteligente de tarefas: atribuição de tarefas numa equipa",
    "abstract": "O processo de tomada de decisão revela-se um fator cada vez mais diferenciador nas empresas atuais. Sejam elas empresas diretamente relacionadas com as Tecnologias de Informação, sejam empresas de outros sectores. Nesse aspeto, a recolha e tratamento de informação crítica a cada atividade, pode ser bastante útil para ser usado no processo de tomada de decisão, sendo sempre necessário avaliar até que ponto pode a informação ser válida e qual a sua qualidade e importância no processo em que é necessária.Recorrendo a um sistema de Case-based reasoning tentar-se-á criar-se um sistema que de forma autónoma aprenda a melhor maneira de atribuir tarefas dentro de uma equipa multifacetada, aprendendo a discernir quais os colabores que melhor se adaptam a cada tarefa e cujos resultados podem ser melhores para a empresa. Esse mesmo sistema também estará apto para lidar com equipas dinâmicas aproveitando os melhores recursos que tem em cada altura.",
    "authors": [
      "Silva, António Oliveira da"
    ],
    "keywords": [
      "Raciocínio baseado em casos",
      "Redes neuronais",
      "Sistemas inteligentes",
      "Tarefas",
      "Equipas",
      "681.3"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84160",
    "title": "Analysis of message passing software using electrum",
    "abstract": "Automation developments are enabling industrial restructuring through the incorporationof more efficient and accurate processes with less associated cost. Consequently, robots arebeing increasingly used in the most various scenarios, including in Safety Critical domains.In such cases, the use of suitable methods to attest both the system’s quality and their safetyis absolutely essential.Following the current increase of complexity of cyber-physical systems, safety guardswhich used to be fully hardware dependent, are constantly migrating to software. Here upon, middleware software to abstract systems hardware are constantly evolving and arebeing increasingly adopted. The common feature of these systems is usually associated withits modular architectures based on message-passing communication patterns. A notoriouscase is the ROS middleware, where highly configurable robots are usually built by composingthird-party modules. The verification of such systems is usually very hard, and its implemen tation in real industrial environments is, in most cases, impracticable. To promote adoption,this work advocates the use of lightweight formal methods associated with semi-automatictechniques that require minimal user input and provide valuable intuitive feedback.This work explores and proposes a technique to automatically verify system-wide safetyproperties of ROS-based applications in continuous integration environments. It is basedon the formalization of ROS architectural models and nodes behaviours in Electrum, aspecification language of first-order temporal logic supported by a model-finder over which,system-wide properties are subsequently model-checked. In order to automate the analysis,the technique is deployed as an HAROS plug-in, a framework for quality assessment of ROSsoftware, specially aimed to its community.The technique proposal and its implementation under the HAROS framework are eval uated with positive results on a real agricultural robot, AgRobV16, whose dimension andcomplexity are industrially representative.",
    "authors": [
      "Carvalho, Bruno Renato Fernandes"
    ],
    "keywords": [
      "Software verification",
      "Model checking",
      "Safety",
      "Robotics",
      "Electrum",
      "ROS",
      "HAROS",
      "Verificação de software",
      "Robótica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47440",
    "title": "Development of a data integration pipeline for human metabolic models and databases",
    "abstract": "Systems Biology aims to integrate experimental and computational approaches with the purpose of explaining and predicting the organisms' behavior. The development of mathematical models in silico gives us a better in-depth knowledge of their biological mechanism. Bioinformatics tools enabled the integration of a large amount of complex biological data into computer models, but also capable to perform computational simulations with these models, that can predict the organisms' phenotypic behavior in different conditions.Up to date, genome-scale metabolic models (GSMMs) include several metabolic components of an organism. These are related to the metabolic capabilities encoded in the genome. In recent years, multiple GSMMs have been built by several research groups. With the increase in number, of these models, important issues regarding the standardization have arisen, a common problem is the different nomenclatures used by each of the research groups.In this work, the major focus is to address these problems, specifically for the human GSSMs. Therefore, the two most recent human GSMMs were selected to go through a data integration process.Integration strategies of these models most important entities (metabolites and reactions), were defined based on an exhaustive analysis of the models. The broad knowledge of their attributes enabled the creation of effective and efficient integration methods, supported by a core database developed in the local research group.The final result of this work, is a unified repository of the human metabolism. It contains all the metabolites and reactions that were automatically integrated along with some manual curation.",
    "authors": [
      "Barbosa, Susana Raquel da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64122",
    "title": "PhagePromoter: phage promoters online analysis tool",
    "abstract": "In the last decades, the emergence and evolution of the Next Generation Sequence technologies have revolutionised genomic research, leading to an exponential increase in the number of sequenced genomes. Many of the sequenced genomes belong to bacteriophages (phages), mostly due to their therapeutic potential against bacterial infections. This abundance of genomic data demands the creation of user-friendly bioinformatics tools for performing genome annotation. The most challenging step in phage genome annotation is the identification of regulatory elements, primarily promoters, to understand phage transcription regulation mechanisms.Thus, in this work, PhagePromoter, a tool for promoter prediction in phage genomes, was developed, using machine learning methods. Several models were created using different datasets and machine learning algorithms, such as support vector machines (SVM), artificial neural networks (ANN) and Random Forests (RF). All models were tested using a 5-fold cross-validation process. The datasets were composed by known phage promoter sequences, mainly retrieved from the phiSITE database, and by a different number of negative cases. After optimization, the performance was similar for all models and two were selected to be integrated in the tool: the ANN model created with the dataset containing 1600 negative examples and the SVM model created with the dataset containing 2400 negatives. The ANN model presented 92% of accuracy, 89% of precision and 87% of recall, whereas the SVM model presented 93% of accuracy, 91% of precision and 80% of recall. Hence, the first model will predict more sequences as promoters and may lead to more false positives. The SVM model will return few positive results, but most of them will be correct classified while some real promoters may not be identified by the model.PhagePromoter was integrated in the widely used Galaxy framework, available at https://galaxy.bio.di.uminho.pt/?tool_id=get_proms&version=0.1.0&__identifer=4u05obc3o5w, which provides a graphical user interface. This tool returns better results when compared to other tools, such as BPROM, PromoterHunter and CNNpromoter_e.",
    "authors": [
      "Sampaio, Marta Sofia Costa"
    ],
    "keywords": [
      "Bacteriophages",
      "Genome annotation",
      "Promoters",
      "Machine learning",
      "Models",
      "Bacteriófagos",
      "Anotação de genomas",
      "Promotores",
      "Aprendizagem máquina",
      "Modelos",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82584",
    "title": "Desenho e implementação de processos de automação de IaaS em ambientes Cloud",
    "abstract": "Com o crescimento exponencial dos serviços hospedados em ambientes digitais e consecutivamente com o aumento da criticidade dos mesmos, tem-se verificado uma procura constante por parte de desenvolvedores e gestores de projetos por plataformasque disponibilizem agilidade, flexibilidade e baixa complexidade para a disponibilização ao público das suas soluções o mais rápido possível com o mínimo de esforço por parte dos mesmos. Este trabalho tem como objetivo precípuo o estudo e implementação de uma plataforma de IaaS em Cloud privada, com a finalidade de disponibilizar a várias entidades da área da saúde uma plataforma centralizada de gestão de ativos de computação e de rede, com o intuito de facilitar, perante o paradigma passado, a logística associada à criação e coordenação dos ativos por parte das mesmas. Para tal o presente trabalho propõe um estudo de mercado, seguido de uma análise de formas e plataformas de automação de processos a serem implementados intrinsecamente e/ou extrinsecamente à plataforma de Cloud privada, de modo a trabalharem em simbiose. São também apresentadas metodologias de desenho de scripting necessário para a realização dos casos de uso propostos pela entidade SPMS, assim como o processo utilizado para a integração da solução com plataformas e serviços terceiros.Com este trabalho intenciona-se proporcionar à SPMS a otimização dos seus recursos computacionais e de rede, bem como diminuir drasticamente as horas humanas dedicadas a processos repetitivos e iterativos, canalizando-as para processos mais nobres.",
    "authors": [
      "Ferreira, João Pedro de Vasconcelos Cadavez"
    ],
    "keywords": [
      "IaaS",
      "Cloud privada",
      "Automação",
      "Scripting",
      "Otimização",
      "Private Cloud",
      "Automation",
      "Optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92596",
    "title": "Aplicação de monitorização de rede baseada em Blockchain",
    "abstract": "Na era atual dos avanços tecnológicos, a blockchain emergiu como uma grande inovação, ganhando reconhecimento generalizado pelas suas capacidades de manutenção de registos seguros e transparentes.Ao mesmo tempo, a gestão e a comunicação eficazes de dados críticos da infraestrutura de tecnologiasda informação (TI) continuam a ser uma necessidade premente. Esta dissertação aborda a intersecçãodestes domínios, com o objetivo de aproveitar o potencial da tecnologia blockchain para otimizar e proteger os processos de comunicação de dados. Patrocinada pela dstelecom, um operador de rede de fibraótica neutra em Portugal, esta investigação investiga o papel transformador que a blockchain pode desempenhar na monitorização da rede. Com base numa análise abrangente da literatura, o estudo examinaa capacidade de aplicação e os desafios da blockchain, apresentando um caso de estudo prático, queutiliza o Hyperledger Fabric, para destacar o seu potencial na monitorização da rede.",
    "authors": [
      "Amorim, João Manuel Silva de"
    ],
    "keywords": [
      "Blockchain",
      "Hyperledger fabric",
      "Monitorização de rede",
      "Network monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84071",
    "title": "Epidemic broadcast algorithms in a Byzantine environment",
    "abstract": "Peer-to-peer broadcasting algorithms are a scalable and cheap way of disseminating information to a largenumber of participants. However, most of these algorithms do not consider the possibility of some membersacting in an unintended way, with malicious or selfish motives. In order to be useful in a real world scenario,these algorithms must be secure, robust and efficient, even in the presence of adversaries. This thesis presents an overview of the challenges that peer-to-peer broadcasting algorithms face, as well as some of the security mechanisms that can be employed to mitigate them. Thus, we present Bycast, a secure and efficient peer-topeer broadcasting algorithm that is able to tolerate up to 45% of malicious nodes in the system. In order to achieve a high level of security, Bycast relies on strong membership integrity guarantees that make it harder for attackers to successfully compromise other nodes. In order to force nodes to cooperate, and contribute to the good performance of the system, Bycast employs an innovative auditing scheme that is able to detect nodes that are not cooperating with their resources, and evict them from the system.",
    "authors": [
      "Costa, Tomás Francisco Cruz"
    ],
    "keywords": [
      "Peer-to-peer",
      "Gossip",
      "Broadcasting",
      "Byzantine",
      "Disseminação",
      "Byzantino"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79934",
    "title": "Plataforma online para visualização de tendências e opiniões de eventos desportivos com base em redes sociais",
    "abstract": "A extração e o processamento de dados nas redes sociais são complementares e, por vezes, fundamentais no apoio à tomada de decisões em diversos negócios. Os eventos desportivos não são exceção, podendo dessa forma beneficiar dessas técnicas. Nesse contexto, os clubes desportivos podem aproveitar as informações obtidas nas redes sociais durante determinados eventos para dar suporte nas decisões, podendo dessa forma melhorar a experiência do espectador e, assim, atrair mais audiência. Foi necessário realizar uma análise ao tipo de dados a serem recolhidos das redes sociais, assim como a melhor forma para os exibir ao utilizador, dando ao mesmo uma boa experiência. Foi elaborado um protótipo como solução para o desafio, apresentando assim uma plataforma web, alinhada às melhores práticas de User Experiente e User Interface, que auxilia clubes desportivos e produtores de conteúdo televisivo a visualizar facilmente dados sociais, durante ou após um evento desportivo em específico. Foram apresentados os resultados obtidos com esta proposta, demonstrando que é possível obter informações valiosas e relevantes para os clubes desportivos, conseguindo captar e reunir certos conhecimentos em tempo real.",
    "authors": [
      "Pinto, Maria Inês Vieira"
    ],
    "keywords": [
      "Dashboard",
      "Estatísticas desportivas",
      "Análise semântica",
      "Análise de dados",
      "Eventos desportivos",
      "Sports statistics",
      "Semantic analysis",
      "Data analysis",
      "Sports events",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60041",
    "title": "Monitoring attention and performance on critical situations",
    "abstract": "In our current lifestyle we often face situations that push us a bit further, over ourordinary limits. These situations can be considered as stress moments that make us reactdifferently from a normal situation, both in a physical or psychological way. While a breakcan be beneficial to our mind and body when we guarantee proper rest after experiencingthis types of moments, when, for instance, we do a physical activity, a chain of stressfulmoments can have negative impact on human beings leading to serious health problemson the long run if there has not been any preventive action. Whereas the physical aspect iseasily understood when looking at, the psychological side is usually forgotten orundervalued because one cannot waste time and focuses on better achievements or simplylacks understanding of this matter. One of the reasons behind this inevitable fatigue is thehigh competitiveness in the markets, which forces employees to work harder or for longerperiods so as to accomplish the same results as in shorter periods of time. In theoryworking harder can lead to more productivity thanks to challenging factors or it can havethe opposite effect when workers suffer from situations like stress or increase of fatigue. Inthis dissertation we will examine the relation between performance and mental fatigueand will prove how this association works with the help of a simulation environmentcreated for this purpose.",
    "authors": [
      "Cardoso, Paulo Gonçalves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47450",
    "title": "Machine learning approaches for predicting effects of drug combinations in cancer",
    "abstract": "Drug combination therapies are commonly used to overcome tumor drugresistance. Computational methods can be helpful tools in drug combinationdiscovery, but there are currently no e stablished methods for the prediction ofdrug combination effects.This work, integrated in the AstraZeneca -Sanger Drug CombinationPrediction challenge launched by the Dialogue for Reverse EngineeringAssessments and Methods (DREAM) community, aimed to develop machinelearning methods to estimate the effects of drug combinations on cancer celllines. The challenge was divided into three subchallenges (1A, 1B, and 2)addressing different clinical scenarios.A variety of machine learning models were devel oped and evaluatedusing cross-validation. Tree-based ensembles, particularly GB, performed bestfor this problem. Among the different the genomic datasets provided, themonotherapy, mutation and CNV datasets were the most informative and werethe only ones used in the final models.The best model, submitted to 1A, was an ensemble of gradient boosting(GB), random forest (RF), and partial least squares (PLS) regression models,having achieved an average weighted Pearson correlation of 0.30, and ranking24th among 76 submissions. The 1B model (average weighted Pearsoncorrelation of 0.18; 47th/62 submissions) was also an ensemble of GB, RF,and PLS models. For subchallenge 2, a GB model was selected. It had aperformance score (based on a three-way analysis of variance (ANOVA) ) of 5.15and ranked 20th out of 39 submissions.The strategies explored in this work and by the DREAM challengecommunity will help to further the development of computational methods forthe rational design of effective drug combinations for cancer therapy.",
    "authors": [
      "Baptista, Delora Soeiro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91563",
    "title": "iQbricks: integration of a fully-featured quantum language in the framework Qbricks",
    "abstract": "Quantum Computing has noticeably grown over the last two decades, making it a revolu tionary field of investigation in the current era of technological research.Such a growth has been leading to an increasing demand in research by several big enter prises such as IBM, Google and Microsoft, paving the way for a richer ecosystem and untoldbenefits among the Quantum Computing community.Verification is a crucial aspect of software development, as it ensures that a program per forms as intended and reduces the risk of introducing errors. This is especially importantin the field of Quantum Computing, where the complexity of programs is high and thebehavior of quantum systems is often counterintuitive. Verification of quantum programscan help detect errors that may lead to incorrect results, which is of utmost importance whendealing with quantum algorithms and quantum simulations. As a result, having a formalverification framework for quantum programs can greatly benefit the development of reliableand accurate quantum software. Qbricks is a verification framework for building quantumprograms, and corresponds to the framework on which this project has been integrated.During the course of this thesis, iQbricks – an intuitive and user-friendly language tobuild and formally verify quantum programs – was developed, along with a framework totranslate and generate verifiable Qbricks programs from iQbricks.This project’s main achievements were: (1) the design and implementation of a high-levelprogramming language for describing quantum circuits in an intuitive and user-friendlyway and (2) the implementation of a translator, embedded in Qbricks’ framework, thatconverts iQbricks programs to Qbricks ones.The developed framework was evaluated against two different quantum algorithms: theQuantum Fourier Transform and Grover’s algorithm.This project was accompanied by an internship at the Commissariat à l’énergie atomique etaux énergies alternatives (CEA) - LSL, where this implementation was developed in directinvolvement with Qbricks’ team of investigators.",
    "authors": [
      "Carneiro, Tomás Barros"
    ],
    "keywords": [
      "Quantum computing",
      "Qbricks",
      "Formal verification",
      "Quantum programming language",
      "Quantum fourier transform",
      "Grover’s algorithm",
      "Computação quântica",
      "Verificação formal",
      "Linguagem para programação quântica",
      "Transformada de fourier quântica",
      "Algoritmo de Grover",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92697",
    "title": "Simulation of hybrid systems regulated by newtonian mechanics",
    "abstract": "The evolution of software products that interact with the physical world has led to a greater need to simulatetheir behavior in order to verify their effectiveness and safety in different scenarios. This dissertation projectaims to enhance a simulation tool for hybrid programs called Lince, more specifically to provide morepowerful simulation capabilities to hybrid programs regulated by Newtonian mechanics. These include theaddition of new language constructs (such as the division operator and the trigonometric functions), theimplementation of non-linear expressions, grammar relaxation and organization, improved error detection,and the mitigation of existing tool-related issues.Throughout this dissertation, it is discussed how the implementation of these improvements benefitsthe simulation of hybrid programs and are explained the key methods adopted for their conception. Finally,this new version of Lince is put to the test by handling case studies related to autonomous driving (forexample, adaptive cruise control and a missile targeting a moving object) and other types of systemsas well, such as purely physical systems and the so-called on-off systems. The results obtained in thetreatment of these case studies attest to the enhanced capabilities of this tool and the contribution of thisdissertation to the scientific community, demonstrating its relevance in simulating integrated systems ineveryday life.",
    "authors": [
      "Correia, Ricardo da Silva"
    ],
    "keywords": [
      "Lince",
      "Hybrid programming",
      "Autonomous driving",
      "SageMath",
      "Programação híbrida",
      "Condução autónoma",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79938",
    "title": "Energy-aware Gossip Protocol for Wireless Sensor Networks",
    "abstract": "In Wireless Sensor Networks (WSNs), typically composed of nodes with resource constraints, leveraging efficient processes is crucial to enhance the network longevity andconsequently the sustainability in ultra-dense and heterogeneous environments, such assmart cities. Epidemic algorithms are usually efficient in delivering packets to a sink orto all it’s peers but have poor energy efficiency due to the amount of packet redundancy.Directional algorithms, such as Minimum Cost Forward Algorithm (MCFA) or DirectedDiffusion, yield high energy efficiency but fail to handle mobile environments, and havepoor network coverage.This work proposes a new epidemic algorithm that uses the current energy state of thenetwork to create a topology that is cyclically updated, fault tolerant, whilst being ableto handle the challenges of a static or mobile heterogeneous network. Depending on theapplication, tuning in the protocol settings can be made to prioritise desired characteristics.The proposed protocol has a small computational footprint and the required memory isproportional not to the size of the network, but to the number of neighbours of a node,enabling high scalability.The proposed protocol was tested, using a ESP8266 as an energy model reference, in asimulated environment with ad-hoc wireless nodes. It was implemented at the applicationlevel with UDP sockets, and resulted in a highly energy efficient protocol, capable of leveraging extended network longevity with different static or mobile topologies, with resultscomparable to a static directional algorithm in delivery efficiency.",
    "authors": [
      "Ferreira, Bruno Chianca"
    ],
    "keywords": [
      "Energy-aware",
      "Epidemic",
      "IoT",
      "Routing",
      "WSN",
      "Epidemico",
      "RSF",
      "Roteamento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81354",
    "title": "Optimization of deep learning algorithms for an autonomous RC vehicle",
    "abstract": "This dissertation aims to evaluate and improve the performance of deep learning (DL)algorithms to autonomously drive a vehicle, using a Remo Car (an RC vehicle) as testbed.The RC vehicle was built with a 1:10 scaled remote controlled car and fitted with anembedded system and a video camera to capture and process real-time image data. Twodifferent embedded systems were comparatively evaluated: an homogeneous system, aRaspberry Pi 4, and an heterogeneous system, a NVidia Jetson Nano. The Raspberry Pi 4 withan advanced 4-core ARM device supports multiprocessing, while the Jetson Nano, also witha 4-core ARM device, has an integrated accelerator, a 128 CUDA-core NVidia GPU.The captured video is processed with convolutional neural networks (CNNs), whichinterpret image data of the vehicle’s surroundings and predict critical data, such as lane viewand steering angle, to provide mechanisms to drive on its own, following a predefined path.To improve the driving performance of the RC vehicle, this work analysed the programmedDL algorithms, namely different computer vision approaches for object detection and imageclassification, aiming to explore DL techniques and improve their performance at the inferencephase.The work also analysed the computational efficiency of the control software, while runningintense and complex deep learning tasks in the embedded devices, and fully explored theadvanced characteristics and instructions provided by the two embedded systems in thevehicle.Different machine learning (ML) libraries and frameworks were analysed and evaluated:TensorFlow, TensorFlow Lite, Arm NN, PyArmNN and TensorRT. They play a key role todeploy the relevant algorithms and to fully engage the hardware capabilities.The original algorithm was successfully optimized and both embedded systems couldperfectly handle this workload. To understand the computational limits of both devices, anadditional and heavy DL algorithm was developed that aimed to detect traffic signs.The homogeneous system, the Raspberry Pi 4, could not deliver feasible low-latency values,hence the detection of traffic signs was not possible in real-time. However, a great performanceimprovement was achieved using the heterogeneous system, Jetson Nano, enabling theirCUDA-cores to process the additional workload.",
    "authors": [
      "Pereira, André Filipe Amorim"
    ],
    "keywords": [
      "Computer vision",
      "Parallel computing",
      "Deep learning",
      "Inference",
      "Homogeneous programming",
      "Heterogeneous programming",
      "Optimization",
      "Autonomous driving",
      "Visão por computador",
      "Computação paralela",
      "Aprendizagem profunda",
      "Inferência",
      "Programação homogénea",
      "Programação heterogénea",
      "Otimização",
      "Condução autónoma",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84671",
    "title": "Automate the learning process of an item identification model in palletized packages",
    "abstract": "Numa determinada empresa, está a ser desenvolvido um sistema que visa identificar e classificarcertos objetos em embalagens paletizadas O sistema é composto por um conjunto de câmaras colocadas numa estação de carregamento que executa várias capturas de imagens. A isto segue-se umprocessamento que termina numa deteção e identificação de certos objetos que determinam as regrasde empacotamento. Estas regras devem estar de acordo com as regras definidas pelo cliente. A soluçãoatual passa por anotar manualmente as imagens exemplo (processo lento e demorado) para serem utilizadas para o treino do algoritmo de deteção de objetos. Depois do treino estar terminado, o modelo dedeteção fica imutável, o que significa que este é incapaz de evoluir com os resultados de verificação quese podem seguir.O âmbito desta dissertação é o de estudar,propôr e desenvolver uma solução para automatizar partes do processo de treino de um sistema de visão por computador que executa deteção de objetos deitems paletizados através da implementação de geração automática de imagens e a automatização dasua anotação. Já foram desenvolvidos e cientificamente aprovados alguns métodos que produzem anotações automáticas recorrendo a processos de active learning, classificação e feature transferring, bemcomo softwares de anotação de imagens que são capazes de executar tal tarefa. A proposta apresentadaintroduz uma estratégia para gerar imagens sintéticas e as anotações associadas para serem utilizadaspara treinar modelos de deteção de objetos de modo a tornar mais ágil o processo de coleção e anotação de imagens. Os resultados mostram que, no geral, os modelos treinados com imagens sintéticasdemonstram melhores resultados que o modelo existente que foi treinado com imagens reais anotadaspor um anotador. Demonstrando que é possivel criar modelos de deteção de objetos que obtenham bonsniveis de desempenho, que foram treinados apenas com dados sintéticos.",
    "authors": [
      "Veloso, Gonçalo Jorge Vilas-Boas"
    ],
    "keywords": [
      "Anotação de imagens",
      "Continual learning",
      "Transfer learning",
      "Deteção de objetos",
      "Image annotation",
      "Object detection",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/87351",
    "title": "From the choroid plexus to the (sub)ventricular cells for oligodendrocyte (re)generation",
    "abstract": "Multiple sclerosis (MS) is an autoimmune demyelinating disorder that affects the central nervoussystem by damaging myelin and axons, but the exact cause of MS remains unclear. It is knownthat the immune system destroys oligodendrocytes (OLs), the myelinating cells of the CNS, andregardless of the efforts, it remains challenging to induce replacement of the lost OLs. Thechoroid plexus (CP) is fundamental for brain homeostasis as it secretes the cerebrospinal fluid.It is also a key modulator of neurogenesis and constitutes a site of neuroinflammation. Since MSis an inflammatory disorder, studying the relation between the CP and ventricular-subventricularzone (V-SVZ) is of interest. In this dissertation we explored the role of the CP in the modulationof neural stem cells and OLs progenitor cells (OPCs), located in the SVZ, in order to assess itspotential to induce OL (re)generation. For that we used single cell RNA sequencing (scRNA-seq)data from both control and an MS model generated by our team.Here we explored the cell types, RNA velocities and cell-cell communication (CCC) inference.We have characterized the cell populations present in the V-SVZ of controls and MS model, andfound differences in the clusters identified. RNA velocity revealed two central cores from wheremost of progenitor cells seamed to arise from, being one located at the neuronal intermediateprogenitor cells cluster and the other one located in the astrocytes cluster. Further analysishas to be performed in order to confirm these results. Concerning CCC inference, we foundevidence of signalling pathways between endothelial cells and pericytes with OPCs, whichcorroborates previous studies where this communication was reported. Together with literature,our data indicates that endothelial cells and pericytes regulate OPCs proliferation through Pdgfsignalling. When analyzing the results from CCC inference in the CP-SVZ integrated data wefound evidence of a communication from mesenchymal cells at the CP and ependymal cellsthrough Wnt signalling. Another interaction was between OPCs and ependymal cells throughTENASCIN signalling. Nevertheless, the existence of this interactions in vivo needs to be furthervalidated.",
    "authors": [
      "Fernandes, Mónica da Silva"
    ],
    "keywords": [
      "scRNA-seq",
      "Multiple Sclerosis",
      "Choroid plexus",
      "Subventricular zone",
      "Oligoden-drocytes",
      "Esclerose Múltipla",
      "Zona subventricular",
      "Oligodendrócitos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86736",
    "title": "Graph kernels and neural networks for predicting yields of chemical reactions",
    "abstract": "Predicting chemical reaction yields is a widely investigated problem in drug discovery due to the natu ral appearance of diseases and viruses worldwide. With the evolution and further study of machine learn ing, this area of computer science has provided alternatives that help chemists search for more effectivemolecule combinations. This dissertation presents two research hypotheses with different bases that seekto improve this prediction problem. The first research hypothesis is related to the support vector regressionalgorithm, which uses graph kernels to measure similarity between molecules and then perform the pre diction. We propose the application of non-linearity in the Weisfeiler-Lehman graph kernel to improve themeasure of comparison between molecules and thus enhance the complexity of the support vector regres sion models. The second research hypothesis is related to the class of neural networks. We propose a deeplearning base to solve this problem through graph neural networks, which use graph convolutional layersand global read-out operations to extract molecular features from graph-structure data. The main focus isto ensure that all models generalise well to obtain good results in experiments with unknown molecules.We performed tests on chemical data for both methods and achieved improvements. The non-linearity ingraph kernels proved to be the most advantageous, having surpassed the state-of-the-art methods in oneof the two global tests performed. The graph neural networks were not as effective, although they showedcompetitive results. Concerning neural networks, we highlight the creation of the deep learning base andthe in-depth analysis of the hyperparameters to enhance further research on the reaction yield predictionproblem, as this area shows immense potential in drug discovery.",
    "authors": [
      "Braga, Diogo Filipe Ribeiro Ferreira"
    ],
    "keywords": [
      "Machine learning",
      "Chemistry",
      "Graphs",
      "Kernel methods",
      "Graph neural networks",
      "Química",
      "Grafos",
      "Redes neuronais de grafos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80228",
    "title": "Learning user well-being and comfort through smart devices",
    "abstract": "The growth of concepts such as Intelligent Environments and Internet of things allows us to understand the habits of users and consequently act to improve people’s daily lives. Through information gathering, it is thus possible to gather patterns about different kinds of human behavior and consequently build a learning model with predictive capabilities. In addition, there are increasing concerns from large companies about the influence, positive or negative, that aspects such as comfort and well-being have on the behavior and health of the population. In fact, as human beings, we are greatly influenced by the environment in which we are inserted. There are therefore conditions in a place that give us certain levels of comfort that will eventually interfere with our well-being. However, it is difficult to identify which of these factors are relevant and how they intervene in our daily lives. Also, the habits we adopt as a result of the routines we follow can contribute to improving or worsening any of these indicators With the help of the various types of sensors present, for example, in the smart devices (smartphones, smartwatches, wristbands), it is increasingly possible to collect information on these factors, easily and comprehensively. In this sense, firstly the main objective of this dissertation is thus to collect data on factors that may influence the user in order to create a user profile. These factors can be inferred through its interests, the visited locations, and its main activities. This objective involves a large-scale analysis, where there are no geographical restrictions. Furthermore, the study will be independent of the type of space (open or closed) that is explored. In that way, the perspective that will be used is from the user. Then there is an exploration of the data so that some intelligence can be inferred, and in this sense, build a mobile application capable of providing smart notifications based on user needs.",
    "authors": [
      "Sousa, David José Teixeira de"
    ],
    "keywords": [
      "Ambient Intelligence",
      "Comfort",
      "Deep Learning",
      "Machine Learning",
      "Smart Devices",
      "Well-being",
      "Aprendizagem Máquina",
      "Aprendizagem Profunda",
      "Bem-estar",
      "Conforto",
      "Dispositivos Inteligentes",
      "Inteligência Ambiente",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86806",
    "title": "Acordo distribuído para arquiteturas de microsserviços",
    "abstract": "Alcançar o consenso distribuído é fundamental para que se consigam construir sistemas tolerantes a faltas, pois permite que uma coleção de processos opere como um grupo coerente que pode sobreviver a falhas de alguns dos seus membros. O algoritmo Raft resolve o problema de consenso e visa ser o mais compreensível possível, porém as atuais implementações deste algoritmo são dedicadas a casos de uso específicos. Consequentemente, quem desenvolve sistemas que tenham que contemplar componentes replicados vê-se obrigado a construir o seu próprio mecanismo de replicação, o que pode ser contraproducente e até ter certas implicações no código da aplicação. Paralelamente, as arquiteturas de microsserviços passaram a ser o novo normal, fazendo oposição à construção de sistemas monolíticos. Dada a sua natureza, este tipo de arquiteturas permite endereçar problemas que dizem respeito à resiliência e coerência dum dado serviço, existindo por isso uma oportunidade para cruzar algoritmos de consenso distribuído com microsserviços. Nesta dissertação propõe-se a construção de duas implementações de Raft num toolkit aplicacional típico de microsserviços, mais especificamente Spring Boot. Cada implementação deverá utilizar uma das diferentes stacks da framework, nomeadamente, a serviet stack ou a stack reativa. Ambas as implementações deverão ser modulares e genéricas o suficiente, para que possam ser simultaneamente configuráveis e aplicáveis a diferentes casos de uso. Para o efeito, começa-se por delinear as configurações em que o middleware poderá operar, assim como a arquitetura interna do mesmo, seguindo-se da fase de implementação, que detalha decisões tomadas ao longo da mesma. A fase de avaliação começa com a implementação, em ambas as stacks, de uma aplicação de armazenamento chave-valor que é configurada com diferentes parâmetros, para que finalmente possa ser comparada com o etcd que é um armazenamento chave-valor replicado. Desde logo, os resultados recolhidos fazem prever o desempenho de ambas as soluções de middleware construídas, que ficam aquém dos desempenhos alcançados por um cluster etcd, mas que dão garantias de viabilidade e extensibilidade, uma vez que as soluções são modulares para integrarem novas otimizações, e são genéricas e úteis para qualquer aplicação que necessite de assegurar garantias de coerência forte.",
    "authors": [
      "Silva, João Pedro Oliveira da"
    ],
    "keywords": [
      "Tolerância a faltas",
      "Consenso",
      "Raft",
      "Microsserviços",
      "Sistemas monolíticos",
      "Spring boot",
      "Middleware genérico",
      "Modularidade",
      "Fault tolerance",
      "Consensus",
      "Microservices",
      "Monolithic systems",
      "Generic middleware",
      "Modularity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86610",
    "title": "Adequa, a platform for choosing games suitable to students’ profile",
    "abstract": "Computational Thinking has been increasingly explored in the area of teaching. Manyresearchers believe that the early introduction of this concept leads to a better understandingof multiple fields like Computer Science, Math and Engineering. However, the inclusion ofComputational Thinking as part of the educational program needs to be carefully done. Forthat, we need to choose the right Learning Resources.As Game-Based Learning was proven to be effective by numerous researchers, in thisproject is argued that games are proper Learning Resources to develop ComputationalThinking. With Game-Based Learning, this work aims to improve students’ motivationand learning experience on Computational Thinking by choosing the most suitable gamesfor each student. To find the relation between students and games, it is necessary toanalyze each of them. First, to differentiate types of games, OntoJogo, an ontology for gameclassification, was built. The usability and coverage of OntoJogo were tested in an experimentconducted with five participants. Secondly, it was required to profile the students throughthe analysis of sociodemographic, competencies, and psychological factors. For that, a profilequestionnaire was developed with the collaboration of two child psychologists. Lastly, agame evaluation questionnaire was designed for the students to complete, making it possibleto connect game classifications with students’ profiles.With these tools, it was possible to develop a platform for games suggestion, fulfilling theprimary goal of this project. The platform Adequa supports the registration of games andstudents and the evaluation of games. Additionally, Adequa recommends the most suitablegames for each student. For the recommendation of games, it was designed an algorithmthat uses the data collected from the questionnaires and returns a list of suitable games. Thealgorithm was developed from the results of an experiment conducted with twenty-fourparticipants, where it was searched patterns between the participants and game types. Fromthe results, it was found that variables like gender, gaming habits, and emotional factorscan influence the motivation a student feels towards a game. This experiment was essentialto prove the hypothesis that it is possible to relate students and games. Based on thisconclusion, it is right to affirm that the future of education must pass through a personalizedexperience, starting with the learning resources used.",
    "authors": [
      "Teixeira, Maria de La Salete Dias"
    ],
    "keywords": [
      "Computational thinking",
      "Game-based learning",
      "Game types",
      "Learning resource",
      "Ontology",
      "Student profile",
      "Aprendizagem baseada em jogos",
      "Ontologia",
      "Pensamento computacional",
      "Perfil do aluno",
      "Recurso de aprendizagem",
      "Tipos de jogo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55646",
    "title": "Using software defined networking for flexible network measurements",
    "abstract": "Network management evolved in a way where implementing complex, high level networkpolicies, implies dealing with some attributes that depend on low-level specific configuration.This reflects on a difficulty of changing the underlying infrastructure. SDN (Software-Defined Networking) concept opens a road for new developments due to the centralizednon vendor-specific control of the network, most of it related with the separation of dataand control planes. Since collecting actual data to create information is important at thetime of taking decisions, network operators need to understand the dynamic of their networkthrough monitoring and sampling. An SDN approach offers different possibilitiesto solve network managing problems, raising new points of view on how networks canoperate and, consequently, how they can be managed and monitored. This study is mainlyfocused on exploring the SDN architecture, and its elements, for applying sampling techniquesthrough flexible network measurements. To pursue this, SDN elements will be presentedand explained, alongside with existing monitoring solutions. These solutions, afterexplored and analysed, will lead to a new approach on applying and configuring flexiblesampling techniques on SDN.",
    "authors": [
      "Silva, Catarina Isabel Pires da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66578",
    "title": "PlaCoR: plataforma para a computação orientada ao recurso",
    "abstract": "A Plataforma para a Computação orientada ao Recurso (PlaCoR) foi desenhada como umambiente de programação e execução de aplicações baseadas no modelo da computação orientadaao recurso (CoR), especificado em CoRes, integralmente escrito em C++ Moderno.A escolha do C++ trouxe enormes vantagens, no suporte à: i) programação orientada aosobjetos, através da herança múltipla (na construção dos recursos); ii) programação genérica(permitindo abstrair na API as diferentes classes de recursos); iii) programação concorrente(para tirar partido de fios de execução e estruturas de sincronização nativas ao C++).A plataforma possui facilidades para: i) comunicação inter-domínios, ii) passagem de mensagensentre recursos comunicantes, iii) memória partilhada distribuída (DSM), iv) ativaçãoremota de fios de execução (RPC), v) criação e gestão de recursos e vi) gestão da consistênciaentre todas as réplicas de um recurso.Atualmente, o desenho de aplicações CoR assenta nos recursos domínio, grupo, clausura,agente, proto-agente, dado, barreira, guarda e guarda para leituras/escritas. Os domíniosestabelecem o primeiro nível de concorrência/paralelismo, quer sejam criados no início daaplicação ou lançados dinamicamente. Os agentes, pelo seu lado, estão associados ao grãofino de paralelismo e de comunicação por passagem de mensagens.O domínio, o grupo e a clausura são recursos estruturados que disponibilizam operaçõesde adesão/saída de recursos; distingue-os o facto dos dois primeiros serem dinâmicos enquantoa clausura é estática, na medida em que as operações de adesão/saída são coletivase o número total de membros é fixado inicialmente - características necessárias para o arranqueparalelo de aplicações do tipo SPMD e a passagem de mensagens intra-clausura.A guarda é usada para a criação de zonas de exclusão mútua distribuídas (leituras/escritas),a barreira para a sincronização entre agentes, enquanto o dado contempla os mecanismosde memória partilhada distribuída, usado para disponibilizar os dados do utilizador numambiente de domínios distribuídos.A avaliação da plataforma tomou como exemplo de aplicação a leitura e processamentode eventos registados em TTree, recorrentemente usados na experiência ATLAS. As váriasversões desenvolvidas justificaram a criação de um módulo específico, a unidade Pool, querealiza o modelo fork-join.O experimento confirmou a viabilidade da orientação ao recurso como paradigma de programaçãohíbrido que integra múltiplos fios de execução e sincronização distribuída, comfacilidades de comunicação de grão fino para a passagem de mensagens e de comunicaçãoem contextos seguros, o acesso remoto a memória e a ativação remota de agentes.",
    "authors": [
      "Ribeiro, Bruno Manuel Gonçalves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82512",
    "title": "iOS development: increasing Home Banking reliability with integration of strong authentication mechanism",
    "abstract": "Neste momento, as aplicações móveis encontram-se cada vez mais presentes no quotidiano de cada individuo, permitindo desempenhar diferentes tarefas, tais como gerir contas bancárias e transação de fundos monetários. Devido à rápida adoção e desenvolvimento de IoT, é também importante garantir a proteção da segurança dos Utilizadores de ataques cibernauticos impedindo o acesso destas contas e execução de determinadas operações não autorizadas pelos respetivos títulares de contas via aplicações de Home-Banking.Tendo isto em consideração, esta Dissertação tem como principal objetivo analisar e integrar uma camada de segurança para Autenticação e Validação de transações de fundos, designada por TrustFactor, numa aplicação existente de Home-Banking.Visto que a implementação será vocacionada para dispositivos móveis serão abordados temas relacionados com os Paradigmas de Desenvolvimento de Aplicações e tecnologias usadas no ecossistema Apple.",
    "authors": [
      "Solans, Carlos Miguel Rebelo"
    ],
    "keywords": [
      "Home-Banking",
      "Desenvolvimento iOS",
      "Desenvolvimento móvel",
      "Segurança móvel",
      "CocoaTouch",
      "iOS Development",
      "Mobile development",
      "Mobile security",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92706",
    "title": "Development of a front-end application for a human-robot collaborative framework",
    "abstract": "ErgoAware aims at developing technology that reduce the worker’s exposure to ergonomic risk (e.g.poor postures) as there is a high incidence of Work-Related Musculoskeletal Disorders (WRMSDs), whichrepresents an economic burden of 240 billion euros and is driven by the Industry 4.0 paradigm. Anothergoal of ErgoAware is also to optimize Human-robot Collaborative processes to shape the robot’s assistanceaccording to the individual physiological requirements of each user.This dissertation main goal is the development of a desktop graphical interface application to allowvisualization of the workers kinematic model, allow real-time monitoring of postural and fatigue metrics,and provide an abstraction layer for the ROS backend in order to allow the configuration of the integratedsensing technologies and develop control strategies. The objective will be for users to be able to manipu late ErgoAware’s adjustable parameters in an intuitive manner, with a user-friendly interface for easy toolusability.",
    "authors": [
      "Martins, Guilherme da Silva Amorim"
    ],
    "keywords": [
      "Ergonomics",
      "Human-robot collaboration",
      "User interface",
      "Posture control",
      "Ergonomia",
      "Colaboração homem-robot",
      "Interface do utilizador",
      "Controlo de postura",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80875",
    "title": "Implementation and evaluation of tagged causal multicast as a rust library",
    "abstract": "Causal Consistency is gaining importance in modern geo-replicated distributed services: itis the strongest consistency model that does not sacrifice availability under high latency andnetwork partitions. However, traditional causal delivery middleware, while ensuring a de livery order consistent with causality, does not provide client applications with knowledgeabout the end-to-end (as seen by each client process) happens-before relation. An end-to-endhappens-before is essential to modern applications, namely for the semantics of operation based CRDTs, but also for traditional applications, in which its absence may cause incorrectbehavior when using traditional causal delivery middleware. This thesis designs and im plements a Tagged Causal Multicast middleware service as a Rust library. Rust was chosenbecause it is a safe concurrent and fast programming language supporting both functionaland imperative paradigms. This allows an efficient implementation where the use of com plex data structures does not decrease the performance as would be the case of usingfunctional languages like Erlang. Finally, an empirical evaluation of the performance ofthis middleware service is made, comparing the novel graph-based implementation againsta more traditional one based on vector clocks.",
    "authors": [
      "Pereira, Carlos Duarte Afonso"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79397",
    "title": "High availability architecture for cloud based databases",
    "abstract": "Com a constante expansão de sistemas informáticos nas diferentes áreas de aplicação, aquantidade de dados que exigem persistência aumenta exponencialmente. Assim, porforma a tolerar faltas e garantir a disponibilidade de dados, devem ser implementadastécnicas de replicação.Atualmente existem várias abordagens e protocolos, tendo diferentes tipos de aplicaçõesem vista. Existem duas grandes vertentes de protocolos de replicação, protocolos genéricos,para qualquer serviço, e protocolos específicos destinados a bases de dados. No que tocaa protocolos de replicação genéricos, as principais técnicas existentes, apesar de completa mente desenvolvidas e em utilização, têm algumas limitações, nomeadamente: problemasde performance relativamente a saturação da réplica primária na replicação passiva e odeterminismo necessário associado à replicação ativa. Algumas destas desvantagens sãomitigadas pelos protocolos específicos de base de dados (e.g., com recurso a multi-master)mas estes protocolos não permitem efetuar uma separação entre a lógica da replicação eos respetivos dados. Abordagens mais recentes tendem a basear-se em técnicas de repli cação com fundamentos em mecanismos distribuídos de logging. Tais mecanismos propor cionam alta disponibilidade de dados e tolerância a faltas, permitindo abordagens inovado ras baseadas puramente em logs.Por forma a atenuar as limitações encontradas não só no mecanismo de replicação ativae passiva, mas também nas suas derivações, esta dissertação apresenta uma solução dereplicação híbrida baseada em middleware, o SQLware. A grande vantagem desta abor dagem baseia-se na divisão entre a camada de replicação e a camada de dados, utilizandoum log distribuído altamente escalável que oferece tolerância a faltas e alta disponibilidade.O protótipo desenvolvido foi validado com recurso à execução de testes de desempenho,sendo avaliado em duas infraestruturas diferentes, nomeadamente, um servidor privadode média gama e um grupo de servidores de computação de alto desempenho. Durante aavaliação do protótipo, o standard da indústria TPC-C, tipicamente utilizado para avaliarsistemas de base de dados transacionais, foi utilizado. Os resultados obtidos demonstramque o SQLware oferece uma aumento de throughput de 150 vezes, comparativamente aomecanismo de replicação nativo da base de dados considerada, o PostgreSQL.",
    "authors": [
      "Abreu, Hugo Miguel Ferreira"
    ],
    "keywords": [
      "Middleware",
      "Replicação",
      "Logs",
      "Logs distribuídos",
      "Bases de dados",
      "Replicação híbrida",
      "Replicação ativa",
      "Replicação passiva",
      "Tolerância a faltas",
      "Alta disponibilidade",
      "Replication",
      "Distributed logs",
      "Databases",
      "Hybrid replication",
      "Active replication",
      "Passive replication",
      "Fault tolerance",
      "High availability",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83672",
    "title": "Sampling techniques applied to anomalous events detection",
    "abstract": "Nowadays, one of the major worries about a network is security. Since the network has becomethe big platform it is, the number of attacks or attempts to steal information or just harmsomeone or something is getting bigger to handle or harder to find. Sampling techniques helpto solve these problems as they are used to reduce the scope of the analysis, as well as theresources needed to perform it. By using sample techniques to search and find the attacks inthe network traffic it will become easier to detect attacks and keep the network secure. Aswill be seen in the following sections, joining sampling and security is not an easy task todo. Questions such as, what are the best techniques to be used, what are the best methodsto be implemented, are inevitable when using sampling. However, sampling can bring moreadvantages than disadvantages. Besides that, depending on the chosen measurement method,sampling technique or algorithm performed to analyse the samples, the results can change a lotaccording to the target for the technique. To achieve results for evaluation, a Network-basedIntrusion Detection System (NIDS) will be used to identify anomalous events present in thesamples.",
    "authors": [
      "Gama, Joel Filipe Esteves"
    ],
    "keywords": [
      "Sampling",
      "Sampling techniques",
      "Security",
      "Measurement methods",
      "Active methods",
      "Passive methods",
      "Amostragem",
      "Técnicas de amostragem",
      "Segurança",
      "Métodos de medição",
      "Métodos ativos",
      "Métodos passivos"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27847",
    "title": "An evaluation of the GAMA/StarPU frameworks for heterogeneous platforms : the progressive photon mapping algorithm",
    "abstract": "Recent evolution of high performance computing moved towards heterogeneous platforms:multiple devices with different architectures, characteristics and programming models, shareapplication workloads. To aid the programmer to efficiently explore these heterogeneousplatforms several frameworks have been under development. These dynamically manage theavailable computing resources through workload scheduling and data distribution, dealingwith the inherent difficulties of different programming models and memory accesses. Amongother frameworks, these include GAMA and StarPU.The GAMA framework aims to unify the multiple execution and memory models ofeach different device in a computer system, into a single, hardware agnostic model. It wasdesigned to efficiently manage resources with both regular and irregular applications, andcurrently only supports conventional CPU devices and CUDA-enabled accelerators. StarPUhas similar goals and features with a wider user based community, but it lacks a singleprogramming model.The main goal of this dissertation was an in-depth evaluation of a heterogeneous frameworkusing a complex application as a case study. GAMA provided the starting vehiclefor training, while StarPU was the selected framework for a thorough evaluation. The progressivephoton mapping irregular algorithm was the selected case study. The evaluationgoal was to assert the StarPU effectiveness with a robust irregular application, and make ahigh-level comparison with the still under development GAMA, to provide some guidelinesfor GAMA improvement.Results show that two main factors contribute to the performance of applications writtenwith StarPU: the consideration of data transfers in the performance model, and chosenscheduler. The study also allowed some caveats to be found within the StarPU API. Althoughthis have no effect on performance, they present a challenge for new coming developers.Both these analysis resulted in a better understanding of the framework, and a comparativeanalysis with GAMA could be made, pointing out the aspects where GAMA could be furtherimproved upon.",
    "authors": [
      "Palhas, Miguel Branco"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81201",
    "title": "Implementação de sistema de monitorização e controlo de interoperabilidade clínica",
    "abstract": "Atualmente tem-se verificado um aumento significativo da quantidade de informaçãoarmazenada digitalmente, devido a cada vez mais as infraestruturas de saúde recorrerem asistemas digitais para armazenar a informação relativa aos intervenientes no seu universo.É, portanto, fulcral que essa informação seja regida por um conjunto de normas, de modo apermitir que seja compreendida sem se perderem dados importantes. Com o aumento douso de ferramentas digitais para armazenamento e troca de informação também se tornaimportante a monitorização dos dados trocados entre os sistemas, de modo a garantir aprivacidade e segurança dos intervenientes, bem como garantir o bom funcionamento dosistema.Nesse sentido, o trabalho consistiu na implementação de uma ferramenta para a monitorizaçãode mensagens trocadas na plataforma AIDA - Agência de Interoperação, Difusão e Arquivo- de maneira a contribuir de forma ativa para resolver as questões abordadas no RGPD. Estaferramenta é baseada em tecnologias como containers Docker, a base de dados ElasticSearche a interface de monitorização Kibana.",
    "authors": [
      "Castanheira, António Manuel de Melo"
    ],
    "keywords": [
      "RGPD",
      "ElasticSearch",
      "Interoperabilidade",
      "Registo médico eletrónico",
      "GDPR",
      "ElasticSearch",
      "Interoperability",
      "Electronic health record",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80868",
    "title": "Development of algorithms for the analysis and data mining of chemical compound prices",
    "abstract": "Nowadays, the products deriving from the biotechnology industry have become quite valu able in the world market. Hence, it is highly advantageous to find out how the pricesof the different chemical compounds needed for biotechnological processes behave in thebioeconomy. The SISBI project was developed to allow the retrieval and collection of differentprices associated with certain chemical compounds through different available sources anddatabases. With access to this information, some behaviours and patterns can be detectedin the price variations, indicating other relevant knowledge, such as the biotechnologicalinterest of this compound in the field. However, it is necessary to take into account thatSISBI data, although relevant, have inconsistencies that do not support an efficient analysisof these data, which is the case for the existence of duplicates, different units and problemsin the price integration. As a result, this study developed algorithms to identify and solvethese problems and to analyze the prices of compounds through time series. To effectivelyevaluate these data, a new database, bioanalysis, was built based on the data from the SISBIproject. Then, several preprocessing methods were applied, including the elimination ofduplicates, conversion of units, removal of defective and inconsistent prices, which led tothe solution of the various complications encountered. Consequently, once the data wasprepared for analysis, the prices pertaining to two specific metabolites, 4-aminopyridine andmethane, were examined. Thus, different price variations over time were compared betweendifferent configurations (quantity + unit) of the same metabolite and between differentmetabolites. These variations were divided by the different price providers to identify anyspecific relationship or pattern depending on where the data originate. However, in thisstudy, no particularly cheap provider was detected between 4-aminopyridine configurationsor between the two metabolites. The only association found occurred only between certainmethane configurations. In addition, the price variations analyzed are mostly constant, andwhen they are not, they do not show any pattern or seasonality. These results revealed that,using only the prices available to date, no correlation was determined by identifying theproviders associated with low prices when comparing different metabolites or configurations.",
    "authors": [
      "Faria, Sofia Maria Alves"
    ],
    "keywords": [
      "Biotechnology",
      "Chemical compounds",
      "Time series",
      "Algorithms",
      "Preprocessing",
      "Biotecnologia",
      "Compostos químicos",
      "Series temporais",
      "Algoritmos",
      "Pré- processamento",
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84185",
    "title": "Benchmarking deep learning for predicting telecommunications recurring problems",
    "abstract": "Nowadays, companies live in a scenario of strong competitiveness. The telecommunicationsmarket is not an exception and it is possible to offer a differentiation from competition throughbetter service quality, differentiated support and even better value proposals. With the evolution oftechnologies, companies have more data about their customers and the usage profile of each oneof them. With this information it is possible to establish a better relationship with the customerthrough a more efficient support service.The evolution of artificial intelligence and computational power, combined with existing data, al lows for several comparisons between different machine learning algorithms. In this dissertation,a prediction model capable of predicting recurrences of contacts with the customer service is pro posed. The aim is to predict whether a particular problem reported by the customer will repeat andrequire a new contact, so that it is possible to correct those problems in advance, making the userexperience more pleasant and fluid. In order to achieve the best possible model, different classicalmachine learning approaches were tested, along with several deep neural network architectures.In recent years, deep neural networks have shown interesting results in several non-tabular appli cations, therefore being interesting to test them in tabular applications like the one present in thiswork. TabNet, developed by Google, is a deep neural network adjusted to perform the better in tabu lar datasets, and was also tested, as it has shown better performance than several neural networksor decision-tree bases algorithms.The used data were collected by various internal systems, the most important of which being theone related to customer support calls. The customer service, due to its size and complexity, has asystem that monitors all calls and their motivations, as well as the parties involved (both operatorand customer) and other additional data such as time spent and the call outcome. Data from othersystems is related to billing, service usage and customer profile, and is added to help to understandthe context of the call.The model that shown the best results was CatBoost, a decision trees based algorithm, showingan AUC_ROC of 79%, with a Recall of 61% and a Precision of 62%, allowing the identification of about8,6% of the 3.9 million calls made to the support service as recurrences even before they occur,about 340k cases. In an ideal scenario, all these calls would be avoided, allowing a substantial costreduction for the company, as well as a consequent increase in customer satisfaction in relation tothe service.The CatBoost model showed better training times and less memory needs, while achieving a better performance than the different architectures of deep neural networks proposed. Only TabNetwas able to achieve a similar performance, while maintaining a higher training time. However, infutures uses, where the CatBoost model achieves a plateau and is not benefiting for the increasingdata, it could be useful to use TabNet as the model in production. TabNet has the advantage of beinga neural network and, for that reason, being more capable of breaking the plateau that classicalmodels often achieve.",
    "authors": [
      "Castro, Vitor José Ribeiro"
    ],
    "keywords": [
      "Customer",
      "Telecommunications",
      "Data mining",
      "Quality of service",
      "Artificial intelligence",
      "Cliente",
      "Telecomunicações",
      "Mineração de dados",
      "Qualidade de serviço",
      "Inteligência artificial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82460",
    "title": "Integrating kinetic and constraint-based models of metabolism",
    "abstract": "Mathematical models are fundamental tools for explaining biological behaviors. Dynamical andconstraint-based models are two different formulations that attempt to capture the phenotypiccapabilities of organisms.Dynamic models are formulated as ordinary differential equations (ODEs) that simulate metabolicconcentration over time. These models, however, only depict changes in metabolic concentration andrely on mechanistic details and kinetic parameters that are not always available. Constraint-basedmodels, on the other hand, have a better cellular perspective. By performing constraint-basedoptimizations, they simulate cell behavior under different genetic and environmental conditions.Metabolic models also have some drawbacks. In addition to providing no mechanical knowledge of anychemical reactions (beyond their stoichiometry) and no information regarding metabolic concentrationsor reaction flux dynamics, they are based on a steady-state assumption that production andconsumption of metabolites are balanced within the cell. Constraint-based optimizations, Flux BalanceAnalysis (FBA) methods, generally return an infinite set of solutions, requiring the imposition of additionalassumptions to identify unique flux distributions.While individually, both modeling approaches have several advantages, one lacks the benefitsprovided by the other. With this in mind, we implemented a tool in MEWpy capable of hybridizing kineticand constraint-based models. With it, we were able to reduce the constraint-based model solution spaceby overlapping the kinetic solution space and sampling the kinetic model, analyze the impact of differentstandard deviation values on the sampling, perform hybridization of enzymatic constrained models, andfurther compare distinct hybridization approaches. To demonstrate the potential of our tool and itsapplicability in strain optimization, we performed hybrid optimization of succinate production, where wediscovered a set of genetic mutations that boosted its production.",
    "authors": [
      "Pereira, Mariana Marques"
    ],
    "keywords": [
      "Constraint-based model",
      "Kinetic model",
      "Hybrid model",
      "MEWpy",
      "Sampling",
      "Hybrid simulation",
      "FBA",
      "Succinate production",
      "GECKO",
      "Modelos com base em restrições",
      "Modelos cinéticos",
      "Modelos híbridos",
      "Amostragem",
      "Simulação hibrida",
      "Produção de succinato",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83353",
    "title": "Space colonization for the procedural generation of lightning",
    "abstract": "The procedural generation of geometry within the space of computer graphics has been a topic of study for quitesome time, benefiting from a more unpredictable brand of randomness. Similarly, the exploration of lighting as aphenomenon within virtual space has been a field of study of comparable age.Despite its age and early adoption, there is a surprising lack of research in emulating the phenomenon oflighting past its interactions with the world. Most implementations of procedurally generated lightning within videogames are based on randomized data trees. When part of the skybox, 2D meshes or textures are randomlyselected from a pre-made pool. There are, however, methods based entirely on the dielectric breakdown model,using approximations to solve a Laplacian equation.This dissertation aims to present an alternative approach to the randomized and procedural generation oflightning bolts based on the Space Colonization algorithm. While the algorithm was first conceived for use inbotanical applications, modeling the growth of biological structures, the similarities between the results producedby the dielectric breakdown model and botanic modeling algorithms coupled with the visual likeness of a lightningbolt and certain trees, made for solid groundwork upon which to establish this unique approach.As such, this work largely aims to be a first step into this particular realm, showing Space Colonizationas a suitable algorithm for this specific purpose. That being said, a large portion of time was spent iterating,modifying and experimenting with ideas that were either discarded or adapted, an effort primarily dedicatedtowards controlling and stifling the possible growth of branches in ways beyond the reduction of attractors.The original algorithm was altered, focus put especially on the creation of a singular channel at a time, mixingdiscoveries from previous research with the work done on manipulating Space Colonization. Instead of thevenation patterns observed with the original work, the stifling of any growth means that each node has a chance,when created, of sprouting a branch and each branch is, in turn, a different, modified instance of the sameunderlying concept providing an additional level of control. Effort was equally placed on showcasing differentproperties inherent to a lightning strike, such as its iterative construction when descending from its origin.In the rendering section, along with recreating the bloom and glow effect seen in previous works, effort was putinto recreating the strobing observed in capturing slow-motion footage of lightning bolts with special detail givento this. In addition, parameters were joined with a waypoint system to allow for a great degree of freedom whengenerating new bolts.",
    "authors": [
      "Reis, Nuno Filipe Maranhão dos"
    ],
    "keywords": [
      "Animation",
      "Lightning",
      "Physics",
      "Electric breakdown",
      "Patterns",
      "Rendering",
      "Real-time",
      "Computer graphics",
      "Physically plausible",
      "Space colonization",
      "Animação",
      "Trovoada",
      "Relampagos",
      "Física",
      "Quebra dielétrica",
      "Padrões",
      "Renderização",
      "Tempo real",
      "Computação gráfica",
      "Fisicamente plausível",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84576",
    "title": "Development of a deep learning-based computational framework for the classification of protein sequences",
    "abstract": "Proteins are one of the more important biological structures in living organisms, since theyperform multiple biological functions. Each protein has different characteristics and properties,which can be employed in many industries, such as industrial biotechnology, clinical applications,among others, demonstrating a positive impact.Modern high-throughput methods allow protein sequencing, which provides the proteinsequence data. Machine learning methodologies are applied to characterize proteins usinginformation of the protein sequence. However, a major problem associated with this methodis how to properly encode the protein sequences without losing the biological relationshipbetween the amino acid residues. The transformation of the protein sequence into a numericrepresentation is done by encoder methods. In this sense, the main objective of this project is tostudy different encoders and identify the methods which yield the best biological representationof the protein sequences, when used in machine learning (ML) models to predict different labelsrelated to their function.The methods were analyzed in two study cases. The first is related to enzymes, sincethey are a well-established case in the literature. The second used transporter sequences, alesser studied case in the literature. In both cases, the data was collected from the curateddatabase Swiss-Prot. The encoders that were tested include: calculated protein descriptors;matrix substitution methods; position-specific scoring matrices; and encoding by pre-trainedtransformer methods. The use of state-of-the-art pretrained transformers to encode proteinsequences proved to be a good biological representation for subsequent application in state-of-the-art ML methods. Namely, the ESM-1b transformer achieved a Mathews correlation coefficientabove 0.9 for any multiclassification task of the transporter classification system.",
    "authors": [
      "Barros, Miguel Ângelo Pereira"
    ],
    "keywords": [
      "Computational biology",
      "Protein classification",
      "Machine learning",
      "Deep learning",
      "Biologia computacional",
      "Classificação de proteínas",
      "Aprendizagem de máquina",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59827",
    "title": "Geração de aplicações multi-plataforma a partir de modelos",
    "abstract": "Na área de Engenharia de Software, a modelação de sistemas com recurso a diagramas,permite representar um sistema de forma padronizada, com o intuito de facilitar a compreensão da especificação, estrutura lógica, e documentação dos mesmos.Hoje em dia, no mundo empresarial, a utilização de diagramas através de ferramentaspróprias para o efeito tem como objetivo a comunicação entre equipas, inserindo-se na fasede modelação dos projetos. No entanto, a construção de aplicações com recurso a técnicasde low code, ou mesmo zero code, é uma realidade cada vez mais atual.A evolução natural deste conceito resultará na geração automática de código através deuma linguagem visual, como os diagramas, facilitando, assim, a produção de código, e aomesmo tempo, conseguir-se-á uma poupança de tempo aproveitando o trabalho realizadonuma fase mais precoce do projeto. Posto isto, a utilização de modelos, mais ou menosstandard, como forma de especificar e prototipar aplicações é e será, cada vez mais, umarealidade bem fundada e com sucesso assinalável, permitindo também gerir de forma maiseficaz questões de multi-plataforma, visto que a geração de código não é exclusiva a nenhumparadigma nem linguagem de programação específica.Com esta dissertação pretende-se, então, utilizar modelos UML como mecanismo únicode especificação de aplicações, automatizando o processo de construção do respetivo códigoe os aspetos tecnológicos relativos ao seu deployment e instalação, disponibilizando umaferramenta que possibilite o processo de criação de aplicações web e android a partir dediagramas UML.Assim, foi criada uma aplicação que, através da interação do utilizador, recebe diagramasde classe exportados em formato XML interpretando-os e gerando aplicações android eaplicações web. Estas aplicações realizam as operações CRUD para cada entidade representadano diagrama de classe.",
    "authors": [
      "Mendes, Frederico Jorge Falcão Torres de Castro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92663",
    "title": "Soluções de business intelligence - gestão de oficina automóvel",
    "abstract": "Diariamente existem mudanças no ambiente organizacional e as organizações procuram adaptar-se e acompanhar as tendências do mercado. É necessário inovar nas tecnologias, haver flexibilidade e adaptabilidade por parte dos colaboradores e retirar o melhor proveito dos dados.Nesse sentido, o conceito de Business Intelligence surge apresentando um conjunto de métodos e ferramentas para disponibilizar informação e suportar a tomada de decisão. O objetivo é otimizar e simplificar o processo de tomada de decisão, agilizar a extração de dados e sua divulgação como informação valiosa e possibilitar soluções de qualidade e confiáveis.Neste projeto apostou-se no desenvolvimento e implementação de uma solução de Business Intelligence capaz de suportar a gestão de oficina no ambiente organizacional em questão.Ao longo do desenvolvimento do projeto realizou-se a construção de um Data Warehouse, o levantamento de requisitos e métricas para implementação da solução, a construção de sistemas de processamento analítico e aplicações de front-end.O principal objetivo desta solução para a organização é a recolha e monitorização da informação para observar valores passados e atuais, de modo a compreender as tendências, observar pontos a melhorar e visualizar dados futuros.Através da solução desenvolvida, a organização sentiu melhorias nas decisões diárias, uma maior capacidade de acesso ao detalhe e uma apresentação da informação de forma fidedigna e de qualidade.",
    "authors": [
      "Alves, Carolina Gonçalves"
    ],
    "keywords": [
      "Business intelligence",
      "Data warehouse",
      "Data mining",
      "Processo de tomada de decisão",
      "Key performance indicators",
      "Dashboards",
      "Decision making process",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55659",
    "title": "Learning frequent behaviours patterns in intelligent environments for attentiveness level",
    "abstract": "Nowadays, when it comes to achieving goals in business environments or educationalenvironments, the successful on a person performing a task has an important role.However, this performance can be affected by several factors. One of the most commonis the lack of attention. The individual’s attention in performing a task can bedeterminant for the final quality or even at the task’s conclusion.In this project is intended to design a solution that can help on the reduce or eveneliminate the lack of attention on performing a task. The idea consists on developa software that capture the user behaviour through the mouse and keyboard usage.Furthermore, the system will analyse how the devices are used. It will be quantifiedthe attention level and, after several captures for each user, it will be defined for eachuser an user profile. Through standardization of user’s behaviour it will be possible todetermine the learning style of each user.",
    "authors": [
      "Cardoso, Catarina"
    ],
    "keywords": [
      "Ambient intelligent system",
      "Decision support system",
      "Attention",
      "User behaviour",
      "Sistemas de ambientes inteligentes",
      "Sistema de apoio à decisão",
      "Atenção",
      "Comportamento do utilizador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84053",
    "title": "Arquiteturas assíncronas na comunicação entre serviços frontend e backend",
    "abstract": "Hoje em dia a comunicação assíncrona entre serviços, independentemente da plataforma (desktop,mobile, smart tv, smartwatch, etc), é cada vez mais frequente.Parte do software produzido pelas empresas de telecomunicações, neste caso, pela empresaCelfocus, consiste em realizar operações assíncronas e, por conseguinte, receber notificações sobre oestado dessas operações. A título de exemplo, quando um funcionário numa loja de telecomunicaçõesrealiza uma operação (assíncrona) como alterar o tarifário de um cliente, é necessário feedback dessaoperação (ou das várias operações espoletadas pela mesma), através de uma notificação com origemno servidor e destino para o browser do funcionário.Para conseguir implementar esta comunicação em tempo real, tecnologias denominadas por serverpush foram desenvolvidas. Cada uma delas tem um cenário ideal de uso, diferentes características,vantagens e desvantagens.A presente dissertação consiste em investigar as tecnologias existentes para a comunicação entre ofrontend e backend e, depois, desenvolver um sistema que permite enviar e receber notificações.Por fim, para comprovar que a solução conceptual proposta, efetivamente, funciona na prática, sãorealizados testes funcionais.",
    "authors": [
      "Petronilho, Ricardo André Gomes"
    ],
    "keywords": [
      "Asynchronous communication",
      "Server push technologies",
      "Microservices",
      "Hexagonal pattern",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23500",
    "title": "Planeamento cirúrgico ortopédico assistido por computador: uma abordagem 3D",
    "abstract": "O planeamento tem vindo a ganhar preponderncia entre a comunidademédica de cirurgiões. A elaboração de um plano para a cirurgia é fundamentalpara que esta se desenrole da melhor forma possível, encurtando assimos tempos de recuperação do paciente. No caso da cirurgia ortopédica, oplaneamento tem uma importância ainda mais acentuada, devido à relaçãoestreita entre os tempos de recuperação e fiabilidade a que o paciente ficasujeito e o sucesso da cirurgia. Assim, é importante que os cirurgiões disponhamde ferramentas que os auxiliem nessa tarefa, por forma a torná-la menosmorosa e complexa. No entanto, isso não acontece. As soluções ao dispordos cirurgiões revelam-se insuficientes, não possibilitando uma visão globalda extensão da lesão e possíveis intervenções. Dessas soluções, apenas umpequeno número permite a modelação tridimensional do estudo imagiológicode tomografia computorizada. Porém, não possibilitam que a análise daintervenção desenvolvida pelo cirurgião possa ser feita no mesmo universogeométrico. Isto deve-se essencialmente à dificuldade de interoperação entretipos de formato imagem diferentes, dado que o estudo imagiológico é do tipomatricial e os templates representativos dos apoios físicos, vectorial. Postoisto, o presente trabalho pretende apresentar uma solução para este problemade interoperação, bem como a sua implementação. Através da soluçãoapresentada, o cirurgião tem a possibilidade de manipular uma isosuperfícierenderizada tridimensionalmente a partir do estudo imagiológico selecionado.De seguida, é-lhe permitido adicionar as representações digitais dos apoiosfísicos utilizados, por forma a avaliar a viabilidade da sua abordagem. Emconjunto, tem a possibilidade de gerar novas isosuperfícies de valores diferentes,bem como cortar o modelo final num plano previamente definido, oque permite uma análise, agora em duas dimensões, da intervenção planeada.Posto isto, é possível concluir que a solução apresentada auxilia o cirurgiãono desenvolvimento de um planeamento mais adequado, podendo analisartridimensionalmente o impacto da sua intervenção no paciente.",
    "authors": [
      "Ribeiro, João Pedro de Araújo"
    ],
    "keywords": [
      "617.3:681.3",
      "681.3:617.3",
      "616-089.8"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "617.3:681.3",
      "681.3:617.3",
      "616-089.8"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80609",
    "title": "Otimização distribuída de indicadores logísticos com recurso a ferramentas Apache",
    "abstract": "The success of a manufacturing company is linked to the efficiency of its supply chain. More specifically, procure ment, which refers to the scheduling and quantity planning of raw materials to order from suppliers, can have abig impact on the company’s operating costs.To be able to avoid stock shortages and assure the necessary stock levels for production, there are a fewsafety measures such as: Safety Time and Safety Stock. In Bosch’s plant located in Braga, the assignment ofthese values is done infrequently and the data taken into consideration is not very specific.This dissertation proposes a dynamic solution that should be able to provide a set of optimal pair solutions ofSafety Time and Safety Stock according to certain objective measures, for each of the raw materials currentlybeing used. This calculation is based on logistic data, which of some are static (master data) and other is historicdata.This solution was developed using distributed data processing frameworks, such as Apache, for executionspeedup. More specifically, HDFS (Hadoop Distributed File System), a technology from Apache Hadoop to storedata in a distributed manner and Apache Spark to load and process the data, by splitting processing tasks andassigning them to different servers inside a cluster.",
    "authors": [
      "Oliveira, Miguel Pimentel de"
    ],
    "keywords": [
      "Supply chain analytics",
      "Big Data",
      "Apache",
      "Apache Spark",
      "Distributed data processing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79908",
    "title": "Developing deep learning methods to predict cancer and its outcome from transcriptomics data",
    "abstract": "Cancer is one of the major causes of death in developed countries. It is not a single disease,but a group of different types of diseases with specific symptoms, treatments and prognosis.Early diagnosis and prognostic assessment are essential to select the best treatment for eachcase.Deep learning is a branch of machine learning that became popular in recent years. Deeplearning methods have been employed in a broad range of areas including self-driving cars,natural language processing, computer vision, health, among others.The main goal of the thesis is to develop deep learning methods to predict cancer and itsoutcome from transcriptomics data. Reviewing literature, exploring datasets, developingpipelines and validating the methods using a case study are some of the tasks needed toachieve the goals of the thesis.The developed methods are implemented as a pipeline for creating models from geneexpression data. The framework is capable of reading and pre-processing these data, andtraining, optimizing and evaluating traditional machine learning and deep learning models.The framework was showcased by using the METABRIC dataset as a case study, whichcontains samples from breast cancer patients. The gene expression microarray data from thedataset was used to generate traditional, deep learning and multi-task models. The modelswere used to predict the expression of Estrogen Receptor (ER), the subtype of breast cancerregarding ER, Human Epidermal Growth Factor (HER-2) and Progesterone Receptor (PR) andthe prognosis of breast cancer patients with Nottingham Prognostic Index (NPI), respectively.Another dataset allowed the use of single-cell RNAseq data and confirmed the main trendsof the results.Overall, the results were promising with classification tasks obtaining good results whileregression models had a poorer performance. While the best results were obtained withtraditional machine learning models, deep learning models were near and could providebetter results if the dataset contained a larger number of samples.",
    "authors": [
      "Soares, Óscar Marques"
    ],
    "keywords": [
      "Cancer",
      "Deep learning",
      "Machine learning",
      "Transcriptomics",
      "Cancro",
      "Aprendizagem máquina",
      "Transcriptómica",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/25758",
    "title": "Avaliação de QoS-QoE no serviço de videochamada SkypeTM",
    "abstract": "Web-based videotelephony services currently occupy a prominent place in the wide rangeof services and generated network traffic. With the growing use of such services comes also anincreasing need for the evaluation of user experience. Depending on network conditions, thereare relevant QoS parameters (bandwidth, delay, loss ratio, jitter, etc.) that have an impact on thequality of experience of the service.The objective of this thesis is to find out which network parameters affect videocalls qualityof experience, which are the threshold values for which the quality of experience levels areaffected, in what way is the quality of experience impacted and what is their relation.The quality of experience was evaluated using objective methods for all video and audiosamples collected during the experimentation phase. Conclusions were made based on the resultswhich are presented in this dissertation.",
    "authors": [
      "Lopes, André D."
    ],
    "keywords": [
      "621.39"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "621.39"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27866",
    "title": "Captura de dados em tempo real em sistemas de data warehousing",
    "abstract": "massificação dos sistemas de informação tem contribuído significativamente para aforma como os utilizadores interagem com as empresas e seus sistemas. Esta nova relaçãoentre cliente e fornecedor tem aumentado significativamente o volume de dados geradospelas organizações, criando novas necessidades de como manter e gerir toda estainformação. Assim, as empresas têm investido cada vez mais em soluções que permitammanter toda a informação tratada e consolidada num repositório único de dados. Estessistemas são vulgarmente designados por sistemas de data warehousing. Tradicionalmente,estes sistemas são refrescados em modo offline, em períodos de tempo que podem serdiários ou semanais. Contudo, o aumento da competitividade no mundo empresarial tornaeste tipo de refrescamentos desadequados, originando uma reação atrasada à ação quedespoletou essa informação. Na realidade, períodos longos de refrescamento tornam ainformação desatualizada, diminuído consequentemente a sua importância e valor para aorganização em causa. Assim sendo, é cada vez mais necessário que a informaçãoarmazenada num sistema de data warehousing, seja a mais recente possível, evitandointerrupções na disponibilização da informação. A necessidade de obter a informação emtempo real, coloca alguns desafios, tais como manter os dados acessíveis 24 horas por dia,7 dias por semana, 365 dias por ano, reduzir o período de latência dos dados ou evitarestrangulamentos operacionais nos sistemas transacionais. Assim, é imperativo a utilização de técnicas de coleta de dados não intrusivas, que atuem no momento em que determinadoevento ocorreu num sistema operacional e reflitam a sua informação de forma imediata (ouquase imediata) num sistema de data warehousing. Neste trabalho de dissertação pretendeseestudar a problemática relacionada com a captura de dados em tempo real e conceberum componente que capaz de suportar um sistema de extração de dados em tempo realuniversal, que capture as mudanças ocorridas nos sistemas transacionais, de forma nãointrusiva, e as comunique na altura certa ao seu sistema de data warehousing.",
    "authors": [
      "Dias, Miguel Gonçalves"
    ],
    "keywords": [
      "Sistemas de data warehousing",
      "ETL",
      "Integração em tempo real",
      "Captura de dados novos ou alterados",
      "Data warehousing em tempo real",
      "Data warehousing systems",
      "Real time integration",
      "Change data capture",
      "Real time data warehousing",
      "681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27878",
    "title": "Development of an environment for the generation, mutation and execution of test cases",
    "abstract": "Testing graphic user interfaces (GUI) involves, mainly, lengthy and expensive processesinvolving user testing. Finding simpler and easier alternatives to use than these processesbecomes an exciting proposal. This project presents an alternative to existing processes throughthe use of Model-based Testing - MBT.The MBT technique takes advantage of models that describe the correct operation of thesystem (for this project task models). The use of MBT may thus become a new approach totesting GUI's, since the implemented GUI is tested against the model that specifies it the correctbehavior. All inconsistencies found during the tests will be treated as potential errors that mustbe corrected.This report describes the development of a prototype for an environment able to generateand execute test cases applying MBT to GUI's.",
    "authors": [
      "Cruz, Paulo Filipe Jesus"
    ],
    "keywords": [
      "Model-based testing",
      "Graphic user interfaces",
      "Task models",
      "Testes baseados em modelos",
      "Interfaces gráficas",
      "Modelos de tarefas",
      "681.3:65.015.11",
      "65.015.11:681.3",
      "681.326"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:65.015.11",
      "65.015.11:681.3",
      "681.326"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84635",
    "title": "Modular framework for a breast biopsy smart navigation system",
    "abstract": "Breast cancer is currently one of the most commonly diagnosed cancers and the fifth leading cause ofcancer-related deaths. Its treatment has a higher survivorship rate when diagnosed in the disease’s earlystages. The screening procedure uses medical imaging techniques, such as mammography or ultrasound,to discover possible lesions. When a physician finds a lesion that is likely to be malignant, a biopsyis performed to obtain a sample and determine its characteristics. Currently, real-time ultrasound is thepreferred medical imaging modality to perform this procedure. The breast biopsy procedure is highly relianton the operator’s skill and experience, due to the difficulty in interpreting ultrasound images and correctlyaiming the needle. Robotic solutions, and the usage of automatic lesion segmentation in ultrasoundimaging along with advanced visualization techniques, such as augmented reality, can potentially makethis process simpler, safer, and faster.The OncoNavigator project, in which this dissertation integrates, aims to improve the precision ofthe current breast cancer interventions. To accomplish this objective various medical training and roboticbiopsy aid were developed. An augmented reality ultrasound training solution was created and the device’stracking capabilities were validated by comparing it with an electromagnetic tracking device. Anothersolution for ultrasound-guided breast biopsy assisted with augmented reality was developed. This solutiondisplays real-time ultrasound video, automatic lesion segmentation, and biopsy needle trajectory displayin the user’s field of view. The validation of this solution was made by comparing its usability with thetraditional procedure. A modular software framework was also developed that focuses on the integrationof a collaborative medical robot with real-time ultrasound imaging and automatic lesion segmentation.Overall, the developed solutions offered good results. The augmented reality glasses tracking capabilitiesproved to be as capable as the electromagnetic system, and the augmented reality assisted breast biopsyproved to make the procedure more accurate and precise than the traditional system.",
    "authors": [
      "Costa, José Nuno Martins da"
    ],
    "keywords": [
      "Breast cancer",
      "Breast biopsy",
      "Ultrasound",
      "OncoNavigator",
      "Augmented reality",
      "Cancro da mama",
      "Biópsia mamária",
      "Ultrassom",
      "Realidade aumentada",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/49099",
    "title": "Classification and structure-based inference of transcriptional regulatory proteins",
    "abstract": "Transcription factors (TFs) are proteins that mediate the cellular response to the changes of thesurrounding environment. Studying their functional domains and protein structure is fundamentalin order to gain insight of the way they are triggered and how they shape genetic transcription.The current work aimed for classifying both TFs and functional domains, understanding whichfeatures can be related to the different functions of the TFs.By using UniProtJAPI, a JAVA library that allows remote access to UniProt, the informationof 200 Escherichia coli’s (E. coli) TFs has been retrieved. This data was manually curated, inorder to remove domain duplicates and other excess information, and to add missing domains.The obtained functional domains were classified according to their molecular function, whilethe TFs were classified according to their regulatory function. TFs that exclusively induce geneexpression were classified as activators, while TFs that only perform gene repression were classifiedas repressors. On the other hand, TFs that perform both the activation and repression oftranscription were classified as duals. The information was then analysed altogether in orderto understand what relationships between the TFs’ function and functional domains could exist.Several analysis were performed, which include statistical tests and clustering methods. Alongwith the analysis of the full list of TFs, TFs that are part of two-component signal transductionsystems and global TFs were given special focus, due to their important role in cellular function.The results showed that there is a relationship between the functional domains and the regulatoryfunction of the different TFs. This may be related to the evolutionary relationships betweenrepressors and activators. It is also understandable that dual regulators are closely related to activatorsand repressors than what activators and repressors are to each other. Moreover, TFs oftwo-component signal transduction systems are similar to each other, given that they performsimilar functions. Their domain architectures are also predictable and do not vary from whatwas expected of these TFs. However, in global TFs the results are opposite of the ones obtainedfor two-component system TFs: their structures are very different from each other and each TFis specific. The amount of different domains is high when comparing to the full sample of TFs,since the number of domains exceeds the number of TFs. Domains of all classification types arepresent in their structure and the domain architectures are varied, which reflects their differentactivities within the cell.",
    "authors": [
      "Barros, Diana Manuela Pinto"
    ],
    "keywords": [
      "Transcription factors",
      "Protein functional domains",
      "Escherichia coli",
      "Regulatory function",
      "Two-component signal transduction systems",
      "Global TFs",
      "Factores de transcrição",
      "Domínios proteicos funcionais",
      "Escherichia coli",
      "Funcção regulatória",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47824",
    "title": "Towards an efficient lattice basis reduction implementation",
    "abstract": "The security of most digital systems is under serious threats due to major technology breakthroughswe are experienced in nowadays. Lattice-based cryptosystems are one of the mostpromising post-quantum types of cryptography, since it is believed to be secure againstquantum computer attacks. Their security is based on the hardness of the Shortest VectorProblem and Closest Vector Problem.Lattice basis reduction algorithms are used in several fields, such as lattice-based cryptographyand signal processing. They aim to make the problem easier to solve by obtainingshorter and more orthogonal basis. Some case studies work with numbers with hundredsof digits to ensure harder problems, which require Multiple Precision (MP) arithmetic. Thisdissertation presents a novel integer representation for MP arithmetic and the algorithmsfor the associated operations, MpIM. It also compares these implementations with other libraries,such as GNU Multiple Precision Arithmetic Library, where our experimental resultsdisplay a similar performance and for some operations better performances.This dissertation also describes a novel lattice basis reduction module, LattBRed, whichincluded a novel efficient implementation of the Qiao’s Jacobi method, a Lenstra-LenstraLovasz(LLL) algorithm and associated parallel implementations, a parallel variant of the ´Block Korkine-Zolotarev (BKZ) algorithm and its implementation and MP versions of thethe Qiao’s Jacobi method, the LLL and BKZ algorithms.Experimental performances measurements with the set of implemented modifications ofthe Qiao’s Jacobi method show some performance improvements and some degradationsbut speedups greater than 100 in Ajtai-type bases.",
    "authors": [
      "Gonçalves, Hélder José Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27838",
    "title": "Voodoo : a system that allows children to create animated films with action figures as interface",
    "abstract": "Using any kind of dolls as a tangible interface, has the potential to provide a friendly andeasy to learn interface that allows children to control virtual characters in a more intuitiveway. The research effort in this domain has been motivated by the shortcomings of conventionalinterfaces, typically mouse and keyboard, which in this context are neither compellingnor do promote immersion.This dissertation focuses on the design and evaluation of a system which can interpret thebehaviors that children give to a doll in order to provide this behavioral information to the virtualcharacters. With this system, the user (children) gets the role of movie director, directingvirtual characters through this natural form of interaction. This dissertation aims to evaluatethe hypothesis that dolls behaviors recognition based on the context of a well-known story,may enhance the ability of children in the creation of an animated film (virtual charactersanimations). Unlike many approaches that use a direct mapping of the doll movements tothe virtual character, it is intended to test the mapping based on the crosses between, userbehavioral intention, and the context where the doll it is inserted (the role and the locationof the character in the story). The results show that the concept of interaction proposed toempower the children with a way to create animated films is actually very intuitive and easyto use. However, due to the technology used, it was not possible to assess to what extentthis concept really empowers children to easily and joyfully create animated films.",
    "authors": [
      "Ribeiro, Pedro do Rêgo"
    ],
    "keywords": [
      "Virtual Characters",
      "Tangible Interfaces",
      "Affective Interfaces",
      "Toy",
      "Children",
      "Personagens virtuais",
      "Interfaces tangíveis",
      "Interfaces \"Affective\"",
      "Brinquedo",
      "Criança",
      "681.3:778.5",
      "778.5:681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:778.5",
      "778.5:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80750",
    "title": "Desenvolvimento de uma plataforma de apoio à decisão clínica relativamente ao problema do absentismo numa empresa",
    "abstract": "O absentismo é um problema que tem vindo a crescer ao longo dos últimos anos e que afeta gravementea economia das empresas. Para além disso, esta questão está intrinsecamente relacionada com a saúdedos trabalhadores, dado que doenças e acidentes de trabalho são duas das maiores causas de absentismopor todo o mundo.De modo a perceber melhor a razão por detrás da crescente taxa de absentismo, assim como arelação entre o absentismo e a saúde e estilo de vida dos colaboradores de uma empresa portuguesa,tornou-se necessário recorrer às Tecnologias de Informação (TI), nomeadamente ao Business Intelligence(BI). Esta tecnologia permite uma rápida análise de dados e uma melhor compreensão da informaçãoexistente.Assim sendo, esta dissertação tem como objetivo desenvolver uma aplicação Web, recorrendo aferramentas de BI, que permita o estudo dos dados de absentismo através de indicadores clínicos deforma a identificar as principais causas, bem como a implicação que o trabalho e o estilo de vida possater na saúde dos colaboradores e, consequentemente, no absentismo. Por conseguinte, esta aplicaçãofornece apoio à tomada de decisão e prática clínica por parte dos profissionais de saúde e permitetambém, através da análise dos dados, que sejam encontradas soluções para a diminuição da taxa deabsentismo da empresa.Esta solução pretende ainda apresentar toda a informação relativa aos colaboradores e às suasausências de uma forma organizada e de fácil leitura, sendo, deste modo, menos suscetível a erros,assim como mais rápido e mais eficiente que o atual sistema utilizado para a análise do absentismo,facilitando assim o trabalho dos profissionais.",
    "authors": [
      "Silva, Sara Daniela Oliveira da"
    ],
    "keywords": [
      "Absentismo",
      "Aplicação web",
      "Business intelligence",
      "Processo de tomada de decisão",
      "Tecnologias de Informação",
      "Absenteeism",
      "Decision-making process",
      "Information and communication technology",
      "Web Application",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64178",
    "title": "Melhoria da qualidade de serviço e monitorização clínica da gravidez",
    "abstract": "O Boletim de Saúde da Grávida é um livro de registos para grávidas. Este permite o registo de informações alusivas à história clínica da grávida, bem como o registo diário do peso, da pressão arterial, dos movimentos fetais, e entre outros valores que serão relevantes para a sua monitorização e o seu acompanhamento por parte do médico. Desta forma, considerando a importância clínica associada, é prioritário proceder-se à melhoria desse sistema. Além disso, as utentes quando encorajadas a serem mais independentes no ato de registo, considerando-o mais relevante, contribuem para a minimização da perda de informação relevante. Tudo isto favorece o diagnóstico antecipado de potenciais riscos à saúde da grávida ou à condição do seu feto.O cálculo da data provável de parto e da semana gestacional da gravidez é um processo extremamente importante no acompanhamento da grávida. Contudo, os métodos comuns são demorados e, facilmente, induzem em erro. Assim, uma ferramenta capaz de calcular esses valores é vantajosa quer para o médico, quer para a paciente.Este trabalho visa o desenvolvimento de uma progressive web app capaz de proporcionar um suporte à gravida em termos de informação e monitorização, disponibilizando o Boletim de Saúde da Grávida em formato digital, sendo também uma ferramenta que facilitará o cálculo da data provável de parto e da idade gestacional.O desenvolvimento do projeto foi auxiliado pelo Centro Materno Infantil do Norte (CMIN), com o acompanhamento do Doutor Jorge Braga – Diretor Geral de Obstetrícia do CMIN.",
    "authors": [
      "Peixoto, Catarina Sofia Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27839",
    "title": "Computational tools for pathway optimization towards metabolic engineering applications",
    "abstract": "Metabolic Engineering targets the microorganism's cellular metabolism to designnew strains with an industrial purpose. Applications of these metabolic manipulationsin Biotechnological derive from the need of enhanced production of valuablecompounds. The development of in silico metabolic models proposes a quantifiableapproach for the manipulation these microorganisms. In this context, constraintbased modelling is one of the major approaches to predict cellular behaviour. Itallows to prune the feasible space of possibilities describing possible phenotypeoutcomes in terms of metabolic fluxes. Under these conditions, cellular metabolismcan be represented as an algebraic system constrained by the laws of massbalance and thermodynamics.These systems are prone to be represented as networks, taking advantage of differentgraph-based paradigms, including bipartite graphs, hypergraphs and processgraphs. This thesis explores these representations and underlying algorithms formetabolic network topological analysis. The main aim will be to identify potentialpathways towards the optimized biochemical production of selected compounds.Related to this task, algorithms will also be designed aiming to complement networksof specific organisms, taking as input larger metabolic databases, insertingnew reactions making them able to produce a new compound of interest.To address these problems, and also related tasks of data pre-processing and evaluationof the solutions, a complete computational framework was developed. Itintegrates a number of previously proposed algorithms from distinct authors, togetherwith a number of improvements that were necessary to cope with large-scalemetabolic networks. These are the result of problems identi ed in the previousalgorithms regarding their scalability.A case study in synthetic metabolic engineering was selected from the literature tovalidate the algorithms and test the capabilities of the implemented framework. Itallowed to compare the performance of the implemented algorithms and validatethe proposed improvements.",
    "authors": [
      "Liu, Filipe Alexandre Wang"
    ],
    "keywords": [
      "Metabolic networks",
      "Flux analysis",
      "Synthetic biology",
      "Pathway optimization",
      "Network topological analysis",
      "Subgraph extraction",
      "Redes metabólicas",
      "Análise de fluxo",
      "Biologia sintética",
      "Optimização de vias metabólicas",
      "Análise topológica de redes",
      "Extração de sub-grafos",
      "681.3:577",
      "577:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:577",
      "577:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56116",
    "title": "Genome-Scale Metabolic Network Reconstruction of the dairy bacterium Streptococcus thermophilus",
    "abstract": "The dairy food industry is constantly changing as novel biotechnological techniquesimprove the manufacturing process of dairy products. Widely used over the years in the yogurtand cheese manufacturing, Streptococcus thermophilus is now considered as an extremelyvaluable lactic acid bacterium for the annual market of the dairy industry. A specific, butof easy-access knowledge regarding the thermophilic bacteria metabolism would be a plus forthe continuous growth of such industry.In this work, we present the Genome-Scale Metabolic (GSM) model for the LMD-9 strain of S. thermophilus together with the detailed description of the species metaboliccapabilities at the cellular level. The reconstruction of the genome-scale metabolic model, wasperformed using Metabolic Models Reconstruction Using Genome-Scale Information (merlin)together with COBRApy tool and OptFlux platform.S. thermophilus LMD-9 genome was functionally annotated and the encoded metabolicinformation was afterwards used to assemble a draft network. After extensive manual curation,the metabolic network was converted to a comprehensive metabolic model. The assembledGSM model was then validated against experimental data.The metabolism of this important stater for the dairy industry has been accessed indetail through the reconstruction. The organism possesses a simple machinery for central carbonmetabolism and shows a narrow spectrum of carbohydrate utilization. The genome-scalemetabolic model additionally suggests the existence of several pyruvate dissipating pathwayswhich end in the synthesis of various compounds of interest. In silico simulations demonstratedthe production of lactate and residual amounts of formate, acetolactate and acetaldehyde.Regarding the amino acid metabolism, the organism possesses complete pathwaysfor the biosynthesis of all amino acids, except for lysine, methionine and cysteine. Furthermore,the GSM model can be used to simulate other relevant features of the S. thermophilusmetabolism, such as the aroma compounds and Exopolysaccharides (EPS) synthesis, oxygentolerance, absence of complete citrate cycle and pentose phosphate pathway, urea metabolismor amino acid catabolism.",
    "authors": [
      "Cruz, Fernando João Pereira da"
    ],
    "keywords": [
      "Systems biology",
      "Genome-scale metabolic models",
      "Metabolic networks",
      "Constraint-based modeling",
      "Merlin",
      "Streptococcus thermophilus",
      "Lactic acid bacteria metabolism",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84573",
    "title": "A software to manage rehabilitation sessions with a robotic walker",
    "abstract": "Cerebellar ataxia arises from damage or dysfunction that affects the cerebellum and its pathways. As aresult, the motor abilities of individuals with this condition become weakened. Robotics-assisted therapy is still anemerging area, but it has several advantages that could boost the rehabilitation of these individuals. Consideringthis problematic, WALKit Smart Walker is being developed. Its main purpose is to improve the treatment of ataxicpatients through intelligent and multidisciplinary rehabilitation sessions. Thus, it is equipped with several sensorsthat provide monitoring capabilities through a continuous evaluation of the end-user gait and posture.A vast amount of data is acquired during each session by the walker sensors. For health professionals toanalyse this data and have feedback on the patient’s status throughout therapy, tools are needed to control,manage, and monitor sessions in a clear, practical and intuitive way. Therefore, the main goal of this dissertationis centred on implementing an effective way to store the acquired data, along with the development of softwarethat satisfies these requirements.To address these goals, a polyglot persistence database system, composed of a relational and a non-relationaldatabase, was implemented to store the required data while maintaining efficiency. Furthermore, a web applicationwas developed to provide, not only to health professionals, but also to patients themselves, the managementof the rehabilitation sessions with the walker. The application provides an individual and temporal analysis ofthe sessions through interactive graphics adapted to each patient. Additionally, it allows the management of theseveral patients who are/were in treatment and the addition of clinical ratting scales, which are useful to assesstheir motor condition and adapt therapies as needed. In this way, professionals can have a better perception ofthe patient’s condition, and can show patients their evolution, possibly contributing to increase their motivation intherapy.Moreover, in the context of this dissertation, the embedded software of WALKit SmartW, which allows thetherapy configuration, was optimized. This software had no security mechanisms, thus the main goal was on theimplementation of techniques capable of making the software secure. Additionally, other functionalities such asfeedback alerts, were added to the existing application.Throughout the development of this project, it was possible to have continuous feedback from health professionalsof the Hospital of Braga. Usability tests and questionnaires were also applied, and the results were verypromising, enhancing the need for a system with these characteristics. Professionals claimed the system mayhelp in analysing the patient clinical status in an intuitive form while keeping them motivated during treatments.",
    "authors": [
      "Campos, Ana Margarida Reis Maia"
    ],
    "keywords": [
      "Software",
      "Database",
      "Monitoring",
      "Smart walker",
      "Rehabilitation sessions",
      "Base de dados",
      "Monitorização",
      "Andarilho inteligente",
      "Sessões de reabilitação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/77437",
    "title": "Towards procedural music-driven animation: exploring audio-visual complementarity",
    "abstract": "This thesis intends to describe our approach towards developing a framework for the interactivecreation of music driven animations.We aim to create an integrated environment where real-time musical information is easilyaccessible and is able to be flexibly used for manipulating different aspects of a reactivesimulation. Such modifications are specified through the use of a scripting language andinclude, for instance, geometrical transformations and geometry synthesis, gradual colourchanges as well as the application of arbitrary forces.Our framework thus represents a proof-of-concept for converting musical informationinto arbitrary modifications to a dynamic simulation, producing a variety of animations.This is possible due to a bargaining between control and automation, where control ispresent by allowing the user to program these modifications with a scripting languageand automation is present by using physics and interpolation to estimate the visual effectsresulting from those modifications.The particular test case for our system was the animation/simulation of a growing treereacting to wind. In order to control or influence both the tree growth and wind field,as well as other visual parameters, the system accepts two different but complementaryrepresentations of music: a MIDI event stream and raw audio data. Different musicalfeatures are obtainable from each of these representations. On one hand, by using MIDI, weare able to discretely synchronise visual effects with the basic elements of music, such as thesounding of notes or chords. On the other, using audio, we are able to produce continuouschanges by obtaining numerical data from basic spectral analysis. Our framework providesa common interface for the combined application of these different sources of musicalinformation to the generation of visual imagery, under the form of procedural animations.We will describe algorithms presented in multiple research papers, namely for tree generation,wind field generation and tree reaction to wind, briefly detailing our implementationand architecture. We also describe why each of these particular methods was chosen, howthey are organised in our platform and how their parameters may be modified from ourscripting environment leading to what we regard as the procedural generation of animations.By allowing the user to access musical information and give them control of what we havecome to refer to as animation primitives, such as wind and tree growth, we believe to havetaken a first step towards exploring a novel concept with a seemingly endless expressivepotential.",
    "authors": [
      "Brito, Carlos Faria Aquino de"
    ],
    "keywords": [],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34117",
    "title": "Business intelligence: indicadores da interrupção voluntária da gravidez",
    "abstract": "Nos últimos anos, tem surgido um grande interesse na aplicabilidade dastecnologias de Business Intelligence (BI) na área da saúde. A grande satisfaçãoresultante da sua implementação em outras áreas fez com que os profissionaisde saúde juntamente com os profissionais de Tecnologias de Informação(TI) cooperassem para a conceção e o desenvolvimento de plataformas de BIem ambiente clínico.A grande motivação para a sua implementação adveio da possibilidadede conceber um Sistema de Apoio à Decisão (SAD) médica, que suportasseo processo de tomada de decisão e que permitisse que este fosse realizado deforma mais rápida e eficaz. Além disso, com a implementação do ProcessoClínico Eletrónico, surgiu a necessidade de dar usabilidade aos dados dosutentes armazenados nas bases de dados, tornando-se isso possível com aimplementação da plataforma de BI.Neste projeto, incidiu-se numa área médica específica dentro da especialidadede Ginecologia e Obstetrícia, relacionada com a Interrupção Voluntáriada Gravidez (IVG), por ser uma unidade relativamente recente, onde nãoexiste nenhuma tecnologia semelhante implementada e, ainda, por ser umfoco de interesse para os profissionais de saúde. O local de estudo foi o CentroMaterno Infantil do Norte (CMIN), pertencente ao Centro Hospitalardo Porto (CHP), onde se teve acesso às fontes de dados necessárias para odesenvolvimento da plataforma de BI.Além disso, foram também desenvolvidos modelos de Data Mining (DM),igualmente integrados na plataforma de BI, que permitem prever quais asutentes que recorrem à IVG que constituem um grupo de risco e quais asutentes que necessitam de acompanhamento da equipa de enfermagem durante o processo da IVG. Os resultados obtidos foram bastante satisfatórios,uma vez que foram registados valores de 93% para a métrica da sensibilidadena questão relacionada com a probabilidade das utentes pertenceremao grupo de risco, e valores de 91% e de 87% para a sensibilidade e acuidade,respetivamente, no problema relacionado com a previsão do local derealização de uma das etapas do processo de IVG.Na seleção da tecnologia a utilizar para o desenvolvimento da plataformade BI, optou-se pelo Pentaho BI Suite, depois de realizada uma pesquisaaprofundada sobre ferramentas open source.Após a implementação da plataforma de BI, pode-se afirmar que o resultadofoi satisfatório, uma vez que todos os indicadores de desempenhorequisitados foram apresentados. Nestes indicadores, estão incluídos a distribuiçãodas utentes por idades, por profissão, por localidade, pela presençana última consulta de avaliação, entre outros. Além disso, a informação representadaé credível, pois esta foi submetida a um processo de validação porparte dos profissionais de saúde.",
    "authors": [
      "Brandão, Andreia Manuela Couto"
    ],
    "keywords": [
      "681.3:618",
      "618:681.3",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:618",
      "618:681.3"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46679",
    "title": "Aprendizagem não supervisionada de padrões de interação homem-computador",
    "abstract": "Durante longos períodos de atividade cognitiva é comum a sensação de cansaço e falta de energia, acompanhadade um decréscimo de desempenho. Este estado, geralmente denominado de fadiga mental, é considerado uma dasprincipais causas de erro humano. Os efeitos da fadiga mental no desempenho de tarefas complexas e que requeremaltos níveis de concentração devem ser estudados e antecipados de forma a minimizar erros. Exemplo disto é o casoda aviação, ou medicina onde pequenas distrações podem gerar graves acidentes.O impacto negativo da fadiga mental na performance, saúde e bem estar dos indivíduos, torna-se, desta forma,um dos principais motivos que leva ao desenvolvimento de metodologias de deteção destes estados mentais, de formaa preveni-los. Efetivamente, ao longo do tempo diversas metodologias de deteção de fadiga têm sido desenvolvidas,contudo a maioria é pouco objetiva ou requer um grande investimento económico.A presente monografia apresenta um estudo de um sistema de aprendizagem não supervisionada de casos defadiga mental, utilizando padrões de interação homem-computador, recolhidos através da sensorização de rato e teclado.Este estudo permitiu uma classificação correta de 83,3% de novos casos de fadiga mental.",
    "authors": [
      "Quintas, Ana Sofia Martins Sá"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46411",
    "title": "Sistema de monitorização de vibrações baseado numa arquitetura REST para IoT",
    "abstract": "A Internet das Coisas é um fenómeno que, embora não seja recente, tem sentido um enormecrescimento nos últimos anos. Atualmente existem cerca de 10 biliões de dispositivos ligados à Internetcom a expectativa de se alcançar entre 20 a 50 biliões de dispositivos ligados dentro dos próximos cincoanos. Através da introdução de serviços inovadores concebidos para diversas áreas, tais como aindústria, os cuidados de saúde, a domótica, os transportes, a agricultura, o retalho, a segurança, entremuitas outras áreas do nosso quotidiano, a Internet das Coisas promete melhorar as nossas vidas, poiscom as capacidades de monitorização, processamento e comunicação dos dispositivos IoT é possíveltornar as “coisas” do nosso dia-a-dia parte de algo maior.A monitorização, sendo uma parte integrante das soluções baseadas em IoT, pode ser utilizada paramedir vários parâmetros. Alguns dos mais comuns são a temperatura, humidade, pressão, som e avibração. Embora a vibração possa ser vista de diferentes formas, na prática geotécnica a vibraçãocorresponde a uma resposta elástica do terreno (solos e/ou rochas) aquando da passagem de uma ondade tensão, tendo como origem um evento de génese natural (como por exemplo sismos ou odeslizamento súbito de massas rochosas ao longo de falhas geológicas) ou artificial (explosões, cravaçãode estacas, trabalhos de construção, utilização de equipamentos diversos, linhas ferroviárias, tráfegorodoviário, entre outros). Esta vibração pode ser monitorizada recorrendo a diferentes tipos de sensores,pelo que a proposta apresentada opta por recorrer aos acelerómetros MEMS, tirando partido do facto deestes serem extremamente pequenos, baratos e com uma baixa necessidade de consumo de energia.A proposta está dividida em três componentes principais: o Coletor, o Servidor e o Monitor. O Coletor éo componente físico da proposta e tem como responsabilidade registar os eventos de natureza vibratóriaao longo do tempo. O Servidor é o componente central e é responsável por armazenar o histórico detoda a informação recolhida pelo Coletor. Por fim, o Monitor é o componente que é responsável porfornecer uma interface capaz de aceder à informação recolhida pelo Coletor e gravada pelo Servidor.Com o trabalho desenvolvido foram executados alguns testes de forma a avaliar o funcionamento dosdiferentes componentes da proposta, em especial o desempenho do Coletor",
    "authors": [
      "Lima, João Francisco Miranda de Peixoto"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92699",
    "title": "An observability approach for microservices architectures based on opentelemetry",
    "abstract": "The rapid adoption of microservices and cloud-native architectures has revolutionized the way modernapplications are developed and deployed. However, this shift has introduced new challenges in termsof ensuring the reliability and performance of these distributed systems. In response, observability isproposed as a new methodology to address these challenges.Observability refers to the collection of telemetry data (including traces, metrics, and logs) from asystem components in real time, allowing for a comprehensive understanding of its internal status andbehavior. This capability is essential for troubleshooting, performance optimization, and enhancing systemreliability by facilitating the detection of errors and anomalies.The main objective of this thesis is to implement an observability concept within a Python Flask based system. The system follows a cloud-native, microservices, and event-driven architecture. The mainmotivation for this study is the recent, but important development of observability and the culture ofDevelopment and Operations (DevOps).The chosen method for implementation is OpenTelemetry, a neutral and open-source approach toobservability. This decision aims to avoid vendor lock-in, which can be a concern with vendor-specificagents.Furthermore, a study is carried out to make a choice among the vendors considered which are compat ible with OpenTelemetry, e.g. Jaeger, Zipkin, Prometheus, Elastic Search, New Relic, Datadog, Dynatrace,Grafana, Splunk, and AppDynamics. Each vendor offers different approaches to observability and visual ization of the telemetry data. In addition, a weighted decision matrix is used to aid in the decision alongwith a decision criterion defined by the development team.The results of this study not only highlight the vendor selection process for telemetry data visualizationbut also emphasize that OpenTelemetry is a viable and standardized approach to observability, offering aneffective means to prevent vendor lock-in.",
    "authors": [
      "Moreira, André Cunha Azevedo"
    ],
    "keywords": [
      "Observability",
      "Monitoring",
      "Cloud-native",
      "Microservices",
      "Opentelemetry",
      "Tracing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86577",
    "title": "Análise de algoritmos de machine learning para deteção de violência em áudio",
    "abstract": "A violência tem sido parte integrante da humanidade. Existem diferentes tipos de violência, sendo a violênciade cariz físico mais recorrente no nosso quotidiano, afetando cada vez mais a vida de muitas pessoas.O reconhecimento da ação humana tem sido crescentemente estudada nos últimos anos. O áudio (microfones)e vídeo (câmaras) são as formas mais utilizadas na captação de violência. O reconhecimento da ação humanaatravés do vídeo representa uma importante área na visão por computador. No entanto, a captação de vídeo requeruma grande capacidade de processamento e de desempenho, tanto de hardware como software. O áudio surgeassim como um fator capaz de colmatar estes problemas. No entanto, a deteção de áudio é altamente suscetívela grandes flutuações de precisão, dependendo do ambiente acústico em que está inserido.Na presente dissertação, pretendeu-se comparar os diferentes algoritmos de Machine Learning com o intuitode averiguar qual o melhor algoritmo a utilizar para detetar violência em áudio.A revisão da literatura revelou que o áudio pode ser classificado usando algoritmos de Machine Learning, sendoa sua conversão em imagens (mel spectrogram) a metodologia habitualmente utilizada, tendo sido a abordagemtomada. Além disto, estudaram-se os algoritmos frequentemente utilizados na classificação de áudio, tendo estessido utilizados para posterior avaliação.Os resultados obtidos demonstram um bom desempenho das redes neuronais EfficientNet, sendo que as redesque obtiveram melhor precisão foram a EfficientNetB1 e EfficientNetB0, com 95.06% e 94.19%, respetivamente.Adicionalmente, verificou-se que a rede MobileNetV2 é a mais incapaz de classificar entradas de violência, comuma classificação de 92.44%.A rede neuronal EfficientNetB1 apresentou uma melhor capacidade na classificação de violência em áudio.",
    "authors": [
      "Veloso, Bruno Cruz"
    ],
    "keywords": [
      "Deteção de violência",
      "Machine learning",
      "Deep learning",
      "Transfer learning",
      "Reconhecimento da ação humana",
      "Áudio",
      "Violence detection",
      "Human action recognition",
      "Audio",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83363",
    "title": "Self-service kiosk-based anamnesis system for emergency departments",
    "abstract": "Emergency departments have a higher number of visits compared to other hospital de partments. Technology has played a crucial role in promoting improvements in hospitalmanagement and clinical performance. The number of visits to emergency departments hasincreased considerably, giving rise to crowding situations that cause several adverse effects.This situation negatively affects the provision of emergency services, impairs the quality ofhealth care and increases the time patients wait for medical check-up. One of the leadingcauses contributing to the crowding is the high number of patients with low severity clinicalcondition. These are referred to as non-urgent or inappropriate patients, whose clinicalsituation should be taken care through self-care or primary health care.It is the responsibility of the institutions to analyse and quantify the possible causes ofcrowding to find the best solution to mitigate the adverse effects caused. It is believed thatnon-urgent patients can use the time spent in the waiting room more productively, namelyby using a self-service kiosk to which they can provide valuable information to facilitate andaccelerate the clinical processing.This work proposes a solution to be used in the waiting room of emergency departments,which aims to reduce the period of medical check-up. The solution uses a self-service kioskfor the patient to provide relevant clinical data that would otherwise have to be collectedby the physician during the clinical observation process. In particular, the kiosk will collectvital signs, past medical history, main complaint and usual medication. This data willbe processed and provided to the physician in a structured and uniform way before eachmedical check-up. The primary purpose of this solution is to reduce the period of patients’medical check-up and thus improve the response capacity of the emergency departmentswith the same resources.During the Master’s work period, an Android application was implemented for patientsto enter the clinical data mentioned above, and a Web application for physicians to access it.Additionally, a data warehouse was implemented to store the data in a consolidated wayto discover hidden relationships and patterns in the data. The first moment of evaluation,undertaken in a non-hospital facility, shows positive acceptability by participants, with alarge majority considering the system user-friendly. Due to the pandemic, it was impossibleto perform the second planned evaluation moment in a real emergency environment.",
    "authors": [
      "Pacheco, Paulo Alexandre Gonçalves"
    ],
    "keywords": [
      "Health Kiosk",
      "Self-Service Kiosk",
      "Emergency department",
      "Crowding",
      "Primary care",
      "Quiosque de saúde",
      "Self-Service quiosque",
      "Serviço de urgências",
      "Lotação hospitalar",
      "Cuidados primários",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83933",
    "title": "Multiplatform application for monitoring services in a hospital environment",
    "abstract": "Many people believe that information technology has the potential to change the way the healthcare industry approaches its current challenges by improving healthcare quality, safety, and efficiency by bringing decision support to the point of care and enabling routine quality measurement. In the medical field, healthcare information technology refers to any information technology tool or software that is intended to increase hospital and administrative productivity, provide new information about medications and treatments, or improve overall quality of care. Infrastructures in hospitals must manage both information technology and specialized healthcare systems and protocols. It is important in this type of structure to ensure that all operations and information technology run smoothly and one of the ways to achieve this is by continuously and automatically monitor the hospital environment’s systems. The right monitoring and reporting tools can help keep medical staff efficient without worrying about failing systems, provide visibility into usage trends, equipment performance, downtime, and much more, saving time and resources. A monitoring application of this type is regarded as an important source of information. The main goal of this Dissertation within the scope of this project is to develop a monitoring web platform for healthcare information technology administrators that is based on a multi-site and multi-organization scheme. The proposed solution will monitor various hospital services and is expected to provide the current state of the system in a timely manner through a set of graphs and reports, allowing for appropriate operational decisions and ensuring that the system functions as expected. A web application for monitoring hospital services was developed, implemented, and evaluated during the dissertation work period. Using questionnaires, the platform was evaluated and validated in order to understand if this approach may improve information technology availability and, in the long run, alleviate some of the healthcare industry’s pains. A formal evaluation of the solution was also performed, which comprised a strengths, weaknesses, opportunities and threats analysis and a risk assessment report, both of which gave helpful insights into the system’s strengths and shortcomings, as well as potential improvement areas.",
    "authors": [
      "Marques, Carolina Resende"
    ],
    "keywords": [
      "Information technology",
      "Monitoring systems",
      "Web application",
      "Healthcare",
      "Tecnologia da informação",
      "Sistema de monitorização",
      "Aplicação web",
      "Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84541",
    "title": "Development of language modelling techniques for protein sequence analysis",
    "abstract": "Nowadays, the ability to predict protein functions directly from amino-acid sequences alone remains a major biological challenge. The understanding of protein properties and functions is extremely important and can have a wide range of biotechnological and medical applications.Technological advances have led to an exponential growth of biological data challenging conventionalanalysis strategies. High-level representations from the field of deep learning canprovide new alternatives to address these problems, particularly NLP methods, such as wordembeddings, have shown particular success when applied for protein sequence analysis.Here, a module that eases the implementation of word embedding models toward proteinrepresentation and classification is presented. Furthermore, this module was integrated in theProPythia framework, allowing to straightforwardly integrate WE representations with the trainingand testing of ML and DL models.This module was validated using two protein classification problems namely, identification ofplant ubiquitylation sites and lysine crotonylation site prediction. This module was further usedto explore enzyme functional annotation. Several WE were tested and fed to different ML andDL networks. Overall, WE achieved good results being even competitive with state-of-the-artmodels, reinforcing the idea that language based methods can be applied with success to awide range of protein classification problems.This work presents a freely available tool to perform word embedding techniques for proteinclassification. The case studies presented reinforce the usability and importance of using NLPand ML in protein classification problems.",
    "authors": [
      "Gomes, Ivan Alexandre Pereira"
    ],
    "keywords": [
      "Protein classification",
      "Word embedding",
      "Natural language processing",
      "Machine learning",
      "Deep learning",
      "Classificação de proteínas",
      "Processamento de linguagem natural",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/40858",
    "title": "Ferramenta de suporte à decisão e prática clínica em unidades de cuidados neonatais e pediátricos",
    "abstract": "As crianças são uma população especialmente vulnerável, nomeadamenteno que diz respeito à administração de medicamentos e necessidade de nutrição.Estima-se que os doentes neonatais e pediátricos são pelo menos trêsvezes mais vulneráveis a danos causados devido a eventos adversos e erros demedicação do que a população adulta.O desenvolvimento de uma plataforma que suporte os médicos pediatrasno exercício das suas funções diárias, de forma a reduzir o erro médico, éo principal objetivo deste projeto. A sua necessidade foi identificada porum médico pediatra em exercício de funções no Hospital de Santo Antóniono Porto, de forma a que falhas existentes na ferramenta em uso fossemcolmatadas e ainda novas funcionalidades fossem desenvolvidas.Com a presente dissertação foi procurada ainda uma abordagem que permitisseo desenvolvimento de um canal de passagem de informação entre osmédicos e a Farmácia Hospitalar, e que este sistema pudesse ser altamenteescalável, sendo facilmente replicado em qualquer Instituição de Saúde.O desenvolvimento do sistema foi sempre acompanhado por um médicopediatra, sendo este testado e refinado ao longo desse período. Por fim, umaversão para testes da aplicação é lançada assim como um questionário quepretende avaliar a mesma.",
    "authors": [
      "Guimarães, Tiago André Saraiva"
    ],
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56103",
    "title": "Development and implementation of bioinformatics tools for the reconstruction of GiSMos",
    "abstract": "The reconstruction of Genomic-Scale Metabolic Model (GiSMo)s is an increasinglygrowing methodology, which allows to develop models that can be used to perform in silicopredictions on the phenotypical response of an organism to environmental changes andgenetic modifications. These predictions allow focusing in vivo experiments on methodologiesthat will, theoretically, present better results, thus reducing the high costs on timeand money spent in laboratorial experiments. GiSMos are a mathematical representationof the organism’s genome, in the form of metabolic networks. As complex as these canbe, because of the large number of compounds involved in many different reactions andpathways, the treatment of all such data is not easily manually performed.Several bioinformatics software were developed with the aims of improving this procedure,by automating many operations in the reconstruction process. Metabolic ModelsReconstruction Using Genome-Scale Information (merlin) is one of such tools, following aphilosophy that thrives on providing an intuitive and powerful graphical environment, toannotate data on key metabolic components and building a complete genome-scale model.While already encompassing a wide range of tools, it is still a work in development.Upon analyzing its functioning, several improvement opportunities were identified, mainlyin existing operations. Moreover, missing important features for the reconstruction ofGiSMos were as well identified.This work details the results of this analysis and the improvements performed toenrich merlin’s toolbox.",
    "authors": [
      "Dias, António Carlos Fortuna Ribeiro"
    ],
    "keywords": [
      "Merlin",
      "Genome-Scale Metabolic Models",
      "Software",
      "Development",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79746",
    "title": "Sistema de recomendação para uma loja online de livros",
    "abstract": "Nas últimas décadas, a tecnologia tem sofrido uma evolução exponencial, assumindo um papel fundamen tal no nosso quotidiano e quando aplicada nas mais diversas áreas proporciona-nos melhorias significati vas na nossa qualidade de vida. Por exemplo, a tecnologia permite-nos fazer compras sem sair de casa,que é um hábito que muitas pessoas têm e nos últimos tempos, devido ao panorama em que nos encon tramos, o seu crescimento tem sido ainda mais notável. Frequentemente, enquanto compramos online,são nos apresentados itens que não têm qualquer interesse para nós. Diante este problema, surgiram ossistemas de recomendação, que se tornaram uma parte fundamental e um dos fatores diferenciadores anível aplicacional. O principal objetivo de um sistema de recomendação é, recorrendo a algoritmos de Ma chine learning, produzir uma lista de itens ordenados de acordo com o grau de relevância esperado paraum determinado utilizador, permitindo, por exemplo, evitar o problema de recomendações indesejadas.Mais especificamente, nesta dissertação desenvolveu-se e incorporou-se um sistema de recomendaçãonuma aplicação web de venda online de livros, onde as técnicas concebidas e implementadas permitirammelhorar a qualidade da recomendação e consequentemente a experiência aplicacional, com sugestõesque vão de encontro às preferências do utilizador.",
    "authors": [
      "Cardoso, Telmo André Moreira"
    ],
    "keywords": [
      "Comércio online",
      "Experiência aplicacional",
      "Machine learning",
      "Sistema de recomendação",
      "Machine learning",
      "Online shop",
      "Recommender system",
      "User experience",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46642",
    "title": "Detecção inteligente de fugas de informação por analise comportamental",
    "abstract": "The information that a company possesses is one of its most valuable assets. This informationis nowadays digitally managed, which is the reason for the exponential increase in securitybreaches, where information is defiled or even stolen. Seeking to solve this problem, WatchfulSoftware developed a product, RightsWATCH, that allows for an organization to protect andwatch over its information.By monitoring what happens to information, RightsWATCH provides, in case of an incident,the means to undertake a very complete post-mortem analysis. Nevertheless, by the time thisanalysis is complete, it might have been hours (or days) since the incident occurred. To makematters worse, nowadays most threats actually come from the inside of the company. Thatbeing said, this dissertation defines as its main objective the need to understand if it is possibleto detect data leaks in an intelligent way, through a real time analysis of the user’s behaviourwhile he handles the classified information. This possibility was indeed confirmed through aninvestigation comprising experiences with real world use cases and a variety of data preparationand data analysis techniques.",
    "authors": [
      "Costeira, Ricardo Manuel Andrade"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79965",
    "title": "Criação de uma camada de serviço especificada em diagramas de sequência UML",
    "abstract": "Automatic code generation is an increasingly recurring theme these days, constantly manifesting itself in new tools that allow you to generate code from top-level languages that try to make the programmer’s job faster and easier. UML is a long-standing modeling language that is primarily used to model applications during the specification phase and is sometimesalso used for automatic code generation.In this dissertation we introduce UMLayer, a middleware component that allows integration with certain types of applications through the provision of services. This layer acceptsbehavior specifications through UML sequence diagrams, allowing applications to accessservices that are specified from these diagrams.The goal is thus to allow an application, still under development, to have immediate accessto services that correspond to its use cases, having the user to provide only the sequencediagrams that specify them. This, in the user’s view, allows his application’s service layerto be completely replaced by sequence diagrams and the application becomes immediatelyready to use. The code generated from these diagrams will be inaccessible and unalterableon the part of the user, since it is only and exclusively through the diagrams that the userwill specify all the logic of his use cases. All this mechanism of generation and subsequentaccess to the generated code becomes as transparent as possible to the user, having to onlyworry about the correct elaboration of his diagrams.",
    "authors": [
      "Lima, Marcelo Alexandre Matos Fonseca"
    ],
    "keywords": [
      "Automação de código",
      "Camada de serviço",
      "Diagrama de sequência UML",
      "Automatic programming",
      "Service layer",
      "UML sequence diagram",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27844",
    "title": "A new Framework to enable rapid innovation in Cloud Datacenter through a SDN approach",
    "abstract": "In the last years, the widespread of Cloud computing as the main paradigm to deliver a largeplethora of virtualized services significantly increased the complexity of Datacenters managementand raised new performance issues for the intra-Datacenter network. Providing heterogeneousservices and satisfying users’ experience is really challenging for Cloud service providers,since system (IT resources) and network administration functions are definitely separated.As the Software Defined Networking (SDN) approach seems to be a promising way to addressinnovation in Datacenters, the thesis presents a new framework that allows to develop andtest new OpenFlow–based controllers for Cloud Datacenters. More specifically, the frameworkenhances both Mininet (a well–known SDN emulator) and POX (a Openflow controller writtenin python), with all the extensions necessary to experiment novel control and managementstrategies of IT and network resources.Further more, the framework was validated by implementing and testing well known policies.Hybrid allocation policies (considering both network and servers) were also implemented andscalability tests were performed.This work was developed under the ERASMUS student mobility program, in the TelecommunicationNetworks Research Group, Dept. of Information Engineering, University of Pisa,and resulted in the paper Datacenter in a box: test your SDN cloud-datacenter controller athome that was accepted into EWSDN2013.",
    "authors": [
      "Teixeira, José António Barros"
    ],
    "keywords": [
      "Datacenter",
      "Cloud",
      "SDN",
      "OpenFlow",
      "681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64528",
    "title": "A fully configurable virtual laboratory of classical mechanics",
    "abstract": "Nowadays many mathematical applications allow the user to introduce its own equationsin the system and also observe through different possibilities the desired results. Regardingphysics, an extended range of virtual laboratories allow the user to accomplish virtualphysics experiments. These virtual laboratories consist in predefined scenarios where theuser can change the value of the physics variables and then visualise the changes accomplished.Other virtual laboratories uses a physics engine allowing the user to create itsown scenarios. However, the physical behaviour of the objects is hardcoded since it resultsstrictly on the physics equations used internally by the physics engine.This dissertation pretends to investigate how far and with what degree of scientific rigorit is possible to associate the idea of the user introducing its own equations with the idea ofaccomplishing virtual experiments of physics. As a proof of concept, this dissertation focuson a specific area of mechanics: the dynamic of rigid bodies. The result of this research isa virtual laboratory completely different relatively the others.Our system has no knowledge about physics. Even the most general laws of physicssuch as the Newton’s second law are not known by the system. To the system, any equationintroduced is considered just as one more equation without any particular meaningassociated to it. The same happens for any physics entity. For example, if the gravitationalacceleration is introduced by the user, to the system it is just another attribute of the world.Taking into account the dynamics of rigid bodies, an object can be identified as being, atany time, in one of three different states. These are: when a object is not in contact withany other, when an object collides with another object and they immediately separate, andwhen two objects remain in contact over time. The user must specify all the equations thatdrive each of these three states. Using its geometrical knowledge, the engine determines atany time in which state an object is. Also, the system provides all the relevant geometricalinformation. For instance, in a collision between two objects, the point and the two normalsvectors of the collision are provided.The graphical simulations reflects strictly on the equations introduced. Therefore, ifthe equations to solve a collision between two objects does not reflect the real underlyingphysics of the situation, it is possible that the objects simply ends-up penetrating eachother. All the relevant numerical information about an experience can be processed throughdifferent forms. In fact, the user can request plots of variables, the graphical application ofvectors on objects, and even the tracing of the variables at a specific event.",
    "authors": [
      "Silva, Alexandre Ventosa da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23416",
    "title": "Novos paradigmas de interface de utilizador para aplicações na área da saúde",
    "abstract": "As instituições de saúde vivem atualmente num ambiente de crescente densidade deinformação, sendo esta uma área de intensa transferência eletrónica de dados. Comoresultado, tem-se recorrido cada vez mais às interfaces de utilizador como forma de auxilio noprocessamento de dados. Tratando-se de uma questão inovadora, o surgimento de novastecnologias de informação e comunicação tem sido um motor para o desenvolvimento denovos tipos de interfaces de utilizador.Com o aumento do volume de informação, tem surgido problemas no que diz respeito ao seuprocessamento. Cada vez mais são exigidos que os serviços sejam prestados com maioreficiência, implicando maior rapidez na obtenção de dados. É neste contexto que surge anecessidade de direcionar os novos avanços no campo da saúde, tendo como linhaorientadora a busca de um serviço mais eficiente e com maior qualidade. Neste sentido surgea necessidade de optimização dos processos de acesso e utilização dessa informação,alterando a forma como a informação sobre os utentes é obtida. É, essencialmente, nestespontos que se foca esta dissertação.Adicionalmente, e do ponto de vista funcional, pode dizer-se que estes desenvolvimentosapresentam características favoráveis à prevenção e ao controlo de infeções hospitalares,reduzindo a necessidade de contato direto entre os objetos, o que leva, à diminuição dapropagação das ditas infeções.Com a concretização deste estudo procura-se avaliar as potencialidades do reconhecimentogestual aplicado às interfaces de utilizador, na sua implementação na área específica dasaúde.Paralelamente desenvolveu-se um protótipo destinado aos utentes que frequentem asinstituições de saúde. Este protótipo teve como objetivo a validação das interfaces de utilizadoranalisadas, considerando a utilização das tecnologias recentemente introduzidas no mercado.Foi dada especial atenção a interfaces de utilizador sem contato entre dispositivos eutilizadores (e.g. utilizando a tecnologia do Kinect da Microsoft).No final é apresentado um estudo estatístico relativo à avaliação por parte dos utilizadores dainterface do protótipo desenvolvido, onde se conclui a funcionalidade da utilização de gestosse revelou intuitiva e de fácil execução.",
    "authors": [
      "Rodrigues, Marco André Ferreira"
    ],
    "keywords": [
      "614:681.324",
      "681.324:614"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "614:681.324",
      "681.324:614"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/77364",
    "title": "The impact of microservices: an empirical analysis of the emerging software architecture",
    "abstract": "The applications’ development paradigm has faced changes in recent years, with modern development being characterized by the need to continuously deliver new software iterations. With great affinity with those principles,microservices is a software architecture which features characteristics that potentially promote multiple qualityattributes often required by modern, large-scale applications. Its recent growth in popularity and acceptance inthe industry made this architectural style often described as a form of modernizing applications that allegedlysolves all the traditional monolithic applications’ inconveniences. However, there are multiple worth mentioning costs associated with its adoption, which seem to be very vaguely described in existing empirical research, being often summarized as \"the complexity of a distributed system\". The adoption of microservices provides theagility to achieve its promised benefits, but to actually reach them, several key implementation principles haveto be honored. Given that it is still a fairly recent approach to developing applications, the lack of establishedprinciples and knowledge from development teams results in the misjudgment of both costs and values of thisarchitectural style. The outcome is often implementations that conflict with its promised benefits. In order toimplement a microservices-based architecture that achieves its alleged benefits, there are multiple patterns andmethodologies involved that add a considerable amount of complexity. To evaluate its impact in a concrete andempirical way, one same e-commerce platform was developed from scratch following a monolithic architecturalstyle and two architectural patterns based on microservices, featuring distinct inter-service communication anddata management mechanisms. The effort involved in dealing with eventual consistency, maintaining a communication infrastructure, and managing data in a distributed way portrayed significant overheads not existent in thedevelopment of traditional applications. Nonetheless, migrating from a monolithic architecture to a microservicesbasedis currently accepted as the modern way of developing software and this ideology is not often contested, nor the involved technical challenges are appropriately emphasized. Sometimes considered over-engineering,other times necessary, this dissertation contributes with empirical data from insights that showcase the impact of the migration to microservices in several topics. From the trade-offs associated with the use of specific patterns, the development of the functionalities in a distributed way, and the processes to assure a variety of quality attributes, to performance benchmarks experiments and the use of observability techniques, the entire development process is described and constitutes the object of study of this dissertation.",
    "authors": [
      "Costa, Leandro José Abreu Dias"
    ],
    "keywords": [
      "Microservices",
      "Monolithic",
      "Software architectures",
      "Inter-service communication",
      "Performance evaluation",
      "Microsserviços",
      "Monolítico",
      "Arquiteturas de software",
      "Comunicação inter-serviços",
      "Avaliação de performance",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66580",
    "title": "Modelling interspecies interactions of syntrophic communities of Syntrophobacter fumaroxidans and Methanospirillum hungatei",
    "abstract": "Microbial communities have gained particular interest and have been used for practical applications such as biorefineries, and bioremediation. However, studying these communities has proven to be difficult due to the absence of experimental protocols and computational tools like the ones available for single organisms.In this work, we present Genome-Scale Metabolic models both for Methanospirillum hungatei strain JF1 and Syntrophobacter fumaroxidans strain MPOBT, together with a model that combines both into one community model. The genome-scale metabolic model reconstruction of S. fumaroxidans was performed in merlin whereas, the methane-producing archaeon M. hungatei was reconstructed in KBase’s environment and the model curation was performed in merlin. OptFlux and BioCoISO, a tool implemented over COBRApy developed specifically for debugging model pathways, were used for curating and validating both models.The metabolism of each individual organism was assessed through its model reconstruction. In silico simulations demonstrated the production of various compounds of interest such as formate in M. hungatei and acetate in S. fumaroxidans. The meta-model representing the community composed by both organisms was assembled using FRAMED, and it was able to describe the metabolic exchanges between the formate scavenger M. hungatei and the syntrophic partner S. fumaroxidans.The reconstructed models can be used to study further the metabolic interactions between these bacteria.",
    "authors": [
      "Bastos, José Jorge Sampaio"
    ],
    "keywords": [
      "Systems biology",
      "Genome-scale metabolic models",
      "Metabolic networks",
      "Constraint-based modelling",
      "Merlin",
      "Syntrophobacter fumaroxidans",
      "Methanospirllum hungatei",
      "Syntrophic community",
      "KBase",
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Biotecnologia Industrial"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27779",
    "title": "Normas, nomenclaturas e uniformização do registo clínico",
    "abstract": "A crescente utilização dos Sistemas de Informação (SI) nas unidades desaúde tem um papel muito importante para garantir a qualidade das mesmas.Com as Tecnologias da Informação e da Comunicação (TIC), os dadosarmazenados estão estruturados e organizados de forma a possibilitar umautilização rápida e e caz. O aumento de informações em formato eletrónicono processo de Registo Clínico (RC) apesar de diminuir em grande escalaerros que resultavam da utilização de dados mal entendidos, trouxe um desa o aos técnicos de informática médica. Esse desa o passa por melhorar aqualidade da prestação de cuidados de saúde utilizando a informação armazenada.É neste âmbito que surgem as normas e sistemas de nomenclatura quepossibilitam uma uniformização do RC de forma a evitar dados ambíguose permitir a comunicação entre diferentes pro ssionais de saúde e serviçoshospitalares. Estas normas são divididas conforme a sua nalidade, havendonormas de comunicação, imagem e representação.Pretende-se, neste contexto, implementar o Systematized Nomenclatureof Medicine (SNOMED) na Agência de Interoperação, Difusão e Arquivo(AIDA) no Centro Hospitalar do Alto Ave (CHAA) de forma a utilizar as suaspotencialidades no processo de uniformização do Registo Clínico Eletrónico(RCE).",
    "authors": [
      "Castro, Sara Catarina Oliveira"
    ],
    "keywords": [
      "681.3:61",
      "61:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56063",
    "title": "Arquitetura lógica de software para sistemas de tratamento sistemático de dados: abordagem em ambiente Cloud no contexto da indústria automóvel",
    "abstract": "Com a evolução da Internet of Things, a presença de Tecnologias de Informação e Comunicaçãoestá cada vez mais omnipresente e vários estudos realizados nesse âmbito têmproporcionado o desenvolvimento de novas arquiteturas baseadas no conceito de CloudComputing.Neste contexto, foram desenvolvidos novos modelos de serviços e novas ferramentas quese tornam importantes para a gestão dos dados adquiridos e a descoberta de novos conhecimentosrelacionados com as realidades do nosso mundo e sobre os comportamentos,relacionados com mobilidade automobilística.Devido à importância que os Advanced Driver Assistance Systems na indústria automóvel,pois garantem uma maior segurança e conforto na condução. Com isto, surge uma oportunidadede ter os veículos conectados, a recolher dados durante a condução com a finalidadede gerar informação que seja útil para auxiliar e melhorar o processo de condução.Uma arquitetura lógica para sistemas que visam realizar um Systematic Field Data Exploration,baseada no conceito Cloud Computing, é proposta neste trabalho. A arquitetura visadar uma resposta sistemas de visam realizar uma recolha de dados sistemática, considerandoa aquisição, armazenamento, processamento e partilha de dados.",
    "authors": [
      "Pinto, Eduardo Manuel Oliveira"
    ],
    "keywords": [
      "Computação Cloud",
      "Internet of Things",
      "Tecnologias de Informação e Comunicação",
      "Advanced Driver Assistance Systems",
      "Sistema de Exploração de Dados Sistemática",
      "Cloud Computing",
      "Information and Comunication Tecnology",
      "Systematic Data Field Exploration",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94080",
    "title": "Vídeo streaming em condições adversas de rede: avaliação dos protocolos HTTP/3 e QUIC",
    "abstract": "A Internet está em constante evolução, bem como as aplicações e serviços disponibilizados aos utiliza dores. Com o crescimento na adoção de plataformas como YouTube, Netflix, Disney+, HBO e AmazonPrime Video, é possível ter acesso a um amplo conteúdo em formato de vídeo. Estas mudanças naInternet obrigam a que se criem formas de melhorar a experiência de utilização nestes novos cenários.O QUIC é um novo protocolo de transporte considerado atualmente uma peça chave no suporteà nova norma HTTP/3. Este protocolo opera sobre User Datagram Protocol (UDP) e visa oferecer umserviço de transporte multistream, rápido, robusto e seguro, que permite contornar limitações conhecidasdo protocolo de transporte Transmission Control Protocol (TCP), a base para os atuais protocolos HTTP/1e HTTP/2.Este trabalho, propõe avaliar o protocolo de transporte QUIC usado como base no recém-imple mentado HTTP/3, em alternativa ao HTTP/2, assente em TCP. Como já existem alguns estudos quecomparam estes protocolos no acesso a páginas web, neste trabalho é analisado o protocolo HTTP/3 nocontexto de streaming de vídeo, em cenários que possam existir condições adversas de rede. Pretende se, neste trabalho, desenvolver uma plataforma experimental que consiga transmitir vídeo recorrendo aestes protocolos e analisar esta transmissão para perceber se o protocolo QUIC realmente será útil paraa web em constante evolução.É desenvolvido um software, com recurso à framework hls.js, com o objetivo de receber e reproduziro vídeo, recolhendo determinadas métricas relativas à reprodução. É, ainda, criada uma ferramenta paraanalisar e gerar gráficos, para os dados adquiridos.São apresentados alguns resultados, ainda que preliminares, que permitem observar diferenças entreos protocolos HTTP/2 e HTTP/3. Estas diferenças são mais notórias em condições adversas de rede,em que o HTTP/3 proporcionou uma transmissão de vídeo mais fluída, com menor latência, menor drifte maior carga de buffer, na generalidade dos cenários escolhidos.",
    "authors": [
      "Guimarães, Filipe Miguel Teixeira Freitas"
    ],
    "keywords": [
      "Streaming de vídeo",
      "HTTP",
      "QUIC",
      "TCP",
      "Condições adversas de rede",
      "Video streaming",
      "Adverse network conditions",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81527",
    "title": "Learning the physics of open quantum systems from experiments",
    "abstract": "The ability to efficiently determine the dynamics to which a quantum device conforms is vital forits reliable operation. Thus, as quantum machines evolve, the means for their characterization mustevolve alongside them, especially as they reach the limits of classical tractability. Bayesian inference hasbeen proposed as a solution, as it offers a flexible way of using experimental data to learn the dynamicalparameters - or even models - governing the evolution of quantum systems. It gives rise to noise-resilientprotocols, which are capable of quantifying their own uncertainty, of learning from scarce information, andof real-time estimation. Importantly, and apart from the obvious applications in sensing devices, onlineprocessing enables adaptivity, a stepping stone for achieving fundamental limits of metrology via what iscalled quantum-enhanced estimation. Like so, the exploitation of quantum control as a resource withinthe Bayesian paradigm opens the door to uncertainties scaling at the Heisenberg limit.Most work on the subject has relied on simple methods for portraying the Bayesian posterior, but theyquickly become a bottleneck in realistic scenarios. The difficulty of the task grows with the complexity ofthe devices to be verified, which adds to the fact that they are ultimately open systems and as such undergouncontrollable interactions with their surroundings. While capturing unwanted processes like decoherenceis by itself a valuable endeavor, this poses extra challenges, and requires careful computational treatment.In general, when scaling up, the biggest difficulty of Bayesian learning is numerical integration. In thiscontext, pairing it with advanced Monte Carlo methods makes for remarkably robust algorithms, whichcan succeed under complex features and control a multitude of trade-offs.This dissertation aims to overview both of these methodologies, and to apply them to the characterization of noisy quantum computers. Special emphasis is placed on state-of-the-art methods for statistical sim ulation - namely Hamiltonian and sequential Monte Carlo, variants thereof, and subsampling approaches.Using these strategies to post-process 150 single-shot measurements on a Ramsey sequence, we learn anoscillation frequency and coherence time to 0.4% and 33% uncertainty respectively, compared to 15% and333% using curve fits. With an echoed pulse set-up and 75 shots only, we then achieve an uncertainty of0.3% for the qubit’s precession frequency, whereas standard fitting methods either do worse or fail completely. Switching to online processing and further lowering the total shot count to 15, we use adaptiveexperimental design to infer the frequency to 5% uncertainty, compared to 44% using offline processingunder otherwise identical conditions.",
    "authors": [
      "Alves, Alexandra Francisco Ramôa da Costa"
    ],
    "keywords": [
      "Bayesian inference",
      "Monte Carlo integration",
      "Quantum metrology",
      "Inferência Bayesiana",
      "Integração de Monte Carlo",
      "Metrologia quântica"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84143",
    "title": "PhobiAR, an artefact of augmented reality to support the exposure therapy of specific phobias",
    "abstract": "Phobia is a type of anxiety disorder defined by a persistent and excessive fear of anobject or situation. Currently, exposure therapy is the most practiced method to treatphobias, although it comes with limitations. We can reduce these limitations by combiningAugmented Reality techniques with exposure therapy. Its benefits are a decrease in costs,versatility of the process, and full control of the procedure by the therapist. As shown inmultiple research, Augmented Reality has obtained interesting results in the therapy ofpsychological disorders serving as a foundation for the development of this project. Therecent technological advances in the field also allowed for easier access to Augmented Realitywhich is accessible to use even in old smartphones. The goal of this Master’s dissertationwas to develop an artefact in conjunction with psychologists who treat phobic patients, tocreate a program to support the therapy of phobias with a gradual exposure system. Theirhelp was essential to understand the most important features needed for the platform. Theplatform was deployed in the informatics department servers, which could be accessedby everyone that had internet connection. Multiple psychologists were invited to test theplatform by following a user guide created and give their technical feedback in the end. Theresults gathered were positive, which proves the viability of this system as an extension tothe current methods by providing comfort and efficiency.",
    "authors": [
      "Vilas Boas, Raul"
    ],
    "keywords": [
      "Augmented reality",
      "Virtual reality",
      "Anxiety disorder",
      "Phobia",
      "WebAR",
      "Gradual exposure",
      "3D models",
      "Realidade aumentada",
      "Realidade virtual",
      "Transtorno de ansiedade",
      "Fobia",
      "Exposição gradual",
      "Modelos 3D",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/40868",
    "title": "Arquitetura distribuída para análises multimodais de conectividade cerebral",
    "abstract": "O estudo da conectividade, estrutura, e integração das funções cerebraisé actualmente uma das ferramentas de maior importância na compreensãodo cérebro humano. A realização destes estudos via aquisições de RessonânciaMagnética exige no entanto acessibilidade e disponibilidade constante deinformação. A quantidade de procedimentos e técnicas de análise, associadoà produção de grandes volumes de dados e multitude de soluções de softwaresão alguns dos principais entraves à organização, manutenção e partilha deestudos neuroimagiológicos.Desta forma, o objectivo principal deste trabalho consiste na concepçãode um fluxo de processamento, que possa servir de padrão à conjugaçãode resultados de análises multimodais, através de uma estrutura de estudosdefinida e nomenclatura de ficheiros própria. Tendo por base este fluxo foidesenvolvida uma aplicação, designada BrainArchive, para automatizaçãodo processo de organização e partilha de estudos neuroimagiológicos. Estapermite por sua vez a disponibilização de grandes volumes de informaçãosem necessidade de caracterização manual de cada ficheiro, ao mesmo tempoque se revela uma ferramenta simples e intuitiva na aquisição de dados.O protótipo desenvolvido responde às necessidades do seu contexto de utilizaçãoe, por conseguinte, é espectável que potencie o processo de investigação,através da simplificação e redução do tempo despendido na organizaçãoe partilha de informação.",
    "authors": [
      "Fernandes, Filipe Bernardino"
    ],
    "keywords": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Médicas::Ciências da Saúde",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93916",
    "title": "Aplicação Web Fullstack para auxílio à redação de textos através de mapas mentais",
    "abstract": "Esta dissertação propõe uma solução a um problema existente num publico que abrange desde oestudante comum até ao profissional do quotidiano. Qualquer indivíduo nesta audiência pode enfrentarobstáculos na estruturação de textos, como relatórios académicos ou profissionais, encontrando dificul dades em organizar as ideias para o formato textual desejado.A utilização de métodos de organização como os mapas mentais desempenha um papel crucial comoprimeiro passo na escrita, pois auxiliam no planeamento da escrita, um processo essencial na construçãode qualquer tipo de texto, facilitando a organização de ideias.A dissertação aborda o desenvolvimento de uma aplicação web, tanto na sua vertente de frontendcomo na de backend. A aplicação integra a edição do mapa mental e do texto no mesmo ambiente,agilizando a escrita, algo que segundo a pesquisa realizada, não existe.O desenvolvimento da aplicação incluiu o planeamento da marca, nomeadamente a criação de logó tipos e slogans. Seguindo-se a fase de design, onde foram realizados mockups das páginas. O NextJsfoi selecionado como framework para o frontend e backend com o Typescript. Para a Base de Dados(BD), foi utilizado o MongoDB com o Docker, para um deployment local, escalável e fácil de migrar. Aimplementação do frontend foi estruturada em 3 camadas diferentes, como a camada de apresentação,de aplicação e a de negócios.No desenvolvimento do frontend foram utilizadas várias bibliotecas de React para auxiliar no desenvol vimento dos mapas mentais e do editor de texto, onde foram impostas regras de funcionamento, comopor exemplo, a criação máxima de 5 níveis diferentes no mapa mental e a proibição de promover oudesprover nós que não cumpram com certos requisitos. O texto é estruturado ao longo da construçãodo mapa mental, podendo apenas gerir o conteúdo de cada nível criado. Os dados são todos persistidoslocalmente no localStorage do browser para funcionamento offline.Finalmente, para avaliação da aplicação, foi realizada uma experiência com um grupo de estudantes.Estes responderam posteriormente a um questionário sobre o funcionamento da aplicação e sobre aexperiência realizada. Os resultados obtidos foram em média bastante positivos, tendo sido expostosalguns problemas em pequenas funcionalidades, que serão sujeitas a melhoramentos.",
    "authors": [
      "Martins, Marcos Alexandre Ferreira"
    ],
    "keywords": [
      "Backend",
      "Estruturação textual",
      "Frontend",
      "Mapas mentais",
      "Planificação",
      "Mind Maps",
      "Planing",
      "Text structuring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82582",
    "title": "Metodologias para classificação de tráfego de rede seguro",
    "abstract": "Characterizing network traffic is a very important process for network planning,management, and analysis. Despite having received attention over the years, there arestill many improvements to be developed, for example, how to accurately classify securenetwork traffic. The research community has already presented numerous characterizationmethodologies, and in this dissertation, one of the approaches for the characterization ofsecure network traffic is investigated. First, the most common encrypted traffic protocols onthe Internet are presented. Its architecture and operating mode are shown to carry out datatraffic safely. Next, the methods for capture network traffic are examined and the most usedand efficient methods of classification of network traffic are pointed out in the study of thecharacterization of secure traffic. The advantage of each method, the use of hybrid methods,the accuracy of characterizing certain application protocols are discussed.After selecting the desired method of characterization of secure traffic from among theseveral that were presented, an analysis of the accuracy of this method was made withseveral datasets. In addition to the tests carried out with data capture in an experimentalenvironment, where all generated traffic was controlled, tests with public datasets were alsoaccomplished. Finally, the results obtained from the precision achieved in each environmentare revealed and the results were synthesized with a brief explanation and highlighting theircharacteristics.",
    "authors": [
      "Gonçalves, Matheus dos Santos"
    ],
    "keywords": [
      "Traffic classification",
      "Encrypted traffic",
      "Machine Learning",
      "Traffic analysis",
      "Classificação de tráfego",
      "Tráfego criptografado",
      "Análise de tráfego",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46414",
    "title": "Where@UM: aplicação de posicionamento colaborativo para PC baseada em Wi-Fi fingerprinting",
    "abstract": "A utilização do posicionamento, no âmbito das aplicações fornecidas aos utilizadores, tem vindoa aumentar exponencialmente. Os trabalhos desenvolvidos na área do posicionamento indoor têm vindoa aumentar com o aumento da mobilidade dos utilizadores.As possibilidades de uso destas tecnologias são imensas: aumentar a experiência do utilizadore a lealdade, aumentar as vendas através de marketing de proximidade, ajudar a movimentação deutilizadores em locais públicos, o uso de geofencing para encontrar pessoas, etc.De forma a reaproveitar as infraestruturas já existentes nos edifícios, a tecnologia de Wi-Fifingerprinting tem sido uma escolha frequente por parte das equipas de investigadores e programadores.O principal objetivo desta dissertação é desenvolver uma aplicação para computadores pessoais usandoum sistema de posicionamento baseado em Wi-Fi fingerprinting, integrando-a no sistema Where@UM.A solução apresentada detalhadamente na dissertação implementa funcionalidades jádisponibilizadas na aplicação Android. Foram também desenvolvidas novas respostas a problemas jáexistentes e integrados novos módulos na arquitetura, como a integração com as redes sociais e osuporte multiplataforma, tendo especial cuidado em manter alguma homogeneidade no ambienteaplicacional Where@UM, através do uso de interfaces de utilizador similares.",
    "authors": [
      "Mesquita, Ricardo Miguel Rego"
    ],
    "keywords": [
      "Wi-Fi fingerprinting",
      "Aplicação desktop",
      "MVVM",
      "Posicionamento indoor",
      "Integração com o Facebook",
      "Desktop application",
      "Indoor positioning",
      "Facebook integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66599",
    "title": "Sistema de monitorização de transportes urbanos utilizando SNMP",
    "abstract": "As empresas do sector dos transportes públicos têm procurado introduzir novas tecnologiase aplicações com o objetivo de melhorar o serviço disponibilizado aos passageiros. Nestesentido, os operadores deste tipo de serviço têm investido no aumento da segurança econforto do passageiro, na diversificação das funcionalidades disponibilizadas, no acrescentode novos destinos ou paragens e na eficácia do cumprimento de horários planeados, entreoutros. Do lado das tecnologias de informação, uma das funcionalidades que mais temrecebido atenção é o sistema de informação e monitorização acessível aos utilizadores.Enquanto uma parte substancial da informação mantida por estes sistemas é atualizadapouco frequentemente, podendo ser, inclusive, denominada de informação de carácter fixo,os dados utilizados no sistema de monitorização devem ser atualizados o maisfrequentemente possível. Mas, o objetivo de aumentar a qualidade do serviço de informaçãoprestado melhorando a qualidade da monitorização do sistema, implica, em geral, custoselevados. No planeamento e construção de sistemas e aplicações de informação paramonitorização de serviços de transporte de passageiros numa escala urbana ou regional, aescolha do tipo de infra-estrutura que torna possível obter, processar e utilizar os dados emtempo real, ou com frequência funcionalmente útil, é, assim, fulcral. Uma má escolha dastecnologias associadas e dos sistemas que integram o serviço, podem tornar os custosinerentes à sua implementação no terreno insustentáveis e podem comprometer seriamente asustentabilidade das empresas do ramo.O presente trabalho começa com um estudo aprofundado das várias soluções e tecnologias jáexistentes no mercado, analisando-as criticamente. Desse estudo resultou o desafio deconceber uma proposta dum sistema aplicativo integrado de gestão de informação quedisponibiliza ferramentas para a gestão, optimização e administração do serviço detransportes públicos com base em tecnologias normalizadas, abertas e de sem custo deutilização.A solução desenhada, que se pretendeu modular e escalável, consiste num sistema integradobaseado inteiramente no protocolo Simple Network Management Protocol (SNMP) e que combina aplicações informáticas com a capacidade de obter dados em tempo real dosveículos e também disponibilizar informação diferenciada aos diversos intervenientes dosistema (passageiros, gestores, motoristas).Após a implementação e teste dum sistema completo de aplicações protótipo, fica provada aefetividade funcional da arquitetura proposta. Sendo esta solução modular e baseada numatecnologia normalizada e aberta e de utilização livre de direitos de autor, espera-se, emconsequência, que também seja eficaz no que concerne a eventuais custos de implementaçãoe adoção no mundo real.",
    "authors": [
      "Zenha, Miguel Gomes"
    ],
    "keywords": [
      "SNMP",
      "Localização automática de veículos",
      "Informação em tempo real",
      "Transportes de passageiros",
      "Automatic vehicle localization",
      "Real time data",
      "Public transports",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27964",
    "title": "From natural language requirements to formal descriptions in Alloy through boilerplates",
    "abstract": "Formal Methods are usually applied by specialists in the final phases of software development.They aim to identify programming errors, and through that reduce the probability of a future failure.Usually, errors are more related with misinterpretation of requirements than with bad programming.More than ever, requirements documents deal with complex terms, which programmersaren’t familiar with, resulting in an increase of misinterpretation of requirements and increasingthe costs of the execution of a software project. The use of formal methods could reduce thesecosts, if properly used to verify requirements and not source code. However, most companiesavoid using formal methods due to high costs associated with formal methods application. Programmersor requirements engineers can’t apply formal methods efficiently without previouslyhaving specific training, which implies hiring expensive specialists in formal methods.This dissertation presents methods which aim to bring formal methods closer to requirementsdescriptions. For such, formal modeling is used to verify and validate the descriptions of requirements,and not source code. Initially it’s presented a standard to create formal models, whichmakes a direct correspondence between each requirement and its model. This standard is supportedby a tool which, among other things, automatically generates graphics representationsof requirements using its models. Afterwards it’s presented a connection between requirementsboilerplates and Alloy models. This connection allows to generate formal models in an automaticfashion, without the need of a specialist. This drastically reduces the costs of using formalmethods in software projects. It’s also presented the beginning of an algebra which allows toaggregate these templates. This aggregation allows one to write its requirements documentsthrought boilerplates and at the end have the complete model of all requirements, for free.When one is modeling a requirements document in Alloy and at some point appears requirementswith explicit temporal restrictions, it’s necessary to recreate the whole model in a tool whichallows that kind of specification (eg. Uppaal). This process is highly error prone, because it’s amanual transformation and highly dependent on the interpretation of who is modeling. In thisdissertation it’s presented a method which allows to automatically generate an Uppaal modelfrom an Alloy model. This transformation allows that at any point in the requirements document,the requirements engineer can generate the correspondent Uppaal model and there specify thetemporal properties.",
    "authors": [
      "Cadete, Daniel Nascimento"
    ],
    "keywords": [
      "681.3.062"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.062"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27790",
    "title": "Suporte multidimensional para sistemas de business intelligence",
    "abstract": "Os avanços tecnológicos e industriais, as constantes inovações e a necessidadede melhoria contínua ao longo dos últimos anos pelas instituições, têm proporcionadoum exponencial aumento na quantidade de informação gerada,e consequentemente armazenada por estas. As instituições de saúde, comoqualquer outra organização, geram uma grande quantidade de dados relativosaos seus processos, os quais, muitas das vezes, não são registados e geridosadequadamente, tornando muito difícil a sua posterior gestão e manipulação.Por outro lado, torna-se necessária a compreensão dos custos envolvidosna prestação dos diferentes cuidados de saúde por parte dos gestores hospitalares,para a melhoria da qualidade e e ciência dos diversos processosdiários neste tipo de instituições. Os Serviços de Informação para Gestão(SIG) hospitalares têm entre diversas responsabilidades, o registo de todosos procedimentos hospitalares relacionados com a área de gestão hospitalar,originando um grande volume de dados e informação, a qual necessita de serbem manipulada.Estes dados são de extrema importância em tomadas de decisão. Comotal, os mesmos precisam de sofrer um processo de modelação e organizaçãoatravés da utilização de sistemas projetados especi camente para esta fun-ção, como é o caso dos sistemas de Business Intelligence (BI). O conceitode BI emergiu nas instituições hospitalares como medida para solucionar oproblema existente no tratamento e processamento dos dados na área dasáude, transformando-os em informação e conhecimento útil para os pro ssionais.De uma forma geral, os sistemas de BI representam um conjunto detecnologias e aplicações, que atuam na recolha, análise e difusão dos dadosexistentes, funcionando como suporte para tomadas de decisão e cientes.O principal objetivo deste projeto prende-se, essencialmente, com o desenvolvimentode uma aplicação de suporte multidimensional para sistemasde BI a ser implementada no Centro Hospitalar do Porto, para uso exclusivopor pro ssionais pertencentes aos SIG da mesma instituição. Esta aplicaçãopermite a importação direta de folhas de Excel, que contém registos efetuadospor estes pro ssionais, para uma Base de Dados, alimentando diretamente a Data Warehouse (DW) e as Data Marts (DMs) existentes e a existir para estepropósito. Com isto, estes pro ssionais passam a ter total responsabilidadena manutenção e gestão de registos mantidos em folhas de Excel, o que nãoacontecia anteriormente, mantendo as componentes dimensional e factual daDW e DMs através de dados mantidos em folhas de cálculo.Veri cou-se que a aplicação desenvolvida, enquanto ferramenta de suportepara sistemas de BI, é inteiramente capaz de ser implementada e integradanas operações diárias da organização hospitalar, facilitando a gestão destesdados e o trabalho dos pro ssionais, proporcionando um aumento da uidez,rapidez, tratamento, recolha e análise da informação.",
    "authors": [
      "Gonçalves, Inês Cerqueira"
    ],
    "keywords": [
      "681.3:614.2",
      "614.2:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:614.2",
      "614.2:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81775",
    "title": "Progressive web development: a case study in obstetrics",
    "abstract": "In the past years, with the evolution of technology and the rise of the Internet,Personal Health Records appeared. These records are maintained by patientsand turn them into a more active stakeholder in their own health management.They can be used to record medical parameters or give useful information tothe patient, among others.However, this was not the only result from this evolution. With the Internet,more medical information became available to everyone, most of it not fromreliable sources, which brought additional problems.The obstetrics field was also impacted by these changes. This, combined withfact that a pregnancy is a delicate medical condition, brought the necessity tofind solutions for this case.This dissertation aims to develop a Personal Health Record for pregnant womenthat provides reliable information, and that has a set of features they find useful.With this goal in mind, a literature review on the technologies and methodolo gies that are used nowadays and on the use of technology by pregnant womenis made.Then, all the development process is presented, as well as the final result.This process was supervised by a medical institution, which had the advantageof facilitating the process of having feedback from pregnant women and it alsoprovided all the medical information displayed in the application.The final result is an application that has a wide range of useful features, pro vides trustworthy information, is available in all devices and that was developedusing modern technologies.",
    "authors": [
      "Loreto, Patrícia Alexandra Soares"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27876",
    "title": "Identificação biométrica e comportamental de utilizadores em cenários de intrusão",
    "abstract": "A usurpação de contas e o roubo de identidade são problemas muito frequentes nos atuaissistemas informáticos. A facilidade de acesso à internet e a exposição das pessoas a este meio,torna muito frequente a utilização indevida e a usurpação de contas (tais como: e-mail, redessociais, contas bancárias) por outras pessoas que não as suas legítimas proprietárias.Atualmente o método de autenticação dominante é o da combinação nome de utilizador epalavra-chave. No entanto, este método pode não ser fiável, pois estas credenciais podem serpartilhadas, roubadas ou até esquecidas. Por outro lado podem-se combinar várias técnicas parareforçar a segurança dos sistemas. Cartões de acesso (tokens), certificados digitais e biometrias sãoalgumas delas. Os cartões de acesso, por exemplo os das caixas multibanco, podem ser roubadosou duplicados, como é frequentemente noticiado em fraudes bancárias. Os certificados seguem omesmo caminho dos tokens uma vez que estes podem ser distribuídos por correio eletrónico ou emdispositivos USB. As biometrias físicas (impressão digital, íris, retina ou geometria da mão porexemplo), para além de serem um pouco intrusivas, requerem a aquisição de equipamento caro.Uma possível solução para os problemas inumerados são as biometrias comportamentais.A forma como nos comportamos e agimos num computador pode ser usada comoinformação biométrica. Esta informação pode ser utilizada à posteriori, geralmente complementadacom mais dados, para identificar, inequivocamente, (ou pelo menos com um determinado grau deconfiança) um indivíduo. A informação recolhida pode variar desde o tipo de escrita no teclado,habilidade com o rato, hábitos, cliques, número de páginas abertas, origem do acesso, etc., quedepois será sujeita à utilização de algoritmos comportamentais para autenticar, de formainequívoca, um utilizador.Neste trabalho pretende-se implementar como reforço aos atuais sistemas de autenticaçãoe de deteção de intrusões, a verificação de perfis comportamentais do proprietário da conta. Estesistema não irá apresentar grandes custos, já que só serão usados equipamentos básicos, e serácompletamente invisível para o utilizador, ou seja este será continuamente autenticado de formasilenciosa e não intrusiva.",
    "authors": [
      "Martins, Henrique Fontão"
    ],
    "keywords": [
      "681.3-7"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3-7"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79955",
    "title": "OntoReport: gerador de relatórios sobre ontologias",
    "abstract": "Uma ontologia pode ser definida como um modelo de dados, que representa uma descriçãode conceitos num determinado domínio. Esta tem como principal objetivo, aumentar a compreensão partilhada num determinado domínio, eliminando as diferenças, sobreposições eincompatibilidades em conceitos, estruturas, entre outros, criando assim uma especificaçãoformal legível por um computador, e explícita, no sentido em que as entidades da ontologiasão claramente definidas, distintas e inter-relacionadas entre si. Tendo em conta o aumento exponencial de dados presentes na WEB, ontologias têmsido cada vez mais usadas como modelo de armazenamento de dados. Este aumento deusabilidade leva a que seja necessário o desenvolvimento de ferramentas que nos permitam criar/editar/analisar ontologias, ou seja, que nos permitam não só a interagir com elas, mas também realizar um tratamento dos dados consequentemente recolhidos. Assim, pretende-se nesta dissertação, desenvolver uma aplicação web capaz de gerar, através de uma especificação, um relatório, referente a uma determinada ontologia acessívelatravés de um determinado endpoint. Por outras palavras, pretende-se a criação de uma aplicação que permita ao utilizador obter, da ontologia, apenas os dados que pretende, em vez de uma quantidade enorme de dados sem qualquer tratamento prévio.",
    "authors": [
      "Neves, Diogo Meira"
    ],
    "keywords": [
      "Aplicação web",
      "Ontologia",
      "Relatório de informação",
      "Web semântica",
      "Web application",
      "Ontology",
      "Information report",
      "Semantic web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84344",
    "title": "Framework para análise de comportamentos de objetos interativos em vídeo jogos",
    "abstract": "Com um crescimento exponencial tanto na área da Inteligência Artificial como dos vídeo jogos, acriação de plataformas que auxiliam os jogadores passou a ser fundamental. A criação de uma ferramentaanalítica que estude detalhadamente o comportamento humano, abre portas a jogos mais dinâmicos,competitivos e justos.A análise do ecrã de um jogador permite identificar, detetar e rastrear movimentos de determinadosobjetos, em tempo real, podendo ter o intuito de o ajudar ou de o vigiar. Seja qual for o caso, é necessário,primeiro, identificar e detetar os objetos visualizados, através de algoritmos de Object Detection. Depois,já identificado o objeto, é possível prever a sua próxima localização, bem como rastrear o seu movimento,utilizando algoritmos de Object Tracking.Intercalando o rastreamento com a deteção de objetos, quer quando este desaparece de vista, querpara obter confirmação que se está a seguir o objeto correto, é possível assim analisar o ecrã do jogadorpara o poder ajudar.Esta dissertação tem como objetivo desenvolver um modelo capaz de identificar o movimento de umdeterminado objeto, em tempo real, no ambiente de um jogo, utilizando para isso técnicas de MachineLearning e Computer Vision, mais especificamente métodos de Object Detection e Object Tracking.O ambiente prático foi desenvolvido utilizando a biblioteca OpenCV para Python, que tem ao disporum diverso leque de algoritmos de Computer Vision e ainda permite a utilização paralela de CPU e GPUpara a otimização destes mesmos algoritmos.",
    "authors": [
      "Cruz, Catarina Freitas da"
    ],
    "keywords": [
      "Inteligência artificial",
      "Machine learning",
      "Computer vision",
      "CNN",
      "Object detection",
      "Object tracking",
      "OpenCV",
      "Artificial Intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79848",
    "title": "Deeploy: a neural network computer vision tool (for the NVidia Tegra TX2 Embedded System)",
    "abstract": "Machine Learning (ML) gives a computer system the ability to perform a certain task without being explicitly programmed to do it. Although ML is not a new topic in the fieldof computer science, these techniques have been gaining increasing popularity due to advances in hardware (especially GPUs). More powerful hardware supports more efficienttraining and a more responsive end-system, once deployed. These algorithms have provento be particularly effective in image processing and feature detection, namely with deepneural networks.In the context of a vehicle, autonomous or not, perceiving its external and internal environment enables the ability to detect and identify left behind objects, its misuse or otherpotentially dangerous situations. This captured data is relevant to trigger vehicle intelligentresponses. Bosch is currently developing a system that has these capabilities and plans toleverage deep learning approaches to implement it.This work aimed to test and evaluate the suitability of a given embedded device forthe project. It also determined the best strategy to implement deep learning solutions inthe device. The supplied test bed was a NVidia Software Development Kit (SDK) systemfor the embedded NVidia Jetson TX2 device with the System-on-Chip (SOC) Parker, anheterogeneous computing chip with 2 Denver-cores (a NVidia implementation of ARM-64architecture), 4 CortexA57-cores (also ARM-64), 256 Pascal GPU-cores and support for up to6 video cameras. The SDK includes several software library packages, including for imageprocessing and ML.With the goal of fully exploiting the embedded device compute capabilities, this workstudied several inference frameworks, going as far as implementing an inference enginefrom scratch (named Deeploy) that produces inferences based on two libraries providedby NVidia: cuDNN and TensorRT. Deeploy was evaluated against well known and established frameworks, namely Tensorflow, PyTorch and Darknet, in terms of efficiency, resourcemanagement and overall ease of use, maintainability and flexibility. This work also exploited key performance related features available on the device, such as power modes,half-precision floating point computation and the implemented shared memory architecture between the GPU-cores and the CPU-cores.",
    "authors": [
      "Fernandes, João Pedro Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81090",
    "title": "RDMA mechanisms for columnar data in analytical environments",
    "abstract": "The amount of data in information systems is growing constantly and, as a consequence, thecomplexity of analytical processing is greater. There are several storage solutions to persistthis information, with different architectures targeting different use cases. For analyticalprocessing, storage solutions with a column-oriented format are particularly relevant dueto the convenient placement of the data in persistent storage and the closer mapping toin-memory processing.The access to the database is typically remote and has overhead associated, mainly whenit is necessary to obtain the same data multiple times. Thus, it is desirable to have a cacheon the processing side and there are solutions for this. The problem with the existing so lutions is the overhead introduced by network latency and memory-copy between logicallayers. Remote Direct Memory Access (RDMA) mechanisms have the potential to help min imize this overhead. Furthermore, this type of mechanism is indicated for large amounts ofdata because zero-copy has more impact as the data volume increases. One of the problemsassociated with RDMA mechanisms is the complexity of development. This complexity isinduced by its different development paradigm when compared to other network commu nication protocols, for example, TCP.Aiming to improve the efficiency of analytical processing, this dissertation presents a dis tributed cache that takes advantage of RDMA mechanisms to improve analytical processingperformance. The cache abstracts the intricacies of RDMA mechanisms and is developedas a middleware making it transparent to take advantage of this technology. Moreover, thistechnique could be used in other contexts where a distributed cache makes sense, such asa set of replicated web servers that access the same database.",
    "authors": [
      "Silva, José Miguel Ribeiro da"
    ],
    "keywords": [
      "RDMA",
      "Cache",
      "Analytical processing",
      "Columnar data",
      "Distributed systems",
      "Processamento analítico",
      "Dados colunares",
      "Sistemas distribuídos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83355",
    "title": "Analysis of the impact of remote work on portuguese software professionals during the COVID-19 pandemic",
    "abstract": "COVID-19, a devastating virus that has been more and more controversial, fickle and problematic worldwide. Thispandemic brought multiple changes and restrictions to the way we live, rather like a historical buoyed period beforeand after Christ, translating, nowadays, into before and after COVID-19.This pandemic forced the worldwide population to be subjected to some lockdown periods, and Portugal was notan exception, in which the population could only leave their home for exceptional and essential situations. Fromthose restrictions, a “new” way of work that has gained more and more popularity was born - the remote workor commonly known as work from home. Thus, it becomes pressing to investigate which are the impacts of thisprofound change to remote work, in multiple domains (personal, professional,...).In short, the primordial objective of this dissertation is to study the impact of the referred change to remote work,due to the COVID-19 pandemic, on software professionals in Portugal.Throughout this dissertation, all the objectives and study questions, along with the relevant state of the art on thetheme are exposed. The construction and propagation of the chosen method (survey), the results arising from thecollection and treatment of data originated from this survey, along with their analysis, conclusions, limitations andadvantages are also highlighted.In total, 176 valid answers were collected from software professionals from Portugal (mainly from Braga). After theperformed statistical analysis on the targeted population and focusing on the 10 elaborated research questions,one can conclude with certainty two major findings: (i) having worked in a remote regimen before the pandemicperiod has a strong relation with a higher frequency use of teleconference tools (Microsoft Teams, Skype, Zoom,...)after this period, and (ii) participants who do not feel safe about coming back to a fully on-site regimen are morelikely to prefer a fully remote regimen than the ones who feel safe and the latter group is more likely to prefer ahybrid regimen. Additionally, although not statistically significant and therefore without certainty, one can also implythat, p.e, (i) having dependants and someone’s support in their care could possibly negatively affect participants’work; (ii) having dependants could possibly show a relation to a preference for a mainly on-site hybrid regimen, and(iii) company employee dimension could show a relation to participants’ feel of support to maintain productivity.In conclusion, this project has a strong pioneer investigation profile in Portugal, with an applicability in the softwareengineering area.",
    "authors": [
      "Almeida, Ana João Dias de"
    ],
    "keywords": [
      "COVID-19",
      "Pandemic",
      "Remote work",
      "Software engineering",
      "Survey",
      "Pandemia",
      "Teletrabalho",
      "Engenharia de software",
      "Inquérito",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88577",
    "title": "Inteligência artificial aplicada à infraestrutura de carregamentos para veículos elétricos",
    "abstract": "A capacidade de resolução de problemas pela Inteligência Artificial encontra-se em constante ex pansão, pelo que a otimização do seu funcionamento em sociedade está dependente da capacidade deextração das aplicabilidades da tecnologia. Uma das áreas da Inteligência Artificial em que tem sido visíveluma evolução significativa é a de Machine Learning, cujos algoritmos têm revelado um crescente nível deespecialização na resolução de diversos problemas.Na presente dissertação, pretendeu-se construir um modelo capaz de auxiliar na recomendação deconfigurações ideais para novas estações de carga para veículos elétricos, com a assistência de modelosde Machine Learning. A revisão da literatura revelou uma extensiva análise sobre problemas de previsão naárea de Machine Learning, pelo que algoritmos tradicionais de Machine Learning e algoritmos da subáreade Deep Learning se demonstram adequados para a resolução do problema proposto nesta dissertação.O dataset empregue neste projeto, com dados referentes a Portugal, foi construído com a assistência dediversas API e, posteriormente ao seu tratamento, foram aplicados seis algoritmos de Machine Learning,com o intuito de treinar um modelo que conseguisse prever a utilização futura de postos de carga.De entre os algoritmos avaliados, o Random Forest Regressor e eXtreme Gradient Boosting são aque les que apresentam maior capacidade na resolução do problema em questão, com um MAE 6.6220 e6.6310, respetivamente. O modelo de Random Forest Regressor é aquele que melhor se adequou paraa previsão de utilização futura dos postos de carga, tendo sido utilizado para a construção do modelo derecomendação.",
    "authors": [
      "Silva, Rodolfo António Vieira da"
    ],
    "keywords": [
      "Machine learning",
      "Mobilidade elétrica",
      "Problemas de previsão",
      "Séries temporais",
      "Electric mobility",
      "Forecasting problems",
      "Time series",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27846",
    "title": "Sensorização, fusão sensorial e dispositivos móveis : contribuições para a sustentabilidade de ambientes inteligentes",
    "abstract": "A sustentabilidade está dependente das decisões que o ser humano toma no ambiente em que se envolve. Por outro lado, é necessário ter consciência sobre o impacto das suas ações no meio ambiente. O crescimento da tecnologia e da área científica de sistemas inteligentes tem sido cada vez maior, tornando-se parceiras do ser humano, e o seu potencial para Ambientes Inteligentes e sustentabilidade tem sido evidenciado nos últimos tempos. Para enriquecer a resposta do Ambiente Inteligente aos seus utilizadores poder-se-á recorrer a sensores dispostos no ambiente e à fusão sensorial e de informação. As recomendações e previsões produzidas têm como objetivo a avaliação e o estudo da sustentabilidade do ambiente, nomeadamente, da utilização equilibrada da energia.Dentro dos contextos de sensibilização, de prevenção das ações do utilizador e da sustentabilidade do ambiente em que o ser humano esteja inserido, existem outros objetivos a alcançar, nomeadamente, a fusão de informação como ferramenta na utilização em suporte tecnológicos. A necessidade de aliar este processamento de dados/informação ao desenvolvimento de um conjunto de plataformas que permitam ao utilizador perceber efeitos negativos ou positivos que as condições analisadas têm, passando por possíveis recomendações ao utilizador. Esta plataforma deliberativa e reativa apoiará, processos e práticas de consciencialização para a sustentabilidade, por forma a conseguir mudanças nos padrões de estilo de vida, de produção e consumo de energia.O presente trabalho incide sobre a integração das Tecnologias da Informação e Comunicação (TIC) no meio envolvente, com estratégias de sustentabilidade dentro da dimensão social, ambiental e económica. As TIC associadas a conceitos de inteligência ambiente e elementos físicos, como por exemplo, edifícios, permitem obter formas de melhorar aspectos como o consumo energético e o impacto ambiental, na medida em que podem gerir de forma eficiente o consumo de recursos e contribuir para a redução de desperdícios. Um exemplo de aplicação pode ser encontrado na plataforma de Agentes Inteligentes, denominada por PHESS, que com ligação a sensores que permite centralizar a recolha de dados e obter decisões através de processos deliberativos automaticamente. Esta dissertação foca-se na extensão da plataforma PHESS com processos de fusão sensorial e de informação e, ainda, na criação e da monitorização dos novos indicadores que permitem promover a sustentabilidade social, económica e ambiental proporcionando aos utilizadores novas possibilidades de acesso a serviços e de participação na comunidade. A utilização de sistemas inteligentes, auxiliam na ação sobre o meio e são, por isso, uma mais-valia para o conforto das pessoas e para a sustentabilidade do ambiente.",
    "authors": [
      "Rosa, Luís Miguel Ferreira"
    ],
    "keywords": [
      "Sustentabilidade",
      "Sensores",
      "Fusão de dados",
      "Fusão de informação",
      "Ambiente inteligente",
      "Inteligência ambiente",
      "Sustainability",
      "Sensors",
      "Data fusion",
      "Information fusion",
      "Ambient intelligence",
      "681.3:504",
      "504:681.3",
      "681.586",
      "621.39"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:504",
      "504:681.3",
      "681.586",
      "621.39"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56115",
    "title": "A metagenomic approach to identify and characterize wastewater populations",
    "abstract": "Water scarcity and pollution are two main ecological focus nowadays. Knowledge of wastewater composition, regarding microorganisms and pollutants, is of great importance to improve the capacities of the effluent treatment plants (ETP). Advances in Next-generation sequencing (NGS) methodologies allowed for faster, cheaper and more accurate study of microbial communities. Besides being an extremely powerful analysis resource, whole shotgun metagenomic analysis comprises many challenging aspects, regarding the processing and analysis.In the present work a shotgun metagenomic bioinformatics analysis was performed comprising three samples from common ETPs (CETP) and four samples from a petrochemical complex ETPs (wastewaters with low and high salts collected in two distinct timepoints). The samples were sequenced with Illumina® HiSeq, generating paired-end reads with 2x150bp length. The main goals of this project were to evaluate currently available tools, establish a customized bioinformatics pipeline and to extract relevant biological information from the sequenced datasets.There were generated simulated datasets representative of the target data, in order to evaluate the performance of the available bioinformatics tools. Datasets were generated with three coverage levels and were used to test pre-processing, assembly and taxonomic tools. The target datasets, both with and without coverage split, were then subjected to processing and analysis using the pre-defined pipeline. A preliminary functional study was also performed using MG-RAST and MGX.Results from the evaluation of the performance of the bioinformatics tools showed that different tools behave differently in distinct datasets. The pipeline was defined using BayesHammer and Fastq-mcf as pre-processing tools, SPAdes for assembly and MetaPhlAn v2.0 for the taxonomical analysis.The assembly results for the target datasets showed a higher contiguity for high coverage levels and a lower contiguity for low coverage levels, highlighting the differences in microorganisms’ abundance and diversity and its impact during analysis.Taxonomical composition suggests the presence of putative pathogenic and opportunistic microorganisms on two of the CETP datasets (A2 and AKR12). It also suggests a more hostile environment in petrochemical complex ETPs datasets, which is concordant with a higher abundance of defence mechanisms on this datasets.The present results must be accounted to the effluent treatment processes.",
    "authors": [
      "Martins, Sara Patrícia Monteiro"
    ],
    "keywords": [
      "Whole shotgun metagenomic",
      "Next-generation sequencing",
      "Effluent-treatment plant",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60040",
    "title": "Como compatibilizar paradigmas estruturados da gestão de projetos com metodologias ágeis",
    "abstract": "As organizações dispõem atualmente de standards e de guias de boas práticas de gestão de projetos bem estruturados e com ampla adoção. Tal é o caso, por exemplo, da ISO 21500 e do PMBoK 5. Não obstante, no contexto dos projetos de tecnologias e sistemas de informação realizados com recurso a metodologias ágeis, surgem novos desafios relacionados com a compatibilização das práticas ágeis com os paradigmas estruturados da gestão de projetos. A presente dissertação está focada em procurar respostas para a questão de investigação “Como compatibilizar paradigmas estruturados da gestão de projetos com metodologias ágeis”, através de uma abordagem metodológica baseada na Design Science Research (DSR). Como resultado do trabalho realizado, apresenta-se um novo método de gestão de projetos desenvolvido com base no PMBoK e no Scrum. Este método faz a conciliação das metodologias de gestão de projetos estruturadas com metodologias ágeis.",
    "authors": [
      "Rocha, Patrícia Sofia Duarte"
    ],
    "keywords": [
      "Gestão de projetos",
      "Método",
      "Metodologia",
      "Desenvolvimento de software",
      "Ágil",
      "PMBoK",
      "Scrum",
      "Software project management",
      "Method",
      "Methodology",
      "Software development",
      "Agile",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91575",
    "title": "Optimization under uncertainty for forest fire containment",
    "abstract": "Forest fires are a major problem that affects the entire world, causing tragic loss of life and serious injuries, which have been worsening due to global warming, making it essential to minimize the serious consequences of these phenomena. In this sense, this project addresses the problem of positioning resources to combat forest fires. As uncertainty is an important aspect in fire propagation modeling, stochastic approaches are used, such as the Equivalent Deterministic Model and the Sample Average Approximation. The purpose of these approaches is to determine the best locations to deploy a limited number of combat assets, for example fire crews. Another important point is to study how fire spreads in a forest given the region's topography, wind and other factors to incorporate fire propagation modeling with the management and planning of fire prevention and firefighting resources (optimization). Although there are several fire propagation simulation software, their integration with optimization problems is still very limited. In this work, this integration is achieved through theminimum travel time (MTT) principle that, when representing the forest by a network in which the transmission times between adjacent homogeneous forest zones are known, states the fire takes the quickest paths. This principle is used in mixed integer programming models to optimize the positioning of the available resources, both in a deterministic and in a stochastic setting. Computational experiments are conducted to validate the approach.",
    "authors": [
      "Neto, David António Vieira dos Santos Moura"
    ],
    "keywords": [
      "Forest fires",
      "Fire propagation modeling",
      "Planning of fire prevention and firefighting resources",
      "Optimization",
      "Incêndios florestais",
      "Modelação da propagação do fogo",
      "Gestão e planeamento dos recursos de prevenção e combate a incêndios",
      "Otimização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80044",
    "title": "Spatial normalization and analysis of brain MRI studies – a deep neural network based approach",
    "abstract": "Throughout the years, Deep Learning has proven to be an excellent technology to solve problems that would otherwise be too complex. Furthermore, it has shown great success in the area of medical imaging, especially when applied to segmentation of brain tissues. As such, this dissertation explores a possible new approach, using Deep Artificial Neural Networks to perform spatial normalization on brain MRI studies as well as classify using Brain MRI studies regarding their state of brain atrophy. Spatial normalization of Magnetic Resonance images by tools like the FSL, or SPM turned out to be inefficient for researches as they need too many resources to achieve good results. Theseresources include, for example, wasted human and computer time when executing the commands to normalize and waiting for the process to finish, this can take up to several hours just for one study. Therefore, a new approach was needed, a faster and easier way to normalize the MRI studies. To do so, Deep Artificial Neural Networks were used by creating a python program to deal with said studiesin much less time. This program should free the researchers’ time for other more relevant tasks and help reach conclusions faster in their studies when trying to find patterns between the analysed brains. Several architectures were tried, having better results with U-Net based architecture as well as GAN architecture.At the end, the model couldn’t learn correctly all the brain features to be changed in any of the approaches but showed great potential. Even though the final model did achieve the correct shape it could not yet achieve the final normalization.With some more time invested in perfecting the models, these could, in the future, learn to correctly perform the final normalization and allow the researchers to perform it in less than 10 seconds per exam instead of hours.Regarding the Brain Atrophy models, the models showed some potential too as the predictions were partially correct. With more data, and less unbalanced, the model could probably learn correctly and output the expected results for all classes.",
    "authors": [
      "Jesus, Tiago Rafael Andrade"
    ],
    "keywords": [
      "Brain",
      "Deep learning",
      "MRI",
      "Neuroimaging",
      "Spatial normalization",
      "Cérebro",
      "Neuroimagem",
      "Normalização espacial",
      "Redes neurais artificiais",
      "Ressonância magnética",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47578",
    "title": "Estimation of the glottal flow from the speech or singing voice",
    "abstract": "O processo de produção humana de voz é, resumidamente, o resultado da convolução entre o sinal deexcitação, o impulso glótico, e a resposta impulsiva resultante da função de transferência do tratovocal. Este modelo de produção de voz é frequentemente referido na literatura como um modelo fontefiltro,em que a fonte representa o fluxo de ar que sai dos pulmões e passa pela glote (espaço entre aspregas vocais), e o filtro retrata as ressonâncias do trato vocal e a radiação labial/nasal.Estimar a forma do impulso glótico a partir do sinal de voz é de importância significativa em diversasáreas e aplicações, uma vez que as características de voz relacionadas, por exemplo, com a qualidadeda voz, esforço vocal e distúrbios da voz, devem-se, principalmente, ao fluxo glotal. No entanto, estefluxo é um sinal difícil de determinar de forma direta e não invasiva.Ao longo das últimas décadas foram desenvolvidos vários métodos para estimar o impulso glótico massem o desenvolvimento de um algoritmo eficiente e automático. A maioria dos métodos desenvolvidosbaseia-se num processo designado por filtragem inversa. A filtragem inversa representa adesconvolução, ou seja, procura obter o sinal de entrada aplicando o inverso da função detransferência do trato vocal ao sinal de saída. Apesar da simplicidade do conceito, o processo defiltragem inversa não é simples uma vez que o sinal de saída pode incluir ruído e não é alcançávelmodelar com precisão as características do filtro do trato vocal.Nesta dissertação apresentamos um novo método de filtragem de um sinal de modo a melhorar ummétodo robusto de estimação da fonte glótica, no domínio das frequências, que usa uma característicade fase baseada nos Atrasos Relativos Normalizados (NRD) dos harmónicos. Este modelo é aplicado adiversos sinais de voz (sintéticos e reais), e os resultados obtidos da estimação do impulso glótico sãocomparados com os obtidos usando outros métodos analisados no estado da arte com e sem oreferido método de filtragem.",
    "authors": [
      "Beleza, Hugo Miguel Ferreira"
    ],
    "keywords": [
      "Impulso glótico",
      "Estimação do impulso glótico",
      "Filtragem inversa",
      "Integração no domínio das frequências",
      "Estimação do impulso glótico no domínio das frequências",
      "Glottal pulse",
      "Estimation of the glottal pulse",
      "Filter",
      "Algorithm",
      "Frequency domain glottal source estimation",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56079",
    "title": "Avaliação dos determinantes internos e externos no sucesso de start-ups de software: o caso do cluster de Braga",
    "abstract": "Nos últimos anos tem-se verificado um crescimento tecnológico, empreendedor e de inovação no clusterde Braga. Dadas estas circunstâncias, tem-se verificado um aumento na oferta e procura de produtose serviços de software, o que incentiva o surgimento de novas start-ups. Contudo, devido à elevadacompetitividade do mercado tecnológico e à atual crise económica mundial, nem todas estas empresassobrevivem nos seus primeiros anos de vida.Neste projeto de investigação procurou-se compreender como as start-ups entram no mercado e oque distingue as que vingam no mercado das restantes, através da avaliação dos seus determinantesinternos e externos. Deste modo, será possível contribuir para a compreensão das condições em queos empreendedores devem construir as suas start-ups, aumentando as possibilidades de sucesso naconstrução de produtos e serviços de software com viabilidade de mercado.Nesta dissertação foi explorado o Early-life Decision Model, um modelo composto por diversos tiposde decisão que devem ser tomados pelos empreendedores para a sustentabilidade do negócio. Atravésdo modelo, procurou-se analisar e avaliar os determinantes internos e externos de diversas vertentes donegócio das start-ups. A metodologia de investigação adotada consistiu na entrevista semi-estruturada,por apresentar características mais apropriadas para a finalidade deste estudo.Foram preparadas e realizadas entrevistas em 15 empresas do cluster de Braga, foi efetuada uma análisee tratamento dos dados recolhidos e, posteriormente, foi feita uma análise dos resultados. Obtiveram-sediversas conclusões como o impacto de cada determinante nos grupos de empresas identificados, bemcomo a quantidade de determinantes internos e externos identificados nas diversas vertentes do negócio.Também foi possível verificar que a existência de determinantes que dificultam a entrada das empresasno mercado, influencia o rumo do negócio, e que a aprendizagem adquirida deles e da experiência vividadurante a atividade da empresa, é essencial para a sustentabilidade do negócio.",
    "authors": [
      "Cruz, Isabel Maria Ferreira"
    ],
    "keywords": [
      "Start-up",
      "Braga",
      "Determinante",
      "Modelação da empresa",
      "Desenvolvimento do software",
      "Estabelecimento do mercado",
      "Internacionalização",
      "Metodologia lean startup",
      "Aprendizagem",
      "Determinant",
      "Shaping the company",
      "Developing the software",
      "Establishing the market",
      "Going international",
      "Lean startup methodology",
      "Learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86188",
    "title": "HTM approach to image classification, sound recognition and time series forecasting",
    "abstract": "The introduction of Machine Learning (ML) on the orbit of the resolution of problemstypically associated within the human behaviour has brought great expectations tothe future. In fact, the possible development of machines capable of learning, in asimilar way as of the humans, could bring grand perspectives to diverse areas likehealthcare, the banking sector, retail, and any other area in which we could avoid theconstant attention of a person dedicated to the solving of a problem; furthermore, thereare those problems that are still not at the hands of humans to solve - these are nowat the disposal of intelligent machines, bringing new possibilities to the humankinddevelopment.ML algorithms, specifically Deep Learning (DL) methods, lack a bigger acceptance bypart of the community, even though they are present in various systems in our dailybasis. This lack of confidence, mandatory to let systems make big, important decisionswith great impact in the everyday life is due to the difficulty on understanding thelearning mechanisms and previsions that result by the same - some algorithms representthemselves as ”black boxes”, translating an input into an output, while not being totallytransparent to the outside. Another complication rises, when it is taken into accountthat the same algorithms are trained to a specific task and in accordance to the trainingcases found on their development, being more susceptible to error in a real environment- one can argue that they do not constitute a true Artificial Intelligence (AI).Following this line of thought, this dissertation aims at studying a new theory,Hierarchical Temporal Memory (HTM), that can be placed in the area of MachineIntelligence (MI), an area that studies the capacity of how the software systems canlearn, in an identical way to the learning of a human being. The HTM is still a freshtheory, that lays on the present perception of the functioning of the human neocortexand assumes itself as under constant development; at the moment, the theory dictatesthat the neocortex zones are organized in an hierarchical structure, being a memorysystem, capable of recognizing spatial and temporal patterns. In the course of thisproject, an analysis was made to the functioning of the theory and its applicabilityto the various tasks typically solved with ML algorithms, like image classification, sound recognition and time series forecasting. At the end of this dissertation, after theevaluation of the different results obtained in various approaches, it was possible toconclude that even though these results were positive, the theory still needs to mature,not only in its theoretical basis but also in the development of libraries and frameworksof software, to capture the attention of the AI community.",
    "authors": [
      "Lima, Tiago Azevedo"
    ],
    "keywords": [
      "Hierarchical temporal memory",
      "Machine intelligence",
      "Neocortex",
      "Hebbian learning",
      "Image classification",
      "Sound recognition",
      "Time series forecasting",
      "Artificial Intelligence",
      "Aprendizagem Hebbiana",
      "Classificação de imagem",
      "Reconhecimento de som",
      "Previsão de séries temporais",
      "Inteligência Artificial",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47728",
    "title": "Gestão do produto de software na região de Braga: análise e avaliação",
    "abstract": "A Gestão do Produto de Software é um dos principais mecanismos a usar durante o desenvolvimento e a exploração de um produto de software. A forma como o produto é gerido durante o seu ciclo de vida, toma em conta várias áreas de negócio, que, infelizmente, nem sempre fazem parte da experiência do gestor de produto, devido também ao facto de ainda existir pouca formação para o exercício deste cargo. Como forma de avaliar as práticas de Gestão do Produto de Software na região de Braga, é apresentada a Matriz de Maturidade para a Gestão do Produto de Software, como instrumento de avaliação das práticas implementadas pelas empresas. Uma análise individual e coletiva das empresas é realizada, com a finalidade de analisar a maturidade das empresas em cada uma das áreas de foco da Matriz. Para tal, foram realizadas 13 entrevistas junto de colaboradores de empresas do ramo de software, que permitiram recolher dados essenciais na construção das matrizes de maturidade de cada uma destas empresas. A partir dos resultados obtidos destas matrizes, uma análise mais geral foi realizada, identificando pontos comuns de falha, bem como de áreas de foco nas quais as empresas direcionam mais os seus esforços. Foi possível concluir que apesar de existir uma grande diversidade de resultados dentro de cada uma das Áreas de Foco, bem como o facto de que por vezes o nível de maturidade atingido, não está diretamente relacionado com o número de competências implementadas em cada uma desta Áreas. Também se constatou que o uso de ferramentas e metodologias adotadas por estas empresas, facilitam a Gestão do Produto, o que se traduz no aumento do nível de maturidade para as empresas em determinadas áreas de negócio.",
    "authors": [
      "Carvalho, Frederico António de Sousa"
    ],
    "keywords": [
      "Gestão do produto de software",
      "Matriz de maturidade",
      "Análise e avaliação",
      "Área de foco",
      "Área de negócio",
      "Software product management",
      "Maturity matrix",
      "Analysis and evaluation",
      "Focus area",
      "Business functions",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66046",
    "title": "Monitorização e otimização da eficiência num laboratório de análises clínicas",
    "abstract": "O presente trabalho foi efetuado para obtenção do grau de Mestre em Engenharia de Redes e Serviços Telemáticos na Universidade do Minho e teve como primordial objetivo proceder ao desenvolvimento de um sistema de monitorização e otimização da eficiência num laboratório de análises clínicas.Para isso, começou-se pelo estudo do estado da arte com intuito de tomar conhecimento das áreas em que se vai trabalhar bem como saber se já existem soluções semelhantes existentes. Neste último ponto, não se encontrou nenhuma solução existente.Por consequência, foi necessário o desenvolvimento de um dispositivo autónomo, que, combinando hardware e software recolha a informação enviada pelos equipamentos da Siemens instalados no laboratório.Paralelamente houve a necessidade de implementar um sistema que receba e armazene em base de dados estruturada a informação enviada pelo dispositivo desenvolvido.Por fim, procedeu-se ao desenvolvimento de um dashboard para apresentação do resultado do tratamento dos dados armazenados (informação de business intelligence), relevante para a gestão e otimização do processo analítico.",
    "authors": [
      "Teixeira, Bruno Filipe Martins"
    ],
    "keywords": [
      "sistemas de monitorização",
      "indicadores",
      "otimização",
      "monitoring system",
      "indicators",
      "optimization",
      "dashboards"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80111",
    "title": "Avoiding question-answering congestion on health services using chatbots",
    "abstract": "The proliferation of social networks presents a significant amount of fake news and fake information every day and every second. The COVID-19 pandemic confirms this situation. The general ignorance of this disease causes the spreading of misleading information, harming people's lives and governments' actions to contain it. To fight this infodemic, the populations resorted to the health services' phone lines, congesting them with questions, most of them repeated among different individuals and locations. A chatbot for COVID-19- related questions would redirect this workload from the health services, mitigating such congestion. This chatbot should work for both the English and Portuguese languages. This work provides a background overview about web crawlers, information processing and chatbot development, which are the three components of the application. A systematic literature review was done to provide an analysis of the existing literature on the mentioned thematics. The application presented in this work consists of three main modules: a web crawler, using the ACHE crawler application, which downloads the web pages from the trustworthy sources; a text processor, that parses the web pages and indexes them according to their language to the respective ElasticSearch index; and a chatbot component, composed by a fine-tuned BERT model with the SQuAD 2.0 dataset and a web interface that queries the ElasticSearch indexes for the most relevant pages and extracts the answers to the given questions by the users. To comply with the English and Portuguese requirement, two sets of reliable sources were defined (one for each language) and a translated version of SQuAD 1.1 dataset was used to train the Portuguese BERT model. The chatbot queries the correct model using the web browser's defined language. Our system was evaluated using a set of COVID-19 QA pairs extracted from the United Nations website, and the obtained results are described in this work. These were far from the desirable outcomes, so some improvements were applied to the crawler and to the ElasticSearch indexes. However the results were still not satisfactory, requiring a set of future modifications that are presented in this work.",
    "authors": [
      "Pereira, Henrique Manuel Palmeira"
    ],
    "keywords": [
      "Chatbot",
      "Information processing",
      "Natural language processing",
      "COVID-19",
      "Processamento da informação",
      "Processamento de linguagem natural",
      "web crawling"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80230",
    "title": "Monitoring and real-time simulation of an industrial production pipeline",
    "abstract": "There is a shortage of manufacturing management software solutions for businesses with various manual processes, and that offer a wide range of products. Existing solutions can become very expensive for small and medium-sized enterprises, and can discourage them to take the next step towards the 4th Industrial Revolution. This dissertation consists of joint work with Tipoprado, Artes Gráficas, to develop a package tracking and production performance analysis platform. The company has a notable number of different clients and offers different types of services. This way, different packages may go through different paths on the production pipeline. Given this, to offer a more close and customized service, Tipoprado, wants to develop a package tracking platform. This tracking is not geographical (delivery case), but about the package location over the production pipeline, giving clients the possibility to consult, in real-time, the actual state of their orders. Apart from this, implementing this platform produces a significant level of data about packages and clients. One of the main goals is to treat, process and analyze this data, to improve production efficiency and be able to help the its managers make crucial decisions about the referred pipeline. Production planning and predictions on delivery dates is the ultimate goal. This dissertation studies and implements the tracking method that best applies to Tipoprado production pipeline, together with data analysis, and prediction options. The given platform will transport the company to a tech production vision, and kick start its journey through the fourth industrial revolution. It is also expected to increase customer engagement levels, which correlate with a higher number of sales.",
    "authors": [
      "Viana, José Diogo Lago"
    ],
    "keywords": [
      "Web Application",
      "Web Development Architecture",
      "Queuing Theory and Simulations",
      "Production line",
      "Aplicação Web",
      "Arquiteturas de desenvolvimento Web",
      "Teoria de Filas e Simulações",
      "Linha de Produção",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83491",
    "title": "Spatio-temporal action localization with Deep Learning",
    "abstract": "The system that detects and identifies human activities are named human action recognition. On the video approach, human activity is classified into four different categories, depending on the complexity of the steps and the number of body parts involved in the action, namely gestures, actions, interactions, and activities, which is challenging for video Human action recognition to capture valuable and discriminative features because of the human body’s variations. So, deep learning techniques have provided practical applications in multiple fields of signal processing, usually surpassing traditional signal processing on a large scale. Recently, several applications, namely surveillance, human-computer interaction, and video recovery based on its content, have studied violence’s detection and recognition. In recent years there has been a rapid growth in the production and consumption of a wide variety of video data due to the popularization of high quality and relatively low-price video devices. Smartphones and digital cameras contributed a lot to this factor. At the same time, there are about 300 hours of video data updates every minute on YouTube. Along with the growing production of video data, new technologies such as video captioning, answering video surveys, and video-based activity/event detection are emerging every day. From the video input data, the detection of human activity indicates which activity is contained in the video and locates the regions in the video where the activity occurs.This dissertation has conducted an experiment to identify and detect violence with spatial action localization, adapting a public dataset for effect. The idea was used an annotated dataset of general action recognition and adapted only for violence detection.",
    "authors": [
      "Monteiro, Carlos Filipe Batista Cardoso"
    ],
    "keywords": [
      "Action localization",
      "Action recognition",
      "Spatio-temporal",
      "Violence detection",
      "Localização de ações",
      "Reconhecimento de ação",
      "Espácio-temporal",
      "Deteção de violência",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66581",
    "title": "Prometheus: a generic e-commerce crawler for the study of business markets and other e-commerce problems",
    "abstract": "The continuous social and economic development has led over time to an increase in consumption,as well as greater demand from the consumer for better and cheaper products.Hence, the selling price of a product assumes a fundamental role in the purchase decisionby the consumer. In this context, online stores must carefully analyse and define the bestprice for each product, based on several factors such as production/acquisition cost, positioningof the product (e.g. anchor product) and the competition companies strategy. Thework done by market analysts changed drastically over the last years.As the number of Web sites increases exponentially, the number of E-commerce websites also prosperous. Web page classification becomes more important in fields like Webmining and information retrieval. The traditional classifiers are usually hand-crafted andnon-adaptive, that makes them inappropriate to use in a broader context. We introduce anensemble of methods and the posterior study of its results to create a more generic andmodular crawler and scraper for detection and information extraction on E-commerce webpages. The collected information may then be processed and used in the pricing decision.This framework goes by the name Prometheus and has the goal of extracting knowledgefrom E-commerce Web sites.The process requires crawling an online store and gathering product pages. This impliesthat given a web page the framework must be able to determine if it is a product page.In order to achieve this we classify the pages in three categories: catalogue, product and”spam”. The page classification stage was addressed based on the html text as well as onthe visual layout, featuring both traditional methods and Deep Learning approaches.Once a set of product pages has been identified we proceed to the extraction of the pricinginformation. This is not a trivial task due to the disparity of approaches to create a webpage. Furthermore, most product pages are dynamic in the sense that they are truly a pagefor a family of related products. For instance, when visiting a shoe store, for a particularmodel there are probably a number of sizes and colours available. Such a model may bedisplayed in a single dynamic web page making it necessary for our framework to exploreall the relevant combinations. This process is called scraping and is the last stage of thePrometheus framework.",
    "authors": [
      "Dias, João Tiago Pereira"
    ],
    "keywords": [
      "E-commerce",
      "Web mining",
      "Web page classification",
      "Machine learning",
      "Crawler",
      "Scraper",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27815",
    "title": "Desenvolvimento de um sistema de localização baseado em tecnologia RFID",
    "abstract": "Atualmente existem várias soluções comerciais que utilizam a tecnologia RFID para diversos fins aplicacionais. Na literatura estão também documentados várias abordagens e algoritmos para localização de dispositivos RFID. Partindo deste contexto, o objetivo principal da presente dissertação era modelar e conceber uma solução global de monitorização/localização baseada na tecnologia RFID (pontos de acesso e etiquetas), que fosse adequada para vários tipos de organismos/empresas. Ou seja, nunca foi intenção desta dissertação propor novos algoritmos de localização por RFID.Seguindo uma metodologia de desenvolvimento de software orientada aos modelos, e apoiada na tecnologia UML, o desenvolvimento do sistema percorreu as seguintes fases: (i) levantamento e documentação de requisitos, baseados na identificação das necessidades de três tipos de organismos/empresas e utilizando o modelo de Volere, (ii) identificação dos utilizadores do sistema, (iii) conceção do sistema, tarefa que incluiu a elaboração de casos de uso detalhados, diagramas de sequência ao nível do sistema, o modelo de dados persistentes do sistema, diagrama de classes e diagramas de sequência ao nível da implementação, (iv) implementação de um protótipo em Java e (iv) realização de testes com o protótipo.Muito mais do que um algoritmo, ou conjunto de algoritmos, de localização, o sistema desenvolvido é uma solução integrada de localização baseada na tecnologia RFID. A estratégia seguida no desenvolvimento do sistema assenta em 3 princípios: (1) uma dada empresa ou organização deseja monitorizar pessoas e bens, de forma impedir/permitir o acesso a determinadas zonas do espaço físico do edifício dessa empresa, (2) uma das tecnologias mais versátil para este atingir este fim é a RFID, (3) combinando a potência do sinal RF, enviado pela mesma etiqueta RFID, e recebido em pelo menos três pontos de acesso, é possível obter uma estimativa da localização do emissor desse sinal RF. Deste modo, o sistema desenvolvido pressupõe que existem três pontos de acesso (conjunto de APs) em cada local de acesso a zonas críticas do edifício, geometricamente localizados de forma a facilitar a trilateração de medições do sinal RF. Entre as funcionalidades do sistema de localização desenvolvido incluem-se: acesso ao sistema controlado, gestão de utilizadores do sistema, gestão de etiquetas RFID (atribuição, recolha, programação), definição da geometria de cada piso do edifício (usando os conceitos de ponto, segmento e zona), definição das zonas permitidas e interditas a cada tipo de utilizador, gestão dos pontos de acesso e dos conjuntos de pontos de acesso,localização de etiquetas RFID por trilateração e emissão de alertas, para uma ou mais entidades de segurança, nos casos em que ocorrer a “invasão” de uma zona interdita.Em termos de resultados, pode dizer-se que se concebeu um protótipo funcional que cumpre a maioria dos requisitos propostos, possui uma interface fácil de utilizar e é suficientemente genérico para poder ser usado por empresas de tipos diferentes. Dado que o código e os modelos estão perfeitamente sincronizados, alterar ou adicionar funcionalidades ao sistema é relativamente seguro e com um custo moderado. Em termos da qualidade das estimativas da localização, os resultados não são muito positivos. Duas alternativas para melhorar este aspeto passariam por (i) melhorar o algoritmo de localização e (ii) usar um tipo de hardware RFID mais sofisticado e vocacionado para ser aplicado num sistema de localização.",
    "authors": [
      "Castro, Mauro Nuno Barbosa de"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92611",
    "title": "Geração automática de código Oracle Retail: uma solução baseada em templates e Django",
    "abstract": "O Oracle Retail oferece um conjunto de aplicações de software que ajuda os retalhistas a gerir os seusnegócios, incluindo ponto de venda, gestão de inventário, gestão de relações com clientes e gestão dacadeia de fornecimento.No entanto há necessidade de adaptar a Solução Oracle Retail à realidade do cliente, isto é, fazeralgumas personalizações à Solução. Estas necessidades dos clientes são várias vezes idênticas, sendo ovolume de alterações por vezes elevado e repetitivo, mudando pequenas especificidades de cliente paracliente. Isto, por sua vez, resulta numa perda de tempo valioso que poderia ser alocado a tarefas maisprodutivas.Combinando esta realidade de trabalhos semelhantes com a pressão do mercado onde a disponibilidade e continuidade de recursos é cada vez mais complexa, as organizações devem procurar formas deautomatizar estes processos e beneficiar da redução do tempo gasto tanto no desenvolvimento como naresolução de problemas.À luz destes desafios, o desenvolvimento de uma aplicação destinada a gerar código Oracle Retailpersonalizado é uma solução promissora para responder às necessidades dos clientes na adaptação dasolução Oracle Retail aos seus requisitos.",
    "authors": [
      "Veloso, José Pedro Fernandes"
    ],
    "keywords": [
      "Oracle retail",
      "Django",
      "Gerador de código",
      "Templates",
      "Gerador de código com templates",
      "Code generator",
      "Code generator with templates",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79396",
    "title": "SafeSpark: a secure data analytics platform using cryptographic techniques and trusted hardware",
    "abstract": "Nowadays, most companies resort to data analytics frameworks to extract value from theincreasing amounts of digital information. These systems give substantial competitive ad vantages to companies since they allow to support situations such as possible marketingdecisions or predict user behaviors.Therefore, organizations tend to leverage the cloud to store and perform analytics overthe data. Database services in the cloud present significant advantages as a high levelof efficiency and flexibility, and the reduction of costs inherent to the maintenance andmanagement of private infrastructures. The problem is that these services are often a targetfor malicious attacks, which means that sensitive and private personal information can becompromised.The current secure analytical processing solutions use a limited set of cryptographictechniques or technologies, which makes it impossible to explore different trade-offs ofperformance, security, and functionality requirements for different applications. Moreover,these systems also do not explore the combination of multiple cryptographic techniquesand trusted hardware to protect sensitive data.The work presented here addresses this challenge, by using cryptographic schemes andthe Intel SGX technology to protect confidential information, ensuring a practical solutionwhich can be adapted to applications with different requirements. In detail, this dissertationbegins by exposing a baseline study about cryptographic schemes and the Intel SGX tech nology, followed by the state-of-the-art revision about secure data analytics frameworks.A new solution based on the Apache Spark framework, called SafeSpark, is proposed. Itprovides a modular and extensible architecture and prototype, which allows protecting in formation and processing analytical queries over encrypted data, using three cryptographicschemes and the SGX technology. We validated the prototype with an experimental evalu ation, where we analyze the performance costs of the solution and also its resource usage.For this purpose, we use the TPC-DS benchmark to evaluate the proposed solution, andthe results show that it is possible to perform analytical processing on protected data witha performance impact between 1.13x and 4.1x.",
    "authors": [
      "Carvalho, Hugo Alves"
    ],
    "keywords": [
      "Apache Spark",
      "Cryptographic Schemes",
      "Databases",
      "Intel SGX",
      "Bases de Dados",
      "Esquemas Criptográficos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84105",
    "title": "Animating user interface prototypes with formal models",
    "abstract": "The User Interface (UI) provides the first impression of an interactive system and should,thus, be intuitive, in order to guide users effectively and efficiently in performing their tasks.User interface prototyping is a common activity in UI development, as it supports earlyexploration of the UI design by potential users.UI quality plays a crucial role in safety-critical contexts, where design errors can poten tially lead to catastrophic events. Model-based analysis approaches aim to detect usabilityand performance issues early in the design process by leveraging formal analysis. Theycomplement prototyping, which supports user involvement, but not an exhaustive analysisof the designs.The IVY Workbench emerges as a model-based analysis tool intended for non-expertusage. The tool was originally focused on supporting modelling and verification, but morerecently an effort began to combine the formal model capabilities with UI mock-ups, toproduce more interactive prototypes than traditional mock-up editors support.This work addresses the enhancement of the prototyping features of the IVY Workbench.The improvements of such features include the creation of a dynamic widget library thatcan vastly improve the quality of prototypes. Such a library, however, should be compatiblewith several mock-up editors to attract a broader design community.The results of this work include an analysis of alternative prototyping tools, identifyingpotential features that can enhance the IVY Workbench, the creation of a dynamic widgetlibrary that is compatible with several mock-up editors, and several improvements to IVY’sprototyping plugin, including the addition of code exporting functionalities. Usability testswere conducted to validate the new features of the tool, with positive results. Two mobileapplications were also created, allowing users to test prototypes in their mobile devices.",
    "authors": [
      "Costa, Rafael Braga Gomes da"
    ],
    "keywords": [
      "User interface",
      "Prototype",
      "User-centred design",
      "Widget",
      "Protótipo",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82761",
    "title": "Combining paraconsistent and dynamic logic for Qiskit",
    "abstract": "This dissertation introduces a logic aimed at combining dynamic logic and paraconsistentlogic for application to the quantum domain, to reason about quantum phase properties:Paraconsistent Phased Logic Of Quantum Programs (PhLQP◦).In the design PhLQP◦, firstly the dynamic was built first, Phased Logic Of QuantumPrograms (PhLQP). PhLQP is itself a dynamic logic capable of dealing with quantum phaseproperties, quantum measurements, unitary evolutions, and entanglements in compoundsystems , since it is a redesign of the already existing Logic Of Quantum Programs (LQP), [14],over a representation of quantum states restricted to a space B equipped with only twocomputational basis, standard and Hadamard. As instances of applications of the logicPhLQP, there is a formal proof of the correctness of the Quantum Teleportation Protocol, ofthe 2-party and 4-party of the Quantum Leader Election (QLE) protocol, and of the QuantumFourier Transform (QFT) operator for 1, 2 and 3 qubits .On a second stage, PhLQP was extended with the connective ◦ known as the consistencyoperator, a typical connective of the paraconsistent logics Logics of Formal Inconsistency(LFIs), [8, 21, 22]. The definition of consistent quantum state and a set of proper para consistent axioms for the quantum domain, Fundamental Paraconsistent Quantum Axioms(FParQAxs), were provided.An example of application of PhLQP◦is the possibility of express and prove correctnessof the universal quantum gate, the Deustch gate.",
    "authors": [
      "Faria, Bernardo Almeida Leite"
    ],
    "keywords": [
      "Quantum phase properties",
      "Dynamic quantum logic",
      "Paraconsistent Dynamic Quantum Logic",
      "PhLQP",
      "PhLQP◦",
      "Quantum Teleportation Protocol",
      "Quantum Leader Election Protocol",
      "Quantum Fourier Transform",
      "Deustch Gate",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66945",
    "title": "Sistemas inovadores de segurança em bases de dados",
    "abstract": "Nos dias de hoje, tem-se notado um aumento do número e diversidade de dados digitaisque circulam, são tratados, analisados e utilizados à escala global. Os números são significativos,e, por isso, as empresas começam a tomar partido de serviços de terceiros, parabeneficiar das vantagens de computação que estes proporcionam.Posto isto, serviços de nuvem disponibilizados pela Amazon, Google ou Microsoft, são utilizadospor essas empresas, que procuram garantias não só de disponibilidade mas tambémde proteção dos seus dados. Como temos observado ao longo dos anos, os serviços denuvem têm vindo a sofrer imensos ataques, onde falhas de segurança nos servidores dearmazenamento acabam por ser responsáveis pela libertação de enormes quantidades deinformação confidencial.De modo a resolver as preocupações existentes de aplicações que lidam com dadossensíveis e confiáveis foram propostas várias bases de dados capazes de armazenar e processardados de forma segura na cloud. Contudo, o maior esforço de investigação encontra-seem desenvolver novos esquemas criptográficos que protegem os dados em texto cifrado detal modo que as bases de dados consigam processar interrogações como se fosse texto simples.Esta abordagem apesar de eficiente acaba por libertar informação sensível que podeser utilizada para quebrar a segurança dos sistemas. Para além disso, a investigação existentetem dado prioridade às bases de dados SQL devido à sua grande aplicabilidade. Estadissertação toma uma abordagem diferente e apresenta uma nova base de dados NoSQLcom processamento seguro, TrustNosQL, assente nas propriedades de segurança de hardwareconfiável. Mais precisamente, este trabalho tem três contribuições principais. O primeiro éuma análise compreensiva do estado da arte atual em base de dados com processamentoseguro. Este estudo permite posicionar o sistema apresentado em relação às capacidadese propriedades de segurança dos sistemas existentes. A segunda contribuição é abase de dados NoSQL com processamento seguro, TrustNoSQL, a primeira base de dadosNoSQL que processa de forma segura as interrogações utilizando a tecnologia Intel SGX. Aúltima contribuição é uma extensa avaliação do sistema apresentado com uma plataformade avaliação de base de dados reconhecida pela indústria.",
    "authors": [
      "Marinho, André Alexandre Pinheiro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79910",
    "title": "Identification and classification of transporter proteins using deep learning models",
    "abstract": "Nos últimos anos a identificação e sequenciação de proteínas transportadoras tem crescido, uma vez que estas são de extrema importância no corpo humano e em todos os seres vivos, sendo responsáveis pela absorção e movimentação de moléculas essenciais às células e ainda pela excreção de produtos do metabolismo celular. A identificação de genes que codificam proteínas transportadoras é muito importante em várias áreas, como farmacocinética e reconstrução de modelos metabólicos em escala genómica que permitem perceber a relação entre genótipos-fenótipos.De forma a tentar diferenciar proteínas transportadoras de não transportadoras duas abordagens foram realizadas, treinando e testando modelos de machine learning e de deep learning. Os dados utilizados provêm da base de dados TCDB, que contém proteínas transportadoras, e da base de dados Swiss-Prot, onde as proteínas foram filtradas para serem obtidas proteínas não transportadoras, obtendo no final um conjunto de dados equilibrado. De seguida, através desses dados foram obtidas características das proteínas através das suas sequências, sendo assim utilizado para treinar diferentes modelos de machine learning e deep neural networks. Nesta abordagem os modelos apresentaram um bom desempenho global, atingindo 89% de acerto na identificação de proteínas transportadoras. Todos os modelos treinados apresentam um elevado número de falsos negativos em comparação com o número de falsos positivos, indicando que a maior falha nos modelos prende-se na identificação de proteínas transportadoras como não transportadoras.O principal objetivo deste projeto prendia-se com a utilização de métodos de deep learning para identificar proteínas transportadoras, apenas utilizando as suas sequências de aminoácidos como entrada, comparando assim as duas abordagens realizadas. Desta forma, utilizando apenas as sequencias das proteínas, diferentes redes neuronais foram treinadas e testadas, desde redes neuronais recorrentes a convolucionais, obtendo um desempenho global muito semelhante ao da abordagem anterior, atingindo também um valor de 89% de acerto na identificação de proteínas transportadoras.Assim, foram alcançados modelos de desempenho preditivo semelhante sem a necessidade de calcular características.",
    "authors": [
      "Silva, Andrea Ferreira Meireles"
    ],
    "keywords": [
      "Deep Learning",
      "Machine Learning",
      "Modelos",
      "Proteínas transportadoras",
      "Models",
      "Transport proteins",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80082",
    "title": "Monitorização da rede Eduroam no Departamento de Informática",
    "abstract": "Perante os recorrentes problemas de qualidade na utilização da rede Eduroam no Departamento de Informática (DI) da Universidade do Minho, foi efetuada uma reformulação ereestruturação da rede sem fios. A sua monitorização permite tomar conhecimento de eventuais situações que possam causar o mau funcionamento de serviços, como outras redes(i.e., ad-hoc, não autorizadas, etc.) no mesmo espaço sem fios que afetam negativamenteo desempenho da rede Eduroam ou má utilização dessas redes com intenções maliciosas.Portanto, monitorizar a operação da Eduroam assume um papel importante em garantiruma prestação adequada dos serviços de rede. Para que tal seja possível, irá ser necessárioestudar a rede sem fios do DI, fazer uma recolha e análise de dados sem fios e disponibilizaros resultados da análise em tempo real. Neste contexto, foi desenvolvida uma aplicação webcom interface amigável de forma a monitorizar o ambiente sem fios em tempo real, identificando comportamentos incorretos e/ou degradação de desempenho. Foram utilizadasalgumas linguagens de programação como PHP e Javascript para processamento de dadose interação com o utilizador e ainda RRDTool como forma de armazenamento de dados.Após a análise dos resultados obtidos foi possível concluir que os 33 pontos de acesso doDI estão sobre uma taxa de utilização inferior a 30% sendo suficientes para a quantidadede dispositivos atual.",
    "authors": [
      "Silva, Mário Costa"
    ],
    "keywords": [
      "Análise de dados",
      "Monitorização de redes",
      "Redes sem fios",
      "Data analysis",
      "Network monitoring",
      "Wireless networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92787",
    "title": "Golang avaliação e melhorias na arquitetura e código",
    "abstract": "Com o sucessivo aumento da informatização e robotização de processos, existe uma crescente neces sidade de produção de código que tenha como objetivo a melhoria da escalabilidade, rapidez e eficiênciade uma aplicação com a finalidade de garantir que a plataforma a trabalhar seja mais rápida, escalável emais fácil de testar e modificar.Assim, a presente dissertação aborda a transição da arquitetura de uma aplicação da empresa EM VENCI, com o objetivo de melhorar a sua eficiência e desempenho. A motivação para este estudo surgiuda necessidade da empresa aplicar melhorias à arquitetura atual, visando aprimorar a compreensão docódigo, identificar problemas com maior rapidez e obter ganhos significativos de desempenho.Desta forma, é realizada uma exposição da arquitetura inicial, identificando pontos críticos e anali sando potenciais melhorias, como repetidos acessos á base de dados, código pouco comentado e misturade lógica de negócio com a camada de acesso a dados. Com base nessa análise, foi proposto um novodesign arquitetural, que foi cuidadosamente planeado e fundamentado no uso de Clean Code Archi tecture. Apresentado posteriormente o processo de implementação deste design sempre exemplificadocom recurso a um caso de estudo que incorpora o projeto, nomeadamente um dos relatórios de Phishing.Por fim, foram conduzidos testes de funcionalidade e desempenho para garantir a saúde da aplica ção e realizar uma análise comparativa com a arquitetura anterior. Os resultados obtidos demonstraramclaramente o sucesso da nova arquitetura, com melhorias significativas no desempenho, na compreen são da estrutura e no tempo de resolução de problemas, destacando-se pela facilidade com que novosmembros da equipa, após uma breve formação na nova arquitetura, conseguem navegar e compreendero código da plataforma em comparação com a arquitetura antiga. Os resultados obtidos evidenciam oimpacto positivo dessa transição no contexto da empresa, beneficiando tanto os colaboradores quanto osclientes. O trabalho futuro envolverá a continuidade da transição dos Use Cases para a nova arquitetura,a fim de consolidar ainda mais os ganhos alcançados e manter a EMVENCI na vanguarda tecnológicado seu segmento.",
    "authors": [
      "Carvalho, Carlos Miguel Luzia de"
    ],
    "keywords": [
      "Clean code architecture",
      "Multi-tenant",
      "Phishing",
      "Relatórios",
      "Saas",
      "Reports",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/95645",
    "title": "Efficient modelling of liquid surfaces on multi-core CPU and Xeon Phi devices",
    "abstract": "The assembly of miniature electronic components requires an adequate scale of the size of the weldingterminators in printed circuit boards to minimize the stresses due to deformation. An optimumterminator layout minimizes the surface tension of the liquid solder, but requires efficient simulationalgorithms to compute the results in an acceptable time slot. Current Surface Evolver is a softwaretool to study surfaces, shaped by surface tension and other energies, and its execution efficiency can beimproved to take advantage of shared memory systems based on multi-core and many-core computingdevices.This dissertation aims to analyze the Surface Evolver, identifying the computational bottlenecksand working on solutions to improve the overall performance of the application. Parallel algorithmswere developed to explore the architectural features of current multi-core and many-core computingdevices namely the Xeon Phi, and including the growing vectorization features of newer processingdevices.After an analysis of the application and its profiling, the original data structure was identified asthe critical bottleneck for software performance: it is implemented with linked lists, which preventsthe use of the vectorization features of current devices and leads to inefficient parallel algorithms,both key elements to improve the performance of the Surface Evolver. The modification of the datastructure was a key task in this dissertation.The calculation force was identified as one of the most time consuming tasks of Surface Evolver andit was the target function of this work. This algorithm iterates over all vertices, edges and faces so isa good example to conclude how vectorization and parallelism affects the performance of simulationsoftware used in the variety fields of science and engineering. In the end of this work it is possibleto see that vectorization can greatly improve the performance of an application, bringing significantspeedups to Surface Evolver.The measured execution times are presented and discussed, throughout the various developmentstages of the application, aiming to analyze the impact of the application of high performance techniqueson the Surface Evolver, suggesting yet further future improvements that were well identified inthe end of this work.",
    "authors": [
      "Araújo, Bruno Tiago Abreu de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27830",
    "title": "Internet tomography : network topology discovery and network performance evaluation",
    "abstract": "Due to the security threats and complexity of network services, such as video conferencing,internet telephony or online gaming, which require high QoS guarantees,the need for monitoring and evaluating network performance, in order to promptlydetect and face security threats and malfunctions, is crucial to the correct operationof networks and network-based services. As the internet evolves in size anddiversity, these tasks become difficult and demanding. Moreover, administrativelimitations can restrict the position and the scope of the links to be monitored,while legislation imposes limitations on the information that can be collected andexported for monitoring purposes and almost all organization can't monitor orhave knowledge or evaluate the performance of the entire network. They only cando this to part of the network, which corresponds to their own network.In this thesis, we propose the use of tomographic techniques for network topologydiscovery and performance evaluation. Network tomography studies the internalcharacteristics of the network using end-to-end probes, ie, it does not need thecooperation of the internal nodes of the network and can be successfully adoptedin almost all scenarios. Thus, it is possible to have knowledge of the networkcharacteristics out of the administrative borders.In this thesis we propose a new approach to Probe Packet Sandwich, where weuse TTL-limited probes to infer the delay of a path hop-by-hop. We have shownthat this approach is more effective than existing ones.This work was developed under the ERASMUS student mobility program, in theTelecommunication Networks Research Group, Dept. of Information Engineering,University of Pisa.",
    "authors": [
      "Costa, Fábio Rafael Azevedo"
    ],
    "keywords": [
      "Internet tomography",
      "Network topology discovery",
      "TTL-limited probes",
      "Packet sandwich",
      "Link failure detection",
      "Tomografia de rede",
      "Descoberta da topologia de rede",
      "Detecção de falhas de link",
      "681.324"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82998",
    "title": "An HAROS extension for variability aware ROS code analysis",
    "abstract": "Human kind has proven how challenging and volatile the technological market can be,growing at an exponential rate. The benefits of such evolution are directly reflected in manyways in our everyday life. Robots are a clear example of an advanced technology that maybe completely integrated in our societies in a near future, hopefully in such a way that theiractions will be considered as trustable as human actions are. These machines are permanentlyrelying on software, which has a development process that many times cannot be consideredtrustworthy. This may cause the final product to have multiple malfunctions, which in turnmay result in tremendous economic losses or even harm human lives.Bearing this in mind, software industry and academia have been trying to establish newstandards and techniques that considerably lower the occurrence of the latter problems.The solution is to apply certain formal methodologies and tools when developing software,namely when developing critical software that controls machinery used, for example, inhealthcare sector, aeronautical industry, or in military operations.The present dissertation aims to explore and improve techniques and tools to help devel opers in the process of building robotic systems, namely those developed with the RobotOperating System (ROS). The focus will be on a specific framework named HAROS, whichperforms different types of analyses of ROS-based code. Although it has a solid set of usefulfeatures, some need to be upgraded to enhance efficiency and also to promote a better experi ence to their users, in particular when the the software has many variants, as is often the casewith robotic applications.The proposed extension offers ROS and HAROS users a practical methodology that, bymerging existing ROS and Software Product Line (SPL) development tools and concepts,considerably improves the understanding of the variability in a robotic application, withoutrequiring a steep learning curve.",
    "authors": [
      "Pereira, Ricardo Ribeiro"
    ],
    "keywords": [
      "Formal methods",
      "High Assurance ROS (HAROS)",
      "JavaScript",
      "NodeJS",
      "Robot Operating System (ROS)",
      "Software Product Line (SPL)",
      "Text-based Variability Language (TVL)",
      "Variability",
      "Linha de Produtos de Software (LPS)",
      "Métodos formais",
      "Variabilidade",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80921",
    "title": "Mobilidade de micro serviços em datacenters suportada por Software Defined Networks (SDN)",
    "abstract": "O termo microserviços não é propriamente recente, existem inúmeras referências ao longo da última década sobre este conceito, no entanto não existe um verdadeiro consenso sobre quem foi o primeiro a introduzir esta abordagem. Independentemente da indefinição sobre o autor, as vantagens e os desafios da sua utilização como base ao desenvolvimento de novas aplicações são hoje bem conhecidos. É também possível verificar que esta arquitetura de software, que inicialmente era mais utilizada em desenvolvimentos nativos para a Cloud, é cada vez mais utilizada em centros de dados locais, o que lança novos desafios às infraestruturas de rede dos centros de dados.O simples facto dos microserviços serem independentes entre si, permite que sejam desenvolvidos, distribuídos e atualizados individualmente, desta forma conseguimos atualizações mais rápidas e com maior frequência, endereçando a constante mudança de requisitos aplicacionais que se verifica em variadíssimas áreas de negócio.No entanto a adoção de novas plataformas deve garantir que estes novos paradigmas integram, e idealmente beneficiam de tecnologias ou soluções já existentes. Num ambiente altamente distribuído, como é o caso de arquiteturas baseadas em microserviços, é evidente que a componente de comunicações tem um papel preponderante na qualidade do serviço, pelo que nos casos em que o centro de dados onde se pretende utilizar a plataforma de orquestração utiliza redes baseadas em software (SDN), o ideal é que as soluções integrem de forma bastante profunda. Esta integração é ainda mais relevante se o referido centro de dados apresentar uma arquitetura híbrida, isto é, composto por capacidade de computação em múltiplos datacenter físicos, mas também em provedores de Clouds públicas (Azure, AWS, Google Cloud, etc.).Este trabalho pretende enumerar os principais desafios à utilização de containers em centros de dados, bem como descrever a melhor forma de integrar a solução de gestão de rede de centros de dados do fabricante Cisco (ACI - Application Centric Infrastructure) com a solução de orquestração de containers mais utilizada atualmente (Kubernetes). É também âmbito deste trabalho apresentar uma proposta à integração do ambiente descrito anteriormente (ACI+Kubernetes) com soluções de orquestração de containers alojados em Clouds públicas, nomeadamente na cloud da Microsoft (Azure).",
    "authors": [
      "Valente, Daniel Jorge"
    ],
    "keywords": [
      "ACI",
      "Cloud",
      "Kubernetes",
      "Microserviços",
      "SDN",
      "Microservices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/65345",
    "title": "Performance optimization and reporting platform for esports",
    "abstract": "The gaming industry has undergone some changes with the investments and professionalizationof the sector, changing the way of playing video games from traditional leisure tobe like a sportsman job. There are currently organized multiplayer video games competitionswith professional players, know as Electronic Sports (Esports).These professional video games players can be compared to athletes once they’re partof a team and with training, their performance can be improved as well as, given certainfactors, conditioned. The emergence of team coaches was naturally introduced, and he’sresponsible for optimizing team performance.Due to this fact, arises the need to develop tools with the aim of improving the performanceof these professional players as well as increasing the duration of their careers bytaking care of their physical and mental health.It was proposed for this study the development of a Performance Optimization andReporting Platform for Esports to help the coaches and players, continuously and automaticallycollecting their behavioral states and reporting the obtained results in order toguide the training to improve individual and team performance.This platform was tested in a real environment, with professional teams as a case study,where it was possible to analyze the impact of mental fatigue and behavioral biometricperformance on devices interaction in players’ game results.",
    "authors": [
      "Duarte, Pedro Daniel Pinto"
    ],
    "keywords": [
      "Esports",
      "Player",
      "Team",
      "Competition",
      "Biometrics",
      "Coach",
      "Monitoring",
      "Data analysis",
      "Recommendations system",
      "Prediction",
      "Machine learning (ML)",
      "Jogador",
      "Equipa",
      "Treinador",
      "Competição",
      "Biométricas",
      "Monitorização",
      "Análise de dados",
      "Sistema de recomendação",
      "Previsão",
      "ML",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27850",
    "title": "Efficient computation of the matrix square root in heterogeneous platforms",
    "abstract": "Matrix algorithms often deal with large amounts of data at a time, which impairs efficientcache memory usage. Recent collaborative work between the Numerical AlgorithmsGroup and the University of Minho led to a blocked approach to the matrix square root algorithmwith significant efficiency improvements, particularly in a multicore shared memoryenvironment.Distributed memory architectures were left unexplored. In these systems data is distributedacross multiple memory spaces, including those associated with specialized acceleratordevices, such as GPUs. Systems with these devices are known as heterogeneousplatforms.This dissertation focuses on studying the blocked matrix square root algorithm, firstin a multicore environment, and then in heterogeneous platforms. Two types of hardwareaccelerators are explored: Intel Xeon Phi coprocessors and NVIDIA CUDA-enabled GPUs.The initial implementation confirmed the advantages of the blocked method and showedexcellent scalability in a multicore environment. The same implementation was also used inthe Intel Xeon Phi, but the obtained performance results lagged behind the expected behaviourand the CPU-only alternative. Several optimizations techniques were applied to thecommon implementation, which managed to reduce the gap between the two environments.The implementation for CUDA-enabled devices followed a different programming modeland was not able to benefit from any of the previous solutions. It also required the implementationof BLAS and LAPACK routines, since no existing package fits the requirements ofthis application. The measured performance also showed that the CPU-only implementationis still the fastest.",
    "authors": [
      "Costa, Pedro Filipe Araújo"
    ],
    "keywords": [
      "681.3.02",
      "681.325.59",
      "519.612"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.02",
      "681.325.59",
      "519.612"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80733",
    "title": "Implementação certificada da componente criptográfica do gestor de passwords KeePass",
    "abstract": "Com a enorme quantidade de aplicações e sistemas web que nos são apresentados existe uma constante preocupação com a nossa segurança e privacidade como utilizadores dos mesmos. Todos nós temos o direito à privacidade dos nossos dados e quando fazemos um registo num novo produto de software queremos acreditar que estaremos protegidos de ataques alheios e que, a não ser que a nossa password seja descoberta, nenhuma informação nossa vai ser vazada. Para tal também nos é exigido, consumidores de tecnologia e aplicações, que tomemos uma atitude no sentido de nos protegermos. Uma dessas formas é usar passwords seguras e diferentes para cada conta criada. Como isto facilmente se toma impraticável devido à enorme quantidade de contas e, consequentemente passwords que é necessário decorar, surgiram os Gestores de Passwords. Estes servem para guardar as nossas passwords de forma segura e confiável para que sempre que precisemos de uma password a irmos buscar de forma simples e rápida. Assim este projecto visa re-implementar a componente criptográfica do gestor de passwords KeePass de forma a garantir os mais altos níveis de confiabilidade e segurança. Para isso, dever-se-á tirar partido das soluções tecnológicas mais recentes para assegurar os referidos níveis de confiabilidade e segurança, como sejam o uso de linguagens de domínio específico para codificação de técnicas criptográficas e sistemas de provas que possam assegurar a respectiva correcção. Para o efeito fazer-se-á uso da linguagem Jasmin e do sistema de provas Easycrypt.",
    "authors": [
      "Freitas, Pedro Miguel Marques"
    ],
    "keywords": [
      "Gestor de passwords",
      "Criptografia",
      "KeePass",
      "Jasmin",
      "EasyCrypt",
      "Password managers",
      "Cryptography",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80126",
    "title": "Conceção e desenvolvimento de uma plataforma para a democratização de APIs",
    "abstract": "Numa era em que tudo está interligado, as organizações que mais antecipadamente adotam estraté-gias de conexão entre os seus produtos, dados e consumidores finais, são aquelas que frequentemente conseguem obter uma vantagem competitiva no mercado. A Sonae MC, como presença dominante no mercado português, adotou uma nova evolução de uma Arquitetura Orientada a Serviços, de modo a manter esse destaque. API-led Connectivity visa estabelecer a conexão acima referida, com recurso às APIs previamente existentes, cada uma com um único propósito, facilitando a sua reutilização e modularidade da arquite-tura. Este conceito é simbólico da transformação digital na Sonae MC e surge agora a oportunidade de estruturar, documentar e divulgar todas as APIs existentes no seu portefólio. Sendo assim, esta dissertação pretende expor um modo mais fácil e autónomo de implementação e manutenção, tanto de novos projetos, como daqueles já existentes dentro da empresa. Isto será atingível através de um reforço da governação e visibilidade dos ativos digitais da organização, que terão como face um novo Portal de APIs. A dissertação acompanha a criação deste portal e descreve como todos os conceitos envolvidos dão origem a uma nova e mais acessível forma de adoção de APIs pelas equipas que delas necessitam. Neste processo de criação, são abordadas diversas fases do desenvolvimento do software, nomeadamente o levantamento e modelação dos seus casos de uso e a sua análise, conceção e implementação. É provado que é de facto vantajoso para uma empresa fornecedora de APIs ter um portal para as apresentar e, por fim, são enumeradas formas de como comprovar e aumentar estas vantagens para o caso da Sonae MC.",
    "authors": [
      "Coutinho, André Rodrigues"
    ],
    "keywords": [
      "API",
      "Software Engineering",
      "Software Modeling",
      "Requirements Engineering",
      "Portal de APIs",
      "Engenharia de Software",
      "Modelação de Software",
      "Engenharia de Requisitos",
      "API Portal",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83568",
    "title": "Cálculo do poder de disseminação de um utilizador numa rede social online como fator de determinação de risco para a sua privacidade digital",
    "abstract": "A presença ubíqua das redes sociais faz com que estas sejam plataformas ideais para a disseminação de notícias, mas quando o conteúdo destas notícias é falso estas podem pôr em risco a privacidadedigital de um utilizador. A disseminação de notícias nas redes sociais é principalmente feita com baseem relações de confiança e confiabilidade entre os utilizadores, estas relações estão então diretamenteligadas com o poder de disseminação de um utilizador. A partilha e compartilha das notícias nas redessociais é atualmente feita por utilizadores e contas bot, a disseminação das notícias falsas por bots põema privacidade digital dos utilizadores das redes sociais em risco.De forma a ter a recolher dados que pudessem ser livremente manipulados foi utilizado a ferramenta”FakeNewsNet”, esta permitiu que fosse criado um dataset com os dados recolhidos da rede social Twitter.De forma a melhor compreender as redes de disseminação das notícias, os dados recolhidos foramaplicados ao ”Community Health Assessment Model”, este modelo é baseado nos modelos epidemiológicos,e que permite obter dados sobre a disseminação das notícias nas redes sociais entre utilizadores edentro de comunidade ou ”echo chambers”. Os dados contidos no dataset foram também aplicados ao”Botometer”, um modelo supervisionado de deteção de contas bot no Twitter, em que as contas dos disseminadoresde notícias foram analisadas. Com os resultados obtidos dos modelos aplicados, é medidoo impacto das contas bot na disseminação das notícias. O impacto das contas bot para a disseminaçãodas notícias é entre 2,9% a 6,9% na disseminação de notícias dentro de comunidades e nas notícias falsascontribuem entre 2,8% a 0,7% na disseminação entre utilizadores, sendo que estas contas correspondementre 0,272% a 0,296% da população que dissemina as notícias. Utilizando os dados dos dataset foi tambémfeita uma análise de como as notícias são disseminadas. A análise foi feita em duas partes, umaparte foi dedicada às publicações e as relações que os utilizadores têm com contas bot e a outra partefoi dedicada ao poder de disseminação dos utilizadores. A partir da análise foi identificado que as contasbot desempenham um papel maior na disseminação das notícias falsas.Utilizando esses dados são propostas medidas de modo a que os utilizadores consigam se protegerda influência das contas bot.",
    "authors": [
      "Miranda, João Lobarinhas Fernandes"
    ],
    "keywords": [
      "Poder disseminação",
      "Fake news",
      "Bots",
      "Redes sociais",
      "Modelos epidemiológicos",
      "Dissemination power",
      "Bot identification",
      "Social networks",
      "Epidemiological models",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36620",
    "title": "Development of an integrated computational platform for metabolomics data analysis and knowledge extraction",
    "abstract": "In the last few years, biological and biomedical research has been generating a large amount of quantitative data, given the surge of high-throughput techniques that are able to quantify different types of molecules in the cell. While transcriptomics and proteomics, which measure gene expression and amounts of proteins respectively, are the most mature, metabolomics, the quantification of small compounds, has been emerging in the last years as an advantageous alternative in many applications.As it happens with other omics data, metabolomics brings important challenges regarding the capability of extracting relevant knowledge from typically large amounts of data. To respond to these challenges, an integrated computational platform for metabolomics data analysis and knowledge extraction was created to facilitate the use of several methods of visualization, data analysis and data mining.In the first stage of the project, a state of the art analysis was conducted to assess the existing methods and computational tools in the field and what was missing or was difficult to use for a common user without computational expertise. This step helped to figure out which strategies to adopt and the main functionalities which were important to develop in the software. As a supporting framework, R was chosen given the easiness of creating and documenting data analysis scripts and the possibility of developing new packages adding new functions, while taking advantage of the numerous resources created by the vibrant R community.So, the next step was to develop an R package with an integrated set of functions that would allow to conduct a metabolomics data analysis pipeline, with reduced effort, allowing to explore the data, apply different data analysis methods and visualize their results, in this way supporting the extraction of relevant knowledge from metabolomics data.Regarding data analysis, the package includes functions for data loading from different formats and pre-processing, as well as different methods for univariate and multivariate data analysis, including t-tests, analysis of variance, correlations, principal component analysis and clustering. Also, it includes a large set of methods for machine learning with distinct models for classification and regression, as well as feature selection methods. The package supports the analysis of metabolomics data from infrared, ultra violet visible and nuclear magnetic resonance spectroscopies.The package has been validated on real examples, considering three case studies, including the analysis of data from natural products including bees propolis and cassava, as well as metabolomics data from cancer patients. Each of these data were analyzed using the developed package with different pipelines of analysis and HTML reports that include both analysis scripts and their results, were generated using the documentation features provided by the package.",
    "authors": [
      "Costa, Christopher Borges"
    ],
    "keywords": [
      "Metabolomics",
      "Machine learning",
      "Univariate analysis",
      "Multivariate analysis",
      "681.3",
      "57"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3",
      "57"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79862",
    "title": "Monitorização inteligente da interoperabilidade em ambiente hospitalar",
    "abstract": "Despite the advances made in recent decades, information technologies still have a lotto offer to the health sector. In the scientific community, the idea of technology as avery important part of improving healthcare is already unanimous. Health institutionsare increasingly willing to invest in technologies that support the daily activities life ofhealth professionals, especially at the time of decision making so that it is as fast andas accurate as possible.Thus, the number of hospital information systems has increased, especially the sys tems supporting and easing the diagnosis, treatment and follow-up of the patient.However, these systems are also necessary for decision-making on the part of the ins titutions computer processes, such as intelligent agents, which indirectly influence thequality of the services provided. Of all the choices that have been made within thedeveloped systems, the use of BI has proved quite effective in the presentation of infor mation as well as in the construction of decision support systems. As a consequence,this dissertation project aims to develop a BI platform to continuously monitor theintelligent agents of the CHP as well as their activities.The current digital revolution brings several challenges, the constant emergence ofinnovative technologies often put at stake the work done due to the eventual obsoles cence that it can present in comparison with those that are built with these innovations.Thus, health institutions, should always be alert and keep an eye on their informationsystems, evaluating whether they have become archaic or even if there are new soluti ons that respond better to the problems they have daily at hand. In this way, they willbe updated at technological level and competitive at market level since, inevitably, thequality of the provided services improves significantly.With all these innovations new problems arise, hospital units are increasingly com plex environments at the level of computer systems due to their heterogeneity. Theinformation generated and stored in each system has characteristics and structuresthat can be quite different, which causes the information to be individualized. In thisway, the main issue addressed in this dissertation emerges, the interoperability. , theinteroperability. To answer all these challenges, AIDA was created, a system based on intelligent agents that aim to implement interoperability in health institutions. The in telligent agents have tasks of various types, but have the similarity of communicatingwith heterogeneous systems in order to exchange information of great importance oreven manage and store information in databases. Therefore, the need to monitor theseagents as well as their activities arises in order to maintain the interoperability andquality of the services provided by the institution where they are implemented. Thus,this dissertation aims to developa platform that monitors continuously and in real timethe agents of the CHP.This project was based on the DSR methodology, that initially defines the problemfor which one intends to design a solution, outlining the objectives to be achieved. Theremaining phases deal with the development and evaluation of the developed solution.As proof of concept, the SWOT analysis and the technology acceptance study basedon TAM3 were chosen. The results of the proof of concept were quite positive andrevealed an excellent growth potential for the developed solution.",
    "authors": [
      "Sousa, Ana Regina Coelho de"
    ],
    "keywords": [
      "Instituições de saúde",
      "Tomada de decisões",
      "Sistemas de informação em saúde",
      "Agentes inteligentes",
      "Business intelligence",
      "Plataforma web",
      "Monitorização",
      "Inovações tecnológicas",
      "Interoperabilidade",
      "Health organizations",
      "Health information systems",
      "Intelligent agents",
      "Web platform",
      "Monitoring",
      "Technological advances",
      "Interoperability",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80269",
    "title": "Caracterização de tráfego e desempenho em dispositivos móveis: Web e aplicacional",
    "abstract": "Mobile devices have evolved in a continuous and steady way both in terms of computingcapacity and communication capabilities. This evolution is also reflected in the level ofoperating systems and apps developed for the most diverse areas of activity and interest.However, the mobile data plan negotiated with network providers as well as the capacity ofthe batteries continue to be limitations, implying an efficient management of these resour ces.Although monitoring of data usage and computational resources already exists in mostmobile devices, it is not clear what are the differences between the available platforms, weband application, when these offer the same service. Thus, this study intends to developa process that details the differences of a service in both platforms, mainly focusing onassessing the data involved in its use and the necessary computational resources. Thiscomparative study contributes to assist the end user to elect the most convenient platformfor saving resources.",
    "authors": [
      "Areal, Nuno Gabriel da Silva"
    ],
    "keywords": [
      "Traffic Analysis",
      "Internet Traffic",
      "Data Usage",
      "Youtube",
      "Análise de tráfego",
      "Tráfego Internet",
      "Utilização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92676",
    "title": "Análise de cenários para especificação de interfaces web",
    "abstract": "O Desenvolvimento Orientado pelo Comportamento (Behaviour-Driven-Development, BDD) é um paradigma de desenvolvimento de software que permite especificar as necessidades dos utilizadores e os seus critérios de aceitação. A especificação de um sistema é feita através da descrição de cenários de utilização que serão depois implementados. Rocha Silva (2022) propôs uma linguagem de especificação de cenários de utilização, para interfaces web, que utiliza os widgets da própria interface na especificação, permitindo não só especificar os requisitos do sistema, mas também referir como estes deverão ser implementados. No processo de BDD podem surgir na especificação cenários contraditórios que, passando despercebidos, podem levar a que a especificação tenha de ser revista na fase de implementação. Todo este processo acarreta custos, pelo que surgiu a necessidade de criar um método de determinar se uma interface está especificada sem quaisquer contradições (isto é, se a especificação é ou não consistentena descrição do sistema). O objetivo deste trabalho é então definir um método que permite, para uma interface web especificada na linguagem proposta por Rocha Silva (2022), determinar se a especificação apresenta ou não contradições (ou seja, é inconsistente) nos seus cenários. Para cumprir este objetivo será utilizada a ferramenta IVY workbench que permite analisar, de forma automática, modelos escritos em MAL interactors do comportamento de sistemas interativos (permitindo verificar propriedades sobre estes). O primeiro passo do projeto será, então, desenvolver uma ferramenta (o modelador) capaz de traduzir uma interface webespecificada na linguagem proposta por Rocha Silva (2022) para MAL interactors, de modo a que esta possa ser analisada na ferramenta IVY Workbench. Depois, serão ainda apresentadas as propriedades CTL que serão verificadas na ferramenta IVY Workbench para determinar se uma determinada especificação é consistente. Por fim, será apresentado um método de, utilizando a ferramenta IVY Workbench e o modelo em MAL interactors gerado pelo modelador com as devidas propriedades CTL, determinar não só se a especificação é inconsistente mas, caso o seja, os cenários que dão origem a essa inconsistência.",
    "authors": [
      "Gonçalves, Rui Alexandre da Costa"
    ],
    "keywords": [
      "BDD",
      "Widget",
      "IVY Workbench",
      "MAL interactors",
      "CTL",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28081",
    "title": "Development of a computational platform for the visualization of metabolic models",
    "abstract": "The recent sequencing techniques and omics approaches are generating huge amounts of data that can provide ways to extract meaningful knowledge, by resorting to appropriate computational tools. One important technique resorts to the use of genome scale model reconstructions. These models are widely used in Metabolic Engineering, attempting to optimize an organism's functions, genetically modifying it to produce compounds of industrial interest.Another area that became widely important within the fields of Systems Biology and Bioinformatics was network analysis and visualization. Networks can provide a way to better understand the relationships between biological entities, by allowing their visual representation. However, biological networks usually comprise a large number of entities and interactions, that cannot be easily interpreted by the human eye. Integrating visualization and analysis is, therefore, a goal of high interest in several scientific areas, and this has been tackled by several visualization tools available. However, regarding the integration of metabolic engineering techniques with metabolic network visualization, there are still few examples of success. Usually, it is necessary to use more than one tool and the agility of the methods is limited.In this work, a metabolic network visualization framework is presented, with the goal of being a tool that will help researchers in metabolic engineering projects. This framework is divided in two layers: the first deals with the importation and exportation of networks in different formats, while the other layer provides all the visualization and edition features. A metabolic layout is based on the reactions contained in the metabolic model, and it can represent just a part of the metabolism of an organism. To have the possibility to use the same layout in different models, a strategy was defined to map the entities of the visualization with the entities of the model. The layouts are displayed in a bipartite graph, with different node types and colors. It is possible to visualize additional information of the network by clicking the nodes. Some of the features include dragging, zooming and highlighting. On top of all this, it is also possible to apply filters and overlap information over these networks. The filters can change what is visible in the network, while the overlaps allow defining new labels, colors and shapes to the nodes, and new colors and thickness to the edges. Finally, the framework was also integrated within OptFlux, an open-source software to support metabolic engineering available at www.optflux.org, to provide a connection between visualization and metabolic simulation methods.",
    "authors": [
      "Noronha, Alberto Miguel Silva"
    ],
    "keywords": [
      "681.3:57",
      "57:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:57",
      "57:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80743",
    "title": "Aplicação móvel inovadora de apoio ao planeamento e administração de medicação em lares",
    "abstract": "Nos dias de hoje, devido à acentuada evolução que a tecnologia tem vindo a sofrer, as aplicações informáticas tornaram-se indispensáveis no nosso dia-a-dia. Inúmeras áreas beneficiam, das mais variadas formas, das aplicações que tem ao seu dispor e a área da saúde não foge à regra. No que toca aos lares de idosos, tendo em conta que estes estão cada vez mais lotados, os auxiliares de saúde não têm mãos a medir em relação à saúde dos utentes. No entanto, ainda são usados métodos arcaicos, que passam por manter toda a informação relativa aos tratamentos farmacológicos em formato físico, o que conduz a uma probabilidade de erro humano bastante elevada. Para que seja possível tornar mais eficiente e reduzir a probabilidade de erro nos tratamentos farmacológicos dos utentes nos lares, é essencial que o processo de administração e planeamento destes tratamentos seja agilizado. Desta forma, pretende-se desenvolver uma aplicação que vise minimizar os problemas anteriormente mencionados, assegurando assim informação atualizada a todo o instante, históricos fidedignos, controlo sobre stocks de medicação, entre outros.",
    "authors": [
      "Perneta, Cesário Miguel Pereira"
    ],
    "keywords": [
      "React native",
      "Node.js",
      "MHealth",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92814",
    "title": "Modelação de um problema de otimização em estações de tratamento de águas residuais usando modelos benchmark",
    "abstract": "A procura incessante por água potável devido à sua escassez levou ao interesse crescente na recuperação de recursos em estações de tratamento de águas residuais, o que ao desenvolvimento de estações de tratamento de águas residuais (ETAR) como instalações de reciclagem de recursos hídricos.A modelação matemática tem vindo a ser extremamente importante na implementação, operação e otimização desses processos. A introdução de modelos padrão, incluindo várias subunidades, permitiu uma avaliação objetiva do desempenho das estratégias de controle por simulação de maneira a combater problemas operacionais que geram perdas económicas e ambientais devido ao desequilíbrio nas populações microbianas.Ao longo do tempo foram propostas muitas estratégias de controlo sendo que têm sido usados softwarese benchmarks simulation models para testar e avaliar estratégias, melhorando a operação das ETAR.O modelo de simulação de referência número 2 inclui sedimentadores primários, espessadores de lamas, desaguamento e tratamento de lamas com digestão anaeróbia, ampliando o alcance em comparação com os anteriores, que se focavam principalmente nas lamas ativadas e no sedimentador secundário. Esses avanços são fundamentais para otimizar o tratamento de águas residuais e superar desafios microbiológicos.Este projeto teve como objetivo principal criar um problema de otimização podendo mais tarde ser formulado também como um problema multiobjectivo.Neste estudo, a abordagem híbrida do algoritmo HGPSAL mostrou-se altamente eficaz ao lidar com um modelo mais complexo, o BSM2, sem ultrapassar limites legais nas variáveis críticas. A inclusão de novas variáveis, como fósforo, oxigénio dissolvido, sódio, SNH (composto de enxofre azotado), pH e SNO(composto de azoto nitrado), representou um avanço significativo no tratamento de águas residuais.Em suma, este estudo contribui significativamente para a aplicação de algoritmos de otimização em estações de tratamento de águas residuais, oferecendo soluções adaptáveis e eficazes. Os resultados obtidos representam um avanço importante em direção a soluções mais económicas e sustentáveis no tratamento de águas residuais, com potencial para impactar positivamente a gestão hídrica e ambiental, destacando a importância da recuperação de recursos, a complexidade microbiológica e a necessidade de estratégias inovadoras de controle, bem como o uso de modelos de benchmark para avaliação de desempenho.",
    "authors": [
      "Gonçalves, Carina Filipa Araújo"
    ],
    "keywords": [
      "Benchmark",
      "BSM2",
      "ETAR",
      "Modelação",
      "Modeling",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93921",
    "title": "Health BlueBoard: clinical cockpit to support the healthcare professional",
    "abstract": "In today's world, data is a ubiquitous concept, and the healthcare ecosystem is no exception, as healthcare organisations are inundated with data that is key to the quality of care. In recent years, the digitalisation of healthcare has changed the way this complex system operates, and it is possible to see the potential for new innovative solutions that apply new insights from Big Data, Data Science and Artificial Intelligence to revolutionise healthcare as a whole. The implementation of digital solutions in healthcare can provide new insights from data and be an extremely helpful tool to improve care and population health, leading to increased clinical efficiency and effectiveness, improved cost and resource containment for the healthcare system, and consequently improved professional satisfaction and patient experience. Thus, data science can be a powerful and impactful tool in the healthcare ecosystem, as efforts need to be made in data exploration and visualisation solutions that are appropriate for healthcare professionals to facilitate their decision-making processes. The main objective of this thesis is to use data modelling methods to create a user-friendly data visualisation dashboard that is suitable for specific end users, in this case doctors, to assist them in task and time management and thus healthcare decision making. It will also highlight the potential positive impact of data science in healthcare and the importance of developing data visualisation solutions that are appropriate for healthcare professionals and able to combine the data and information they need on a single viewing platform.",
    "authors": [
      "Coelho, Mariana Carvalho"
    ],
    "keywords": [
      "Healthcare",
      "Digital health",
      "Data Science",
      "Big Data",
      "Artificial Intelligence",
      "Data visualisation solutions",
      "Health Information Systems",
      "Data-driven solutions in healthcare",
      "Cuidados de saúde",
      "Saúde digital",
      "Ciência de dados",
      "Inteligência Artificial",
      "Soluções de visualização de dados",
      "Sistemas de informação de saúde",
      "Soluções baseadas em dados nos cuidados de saúde",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92807",
    "title": "Protótipo de um sistema de apoio à decisão clínica na urgência: caso de estudo exploratório",
    "abstract": "O serviço de urgência é uma das áreas hospitalares com maior afluência, onde a procura e o graude complexidade são elevados e imprevisíveis. Para além disso, o acesso é irrestrito e as exigências sãocrescentes, assim como a necessidade de gestão de recursos para evitar o colapso das instituições ediminuir os tempos de espera excessivos, que são das consequências mais preocupantes na área dasaúde.Este projeto surge da oportunidade de estágio e proposta de tema de dissertação/projeto académico,na empresa de consultoria tecnológica para a área da saúde, Glintt HealthCare Solutions, SA. O principalobjetivo, é o estudo teórico e desenvolvimento de um protótipo funcional de um sistema, tendo em vistaa realização de recomendações/sugestões aos profissionais de saúde a nível hospitalar, mais concreta mente nos Serviços de Urgência. Este sistema irá ter em conta, informação de utentes, como historialmédico e estado de saúde (p.e. doenças crónicas, medicação, informações recolhidas na triagem, alergiasconhecidas, etc.).De forma mais específica, a solução tecnológica proposta para dar resposta ao problema e contextoacima referido, contém várias etapas. Num primeiro momento, é realizada uma investigação relativaaos softwares de urgência já existentes nos hospitais em diferentes contextos(nacional e internacional).Seguidamente, perceber, no panorama português, onde se encontram os dados relevantes em contextode urgência, para que possam ser recolhidos e posteriormente utilizados. Numa etapa mais intermédiae após a definição dos requisitos essenciais, pretende-se a definição e conceção de uma arquiteturainteroperável, que englobe os mesmos. É nesta fase que se procede à análise das abordagens a integrarna arquitetura, bem como ao estudo do funcionamento do motor de inferência, responsável por gerar asrecomendações e sugestões destinadas a apoiar a tomada de decisão por parte do utilizador. A ideia,numa fase final, é que a arquitetura definida, seja implementada e posteriormente sujeita a avaliação evalidação, por parte dos profissionais ligados à empresa.Deste modo, este projeto visa otimizar a gestão de recursos na saúde, agilizando o atendimento emelhorando a eficiência, beneficiando tanto pacientes, como profissionais de saúde.",
    "authors": [
      "Ferreira, Marco António Álvares"
    ],
    "keywords": [
      "Urgência",
      "Saúde",
      "Afluência",
      "Doentes",
      "Agilizar",
      "Tempo de espera",
      "Tecnológica",
      "Protótipo",
      "Sugestões",
      "Recomendações",
      "Suporte à decisão",
      "Emergency",
      "Healthcare",
      "Attendance",
      "Patients",
      "Expedite",
      "Waiting time",
      "Technological",
      "Prototype",
      "Suggestions",
      "Recommendations",
      "Decision support",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84668",
    "title": "Breast tumor segmentation and classification using deep learning methods",
    "abstract": "O cancro da mama é o tipo de cancro mais comum e a principal causa de morte por cancro nasmulheres. A deteção atempada é crucial para o sucesso do tratamento e para a redução da taxa demortalidade. Embora várias modalidades de imagem sejam utilizadas para detetar lesões mamárias, aultrassonografia tornou-se uma das mais frequentes, uma vez que é segura, portátil, de baixo custo epermite uma examinação em tempo real. Deste modo, a segmentação e classificação automáticas detumores em imagens de ultrassom da mama podem auxiliar no seu diagnóstico, proporcionando umasegunda opinião aos especialistas. No entanto, realizar uma segmentação e classificação de lesõescom precisão, através desta modalidade de imagem, é desafiante devido à má qualidade de imagem eelevada variabilidade das lesões. Recentemente, algoritmos de deep learning têm demonstrado grandepotencial na área do processamento de imagem, nomeadamente para a segmentação e classificação emimagens de ultrassom da mama. Contudo, ainda há necessidade de investigação e melhoria para serpossível aplicar com confiança estas abordagens na prática clínica. Adicionalmente, o ultrassom podeser utilizado para orientar a agulha durante a biópsia mamária, um procedimento que exige elevado rigor.O projecto OncoNavigator, no âmbito do qual esta dissertação foi realizada, visa combinar imagemmédica em tempo real com um robô médico colaborativo para melhorar a precisão do rastreio do cancroda mama e na biópsia mamária guiada por ultrassom.Considerando a necessidade de métodos automáticos para a segmentação e classificação para melhorar a prática clínica e de modo a aplicar inteligência artificial no robô médico colaborativo, nesta dissertação, foi desenvolvida uma nova multi-task learning network para a segmentação e classificação simultânea de tumores em imagens de ultrassom de mama. O método proposto foi avaliado em 810 imagensde ultrassom de dois conjuntos de dados, o BUSI e o UDIAT, tendo obtido um Dice de 80.72%, na seg mentação, e area under the curve de 94.34%, na classificação. Em suma, o método demonstrou ser bemsucedido no delineamento e categorização de lesões e revelou potencial para ser incorporado num robômédico colaborativo para intervenções de cancro da mama e para auxiliar no seu diagnóstico.",
    "authors": [
      "Ferreira, Ana Margarida da Rocha"
    ],
    "keywords": [
      "Cancro da mama",
      "Ultrassom",
      "Deep learning",
      "Multi-task learning",
      "Segmentação de tumores",
      "Classificação de tumores",
      "Computer-aided diagnosis",
      "Breast cancer",
      "Ultrasound",
      "Tumor segmentation",
      "Tumor classification",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86527",
    "title": "Automatic generation of ASTs from a programming language grammar",
    "abstract": "Tools that detect security problems are very important nowadays and for the people doingcode reviews it is even more important to have tools that identify vulnerabilities in the code.In this way companies that provide applications can be more confident that the code theydeploy is almost vulnerabilities free.A subset of these tools, known as Static Application Security Testing (SAST) tools, rely onthe analysis of the source code aiming at looking for patterns that correspond to vulnerabili ties. These analyzers use mainly language processors to help them extract from the sourcecode the information they need. Languages exist for many years but they never ceased toexist because they are in constant development.The description of languages is supported bygrammars. Grammars also evolve to sustain the referred languages evolution. They wereprimarily used for compilers to analyse the structure of the language and parse it; nowadaysthey help many other tools like SAST for example.Having a tool that can detect vulnerabilities is very useful like was said, but to identifythose vulnerabilities it is necessary to find patterns in languages. By finding these abstractpatterns the work is simplified since all concrete languages will present similar vulnerabilities.For example, SQl Injection is a vulnerability that is shared across almost all languages; so,it is possible to define one general pattern to capture that common vulnerability in eachlanguage.Those patterns are defined over Abstract Syntax Trees (AST). To build an AST whileparsing a program, Checkmarx uses a set of functions called Visitors that are associated to theproductions of the programming language Grammar. In that context, the Language Factorytool developed by Checkmarx generates automatically some visitors and let programmers togenerate the others by dragging and dropping rules. The main objective of Language Factorytool is to aid programmers understanding how to create visitors and, at the same time, togenerate as many visitors as possible to be used directly. When Checkmarx is working ona new Language to to include support for that language in the CxSAST tool, the use ofLanguage Factory will help creating the appropriate Visitors making this process simplerand faster.The Master’s project reported in this dissertation appears in that framework aiming at theimprovement of the Checkmarx Language Factory making it capable of infer more Visitorsfrom the new language’s Grammar.",
    "authors": [
      "Silva, Pedro Miguel Mimoso Lopes Ferreira da"
    ],
    "keywords": [
      "Programming languages and grammars",
      "Static analysis",
      "Vulnerabilities",
      "SAST",
      "Language-based tools",
      "Linguagens de programação e gramáticas",
      "Análise estática",
      "Vulnerabilidades",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92591",
    "title": "Analysis of trade-offs between performance and energy efficiency of scalable dataframes tools",
    "abstract": "Nowadays, we have the ability to trace everything, to extract valuable data from wherever we want, all tokeep us connected and to improve our lifestyle. This huge amount of information, produced every day,needs to be treated, manipulated, and analysed, requiring convincing data structures to do so.Dataframes, regularly used worldwide, are powerful data structures used to analyse and manipulatedata of any kind. A Dataframe organizes data into a 2-dimensional table of rows and columns, similar toSQL tables or CSV files. Furthermore, it can span alongside thousands of computers or servers, makingit easier to work with huge amounts of data, called big data, using distributed systems and parallel computing.This Dataframe’s distributed nature led to the rise of distinct scalable and parallel Dataframe tools. Themost used Dataframe tool, pandas, only performs on sequential execution and has some limitations whenthere is the need to handle huge volumes of data, and some tools such as Modin, Polars, RAPIDS, andso forth, appeared in order to overcome those limitations. The vast offer of these scalable tools broughtthe need to make an analysis and comparison between these frameworks and pandas, studying theirbehaviour and results with different workflows. This comparison is not linear and there is a need to usea benchmarking tool, in order to produce a homogeneous and reliable evaluation of the different frameworks.To perform this analysis, we worked with several workflows, manipulating real and synthetically produceddata on distributed and parallel environments and on different hardware configurations.We designed and developed a benchmarking tool that supports a set of Dataframe frameworks, is flexibleto the addition of new frameworks, and is able to perform micro-benchmarking evaluation with the analysisof a group of individual and common operations used on data science, and macro-benchmarking evaluation with the analysis of workflows that represent a set of chained operations. Both of these evaluationsaggregate performance and energy consumption results for each framework.",
    "authors": [
      "Martins, André Carvalho da Cunha"
    ],
    "keywords": [
      "Dataframe",
      "Distributed and parallel computing",
      "Performance",
      "Energy consumption",
      "Benchmark",
      "Computação distribuída e paralela",
      "Consumo energético",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82755",
    "title": "Georreferenciação de conteúdos de bases de dados documentais",
    "abstract": "A georreferenciação é o processo de localização geográfica de um determinado objeto espacial através da atribuição de coordenadas. Os sistemas de georreferenciação utilizam umprocessamento espacial automático executado por computador, por exemplo para colocaruma entidade num mapa ou fornecer um recurso espacial. Quando este processo é aplicadoa coleções de documentos textuais, é descrito como uma combinação de reconhecimento deentidades nomeadas. O Livro das Propriedades, também designado como o Tombo da Mitra,contém informação relativa aos tipos de terras, acidentes de terreno, nomes de ruas, proprietários e apontamentos biográficos e genealógicos das várias propriedades que a mesaArcebispal de Braga possuía no século XVII. Este trabalho de dissertação teve como objetivoconceber e implementar um sistema de georreferenciação textual para o conteúdo existenteno Livro das Propriedades, com particular enfoque nos lugares que nele estão referidos, deforma a permitir aos estudiosos destes conteúdos possuírem informação acerca da localização geográfica desses elementos.",
    "authors": [
      "Gomes, João Pedro Carvalho"
    ],
    "keywords": [
      "Aprendizagem máquina",
      "Mineração de textos",
      "Processamento de Linguagem Natural (PLN)",
      "Sistemas de georreferenciação",
      "Livro das Propriedades",
      "Tombo da Mitra",
      "Machine Learning",
      "Text mining",
      "Natural Languague Processing (NLP)",
      "Geo-referencing Systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56107",
    "title": "Metagenomic analysis of a Nitritation - Anammox reactor: community members and processes",
    "abstract": "Nitrogen is a fundamental element for all organisms. It is nevertheless predominantlyfound in the atmosphere, in the form of unreactive nitrogen. In the last century, a manmademethod for nitrogen fixation improved the crops yield, fuelling a populational growth.The exponential increase of anthropogenic nitrogen in soils and water bodies has, however,affected the environment and deregulated the natural biogeochemical nitrogen cycle. Currently,the costs of repairing the damage caused by the reactive nitrogen load from humanactivities have overcome the profits of the agricultural improvement, derived from the applicationof fertilizers. Wastewater treatment plants remove the excessive amounts of nutrientssuch as carbon, nitrogen and phosphorus from wastewater to prevent environmentalimpacts derived from excessive nitrogen in the biosphere, like eutrophication.The current conventional wastewater treatment applied is nitrification coupled with denitrification.However, the requirement for an external carbon source and aeration renderthis process costly. Furthermore, one of the intermediates of denitrification is nitrous oxide,a greenhouse gas with an effect three hundred times worse than carbon dioxide and witha lifespan of one hundred and twenty years in the ozone layer.The Partial Nitrification/Anammox (PNA) process combines aerobic ammonium oxidationwith anaerobic ammonium oxidation while suppressing the activity of nitrite oxidizingbacteria. This efficient process of nitrogen removal from wastewater reduces the aerationcost and the need for external carbon with zero nitrous oxide emissions.Unknown microbial interactions may, on the other hand, impair this process, resulting insuboptimal performance such as, excessive nitrate and nitrous oxide emissions. To betterunderstand the microbial community and its interactions and to find the causes of the treatmentsinstability, metabolic analysis and genomic annotation was performed, using twocomplementary binning methods. The biological samples used in this study were retrievedfrom a high-rate PNA sequencing batch reactor, fed with carbon-free ammonium-rich syntheticwastewater.Fifty-seven draft genomes making up about eighty percent of the total community metagenomewere recovered. In addition to the three genomes each from Nitrosomonas and CandidatusBrocadia, several genomes belonged to Proteobacteria, Chloroflexi, Planctomycetes, Bacteroidetes, Armatimonadetes, Ignavibacteriae, Acidobacteria, Chlorobi, Verrucomicrobia, Actinobacteriaand Gemmatimonadetes phyla.In this study, the heterotrophic organisms encoding partial denitrification could be dividedinto niches accordingly to their role in this pathway, describing their interactions asa community. The complexity of the community was also ascertained with the discovery ofputative heterotrophic hydroxylamine oxidizing bacteria and putative heterotrophic nitriteoxidizing bacteria.Overall, high quality genomes that constitute a high fraction of the metagenome wererecovered, allowing for a precise description of the PNA reactors community and the flowof nitrogen oxides. A complex community with high redundancy was uncovered basingthe main interactions on the partitioning of the nitrogen oxides respiratory pathway.",
    "authors": [
      "Silva, Bruna Daniela Azevedo da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia do Ambiente"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia do Ambiente"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83357",
    "title": "Melodic, using music to train visually impaired kids in computational thinking",
    "abstract": "This document, in context of second year of Integrated Master of Informatics Engineering,reports the development of a project that intends to teach Computational Thinking to kids withspecial educational needs, in this case blindness. The aim of this research is to characterizeboth subjects, Computational Thinking and Blindness, and identify what are the current mostused and best practises to teach this different way of thinking to kids with special needs. Toachieve this, Melodic was created. This is a system composed by a software and a hardwarewhere the user must create sequences with the tactile blocks (the hardware) and then readthem with the mobile application (the software), that converts the sequence created intosound. With this, the user can easily hear the differences that the changes in the blockssequence can make. This can be compared to the Computational Thinking teaching throughthe use of robots, because in that case, users can see the result of their instructions in therobot movement and with Melodic, the user can hear the result of their instruction with themusical note sequence played by the app. In this document more technical aspects such asthe architecture of the application that is proposed to accomplish the goal of the presentMaster’s project, will also be discussed. After this, the project development process that leadto the creation of Melodic is described as well as all the decisions taken. A description ofall functionalities of this system can also be seen in this document. To prove the researchhypothesis initially stated, some exercises were created and described. The referred exerciseswere designed to access if Melodic actually develops Computational Thinking.",
    "authors": [
      "Costa, Rui Diogo da Silva"
    ],
    "keywords": [
      "Computational thinking",
      "Visual impaired students education",
      "Teaching through music",
      "Pensamento computacional",
      "Educação de estudantes invisuais",
      "Ensino através da música",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83362",
    "title": "Geração automática de interfaces de utilizador para aplicações web",
    "abstract": "The main objective of this dissertation is to make a contribution in the automation of web applications' development, starting from prototypes of their graphical user interlaces. The integration of model-based user interface development concepts with the more traditional user-centred development approach allows for a rethinking of GUI design development, independent of implementation details, and redefining models to realize these graphical interfaces. In the end, the intent is to increase the level of abstraction of the development process, promote better adaptation of applications to different devices and execution environments, and decrease the effort required to develop the graphical interlaces. Due to the exponential increase in the use of internet-based services and applications, there is an also increasing demand for Web designers and developers. At the same time, the proliferation of languages, frameworks and libraries illustrates the current state of immaturity of web development technologies. This state of affairs creates difficulties in the development and maintenance of Web applications. An approach is presented that allows designers to use prototyping tools, in this case Adobe XD, to design graphical interfaces, and then automatically converts them to Vue.js + Bootstrap code, thus creating a first version of the implementation. This is done through the interpretation of the SVG file that Adobe XD exports. The goal is not to produce the final version of the Ul. Instead, we aim to produce a first version of the code, which can then be refined by the developer. This enables us to place less requirements on the prototype, regarding the amount of information that it must contain. In the end, we get a skeleton of Vue.js code that is easy to maintain and reuse to further improve the project.",
    "authors": [
      "Machado, Catarina Araújo"
    ],
    "keywords": [
      "Model-based user interface development",
      "Web development",
      "Web frameworks",
      "Prototyping tools",
      "Automatic code generation",
      "Desenvolvimento de interfaces de utilizador baseado em modelos",
      "Desenvolvimento web",
      "Ferramentas de prototipagem",
      "Frameworks web",
      "Geração automática de código",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92813",
    "title": "Decoding activities of daily living and incipient falls using brain signals",
    "abstract": "Falls represent one of the biggest causes of deaths related to unintentional injuries. The increasing numberof occurrences is associated a continuously expanding elderly population, along with its detrimental effectson the survival and well-being of those aged 65 and above, has turned the issue of falls into a globalpublic health concern. It is estimated that 684,000 people worldwide lose their lives due to falls, whichhappen approximately 37.3 million times annually. As a result, the financial expenses associated withhospitalisations are significant and present a complex challenge.The Electroencephalogram (EEG) technique is widely utilised to assess brain electrical activity anddetect indicators of balance disruptions, such as Perturbation Evoked Potentials (PEPs), in brain signals.This is possible because EEG data provides insights into motor planning and intention, making it a valuabletool for monitoring both falls and Activities of Daily Living (ADLs). Accordingly, this dissertation will establishtwo experimental protocols: one for simulating slip-like incidents and another for ADLs, with the aim ofcollecting EEG data. The primary goal of this dissertation is to leverage Artificial Intelligence (AI)-basedsystems to identify slip-like perturbations and various ADLs using the data from both protocols. The ultimateobjective is to integrate these algorithms into assistive robotic devices, e.g. exoskeletons.In the context of the methods employed, the PEP components were identified within a time frameof 75–137 ms after the external perturbation onset. To analyse the pre-processed EEG data, four distinctartificial neural networks were evaluated, each with varying network architecture parameters. Among thesearchitectures, the Convolutional Neural Network (CNN)-Long Short-Term Memory (LSTM) model, trainedto predict EEG perturbations, exhibited superior classification performance, achieving an accuracy rate of86% when using a short time window of 100 ms. In contrast, for classifying ADL, the best result obtainedwas 53% accuracy, and this was also achieved using the CNN-LSTM architecture.",
    "authors": [
      "Manso, José Pedro Rodrigues"
    ],
    "keywords": [
      "EEG",
      "ADL",
      "Falls",
      "Artificial Intelligence",
      "Slip-like perturbations",
      "Quedas",
      "Inteligência Artificial",
      "Perturbações do tipo escorregar",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27971",
    "title": "Formal verification of Ada programs: an approach based on model checking",
    "abstract": "O rápido crescimento da complexidade dos sistemas de software exige, agora mais do que nunca, uma validação rigorosa dos mesmos por forma a manter ou até mesmo aumentar a confiança nestes sistemas. Em particular nos sistemas críticos, onde as falhas podem ter consequências catastróficas podendo até incluir a perca de várias vidas humanas, é de externa importância o desenvolvimento de técnicas capazes de garantir altos níveis de confiança para estes sistemas.Nesta tese é proposta a utilização de uma técnica formal para a verificação de programas Ada, que pretende aumentar a confiança em sistemas cuja implementação seja realizada nesta linguagem de programação. Mais precisamente, pretende-se a aplicação da técnica de verificação de modelos para a análise do código fonte de programas concorrentes Ada, com especial foco para o domínio dos sistemas críticos.A verificação de modelos é uma técnica bem-sucedida no que diz respeito à garantia de um aumento de fiabilidade destes sistemas. No entanto, a aplicação desta técnica a sistemas de software enfrenta ainda vários obstáculos, e as ferramentas e técnicas para ajudar a ultrapassar estes obstáculos estão ainda a ser desenvolvidas. A ferramenta desenvolvida no contexto desta tese (ATOS) visa responder a problemas como (i) a construção de modelos a partir de programas e (ii) a especificação de propriedades para estes modelos de acordo com as pretendidas para os programas.A construção manual de modelos que simulam o comportamento de programas é um processo complexo, temporalmente dispendioso, e sujeito a falhas devido à complexidade destes sistemas. De forma a ultrapassar este problema o ATOS propõe a extração automática de modelos a partir de programas Ada. Por outro lado, o mapeamento das propriedades desejadas dos programas em propriedades dos modelos pode ser urna tarefa com um grau de complexidade elevado, pois requer entre outros a utilização de um formalismo logico ao qual a maioria dos programadores não está acostumada. 0 ATOS ajuda no mapeamento destas propriedades, oferecendo vários mecanismos de suporte à sua especificação.",
    "authors": [
      "Martins, João Pedro Marques da Silva"
    ],
    "keywords": [
      "Ada",
      "Extração de Modelos",
      "Lógica Temporal",
      "Métodos Formais",
      "Sistemas Críticos",
      "SPIN",
      "Verificação de Programas",
      "Verificação Formal",
      "Verificação de Modelos",
      "Verificação de Modelos de Software",
      "Critical Systems",
      "Formal Methods",
      "Formal Verification",
      "Model Checking",
      "Model Extraction",
      "Software Verification",
      "Software Model Checking",
      "SPIN",
      "Temporal Logic",
      "681.3.06"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46731",
    "title": "Exploração e desenvolvimento de soluções interoperáveis em ambiente hospitalar",
    "abstract": "Em contexto hospitalar, elaborar soluções interoperáveis remete para o conceito de Sistemasde Informação (SI) e a forma como estes são capazes de cooperar na partilha deinformação. Aqueles que apresentam maior afinidade com a informática médica são naturalmenteos Sistemas de Informação em Saúde (SIS), que constituem um dos pontos mais importantese mais sensíveis no que à prestação de cuidados de saúde diz respeito. A evoluçãodesde o armazenamento de dados no formato de papel (paper-based) até à utilização de sistemasinformáticos apresenta inúmeras vantagens. A maior rapidez no acesso a dados, aredução de erros (consulta, processamento e apresentação) e o diminuição de perdas deinformação encabeçam sem dúvida uma lista bastante extensa de pontos positivos. Emcontraponto, na altura em que muitos dos sistemas existentes foram implementados, o conceitoda partilha de informação não era equacionado, resultando na criação de “ilhas deinformação” sem qualquer comunicação com o exterior.Com a evolução da Internet e da capacidade de processamento dos sistemas informáticos,a partilha de dados entre sistemas tornou-se uma constante, sendo vital a questão da interoperabilidade.Existem no entanto barreiras à implementação completa dessa ideologia motivadopor divergências em formato de dados ou compatibilidade entre sistemas. Emboramuito já tenha sido estudado e implementado, existem ainda no Serviço Nacional de Saúde(SNS) lacunas que só soluções propostas de raiz em prol da interoperabilidade semânticapodem corrigir.O setor da saúde nestes tempos de mudança deve aproveitar exemplos provenientes deoutras áreas da economia, como por exemplo, a produção em larga escala onde uma novarevolução está em marcha, a Revolução Industrial 4.0. Através da introdução do conceitode indústria 4.0 desenvolveram-se Cyber-physical System (CPS), onde dados relativos ao estadode uma máquina ou um processo são transmitidos pela rede com destino a unidadescentrais de processamento. Estas, através de processos de Extract, Load, Transform (ETL),transformam dados em informação útil acerca do estado da máquina, de uma linha deprodução ou de toda ama unidade fabril, fazendo assim a ponte entre o mundo real e o virtual.Com esses dados recolhidos em tempo real, as ditas unidades centrais criam as basespara sistemas de monitorização, geralmente sob a forma de aplicações Web, permitem umatomada de decisão mais informada por parte dos utilizadores envolvidos.Os dispositivos móveis são hoje em dia um filão de negócio a ser explorado pelas unidadesde saúde numa tentativa de aproximação ao utente e ao mesmo tempo baixando custosde comunicação, fomentando assim uma relação de confiança entre ambos. No desenvolvimento de uma aplicação móvel é percetível a forma de como a interoperabilidade é vitalem sistemas distribuídos. A interoperabilidade física é garantida com utilização de Internete pedidos Hypertext Transfer Protocol (HTTP), a interoperabilidade sintática tira partido deestruturas como o JavaScript Object Notation (JSON) ou Extensible Markup Language (XML)para envio de dados na resposta aos diversos pedidos. Por último, a interoperabilidadesemântica faz com que diferentes layouts sejam apresentados pela aplicação consoante osdados recebidos.Ao longo do presente projeto, desenvolvido no âmbito da dissertação de mestrado emEngenharia Biomédica, ramo de Informática Médica foi primeiro realizado um levantamentode requisitos com base no estado atual dos SI no Centro Hospitalar do Porto (CHP) emediante os resultados, foram desenvolvidos casos de estudo de forma a perceber qual opossível impacto das soluções encontradas. Quatro possíveis áreas de intervenção foramidentificadas, a troca de informação seguindo a estrutura proposta pela Health Level SevenInternation na versão 2 relativa à troca de mensagens em contexto hospitalar, sistema demonitorização de agentes implementados na unidade de saúde (HL7 e AIDA), formuláriode codificação International Classification Diseases 9 Revision Clinical Modification (ICD-9-CM)para posterior integração num sistema Grupo de Diagnósticos Homogêneos (GDH) e por fima comunicação com o utente quando este está fora da unidade de saúde como forma demelhorar a imagem da unidade de saúde.Como referido, utilizando as diretrizes Health Level Seven International para troca de mensagensna versão 2, foi desenvolvida uma interface do tipo interface engine na linguagemPython, que sustenta um “Multi-Agent System” (MAS) para envio e receção de mensagens.Aproveitando as diretrizes da indústria 4.0, ´e fundamental que estes agentes inteligentescomuniquem a cada instante o seu estado de forma a que os responsáveis conseguirem umfeedback do estado atual de todo o sistema.No segundo caso de estudo foi desenvolvida uma plataforma de monitorização com vistaa rastrear o estado de agentes HL7 e AIDA j´a implementados no CHP e ainda daquelesdesenvolvidos no caso de estudo anterior. Primeiro foi implementado um processo de ETLque alimenta um DataWarehouse (DW) onde a aplicação Web faz queries para recolher dados.Toda a plataforma foi desenvolvida, com base no conceito MVC com a framework Angularjs.O terceiro caso de estudo apresenta uma plataforma de codificação ICD-9-CM numaabordagem à interoperabilidade semântica relacionada com representação de diagnósticose procedimentos clínicos. Utilizando também o conceito MVC em Angularjs, a plataformaestá inserida num sistema de GDH com objetivo de facilitar o trabalho do codificador,aumentar a sua produtividade, servir de barómetro ao trabalho desenvolvido e ainda categorizaros resumo de altas hospitalares. Com o GDH pode num futuro ser definido todo ofinanciamento hospitalar ou até compras realizadas por cada unidade. No último caso de estudo, desenvolveu-se uma aplicação Android do enquadrada noconceito de Electronic Health (eHealth) do tipo appointment remInder. O objetivo é aproximaro paciente da unidade de saúde e evitando que este falhe o seu agendamento poupando emúltima instância dinheiro e recursos às unidades de saúde. Utilizou-se o design propostopela Google para o envio de notificações o serviço Firebase Cloud Message também ele propostopela Google. Mais que um lembrete a aplicação regista consultas passadas, futuras epermite fazer a confirmação da consulta. Este caso ainda se trata de um protótipo inicialpara aceitação por parte da unidade de saúde.Como métodologia de desenvolvimento em todos os casos de estudo, utiliza-se o cicloDesignResearch. Primeiro é definido um requisito/problema que se pretende ver resolvidoe todas as fases que se sucedem visam encontrar uma possível solução para tal. Comoprova de conceito, escolheu-se a análise SWOT, situando-se esta, na fase de conclusão dociclo DS. Todos os casos de estudo foram submetidos a essa análise apresentado resultadosconsiderados positivos.Fica assim vincada a importância da interoperabilidade no desenvolvimento de sistemasinteroperáveis, sendo um conceito transversal a todas as soluções propostas, quer nafomentação direta da interoperabilidade ou no seu aproveitamento para a disseminaçãode informação. No primeiro caso, um sistema que favorece diretamente a partilha deinformação utilizando standards de comunicação. No segundo, o controlo de agentes quetrabalham diretamente na partilha de informação garantindo a sua segurança e rápidaatuação em caso de erro. No terceiro caso, uma solução diretamente ligada à interoperabilidadesemântica e por último uma aplicação que necessita de interoperabilidade parafuncionar na sua plenitude.",
    "authors": [
      "Miranda, Filipe Manuel Mota"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92554",
    "title": "Designing and developing an analytics tool using Prometheus in a company setting",
    "abstract": "In the past few years, due to the exponential growth of technology of the world, there hasbeen an increasing interest in the field of data analytics. This interest comes from howimportant data is to everyday life, since data can significantly benefit and influence a lot ofdifferent areas, from security to health or even sports. Large amounts of data allows analyststo predict possible outcomes, evaluate behavioral patterns and study trends.The main goal of this Master’s thesis is to show that a data analytics tool can also be veryuseful in a company setting, since there is a considerable amount of data, generated bythe software tools of the company, that can be collected and used to improve the servicesprovided by the company.The Master’s work here reported begun with a comprehensive study on different kind ofanalytics tools and also an investigation of the company’s product. After this investigationprocess, the next step was to design and implement a custom analytics tool that usesPrometheus as a basis and takes into account the requirements of the company. The toolrequired the implementation of various processes starting with the collection of the generateddata, the transformation and cleaning of that data and finally a process that provides differenttypes of data visualizations. These visualizations present relevant knowledge to customersand also to different departments inside the company.While the implemented analytics tool met all of the proposed requirements and passedthe tests made to verify it, there will be a need to further refine and improve the tool’scapabilities, specifically regarding scalability and data storage.This document provides a better understanding of the use of data analytics in a companysetting by showcasing the full process of designing and developing a custom analytics toolusing Prometheus. Through this document, it is possible to see that using product generateddata and providing data visualizations is beneficial for the company.",
    "authors": [
      "Cabo, João Paulo Mourão"
    ],
    "keywords": [
      "Data analytics",
      "Descriptive analytics",
      "AST",
      "Big Data",
      "Prometheus",
      "Visualizations",
      "Análise de dados",
      "Análise descritiva",
      "Visualizações",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79428",
    "title": "Otimização dos custos de operação de aplicações Web em Cloud",
    "abstract": "O cloud computing tem sido amplamente adotado na área das tecnologias de informação na última década, devido às diversas vantagens que providencia, entre elas a possibilidade de redução de custos com infraestruturas. Embora a utilização da cloud possa minorar os custos de operação de aplicações Web, verifica-se que a definição dos preços praticados pelos fornecedores de serviços tem-se tornado cada vez mais complexa, ameaçando uma das principais razões que leva os utilizadores a migrar para a cloud: a redução de custos. Derivado deste aumento de complexidade, o surgimento de soluções de monitorização e otimização de custos de cloud tem vindo a aumentar por forma a combater este problema. Apesar de existirem algumas soluções capazes de auxiliar na otimização de custos, verifica-se que a visibilidade sobre os custos e dados de utilização é limitada, não sendo possível consultar a informação com a granularidade que os utilizadores pretendem. Por todos estes motivos, a equipa de Investigação e Desenvolvimento da Eurotux Informática, S.A. decidiu investir no desenvolvimento de uma solução que auxiliasse os seus colaboradores e clientes num problema que enfrentam no dia a dia. Após estudar as soluções existentes, identificou-se, junto dos principais intervenientes, os requisitos que a solução deveria cumprir. A criação de uma aplicação em Flask em conjunto com uma Elas& Stack constitui a base tecnológica da solução. A modularidade, escalabilidade e robustez da solução foi tida em conta em todo o processo de elaboração da solução. O resultado final é uma ferramenta totalmente funcional que permite satisfazer as necessidades impostas. A integração com os principais fornecedores de cloud estudados foi amplamente conseguida. A avaliação da mesma foi realizada tendo por base diversos casos de estudo de clientes reais da empresa.",
    "authors": [
      "Machado, Diogo Alexandre Gonçalves"
    ],
    "keywords": [
      "Cloud-computing",
      "Fornecedores de serviços de cloud",
      "Otimização de custos",
      "Cloud service providers",
      "Cost optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82791",
    "title": "Self-sovereign identity decentralized identifiers, claims and credentials using non decentralized ledger technology",
    "abstract": "Current identity management systems rely on centralized databases to store user’s personal data, which posesa great risks for data security, as these infrastructure create a critical point of failure for the whole system. Besidethat service providers have to bear huge maintenance costs and comply with strict data protection regulations.Self-sovereign identity (SSI) is a new identity management paradigm that tries to answer some of theseproblems by providing a decentralized user-centric identity management system that gives users full control oftheir personal data. Some of its underlying concepts include Decentralized Identifiers (DIDs), Verifiable Claimsand Credentials. This approach does not rely on any central authority to enforce trust as it often uses Blockchainor other Decentralized Ledger Technologies (DLT) as the trust anchor of the system, although other decentralizednetwork or databases could also be used for the same purpose.This thesis focuses on finding alternative solutions to DLT, in the context of SSI. Despite being the most usedsolution some DLTs are known to lack scalability and performance, and since a global identity managementsystem heavily relies on these two requirements it might not be the best solution to the problem.This document provides an overview of the state of the art and main standards of SSI, and then focuses ona non-DLT approach to SSI, referencing non-DLT implementations and alternative decentralized infrastructuresthat can be used to replace DLTs in SSI. It highlights some of the limitations associated with using DLTs foridentity management and presents a SSI framework based on decentralized names systems and networks. Thisframework couples all the main functionalities needed to create different SSI agents, which were showcased ina proof of concept application.",
    "authors": [
      "Oliveira, Bruno Miguel Gomes"
    ],
    "keywords": [
      "Decentralized identity",
      "Self sovereign identity",
      "DLTs",
      "Decentralized networks",
      "Identidade descentralizada",
      "Self sovereign identity",
      "Redes descentralizadas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84140",
    "title": "A meta-learning approach for selecting machine learning algorithms",
    "abstract": "One of the major challenges in Machine Learning is to investigate the capabilities and lim itations of the existing algorithms to identify when one algorithm is more adequate thananother to solve particular problems. Traditional approaches to predicting the performanceof algorithms often involve costly trial-and-error procedures or expert knowledge, which isnot always straightforward to acquire. Thus, the main goal of this dissertation is to supportbeginners or even experienced data scientists by automatically indicating which classifica tion algorithm is most suitable for their datasets.This dissertation proposes the use of Meta-Learning as a possible solution to the above mentioned problem. In this respect, we introduced a novel framework for the automaticgeneration of meta-datasets. Taking advantage of the developed framework, several clas sification datasets from public sources were used. The result is the meta-dataset for theexperiment of this research project.Concerning the goal of forecasting the best model for a classification dataset, two differentsolutions are presented: the first toward binary classification and the second on multiclassclassification. A variety of Machine Learning algorithms are tested and compared throughcross-validation.The experiment confirms the feasibility of applying Meta-Learning to select the algorithmthat is expected to obtain the best performance for classification problems.",
    "authors": [
      "Monteiro, José Pedro Santos"
    ],
    "keywords": [
      "Machine learning",
      "Meta-learning",
      "Metadata",
      "Machine learning algorithms selection",
      "Classification",
      "Data mining",
      "Metadados",
      "Seleção de algoritmos",
      "Problemas de classificação",
      "Análise de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80969",
    "title": "Extração automática de documentos médicos da web para análise textual",
    "abstract": "A literatura científica na biomedicina é um elemento fundamental no processo de obtenção de conhecimento, uma vez que é a maior e mais confiável fonte de informação. Com os avanços tecnológicos e o aumento da competição profissional, o volume e diversidade de documentos médicoscientíficos tem vindo a aumentar consideravelmente, impedindo que os investigadores acompanhem o crescimento da bibliografia. Para contornar esta situação e reduzir o tempo gasto pelos profissionais na extração dos dados e na revisão da literatura, surgiram os conceitos de Web Crawling, Web Scraping e Processamento de Linguagem Natural, que permitem, respetivamente, a procura, extração e processamento automático de grandes quantidades de texto, abrangendo uma maior gama de documentos científicos do que os normalmente analisados de forma manual.O trabalho desenvolvido para a presente dissertação teve como foco principal o rastreamento e recolhade documentos científicos completos, do campo da biomedicina. Como a maioria dos repositórios da web não disponibiliza, gratuitamente, a totalidade de um documento, mas sim apenas o resumo da publicação, foi importante a seleção de uma base de dados adequada. Por este motivo, as páginas web alvo de rastreamento foram restringidas ao domínio dos repositórios da editora BioMed Central, que disponibilizam por completo, milhares de documentos científicos na área da biomedicina.A arquitetura do sistema desenvolvido divide-se em duas partes principais: fase online e a fase offline. A primeira inclui a procura e extração dos URLs das páginas candidatas a serem extraídas, a recolha dos campos de texto pretendidos e o seu armazenamento numa base de dados. A segunda fase consiste no tratamento e limpeza dos documentos recolhidos, deixando-os num formato estruturado e válido para ser utilizado como entrada de qualquer sistema de análise de texto. Para a concretização da primeiraparte, foram utilizadas a framework Scrapy, como base para a construção do scraper, e a base de dados de documentos MongoDB, para o armazenamento das publicações científicas recolhidas. Na segunda etapa do processo, ou seja, na aplicação de técnicas de limpeza e padronização dos dados, foram aproveitadas algumas das inúmeras bibliotecas e funcionalidades que a linguagem Python oferece.Para demonstrar o funcionamento do sistema de extração e tratamento de documentos da área médica, foi estudado o caso prático de recolha de publicações científicas relacionadas com Transtornos Obsessivo Compulsivos. Como resultado de todo o procedimento, foi obtida uma base de dados com quatro coleções de documentos com diferentes níveis de processamento.",
    "authors": [
      "Gomes, Inês Fraga"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92822",
    "title": "Reconhecimento de atividades em smartphones usando sensores não intrusivos",
    "abstract": "O reconhecimento de atividades utilizando smartphones tem ganho uma atenção redobrada nos últimos anos devido à adoção generalizada destes dispositivos e consequentemente dos seus vários sensores. Estes sensores são capazes de fornecer dados bastante relevantes para este fim. Os sensores não intrusivos, em particular, oferecem a vantagem de recolher dados sem exigir ao utilizador a realização de quaisquer ações específicas ou o uso de dispositivos adicionais. Primeiramente são discutidos os diferentes tipos de sensores habitualmente utilizados em smartphones, incluindo acelerómetros, giroscópios, entre outros, e o tipo de informação que nos podem guarnecer. Depois serão apresentados vários algoritmos e técnicas de machine learning, incluindo aprendizagem supervisionada, aprendizagem não supervisionada, e ainda aprendizagem por reforço. Dentro destes são também apresentados alguns dos algoritmos mais utilizados. Para finalizar esta primeira parte, serão ainda apresentadas alguns trabalhos realizados por outros investigadores na área. O objetivo desta tese passou, então, pela criação de uma aplicação destinada ao reconhecimento de atividades recorrendo exclusivamente ao uso de sensores não intrusivos presentes em qualquer smartphone. Os dados colecionados por esses sensores forem submetidos a várias etapas de processamento e, após diversas iterações, obteve-se um conjunto de features altamente favoráveis ao treino dos modelos de machine learning criados. O melhor resultado foi obtido pelo modelo utilizando o algoritmo XGBoost, que alcançou uma impressionante taxa de accuracy de 0.979. Este resultado bastante sólido, permite verificar a alta eficácia do uso deste tipo de sensores para o reconhecimento de atividades.",
    "authors": [
      "Fernandes, Pedro Almeida"
    ],
    "keywords": [
      "Comportamento humano",
      "Dispositivos móveis",
      "Machine learning",
      "Reconhecimento de atividades",
      "Sensorização",
      "Smartphones",
      "Activity recognition",
      "Human behavior",
      "Mobile devices",
      "Sensor data",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92830",
    "title": "Development of a software application based on deep learning to predict drug sensitivity of cancer cell lines",
    "abstract": "With the increase in Deep Learning (DL) popularity, the need for efficient and effective model development and optimization has emerged. Neural Architecture Search (NAS) is a research area that aims to automate the time-consuming and iterative tasks involved in building and tuning DL models. This approach has already proven to be effective in various fields, such as computer vision and natural language processing, and holds potential in the field of cancer treatment. Cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. The high degree of diversity among tumors, along with drug resistance acquired during (or before) treatment, makes it challenging to find effective therapies for all types of cancer. High-throughputscreening (HTS) is a laboratory technique used to rapidly test large numbers of compounds against a biological target or assay to identify potential drug candidates. In cancer research, HTS is often used to identify compounds that have the potential to inhibit the growth of cancer cells or interfere with specific cancer-associated processes. Since the number of compounds with drug-like properties is much greater than the number of compounds that can be analyzed in HTS processes, DL has been used recently to predict drug response based on omics data. This work integrated a DL-based framework capable of handling single and multi-input datasets of various types into the company’s internal platform, where the work was carried out. Using Automated Machine Learning (AutoML) tools to reach this goal, namely tools with NAS capabilities, this new module automates the search for the best DL models that produce the most suitable results across a range of prediction tasks. The effectiveness of the developed work was validated in the context of drug response prediction by comparing the performance of the NAS-generated models with manually tuned models previously developed. The models found using this approach achieved comparable performance without theneed for human expertise, proving the potential of NAS in cancer treatment, but also as a useful tool for automating and optimizing the model selection process in Machine Learning.",
    "authors": [
      "Marreiros, Leonardo de Freitas"
    ],
    "keywords": [
      "Cancer",
      "Deep learning",
      "AutoML",
      "NAS",
      "Drug sensitivity",
      "Drug synergy",
      "Cancro",
      "Aprendizagem profunda",
      "Aprendizagem de máquina automática",
      "Pesquisa de arquiteturas neuronais",
      "Sensibilidade a fármacos",
      "Sinergia farmacológica",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93915",
    "title": "Program corpus analysis to characterize the learning process of the programmers",
    "abstract": "This document is a report for the final project of the Master’s in Informatics Engineeringdegree, accomplished at Universidade do Minho in Braga, Portugal.The project consists, in a first phase, on the study of many snapshots of programs archivedby Nuno Fonseca for his doctoral Thesis \"Contributos para a Monitorização do Desempenho deEstudantes de Programação\" at the Universidade de Coimbra.This study’s main goal is to analyse the code snapshots and explore as much as possible theknowledge that can be extracted from that repository in order to understand the students’behavior while solving problems by computer.The research begins with the gathering and analysis of different types of tools whose purposeis to analyze code, both to assist students in learning and to help teachers in evaluation.After this research, the data provided by professor Nuno Fonseca is deeply analysed andadapted to fit this project’s requirements and fulfill the proposed goals.Finally, a web application that allows the teacher to conduct a comprehensive and systematicanalysis of the learning progression of their students was developed. All documentationassociated with this application was also produced.The outcomes of this thesis are expected to contribute to the field of code teaching tools,with the research made in this field and the resulting web application.",
    "authors": [
      "Lourenço, Gustavo Araújo"
    ],
    "keywords": [
      "Code snapshots",
      "Teaching programming",
      "Student performance monitoring",
      "Student evolution analysis",
      "Snapshots de código",
      "Ensino de programação",
      "Monitorização do desempenho de alunos",
      "Análise da evolução de alunos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84365",
    "title": "Plataforma multidisciplinar de registo e monitorização de cirurgia bariátrica baseada em OpenEHR",
    "abstract": "Desde o primeiro contacto da tecnologia com o universo da saúde, que esta agradável e harmoniosarelação tem vindo a ser cada vez mais poderosa e majestosa. A utilização de sistemas de informaçãona saúde tem servido diversos propósitos, seja na gestão de profissionais de saúde, na descoberta denovas curas, no apoio à tomada de decisão ou simplesmente no armazenamento de todas as informaçõesrespeitantes a este ambiente.A prática clínica é uma ciência multidisciplinar, na qual os profissionais de saúde de diferentes especialidades colaboram em prol do bem comum do utente. Nesta linha de pensamento, emerge a cirurgiabariátrica, encarada como uma abordagem multidisciplinar no tratamento da obesidade. Esta cirurgiaengloba o diagnóstico de essencialmente cinco especialidades distintas, já que a “saúde é o estado decompleto bem-estar físico, mental e social e não somente a ausência de doença, segundo a OrganizaçãoMundial de Saúde”, como tal é imprescindível a constante partilha de informação entre estas, de modoa ser proporcionado um correto e eficaz tratamento ao utente.A quantidade extrema de informação clínica produzida diariamente e de forma constante, a diversidade de fontes de conteúdo e os diferentes formatos dos dados têm constituído grandes desafios de interoperabilidade entre sistemas de informação de saúde. Por conseguinte, a presente temática é pautadapela integração da tecnologia openEHR na plataforma desenvolvida aliada ao registo e à monitorizaçãode informações intrínsecas à cirurgia bariátrica.Face ao mencionado, o grande objetivo na conceção de um projeto desta dimensão concentra-sefundamentalmente em facilitar toda a especificidade do processo inerente à cirurgia bariátrica, quer paraos profissionais de saúde (auxiliando-os nas suas tarefas diárias), quer para os utentes envolvidos (usufruírem de cuidados de saúde mais rápidos e orientados). Desta forma, construiu-se uma plataformaque produziu resultados únicos e inovadores, arrastando consigo a solução útil, necessária e interessantepara o preenchimento de uma profunda lacuna nesta área da saúde.",
    "authors": [
      "Afonso, Ana Beatriz Castro"
    ],
    "keywords": [
      "Cirurgia bariátrica",
      "Obesidade",
      "Sistemas de informação na saúde",
      "Registo de saúde eletrónico",
      "Interoperabilidade semântica",
      "OpenEHR",
      "Utentes",
      "Profissionais de saúde",
      "Bariatric surgery",
      "Obesity",
      "Health information systems",
      "Electronic health record",
      "Semantic interoperability",
      "Patients",
      "Health professionals",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79963",
    "title": "Caracterização de tráfego não solicitado em dispositivos móveis",
    "abstract": "Smartphones increasingly play an important role in everyday life, whether to communicateor perform tasks that require more computing power. This has become an indispensableobject in the lives of many people so their use increases more and more. This growth isassociated with an increase in network traffic due to the existing applications and Internetservices.This increase in traffic is also due to the development and growth of 3G and 4G mobilenetworks, which allow Internet access when out of Wi-Fi networks. Such access requiresa mobile data plan that is normally limited to a certain level. Expiring this plan, access tothe Internet from mobile networks is prohibited or limited to a certain rate which is ofteninadequate for the requirements of the applications used.This disturbance is commonly associated with the consumption of bandwidth in orderto reproduce contents downloaded from the network. These contents are often associatedwith the normal operation of the applications, i.e., expected contents, however, there arecontents that were not requested by the user, for instance advertisements contributing todata plan exhaustion.This justifies the need for studying and understanding the traffic involved between theuser device and the network in order to assist the end user in identifying how much datahas been consumed and distinguishing by the type of traffic involved. To answer this need,a systematic methodology is developed and proposed in this work considering as inputspopular user applications - YouTube, Facebook and Instagram - with potencial impact onthe user data plan. Therefore, the present dissertation is a contribution in the field of trafficanalysis and characterization, shedding light in the process of identifying and measuringtraffic not requested by the user.",
    "authors": [
      "Silva, José Pedro Veiga da"
    ],
    "keywords": [
      "Tráfego de rede",
      "Dispositivos móveis",
      "Caracterização de tráfego",
      "Serviços de internet",
      "Aplicações móveis",
      "Análise de dados",
      "Network traffic",
      "Mobile devices",
      "Traffic characterization",
      "Internet services",
      "Mobile applications",
      "Data analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93043",
    "title": "Co-designing log-structured merge key-value stores with a non-volatile storage hierarchy",
    "abstract": "The trend of increasing size of datasets in storage-based applications has promoted the research of newmethods and technologies for efficiently storing, processing, and analyzing large amounts of data. As aresult, Log Structured Merge (LSM) Key-Value Stores (KVSs) have been highly adopted since theirdesign allows high write throughput and enforces sequential disk access patterns. Additionally, with theadvent of Non-Volatile Main Memory (NVMM), new storage technologies have emerged that offerfaster access times compared to traditional block-based storage devices, thus accelerating KVSs.However, while NVMM devices offer faster access to data, they are typically limited in capacity and areoften more expensive. To address this trade-off, contemporary storage solutions harness the capabilities ofheterogeneous storage devices in two fundamental manners: caching and tiering. In this dissertation, weshow that, on one hand, read-dominated workloads benefit from a caching approach, but their performancedegrades under tiering. On the other hand, for write-dominated workloads, the tiering approach presentsbetter performance, while storing the entire dataset on NVMM actually degrades performance.To overcome these challenges, this dissertation proposes KEIGO, a novel storage middleware that al lows LSM-based KVS to efficiently use storage hierarchies composed of NVMM and block-based devices.KEIGO is aware of the different I/O operations done by the KVS (e.g., foreground requests, and backgroundflushes and compactions) and the characteristics of the underlying devices (e.g., concurrency, read/writeasymmetry). This knowledge serves as a pivotal factor in optimizing KEIGO’s performance in the face ofdynamic and mixed production workloads such as those observed in Nutanix and Meta. Moreover, KEIGOrequires minimal code modifications to integrate into production-ready LSM KVSs.Conducted experiments show that KEIGO significantly enhances the throughput of LSM KVS solu tions, including RocksDB, Speedb, and LevelDB, by as much as 12.4×. Furthermore, it substantiallyreduces tail latency by up to 21.3× over both general-purpose storage solutions and LSM KVSs builtfrom the ground up for hierarchical storage.",
    "authors": [
      "Adão, Rúben Daniel Almeida"
    ],
    "keywords": [
      "Key-value store",
      "NVMM",
      "Persistent memory",
      "Hierarchical storage",
      "Armazenamento chave-valor",
      "Memória persistente",
      "Armazenamento hierárquico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27891",
    "title": "Credit scoring as an asset for decision making in intelligent decision support systems",
    "abstract": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.Historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.The work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.At last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
    "authors": [
      "Silva, Fábio"
    ],
    "keywords": [
      "519.86"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "519.86"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84005",
    "title": "Monitoring and optimization of an autonomous learning system",
    "abstract": "In the last years, the number of Machine Learning algorithms and their parameters has increased significantly.This allows for more accurate models to be found, but it also increases the complexity of the task of training amodel, as the search space expands significantly.As datasets keep growing in size, traditional approaches based on extensive search start to become costlyin terms of computational resources and time, especially in data streaming scenarios. With this growth, newchallenges in Machine Learning started to appear. The speed at which data arrives and different ways of storingdata are forcing organizations to address and explore new ways of adapting fast enough so their ML modelsdon’t become obsolete.This dissertation aims to develop an approach based on meta-learning that tackles two main challenges: predict ing the performance metrics of a future model and recommending the best algorithm/configuration for traininga model for a specific Machine Learning problem. Throughout this dissertation, all the study objectives andquestions, along with the relevant contextualization will be exposed.The proposed solution, when compared to an AutoML approach is up to 130x faster and only 2% worse in termsof average model quality, showing it is a good solution for scenarios in which models need to be updated regularly,such as in streaming scenarios with Big Data, in which some accuracy can be traded for a much shorter modeltraining time.",
    "authors": [
      "Palumbo, Guilherme Fraga"
    ],
    "keywords": [
      "Machine Learning",
      "Meta-Learning",
      "Optimization",
      "Monitoring",
      "Algorithm recommendation",
      "Aprendizagem automática",
      "Meta aprendizagem",
      "Otimização",
      "Monitorização",
      "Recomendação de algoritmos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/87196",
    "title": "Accelerating deep learning training on high-performance computing with storage tiering",
    "abstract": "Deep Learning (DL) has become fundamental to the advancement of several areas, such as computervision, natural language processing and expert systems. Utilizing DL techniques demands vast amountsof data and processing power, which raises challenges to the training performance of DL models. High Performance Computing (HPC) systems are becoming increasingly popular to support DL training, byoffering extensive computing capabilities, however, due to convenience and usability, many DL jobs runningon these infrastructures resort to the shared Parallel File System (PFS) for storing and accessing trainingdata. Under such scenario, where multiple Input/Output (I/O)-intensive applications operate concurrently,the PFS can quickly get saturated with simultaneous storage requests and become a critical performancebottleneck, leading to throughput variability and performance loss.To solve these issues, this dissertation presents a storage middleware agnostic to any DL solution,Monarch, that deploys storage tiering to accelerate DL models’ training performance and decrease the I/Opressure imposed over the PFS. It leverages from existing storage tiers of supercomputers (e.g., computenode’s local storage, shared PFS), as well as the I/O patterns of DL solutions to improve data placementacross storage tiers. Furthermore, this middleware is non-intrusive and easily installed in HPC centers,thus enabling its wide adoption and applicability.The performance and applicability of Monarch are validated with the TensorFlow and PyTorch DLframeworks. Results show that, when the training dataset can only be partially stored at the local storagetier, Monarch decreases TensorFlow’s and PyTorch’s training time by up to 28% and 37% for I/O-intensivemodels, respectively. Furthermore, Monarch can reduce the number of I/O operations submitted to thePFS by up to 56%.",
    "authors": [
      "Dantas, Marco Filipe Leitão"
    ],
    "keywords": [
      "I/O optimization",
      "Storage tiering",
      "Deep learning",
      "Otimização de E/S",
      "Armazenamento por camadas",
      "Aprendizagem profunda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/38683",
    "title": "Monitoring and analysis of queries in distributed databases",
    "abstract": "Scalable database services combining multiple technologies, including SQL and NoSQL, areincreasingly in vogue. In this context, the CoherentPaaS research project aims at providingan integrated platform with multiple data management technologies, united by a commonquery language and global transactional coherence.For this integration to succeed, it must provide the same monitoring capabilities of traditional relational databases, namely, for database administrators to optimise its operation.However, achieving this in a distributed and heterogeneous system is in itself a challenge.This work proposes a solution to this problem with X-Ray, that allows monitoring code tobe added to a Java-based distributed system by manipulating its bytecode at runtime. Theresulting information is collected in a NoSQL database and then processed and visualisedgraphically. This system is evaluated experimentally by adding monitoring to Apache Derbyand tested with the standard TPC-C benchmark workload.",
    "authors": [
      "Guimarães, Pedro Miguel Pimentel"
    ],
    "keywords": [
      "Distributed databases",
      "CoherentPaaS",
      "Query monitoring",
      "Query analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79826",
    "title": "Process Mining na análise de tráfego de uma rede de comunicações",
    "abstract": "Nos primórdios a Internet era usada apenas por algumas pessoas. Nessa altura muitas das grandes empresas de tecnologia ainda não tinham aparecido. Porém tudo mudou quando a Internet entrou nos circuitos comerciais, provocando o aparecimento de estruturas para fazer a ligação das pessoas ao seu mundo. Desde aí que as grandes empresas têm adotado novas formas de encarar o mercado, aumentando gradualmente a sofisticação da forma de o fazerem. As empresas começaram a monitorizar a atividade dos seus clientes para que com isso melhorar a oferta dos seus bens e serviços ao público em geral. Todavia o conhecimento acerca daquilo que o cliente gosta não é suficiente para o atrair. Uma empresa também precisa que a informação apresentada ao cliente seja feita da maneira mais rápida possível. Por exemplo, se um cliente esperar mais do que “três” segundos para que o site seja carregado, o cliente irá abandoná-lo e, provavelmente, procurar um outro site de uma empresa concorrente. A Google avalia a rapidez dos sites e com isso dá-lhes uma pontuação. Os sites com piores pontuações são apresentados em últimos, o que tem, como sabemos, um grande impacto na escolha dos clientes. Mas não é só com os clientes que as empresas se tem de preocupar. Internamente os serviços dos funcionários de uma empresa podem ser afetados por uma Internet lenta, o que conduz a uma perda de performance e ao aumento da frustração do próprio funcionário no local de trabalho. Por estas razões é importante que as empresas estejam constantemente a monitorizar o tráfego passado pelos seus servidores, para serem capazes de verificar se os motivos da lentidão dos seus serviços de rede são internos ou não. Neste trabalho de dissertação desenvolvemos um trabalho baseadoem process mining, que através de uma ferramenta de monitorização de rede, wireshark, permite avaliar a qualidade de serviço da rede através da observação e análise das logs produzidas por alguns dos seus equipamentos, em particular, dos seus routers. Como são geradas várias logs para cada um tipo de router foi necessário fazer a sua conciliação, para que, a partir daí, se pudesse obter o percurso que os vários pacotes realizaram na sua movimentação pela rede. Desta forma, é possível criar um modelo matemático capaz de determinar um índice de bem-estar relativo à qualidade de serviço da rede de uma empresa. Basicamente, este índice permitirá avaliar odesempenho da rede e permitir aos seus gestores identificar, por exemplo, quais os pontos da rede que apresentam menor desempenho (ou estrangulamentos de serviço) e prevenir futuras quebras no serviço geral da rede em análise.",
    "authors": [
      "Gonçalves, Gil"
    ],
    "keywords": [
      "Process Mining",
      "Ferramentas de análise de tráfego de rede",
      "Índice de qualidade",
      "Smart Cities",
      "Data warehouse",
      "Network monitoring and analysis tools",
      "Index quality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27921",
    "title": "Selecção de hiper-cubos com base em padrões de exploração OLAP",
    "abstract": "Na literatura do domínio do processamento analítico de dados facilmente se podem encontrar métodos e soluções que respondem ao problema de seleção de vistas multidimensionais no processo de implementação de um cubo OLAP. Uma forma que se evidencia como sendo extremamente vantajosa, é a de fazer a seleção baseada em critérios que se apoiem essencialmente nos conteúdos que são consultados sobre o cubo de dados ao longo das sessões de consulta OLAP. As principais vantagens que advêm desta monitorização, está relacionada com a possibilidade de efetuar correspondências rigorosas com a informação em que os agentes de decisão mais se apoiam para efetuar as suas tomadas de decisão. Ao ser feita a identificação da informação que se evidencia como sendo a mais relevante, ou pelo menos a mais frequentemente consultada, várias ilações se podem retirar, como, por exemplo, a definição de perfis de utilização, a expressão de preferências, a identificação de metodologias de trabalho, ou então a definição de processos que procurem construir cubos iceberg com forte probabilidade de explorações futuras sobre o cubo. Este último aspeto constitui, basicamente, o trabalho desta dissertação. Ao se efetuar a materialização dos conteúdos mais pesquisados no servidor OLAP, obtém-se um melhor desempenho ao nível do servidor, uma vez que o preparamos antecipadamente com os dados que mais vezes são solicitados, reduzindo assim o número de vezes que seria necessário recorrer ao data warehouse para retornar os resultados pretendidos por uma dada query multidimensional. Em termos gerais, neste trabalho de dissertação, desenvolveu-se um estudo detalhado acerca das ideias e práticas que levam ao desenvolvimento de um dado método de seleção, que seja capaz de indicar de forma precisa as partes de um cubo que são mais utilizadas, sugerindo com base nessa informação uma nova estrutura para o cubo em questão que utilize menos recursos computacionais, nomeadamente espaço em disco e tempo de processamento.",
    "authors": [
      "Rocha, Daniel"
    ],
    "keywords": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80087",
    "title": "Development of a web-based platform for Biomedical Text Mining",
    "abstract": "Biomedical Text Mining (BTM) seeks to derive high-quality information from literature in the biomedical domain, by creating tools/methodologies that can automate time-consuming tasks when searching for new information. This encompasses both Information Retrieval, the discovery and recovery of relevant documents, and Information Extraction, the capability to extract knowledge from text. In the last years, SilicoLife, with the collaborationof the University of Minho, has been developing @Note2, an open-source Java-based multiplatform BTM workbench, including libraries to perform the main BTM tasks, also provid ing user-friendly interfaces through a stand-alone application.This work addressed the development of a web-based software platform that is able toaddress some of the main tasks within BTM, supported by the existing core libraries fromthe @Note project. This included the improvement of the available RESTful server, providingsome new methods and APIs, and improving others, while also developing a web-basedapplication through calls to the API provided by the server and providing a functionaluser-friendly web-based interface.This work focused on the development of tasks related with Information Retrieval, addressing the efficient search of relevant documents through an integrated interface. Also, atthis stage the aim was to have interfaces to visualize and explore the main entities involvedin BTM: queries, documents, corpora, annotation processes entities and resources.",
    "authors": [
      "Fernandes, Emanuel Queiroga Amorim"
    ],
    "keywords": [
      "Aplicação baseada na Web",
      "Mineração de literatura biomédica",
      "Recuperação de informação",
      "Biomedical Text Mining",
      "Information retrieval",
      "Web based application",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84345",
    "title": "Formalização da reconfiguração de protocolos de consenso usando Alloy",
    "abstract": "O protocolo de máquinas de estado replicadas (MER) é uma peça fundamental dos sistemas distribuídos. Nocentro deste protocolo estão os algoritmos de consenso, como o Paxos, usados para manter a consistência dasMER. Todavia, os sistemas modernos não podem depender estritamente das técnicas de MER, estes devemtambém implementar estratégias de reconfiguração. Estas estratégias consistem em alterar a configuração dosistema, adicionando, removendo ou substituindo os processos que o compõem. Dada a sua complexidade, aimplementação de protocolos de reconfiguração é muito suscetível a erros, daí que seja aconselhável a especificação,validação e verificação dos mesmos.No presente trabalho apresentamos uma especificação em linguagem Alloy do protocolo de reconfiguraçãoVertical Paxos e do protocolo de consenso Paxos. Além destes, modelamos o protocolo Multi-Paxos, o qualimplementa uma MER. Estes protocolos estão intrinsecamente relacionados e a compreensão do primeiro éfacilitada com o conhecimento dos demais. Atualmente, o Alloy é uma das linguagens de especificação maispopulares, mas pouco explorada na modelação de algoritmos distribuídos e, tanto quanto sabemos, não existeainda nenhuma especificação dos referidos protocolos em Alloy.O presente trabalho visa modelar e validar os referidos protocolos, bem como verificar as suas propriedadesde safety, de modo a obtermos confiança nas especificações. Ademais, realizamos uma avaliação de desempenhode diferentes solvers e estratégias de decomposição nativas do Alloy, bem como uma breve análisecomparativa com o TLA+.",
    "authors": [
      "Soares, Cecília da Conceição de Oliveira"
    ],
    "keywords": [
      "Alloy",
      "MER",
      "Multi-Paxos",
      "Paxos",
      "Vertical Paxos",
      "SMR",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47844",
    "title": "Serviço safe-return-home baseado em análise de big data em tempo real",
    "abstract": "Na nossa sociedade existem várias formas de crimes que ocorrem com bastante frequência.No combate ao crime, vários sistemas safe-return estão a ser desenvolvidos para pessoasvulneráveis. Contudo estes sistemas focam-se principalmente na resposta a situações emque o perigo já aconteceu, não é feita nenhuma previsão de um possível crime. Estasprevisões de situações de perigo podem ser feitas com a informação recolhida, relativa àsproximidades do utilizador, em tempo real.Um sistema de safe-return-home deve permitir que os utilizadores do serviço sejam notificados,atempadamente, de possíveis situações perigosas que ocorram nas proximidadesda localização do utilizador, em tempo real. Estas previsões são feitas através da análisede vários tipos de dados relativos ao que se encontra em redor do utilizador. A recolhae análise de dados deverá incluir informação relativa a notícias relacionadas com crimes,acidentes e desastres, incluindo também a localização do utilizador. A análise dos dadospermite então fazer uma previsão de perigos, de forma que possam ser visualmente reconhecidosno smartphone do utilizador, sendo assim possível reagir de forma ativa a essesperigos, em tempo real.",
    "authors": [
      "Malheiro, Tarcísio"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47706",
    "title": "Medieval coin automatic recognition by computer vision",
    "abstract": "The use of computer vision for identification and recognition of coins is well studied and of renowned interest. However the focus of research has consistently been on modern coins and the used algorithms present quite disappointing results when applied to ancient coins. This discrepancy is explained by the nature of ancient coins that are manually minted, having plenty variances, failures, ripples and centuries of degradation which further deform the characteristic patterns, making their identification a hard task even for humans. Another noteworthy factor in almost all similar studies is the controlled environments and uniform illumination of all images of the datasets. Though it makes sense to focus on the more problematic variables, this is an impossible premise to find outside the researchers’ laboratory, therefore a problematic that must be approached.This dissertation focuses on medieval and ancient coin recognition in uncontrolled “real world” images, thus trying to pave way to the use of vast repositories of coin images all over the internet that could be used to make our algorithms more robust.The first part of the dissertation proposes a fast and automatic method to segment ancient coins over complex backgrounds using a Histogram Backprojection approach combined with edge detection methods. Results are compared against an automation of GrabCut algorithm. The proposed method achieves a Good or Acceptable rate on 76% of the images, taking an average of 0.29s per image, against 49% in 19.58s for GrabCut. Although this work is oriented to ancient coin segmentation, the method can also be used in other contexts presenting thin objects with uniform colors.In the second part, several state of the art machine learning algorithms are compared in the search for the most promising approach to classify these challenging coins. The best results are achieved using dense SIFT descriptors organized into Bags of Visual Words, and using Support Vector Machine or Naïve Bayes as machine learning strategies.",
    "authors": [
      "Salgado, Luís"
    ],
    "keywords": [
      "Computer vision",
      "Image classification",
      "Image segmentation",
      "Machine learning",
      "Coins",
      "Visão por computador",
      "Classificação de imagens",
      "Segmentação de imagens",
      "Moedas"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84363",
    "title": "Automated and intelligent hacking detection system",
    "abstract": "The Controller Area Network (CAN) is the backbone of automotive networking, connecting many Electronic ControlUnits (ECUs) that control virtually every vehicle function from fuel injection to parking sensors. It possesses,however, no security functionality such as message encryption or authentication by default. Attackers can easily inject or modify packets in the network, causing vehicle malfunction and endangering the driver and passengers. There is an increasing number of ECUs in modern vehicles, primarily driven by the consumer’s expectation of more features and comfort in their vehicles as well as ever-stricter government regulations on efficiency and emissions. Combined with vehicle connectivity to the exterior via Bluetooth, Wi-Fi, or cellular, this raises the risk of attacks. Traditional networks, such as Internet Protocol (IP), typically have an Intrusion Detection System (IDS) analysing traffic and signalling when an attack occurs. The system here proposed is an adaptation of the traditional IDS into the CAN bus using a One Class Support Vector Machine (OCSVM) trained with live, attack-free traffic. The system is capable of reliably detecting a variety of attacks, both known and unknown, without needing to understand payload syntax, which is largely proprietary and vehicle/model dependent. This allows it to be installed in any vehicle in a plug-and-play fashion while maintaining a large degree of accuracy with very few false positives.",
    "authors": [
      "Carvalho, Bruno Alves Martins"
    ],
    "keywords": [
      "Intrusion detection system",
      "Machine learning",
      "Controller area network",
      "Connected vehicles",
      "Sistema de deteção de intrusão",
      "Aprendizagem máquina",
      "Controller area network",
      "Veículos conectados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82581",
    "title": "Cidades Inteligentes - Sistema de Monitorização de Pavimento e Tráfego",
    "abstract": "Na atual conjuntura em que vivemos é crucial que seja realizado um estudo aprofundadosobre a utilização dos recursos da cidade, especialmente em zonas urbanas. Além disso,uma rede de estradas bem conservada deveria ser uma prioridade para o desenvolvimentoeconómico e bem-estar dos habitantes de qualquer país.Com o desenvolvimento da tecnologia, em particular na área das Cidades Inteligentes(“Smart cities”), é importante implementar sistemas de apoio, de modo a centralizar toda ainformação existente sobre um aspeto de interesse, de maneira a “virtualizar” as cidades.Neste trabalho pretende-se desenvolver esse suporte em torno de uma rede de transportespúblicos, compilando toda a informação que essa rede nos poderá fornecer para que possaser útil a várias entidades.Neste contexto, a presente dissertação pretende estudar duas vertentes no âmbito dasCidades Inteligentes. Na primeira será realizado um estudo do estado do pavimento atravésde sensores de aceleração (acelerómetros), sendo estes sensores colocados numa rede urbanade transportes públicos já existentes, assim a rota será conhecida pela entidade responsável,como por exemplo a Câmara Municipal do distrito em questão. Assim, espera-se quecom esta informação seja possível uma tomada de decisão mais adequada, face ao estadodo pavimento, de maneira a que possam ser realizados os trabalhos necessários para areconstrução do mesmo. Na segunda vertente será realizado um estudo da monitorizaçãodo trânsito em áreas em que o fluxo é elevado, com base na mesma rede de transportespúblicos, sendo a informação obtida através dos mesmos sensores.",
    "authors": [
      "Rodrigues, Vítor Hugo de Castro"
    ],
    "keywords": [
      "Acelerómetros",
      "Cidades Inteligentes",
      "Estado do pavimento",
      "Monitorização de trânsito",
      "Accelerometers sensors",
      "Road conditions",
      "Smart Cities",
      "Traffic monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94077",
    "title": "Ferramenta de criação de lojas online para programadores baseada em NextJS",
    "abstract": "A presente dissertação baseia-se na expectativa de diminuir o trabalho e tempo necessário para o desenvolvimento de uma loja online por programadores. Com a evolução do comércio, começaram a surgir cada vez mais negócios com lojas online, sendo por isso cada vez maiores as exigências e condições para que estas se tornassem um sucesso. Todas as lojas online costumam ter aspetos em comum como o carrinho de compras, a autenticação, entre outros componentes básicos sobre os quais se poderia salvar tempo de desenvolvimento se não se tivesse de reescrever sempre o mesmo código. Esta dissertação teve início com a investigação dos parâmetros necessários para que uma loja on-line funcionasse, pelo que em seguida foi desenvolvido uma loja online de venda de jogos e produtos relacionados. Após a investigação foi possível identificar vários aspetos e parâmetros necessários para odesenvolvimento web tanto no frontend como no backend, conseguindo-se assim fazer a distinção entre componentes e as suas relações.De seguida foi abordado o principal objetivo da dissertação que consistia em desenvolver uma ferramenta de criação de lojas online automática seguindo as configurações de diferentes programadores. Esta ferramenta pretende diminuir o tempo despendido pelos programadores em cada novo projeto, ao lhes dar a oportunidade de obter o código de lojas online com diferentes layouts e diferentes parâmetros definidos por eles. Após a criação deste projeto template usando a configuração dos programadores, o objetivo é que eles o adaptem aos pedidos dos seus clientes de uma forma simples, tendo sido usada uma programação modular ao longo deste projeto para facilitar a sua utilização. O projeto desenvolvido nesta dissertação focou-se na autenticação, algumas páginas relacionadas com o produto, na organização do código, na programação modular e na capacidade de os inputs dos programadores alterarem o resultado da ferramenta. No final deste trabalho, foi possível obter uma framework funcional com os pontos anteriormente mencionados, pelo que se atingiu o principal objetivo de desenvolver uma ferramenta que simplificasse o trabalho dos programadores.",
    "authors": [
      "Ferreira, José Carlos Peixoto"
    ],
    "keywords": [
      "Backend",
      "Desenvolvimento web",
      "Frontend",
      "Loja online",
      "Programação modular",
      "Web development",
      "Online store",
      "Modular programming",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82454",
    "title": "Padrões arquitecturais e de desenho para aplicações paralelas",
    "abstract": "Parallel programming is becoming increasingly common, given the evolution of hardwareinto multicore and manycore architectures. To take advantage of these architectures it isnecessary to develop parallel code, since the automatic generation of parallel code from asequential base code is not a viable option.Parallel programming tends to be complex and therefore the developed code tends to bedifficult to maintain or even difficult to reuse.This dissertation aims to explore the use of algorithmic skeletons in the developmentof parallel applications in order to allow faster development as well as the production ofhigher quality final software. A skeleton framework was developed. The skeletons weredeveloped through a technique called ”mixin layers”, which makes use of generic classes ofC++. A great emphasis wass given to the use of generic classes and other alternatives inthe construction of the skeletons.Finally, the performance of these solutions were evaluated in comparison with solutionsalready consolidated within parallel computing. For that, the codes of a set of applicationswere restructured in order to make use of the skeletons provided by the library",
    "authors": [
      "Leitão, Vasco Luzio"
    ],
    "keywords": [
      "Software architecture",
      "HPC",
      "Algoritmic skeletons",
      "Arquitectura de software",
      "HPC",
      "Esqueletos algorítmicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83227",
    "title": "Concepção e desenvolvimento de uma API REST com incorporação de mecanismos de segurança aplicacional",
    "abstract": "The constant technological evolution of the last decades makes more and more companies to focus on providing more resources to their customers, showing new perspectives for the development of solutions with highlevels of performance, availability, scalability and flexibility.One of the biggest contributions in this regard was the appearance of Application Programming Interfaces(APIs), increasingly crucial as integration, automation and efficiency become more important. With the abruptemergence of APIs, API security has become a significant topic in the tech world. If an API does not haveadequate security, it can be vulnerable to attacks that can compromise a company’s data or system. Securityshould be considered from the beginning of any API development project and built into each step of the processto ensure that the API is adequately protected.In this dissertation we intended to investigate the functioning of APIs, with special focus on the Representational State Transfer (REST) architecture and their security, allowing us to verify that, despite several techniquesand tools for the creation of solid and robust REST APIs have already been studied in detail and applied to awide variety of domains, REST services still need practical approaches specialized in the design and security oftheir APIs. It is proposed to fill this gap with the definition of a set of metrics capable of helping in the creation ofa REST API with good design principles and absent of any vulnerabilities.In the context of UN1Qnx as a company that develops authenticity solutions, an IT infrastructure capable ofhandling multiple customers and systems is essential for its business.Bearing this need in mind, the opportunity arose to implement in practice the result of all the research carriedout throughout the dissertation through the development of an Application Programming Interface (API) thatfollows the principles of architectural style based on REST in order to allow managing the data flow of theUN1Qnx system together with the definition of mechanisms to integrate the entire UN1Qnx service with third-party applications and services in order to automate procedures for creating and changing data.",
    "authors": [
      "Gonçalves, André da Silva"
    ],
    "keywords": [
      "API",
      "REST",
      "RESTful",
      "Web services",
      "Web security",
      "Serviços web",
      "Segurança web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47314",
    "title": "Delay tolerant networks with traffic differentiation capabilities",
    "abstract": "In the last few decades, an increasing growth of Internet usage was witnessed worldwide.However, infrastructures do not always allow the existence of Internet connectivity everywhere.Therefore, to address this issue, the concept of Delay Tolerant Networks (DTNs) wasdeveloped. DTNs purpose is to provide a different level of intermittent connectivity, dissimulatingconnection problems that arise in complex connectivity scenarios. Examples of suchscenarios are, for instance, cities, where cars exchange information about their location; inunderdeveloped countries, where Internet is inexistent; in freeways, where is not viable toprovide infrastructures for a continuous connectivity, but cars, tolls, and services need to beaware of each other. Thus, DTNs constitute a possible solution for all the aforementionedcommunication environments.However, DTNs still faces some obstacles in terms of delivering a service with quality as itlacks specific mechanisms, such as traffic differentiation. Traffic differentiation is essential toprovide different levels of service quality regarding delivering of messages. Current proposalsto improve service delivery through traffic differentiation on DTNs are still under developmentor lack the proper testing and simulation. The main focus of these proposals is on buffermanagement mechanisms at each DTN node, instead of message prioritisation mechanisms.Message prioritisation allows some messages to be prioritised over others, improving thedelivery rate and, therefore, increasing the probability of a message being correctly delivered.The present thesis implements traffic differentiation in DTNs based on prioritisation strategies,assuming a clear alternative to other buffer management proposals and message prioritisation.Using The One simulation tool, three popular DTNs routing protocols (Epidemic,Spray & Wait, and PRoPHET) are adapted to comply with traffic differentiation. The DTNstraffic prioritisation objective is achieved by designing, implementing and testing four distinctalgorithms that classify and order messages according to their priority levels. Thesealgorithms are based and extend some traditional traffic differentiation mechanisms, namelythe well-known Priority Queuing and Weighted Round Robin strategies.Results from the simulation tests corroborate that the delivery rate of the messages isaffected according to their priorities. Specifically, the simulation shows an increase in thedelivery rate of high priority messages, with low impact on the total number of messages delivered,comparatively to the same scenario without differentiation capabilities. To conclude,DTNs can effectively benefit from traffic differentiation based on message prioritisation techniques,being a promising approach to improve service quality levels in such scenarios.",
    "authors": [
      "Mendes, Nuno Filipe"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27906",
    "title": "Descoberta de padrões num sistema de certificação de empresas",
    "abstract": "A mineração de dados tem como principal objetivo descobrir padrões escondidos num conjunto de dados, isto é, informação útil que ajude a tomar decisões sobre um determinado tema. Nesta dissertação usa-se a mineração numa base de dados que descreve os certificados adquiridos pelas empresas portuguesas no período 2008-2010. A certificação é um processo voluntário, que, apesar de ser bastante demorado, custoso e envolver demasiada burocracia, pode ser crucial para a sobrevivência das empresas. Os certificados demonstram o compromisso da empresa com a qualidade dos produtos e dos serviços, e com o meio ambiente, a saúde e a segurança dos trabalhadores, entre outros. Quais os certificados que uma empresa deve adquirir é uma das principais questões que se colocam a uma nova empresa no mercado. A resposta a esta pergunta pode variar com vários fatores, como a região onde a empresa se encontra localizada ou o seu sector de atividade. É necessária uma análise prévia que forneça um conhecimento do estado do mercado para tomar as melhores decisões. A aplicação de técnicas de mineração de dados permite obter uma descrição do estado do mercado e uma previsão de quais os certificados a adquirir consoante as características de uma empresa. A informação extraída facilita a tomada de decisão relativamente ao conjunto de certificados que melhor se adapta às características da empresa. Este conjunto de certificados varia com a competitividade resultante do número de empresas na região e do número de empresas no sector de atividade em que a nova empresa se encontra inserida. Os resultados apresentados nesta dissertação fornecem às novas empresas uma orientação inicial no mercado competitivo.",
    "authors": [
      "Carvalho, Mariana Reimão Queiroga Valério de"
    ],
    "keywords": [
      "Mineração de Dados",
      "Processos de certificação de Empresas",
      "Classificação e Segmentação de Dados",
      "Descoberta de Padrões de Certificação",
      "Data Mining",
      "Process of Certification of Companies",
      "Data Classification and Data Clustering",
      "Discovery of Patterns of Certification",
      "658.56",
      "681.3:658.56",
      "658.56:681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "658.56",
      "681.3:658.56",
      "658.56:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64163",
    "title": "Developing deep learning computational tools for cancer using omics data",
    "abstract": "There has been an increasing investment in cancer research that generated an enormous amount of biological and clinical data, especially after the advent of the next-generation sequencing technologies. To analyze the large datasets provided by omics data of cancer samples, scientists have successfully been recurring to machine learning algorithms, identifyingpatterns and developing models by using statistical techniques to make accuratepredictions.Deep learning is a branch of machine learning, best known by its applications in artificialintelligence (computer vision, speech recognition, natural language processing androbotics). In general, deep learning models differ from machine learning “shallow” methods(single hidden layer) because they recur to multiple layers of abstraction. In this way, itis possible to learn high level features and complex relations in the given data.Given the context specified above, the main target of this work is the development andevaluation of deep learning methods for the analysis of cancer omics datasets, covering bothunsupervised methods for feature generation from different types of data, and supervisedmethods to address cancer diagnostics and prognostic predictions.We worked with a Neuroblastoma (NB) dataset from two different platforms (RNA-Seqand microarrays) and developed both supervised (Deep Neural Networks (DNN), Multi-TaskDeep Neural Network (MT-DNN)) and unsupervised (Stacked Denoising Autoencoders (SDA))deep architectures, and compared them with shallow traditional algorithms.Overall we achieved promising results with deep learning on both platforms, meaningthat it is possible to retrieve the advantages of deep learning models on cancer omics data.At the same time we faced some difficulties related to the complexity and computationalpower requirements, as well as the lack of samples to truly benefit from the deep architectures.There was generated code that can be applied to other datasets, wich is available in agithub repository https://github.com/lmpeixoto/deepl_learning [49].",
    "authors": [
      "Peixoto, Luís Miguel da Cunha"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81078",
    "title": "RSafeFS: sistema de ficheiros modular para armazenamento remoto",
    "abstract": "File systems are widely used for storing digital information, as they offer abstractions thatallow data to be intuitively separated and organized through files and directories, accordingto the requirements of applications and users. The continuous growth of data volume andcomplexity leads to the constant evolution of these systems. However, the complexity ofintegration of new features and lack of continuous support, leads to many file systems notbeing adopted in practice.In this sense, stackable file systems have emerged, which allow the development of complexfile systems, providing existing systems with new functionalities through independentprocessing layers. Despite this, the development of these systems presents some challenges,namely in terms of speed of implementation, portability, and resilience, since they aredeveloped in kernel. In this way, later solutions emerged that allowed the development offile systems in user space, thus mitigating some of the problems identified in the developmentof this type of file systems. However, these solutions have not been properly explored in thedevelopment of remote file systems.Therefore, this dissertation presents RSafeFS, a platform that extends the SafeFS system toallow developing modular, flexible and extensible remote file systems in user space. Theproposed solution enables extensible remote file system implementations that adjust to therequirements of different types of applications and storage workloads. It was then necessaryto develop a layer that would allow an RSafeFS instance to operate as a system server, anda communication layer, based on remote procedure calls (RPCs), to allow interoperabilitybetween client and server instances. To demonstrate the ease of integration of new features,taking advantage of the modularity and flexibility of RSafeFS, the developed prototype wasequipped with two layers of caching, namely data and metadata, which aim to improvesystem peformance. The results obtained with this prototype reveal that the file systemsdeveloped through RSafeFS obtain performances comparable to remote storage solutionsbased on FUSE. Furthermore, with the processing layers developed it is possible to adjustthe system to different types of workloads, allowing, for example, to improve systemperformance by 1.5× in certain workloads.",
    "authors": [
      "Leitão, Diogo Lúzio"
    ],
    "keywords": [
      "Storage",
      "Remote",
      "Modular",
      "Flexible",
      "Extensible",
      "Armazenamento",
      "Remoto",
      "Flexível",
      "Extensível",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81020",
    "title": "Scalable trace analysis of distributed systems: finding data races",
    "abstract": "Distributed Systems and Protocols are widely employed in the infrastructure that supportsthe Internet and the services available online such as streaming services and social networks.At the same time, they are well known for usually being hard to implement correctly, evenwhen this task is left to experienced programmers. Consequently, Distributed Systems areprone to suffer from distributed concurrency bugs, which are a frequent source of significantservice outages. Thus, it is of the utmost importance to ensure that widely-used distributedsystems are reliable and do not suffer from this kind of bugs.Formal Verification looks like a promising way to achieve this. However, we argue thatthe currently available techniques require too much of an investment in order to verifycorrectness of implementations of complex distributed systems. Instead, we defend theusage of clever testing techniques and tools for all but the most critical of contexts. In thisdissertation, we present one such tool – Spider – designed to automatically detect data racesfrom traced executions of distributed systems. Data races originate when two memoryaccesses to the same memory location occur concurrently and they have been shown to be amajor source of concurrency bugs in distributed systems. Unfortunately, data races are oftentriggered by non-deterministic event orderings that are hard to detect when testing complexdistributed systems.Spider encodes the causal relations between the events in the trace as a symbolic constraintmodel, which is then fed into an SMT solver to check for the presence of conflictingconcurrent accesses. To reduce the constraint solving time, Spider employs a pruningtechnique aimed at removing redundant portions of the trace. Our experiments withmultiple benchmarks show that Spider is effective in detecting data races in distributedexecutions in a practical amount of time, providing evidence of its usefulness as a testingtool.",
    "authors": [
      "Pereira, João Carlos Mendes"
    ],
    "keywords": [
      "Distributed systems",
      "Satisfiability modulo theories",
      "Software testing",
      "Offline monitoring",
      "Dynamic race detection",
      "Sistemas distribuídos",
      "Teste de software",
      "Monitorização offline",
      "Deteção dinâmica de races",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59723",
    "title": "Mobile patient relationship mangement: a case study",
    "abstract": "Há cerca de duas décadas atrás, grande parte dos hospitais portugueses começaram autilizar um sistema de notificação de pacientes, para os alertar dos seus eventos médicosnos hospitais, como consultas, cirurgias, exames, tratamentos, entre outros, através de mensagensde texto. Este sistema de notificação é ainda usado nos dias de hoje, mas enfrentaum grande problema: a quantidade de dinheiro avultada que é gasta com as empresasde telecomunicações. Ainda que cada mensagem custe uma fração de cêntimos, poderáfacilmente ser representada num gasto superior a 50,000 euros anuais para um hospital.Uma vez que a tecnologia e o uso de smartphones tem evoluído de forma tão rápida, é estimadoque na próxima década, quase toda a população portuguesa possuirá ou poderá teracesso indireto a um smartphone. Por essas razões, o objetivo principal deste projeto de dissertação,é desenhar e desenvolver uma aplicação móvel de forma a substituir o serviço denotificação atual, por uma aplicação móvel que notificará o paciente através de notificaçõespush, cuja informação poderá ser salva no calendário do próprio smartphone, traduzindo-senuma maior comodidade para o paciente, assim como a erradicação de custos no envio denotificações por parte do hospital. A principal motivação é, portanto, suprimir esses custospara o hospital, aproximar os pacientes do hospital, integrar outros sistemas na aplicação etornar o sistema de notificação mais eficiente. De forma a gerir os utilizadores da aplicaçãoconsiderou-se necessário o desenvolvimento de uma aplicação web.Neste contexto, a presente dissertação apresenta então uma aplicação web, desenvolvidaem AngularJS, orientada para a gestão de utilizadores da aplicação móvel e apresenta tambéma aplicação móvel com a funcionalidade principal de notificar o paciente dos seuseventos médicos, oferecendo também funcionalidades como a consulta do seu histórico deeventos médicos passados, consulta de eventos médicos futuros, bem como a verificaçãodos eventos médicos próximos da data.",
    "authors": [
      "Moreira, António Pedro da Rocha"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27893",
    "title": "Desenvolvimento de um sistema integrado para o tratamento de dados de sequenciação de próxima geração",
    "abstract": "A sequenciação de próxima geração veio permitir a sequenciação em paralelo de milhões depares de bases de DNA / RNA, tendo tido desde o início um grande impacto, ao ponto de setornar o método escolhido em projetos de grande escala, em detrimento do método de Sanger.Entre as principais aplicações desta tecnologia encontram-se a análise em larga escalada metilação de DNA, o Chip-Seq para análise da interação entre proteínas e DNA ou RNA, eo mapeamento de rearranjos estruturais. Destacam-se, especialmente, a sequenciação denovos organismos ou indivíduos, o estudo de polimorfismos de nucleótido único (DNA-Seq) ea análise de expressão genética (RNA-Seq).Neste trabalho, foi desenvolvido um sistema onde foram integradas ferramentas necessáriaspara estudos de DNA-Seq e RNA-Seq. Inicialmente, foi efetuado um estudo das aplicaçõesexistentes, tendo de seguida sido selecionadas as que se destacaram em parâmetroscomo a facilidade de utilização, documentação e possibilidade de integração com as restantesferramentas do sistema. O sistema foi desenvolvido utilizando-se as linguagens de programaçãoRuby, Java e R, sendo as principais funcionalidades o estudo de polimorfismos, aassemblagem de novo e a análise de expressão genética a partir de dados de RNA-Seq. Estepermite uma utilização simplificada e semiautomática dos vários programas, sendo acessívela utilizadores com poucos conhecimentos informáticos.O sistema foi testado em três casos de estudo: caracterização de duas estirpes deMycobacterium Tuberculosis, assemblagem de novo da Pseudomonas str. M1 e o estudo daexpressão genética em amostras de Saccharomyces cerevisiae.",
    "authors": [
      "Reis, Marco André Ferreira"
    ],
    "keywords": [
      "577.2:681.3",
      "681.3:577.2",
      "61:681.3",
      "681.3:61"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "577.2:681.3",
      "681.3:577.2",
      "61:681.3",
      "681.3:61"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81198",
    "title": "Otimização de recursos de rede para encadeamento de serviços em Segment Routing",
    "abstract": "O Segment Routing (SR) é uma implementação de Software Defined Networking (SDN)para encaminhamento de tráfego. Baseado num paradigma de source routing, cada nodona fronteira da rede adiciona um conjunto de etiquetas ao cabeçalho dos pacotes e defineexplicitamente o caminho que cada fluxo de tráfego deverá percorrer até ao destino. Cadaetiqueta é um segmento que identifica uma instrução topológica, um serviço ou mesmouma instrução de processamento. Embora o SR seja muito parecido com o Multi ProtocolLabel Switching (MPLS) no que respeita ao encaminhamento simples de pacotes, a suautilização simplifica os processos de gestão da rede. Por outro lado, o SR também proporcionasoluções para problemas de escalabilidade existente em implementações SDN comoo OpenFlow. Esta tecnologia está rapidamente a tornar-se um standard e é já suportada porempresas importantes, como a Cisco e a Huawei, desempenhando um papel importante naarquitetura das futuras redes 5G. Face ao desenvolvimento desta nova tecnologia, surgiramnovos tipos de serviço que tiram partido das capacidades da mesma. Devido ao crescentenúmero de dispositivos 5G, surgiu a necessidade de disponibilizar os serviços em diversospontos da rede, fornecendo assim resposta às necessidades requeridas. Como tal, as operadorasde telecomunicações desenvolveram o conceito de Network Function Virtualization(NFV).Todavia existem questões importantes que necessitam ser tratadas, nomeadamente, comodistribuir os serviços pela topologia de forma a reduzir o nível de utilização das ligaçõese nodos da mesma. Este trabalho tem como objetivo a exploração de soluções baseadasem SR e em NFV, recorrendo a técnicas de otimização como algoritmos evolucionários emachine learning.",
    "authors": [
      "Moreira, Gonçalo Dias Camaz"
    ],
    "keywords": [
      "Algoritmos evolucionários",
      "Otimização",
      "Machine learning",
      "Network function virtualization",
      "Segment routing",
      "Evolutionary algorithms",
      "Optimization",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81099",
    "title": "Improving the resilience of microservices-based applications",
    "abstract": "Atualmente, escalabilidade, manutenibilidade e disponibilidade são algumas das medidas mais utilizadas na avaliação qualitativa de software. Com uma presença cada vez maior de produtos de software no nosso dia a dia, há consequentemente a necessidade de torná-los melhores aos olhos do utilizador, surgindo novos desafios a serem explorados e superados na hora de projetar e desenvolver produtos de software. Mais focado neste tema de dissertação de mestrado, a resiliência é de facto um ponto chave para o sucesso de um qualquer produto de software. Cada vez mais as pessoas se encontram diretamente ligadas a produtos de software no seu dia a dia, o que torna o bom funcionamento destes essencial. Assim sendo, o estudo de metodologias que permitam aumentar a resiliência e consequentemente a disponibilidade destes serviços ganha relevância. O principal objetivo desta dissertação é desenvolver uma metodologia para aumentar a resiliência de soluções orientadas aos microsserviços. Assim, é fundamental primeiro entender quais soluções já desenvolvidas para esse fim. Após reunir um conjunto de técnicas para aumentar a resiliência, analisamos um caso de estudo procurando possíveis problemas de resiliência. Para além desta procura de vulnerabilidades, foram apresentadas propostas para a sua resolução, tendo em conta o conjunto de soluções já levantado. Por fim, e avançando para a construção da metodologia alvo da dissertação, procedeu-se à análise de todas as propostas apresentadas, bem como a caracterização das interações problemáticas. Desta forma, foi possível extrair a informação necessária do estudo para a construção da metodologia. Como resultado deste estudo, também foi possível identificar uma nova proposta para aumentar a resiliência diante das necessidades do estudo de caso e da recorrência em que esta se tornou útil.",
    "authors": [
      "Silva, Marco António Rodrigues Oliveira"
    ],
    "keywords": [
      "Microservices",
      "Resilience",
      "Patterns",
      "Service degradation",
      "Distributed systems",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46402",
    "title": "Desenvolvimento de mecanismos de engenharia de tráfego em data centers através de SDN",
    "abstract": "O crescente uso de aplicações que geram altos volumes de tráfego motivou odesenvolvimento de novas abordagens de Engenharia de Tráfego que pudessem melhorar odesempenho e eficiência das infraestruturas de comunicação, e.g. redes dos ISPs (InternetService Providers), Data Centers, etc. Neste contexto, a área denominada por Software DefinedNetworking (SDN) poderá ser útil para a definição de alguns mecanismos inovadores nestescenários. Este paradigma, que tem sido recentemente explorado, oferece novas tecnologias eprotocolos proporcionando novas oportunidades para uma gestão mais expedita e eficiente dasinfraestruturas de rede.Este trabalho propõe-se contribuir para o desenvolvimento de mecanismos de Engenhariade Tráfego na área das SDN. Os mecanismos a estudar estarão orientados para tarefas debalanceamento de carga em redes de Data Centers e implementados com a ferramenta deemulação Mininet. Para tal, será feito inicialmente um estudo das diversas arquiteturas de redesde Data Centers, dos conceitos que englobam o paradigma SDN e uma análise das estratégiasde balanceamento de carga já existentes. De seguida será desenvolvida uma bancada de testese implementados alguns mecanismos de balanceamento de carga. Posteriormente, serãoefetuados testes de desempenho aos mecanismos desenvolvidos",
    "authors": [
      "Leal, Anthony Jonathan da Silva"
    ],
    "keywords": [
      "Software defined networking",
      "Data centers",
      "Engenharia de tráfego",
      "Balanceameto de carga",
      "Mininet",
      "Traffic engineering",
      "Load Balancing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64637",
    "title": "Mineração de opiniões e de sentimentos para o estabelecimento de índices de satisfação",
    "abstract": "Nunca como hoje as opiniões e os sentimentos de cada ser humanodesempenharam um papel tão fundamental no quotidiano da sociedade.A mineração de opiniões e de sentimentos providencia um grau deconhecimento bastante bom sobre o nível de satisfação que um dadonegócio, um acontecimento, ou uma decisão estratégica pode gerar. Estarna posse de um índice de satisfação sobre um determinado bem, serviço,ou acontecimento é dispor de condições que nos permitem alcançarsituações de sucesso que podem ser usadas para melhorar o planeamentoou a estruturação de uma decisão ou mesmo de um negócio, face aofeedback que usualmente provocam.O conhecimento recolhido por este tipo de processo de mineração e,consequentemente, acumulado sobre a experiência que um certo clientedetém num determinado negócio, ou sobre a opinião (ou sentimento) queum certo indivíduo possui relativamente a um acontecimento de granderelevância político ou social permite definir novas estratégias de abordagemem determinados mercados. Neste trabalho de dissertação realizámos aimplementação de um sistema de mineração de opiniões e de sentimentostendo como finalidade de definir e estabelecer um dado índice de satisfação,de forma a que seja possível angariar conhecimento útil sobre aspetos que potenciem a geração de opiniões e de sentimentos, potenciando novasoportunidades de negócio",
    "authors": [
      "Fernandes, Hugo Alexandre Machado"
    ],
    "keywords": [
      "Mineração de opiniões",
      "Mineração de sentimentos",
      "Índices de satisfação",
      "Profiling",
      "Análise de opiniões",
      "Análise de sentimentos",
      "Categorização de opiniões",
      "Categorização de sentimentos",
      "Feedback do consumidor",
      "Mineração de críticas",
      "Métricas de avaliação de performance",
      "Opinion mining",
      "Sentiment mining",
      "Satisfaction indexes",
      "Profiling",
      "Opinion analysis",
      "Sentiment analysis",
      "Opinion categorization",
      "Sentiment categorization",
      "Consumer’s feedback",
      "Reviews mining",
      "Performance evaluation metrics",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59720",
    "title": "Plataforma de apoio à prática de cuidados de enfermagem em contexto hospitalar",
    "abstract": "Nas instituições de saúde, um dos serviços mais exigentes em termos de prestaçãode cuidados de saúde contínuos e atenção por parte dos profissionais, é o serviçode internamento hospitalar. Aqui existe uma constante necessidade de informaçõesatualizadas sobre o registo clínico eletrónico do paciente, através de sistemas de apoioao ato clínico facilmente utilizáveis.Neste contexto, a presente dissertação apresenta uma nova plataforma web para amonitorização diária de pacientes, projetada para ser usada por profissionais de saúde,especialmente enfermeiros. A aplicação é baseada em React, uma Library Open Sourcede JavaScript, para criar User Interfaces (UI).A nova plataforma é constituída por duas principais componentes: Um quadro deenfermagem que contém informação de todos os pacientes internados, numa unidadede saúde, num determinado momento. Este quadro baseia-se em indicadores, como principal objetivo de alertar ações futuras (Exames, Análises, Medicação, Dietas, Jejume Cirurgias), relativamente a cada paciente. A outra componente designa-se porregisto clínico de internamento e contém todas as informações sobre sobre as açõessupramenciadas. De notar que cada unidade de saúde deverá adaptar a plataforma àssuas principais necessidades.",
    "authors": [
      "Oliveira, Daniela Sofia Rijo"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94078",
    "title": "Metagenomics reveal taxonomic diversity of microbial communities along gradients of environmental stress in river basins of North Portugal",
    "abstract": "Uma das consequências do problema emergente das alterações climáticas, devido ao aumento dostress antropogénico, é a falta de informação sobre o impacto específico destas alterações nos ecossis temas e as potenciais medidas para as atenuar. Este facto cria uma necessidade premente de melhoresferramentas de biomonitorização para avaliar o estado de diversos ecossistemas. Especificamente, osecossistemas de água doce que desempenham um papel crucial na prestação de bens e serviços essen ciais à humanidade. No entanto, os esforços de investigação e biomonitorização concentram-se frequen temente nos invertebrados e algas, deixando uma lacuna na monitorização dos microrganismos que sãochave para o funcionamento dos ecossistemas aquáticos. Muitos microrganismos que habitam nestesambientes não são cultiváveis em meios tradicionais, o que constitui um desafio para a compreensão dassuas respostas a vários gradientes de stress ambiental. Para melhorar a compreensão e facilitar a tomadade decisão para a proteção destes ecossistemas, torna-se imperativo dispor de dados mais precisos so bre os microrganismos presentes nestes ecossistemas. Este trabalho teve como objetivo colmatar essalacuna, usando a metagenómica para ajudar a compreender como a comunidade de fungos e bactériasé afetada pelos diferentes fatores de stress ambiental. As amostras biológicas foram recolhidas em 50locais localizados em 4 bacias hidrográficas do Norte de Portugal, dos rios Ave, Cávado, Lima e Minho. Aamostragem consistiu em recolher folhada naturalmente acumulada no leito dos rios. As amostras foramrecolhidas durante o verão de 2020. Para além do material biológico, foram também recolhidos dados devariáveis ambientais nos mesmos locais de amostragem. O material genético foi extraído e foi feita umasequenciação completa do genoma das comunidades presentes. A identificação taxonómica foi feita comrecurso ao servidor web Kaiju. Alguns taxa destacaram-se pela sua dominância nas amostras em relaçãoaos restantes. A riqueza de taxa de fungos apresentou uma resposta negativa à presença combinadados fatores de stress ambiental, à intensificação do uso do solo, e ao aumento da condutividade e daconcentração de azoto inorgânico dissolvido. Por outro lado, a riqueza taxonómica dos fungos aumentoucom o aumento da altitude. A intensificação do uso do solo foi o fator de stress que melhor explicoua riqueza em taxa. A diversidade taxonómica das bactérias apresentou uma resposta à velocidade dacorrente de acordo com o modelo de Gauss. Isto significa que um distúrbio moderado pode favorecer abiodiversidade, o poderá ser explicado pela hipótese do distúrbio intermédio.",
    "authors": [
      "Gonçalves, Pedro Alexandre Cunha Senra Bogas"
    ],
    "keywords": [
      "Metagenómica",
      "Stress ambiental",
      "Ecossistemas",
      "Rios",
      "Microrganismos",
      "Metagenomics",
      "Stressors",
      "Ecosystems",
      "Rivers",
      "Microorganisms",
      "Ciências Naturais::Ciências Biológicas"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências Biológicas"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34136",
    "title": "Meios complementares de diagnóstico e terapêutica no Processo Clínico Electrónico via \"m-Health\"",
    "abstract": "As aplicações Mobile Health (M-Health) desenvolvidas para dispositivosmóveis têm sido alvo de muitas investigações, sendo a área da Saúde umdos principais focos de desenvolvimento, em que estas aplicações pretendemauxiliar nas práticas médicas. Um dos principais desa os da área da saúdeé melhorar a acessibilidade e a disponibilidade da informação clínica dospacientes. Com este desa o em mente, foi desenvolvida a Agência para aIntegração, Difusão e Arquivo de Informação Médica (AIDA) de forma a garantira interoperabilidade entre os vários Sistema de Informação Hospitalar(SIH), estando esta implementada num dos maiores centros hospitalares dePortugal, o CHP.O desenvolvimento de uma aplicação para dispositivos móveis que permitaa visualização e consulta da informação clínica dos pacientes, providenciandoum acesso rápido e fácil a esta informação aos pro ssionais de saúde, é o principalobjetivo do presente projeto. A aplicação pode ser considerada comouma ferramenta adicional do Processo Clínico Eletrónico da AIDA (AIDAPCE)e é para uso dos pro ssionais de saúde pertencentes ao CHP. Esteprojeto expõe assim uma nova metodologia que se baseia nos princípios dacomputação calma para a apresentação dos relatórios dos Meios Complementaresde Diagnóstico e Terapêutica (MCDT) dos pacientes e pretendemelhorar a qualidade e a e ciência na prestação dos cuidados de saúde.Um dos objetivos deste projeto incide na realização de uma prova de conceitoà metodologia abordada, de forma a avaliar o impacto da implementaçãoda aplicação desenvolvida no CHP. Desta forma, foi realizada uma análiseStrengths Weaknesses Opportunities and Threats (SWOT), com o intuito deidenti car as suas forças e fragilidades, e foram realizados testes de e ciênciae usabilidade, em que para este último recorreu-se ao método de avaliação deusabilidade do tipo inquérito. A realização destes estudos permitiu averiguaros resultados que teria a implementação da aplicação, veri cando que estaseria bem aceite e utilizada pelos pro ssionais de saúde.",
    "authors": [
      "Pereira, Ana Sofia Vieira Tavares de Amorim"
    ],
    "keywords": [
      "616-07:681.324",
      "681.324:616-07",
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616-07:681.324",
      "681.324:616-07"
    ],
    "subjects_fos": [
      "Ciências Médicas::Medicina Clínica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84139",
    "title": "On pattern recognition of brain connectivity in resting-state functional MRI",
    "abstract": "The human urge and pursuit for information have led to the development of increasingly complextechnologies, and new means to study and understand the most advanced and intricate biologicalsystem: the human brain. Large-scale neuronal communication within the brain, and how it relates tohuman behaviour can be inferred by delving into the brain network, and searching for patterns inconnectivity. Functional connectivity is a steady characteristic of the brain, and it has been proved to bevery useful for examining how mental disorders affect connections within the brain. The detection ofabnormal behaviour in brain networks is performed by experts, such as physicians, who limit the processwith human subjectivity, and unwittingly introduce errors in the interpretation. The continuous search foralternatives to obtain faster and robuster results have put Machine Learning and Deep Learning in theleading position of computer vision, as they enable the extraction of meaningful patterns, some beyondhuman perception.The aim of this dissertation is to design and develop an experiment setup to analyse functionalconnectivity at a voxel level, in order to find functional patterns. For the purpose, a pipeline was outlinedto include steps from data download to data analysis, resulting in four methods: Data Download, DataPreprocessing, Dimensionality Reduction, and Analysis. The proposed experiment setup was modeledusing as materials resting state fMRI data from two sources: Life and Health Sciences Research Institute(Portugal), and Human Connectome Project (USA). To evaluate its performance, a case study wasperformed using the In-House data for concerning a smaller number of subjects to study. The pipelinewas successful at delivering results, although limitations concerning the memory of the machine usedrestricted some aspects of this experiment setup’s testing.With appropriate resources, this experiment setup may support the process of analysing and extractingpatterns from any resting state functional connectivity data, and aid in the detection of mental disorders.",
    "authors": [
      "Cardoso, Ana Catarina Salgueiro"
    ],
    "keywords": [
      "Brain connectivity",
      "Data visualization",
      "Machine learning",
      "Medical imaging informatics",
      "Pattern recognition",
      "Conectividade cerebral",
      "Descoberta de padrões",
      "Informática de imagem médica",
      "Visualização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80115",
    "title": "Geração automática dum mapa do coberto do solo português",
    "abstract": "A aplicação de técnicas orientadas a objectos e de aprendizagem automática sobre imagens de satélite tem sido alvo de interesse nos últimos anos. O aumento da qualidade e quantidade de imagens, disponibilizadas por programas de observação da Terra como por exemplo o Programa Cope rnicus, levou à geração de uma grande quantidade de dados. De entre as várias aplicações destes dados destaca-se a criação de mapas do coberto do solo. Com a presente dissertação pretendia-se criar modelos de aprendizagem automática capazes de segmentar e classificar com precisão imagens de satélite, gerando automaticamente um mapa do coberto do território Português. Durante a dissertação foram realizadas várias experiências com as bandas espetrais do satélite Sentinel-2, com índices espetrais e com diversos conjuntos de classes do coberto. Foram testadas três arquiteturas nos modelos de aprendizagem automática treinados, que adotam duas técnicas diferentes para classificação das imagens. Numa das técnicas a classificação é orientada ao objeto, e neste caso a arquitetura adotada nos modelos foi uma rede neuronal artificial U-Net. Na outra técnica, a classificação é orientada ao pixel e os modelos de aprendizagem automática testados foram a floresta aleatória e a máquina de vetores de suporte. A acurácia global dos resultados obtidos variou entre 82.32% e 94.75%, dependendo fortemente do número de classes em que se classifica o coberto. O resultado de 94.75% foi obtido quando se classifica o coberto em apenas 5 dasses. Contudo conseguiu-se uma acurácia bem interessante de 92.37%, no modelo treinado para classificar 8 classes.",
    "authors": [
      "Valente, Nuno Afonso Gonçalves Solha Moreira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27778",
    "title": "Avaliação da qualidade do registo clínico eletrónico",
    "abstract": "Os registos clínicos têm como função facilitar a continuação da prestaçãode cuidados, a documentação dos seus processos e a comunicação entre ospro ssionais de saúde. Têm sido desenvolvidos protocolos para registos dedados clínicos, no entanto, existe pouco conhecimento sobre o que realmenteé documentado. O Registo Clínico Electrónico (RCE) encontra-se em amplaexpansão, recorrendo-se, cada vez mais, à sua implementação em unidadeshospitalares, o que proporciona uma maior agilidade no tratamento dos processose uma consequente melhoria na qualidade da abordagem ao historialdo paciente.A crescente focalização na avaliação do desenvolvimento de produtos, baseadaem testes de garantia e no design do produto, está na origem do termousabilidade, que ganha uma importância vertiginosa, à medida que aumentao número de pessoas que dependem de dispositivos técnicos para realizar tarefas.A usabilidade de um sistema informático pode ser de nida como umaqualidade inerente ao sistema, que possibilita que os utilizadores o utilizemcom satisfação, e cácia e e ciência. De facto, a adoção do RCE e a satisfa-ção do utilizador estão intimamente associados à usabilidade do sistema. Aocontrário de muitas indústrias, onde a usabilidade é a norma em design deproduto, a prática de usabilidade no RCE tem sido esporádica, não sistemá-tica, casual e super cial, em parte devido à falta de estruturas especí cas deRCE e de aplicação de métodos que avaliem a usabilidade.Assim, da conjunção da problemática de avaliação do RCE com o desa ode tirar o melhor partido das potencialidades dos sistemas RCE já implementados,surge o presente projeto. Pretende-se apresentar uma abordagemglobal dos vários aspetos relacionados com a avaliação da qualidade do RCE nas unidades de saúde. Pretende-se ainda realizar essa avaliação ao RCEimplementado no CHAA, para se proceder à elaboração de um protótipo quecontemple as alterações mais emergentes ao sistema.Desta forma, foi avaliada a usabilidade dos sistemas SAM, SONHO, SAPE eAIDA, através de questionários, distribuídos a 38 participantes. Constataramseque as maiores di culdades sentidas com os utilizadores dos sistemasrelacionam-se com o elevado tempo de desempenho do sistema e com a faltade motivação para introduçao de registos informatizados. Contudo, os dadosrecolhidos realçam as enormes vantagens desta desmaterialização hospitalar,como sejam a diminuição dos erros e do tempo de leitura e transmição dosregistos clínicos.",
    "authors": [
      "Leal, Maria Filipa Rodrigues"
    ],
    "keywords": [
      "681.3:614",
      "614:681.3",
      "658.56"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:614",
      "614:681.3",
      "658.56"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92678",
    "title": "Energy efficiency aware job scheduling for scalable data processing tools",
    "abstract": "Massive data processing tools for distributed environments such as Spark or Dask allow programmersto process massive amounts of data in data centers. A large portion of the operation costs of theseinfrastructures corresponds to the energy consumption resulting in performing these operations.Current tools use simple algorithms for efficient scheduling of data processing jobs in distributedcomputing, relying on heuristics without considering the workload characteristics. Recent work exploresefficient scheduling of data processing jobs in distributed computing, especially in heterogeneous environ ments, despite these infrastructures being typically homogeneous.This dissertation makes an analysis of job executions in Spark and proposes EASAHUM a new al gorithm for job scheduling in massive data processing tools with energy efficiency concerns using theconclusions drawn. The implementation and evaluation in a simulator using real and synthetic executiontraces in Spark demonstrate that the algorithm can reduce energy consumption by up to 16% and reducejob execution time by up to 12.25% without significant impact on the scheduling time.",
    "authors": [
      "Azevedo, Renato André Araújo"
    ],
    "keywords": [
      "Spark",
      "Scheduling",
      "Energy Efficiency",
      "Agendamento",
      "Eficiência energética",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27829",
    "title": "Resolução de conflitos em linha : OntoLab uma aplicação ao direito laboral",
    "abstract": "A Resolução Alternativa de Conflitos, visa, por um lado, promover o acesso à justiça,sendo um meio alternativo à resolução de conflitos judiciais, neste caso recorre-se aos tribunaisarbitrais e julgados de paz. Por outro lado, visa, também, apoiar a criação e o funcionamento demeios extrajudiciais para resolução alternativa de conflitos, incluindo a mediação, negociação earbitragem.No de Direito, especificamente na área do direito do trabalho, a utilização de métodos deresolução alternativa de conflitos é muito vantajosa. É conhecido o estado actual da justiça emPortugal, grande parte dos casos em litígio arrastam-se ao longo de anos nos tribunais, semexistir previsão de resolução. O recurso à resolução de conflitos em linha, visa retirar dostribunais vários destes casos, tornando assim a resolução do litígio mais célere, bem comomenos dispendiosa para ambas as partes.Estes métodos de resolução online de conflitos são uma abordagem bastante recente, quese serve da Internet e de ferramentas de suporte à decisão. Neste tipo de ambientes as partesinteragem e expõe os seus pontos de vista, em qualquer momento e em qualquer local, uma vezque estes sistemas estão disponíveis em linha.Nesta dissertação procurou-se definir qual a melhor metodologia para o desenvolvimentode uma ontologia base, nesta área do conhecimento. Nesse sentido desenvolveu-se umaontologia e um motor de inferência de conhecimento que actua sobre ela de forma adisponibilizar o conhecimento obtido ao sistema.Com a utilização das ontologias aliadas aos sistemas de resolução de conflitos em linhaexistiram enormes ganhos para a justiça, na medida em que estes processos tendem a ser maistransparentes, mais rápidos e mais justos.",
    "authors": [
      "Ribeiro, João Pedro Pinto Vasques"
    ],
    "keywords": [
      "681.324:349.2",
      "349.2:681.324"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.324:349.2",
      "349.2:681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56109",
    "title": "Development of a database and web tool for the in silico characterization of plasmid data",
    "abstract": "Bacterial plasmids are mobile genetic structures capable of conferring selective advantagesto their hosts, such as resistance to antibiotics, virulence genes and tolerance to pollutants.By associating with other genetic elements, like integrons and transposons, plasmids providea platform for genetic recombination and for gene transfer between different bacterialspecies, allowing them to colonize multiple environments and guaranteeing their persistence.Although there are over 4000 complete plasmid sequences available in GenBank, mosthave absent or non-standardized (disorganized) information regarding their isolation source,environment and year and country of isolation. Furthermore, a prediction about their mobilityand incompability group is also lacking.The goals of this thesis are, besides completing the missing information about plasmiddata, the development of a repository of fully sequenced plasmids and the developmentof easy-to-use web tools for the characterization of plasmid data regardless of their sourceenvironment and bacterial host. For the development of these tools, Shiny was used, whichis a package from the R scientific computing environment.The present work is organized as follows: the core concepts related to plasmids aredescribed, their background is characterized and a critical analysis of the available web toolsfor plasmid classification is carried out. Then, the adopted approach and the development(implementation, outcomes) of the database and web tool are explained. Lastly, the mainconclusions are highlighted.",
    "authors": [
      "Santos, Catarina Freitas de Sousa"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86731",
    "title": "Developing deep learnig methods to predict phenotypes and clinical outcomes from transcriptomics data",
    "abstract": "Personalized medicine is a constantly growing area. Important goals of this field are early diagnosis and the discovery of new personalized treatments. Gene expression data play a key role at this level, as variations in these data can often offer explanations for some phenotypes. To this end, Machine Learning (ML) models capable of predicting biologically relevant information, have been widely used. Deep Learning (DL) is a branch of ML that has become popular over the past few years. The increasing amounts of data that have been generated, and the growing use of this type of models in biomedical areas, have been accelerating the analysis of biological processes associated with cancer and other complex diseases.In this work, we focused on developing a framework that allows to create and evaluate distinct work-flows for the application of a variety of machine and deep learning models, working over gene expression data, including different options regarding data preprocessing pipelines, distinct ML and DL models, including traditional ML models, Dense Neural Networks, Convolutional Neural Networks and Variational Autoencoders. The framework has been validated using different case studies, where the data sources were two of the main repositories of gene expression data (TCGA and GTEx). The goal of each case study was to predict important variables for clinical application. A variety of models were developed and evaluated for each case study, generally with competitive performance. For the first case study, the task was to predict the type of cancer from TCGA data, and the best performing DL model was a dense neural network, being outperformed by a logistic regression model. In the second case, where the task was to predict the hypoxia score, the best DL model was a two dimensional convolutional neural network (2D CNN), being outperformed by the LightGBM model. As for the third case study, where the objective was to predict the aneuploidy score, the best model was an one dimensional convolutional neural network (1D CNN). For the fourth case, where the task was to predict body mass index, the best model was again a 1D CNN. Finally, in the fifth case study, where the main goal was to predict gene expression for a set of genes based on landmark genes, the best DL model was found by an 1D CNN, still slightly outperformed by linear regression. Some of the DL models developed in this work show promising results. However, these need to be improved in the future as they are not clinically applicable at this time. This framework can be reused for new problems and can be easily expanded.",
    "authors": [
      "Vieira, Maria Fernanda Silva"
    ],
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Personalized medicine",
      "Transcriptomics",
      "Aprendizagem máquina",
      "Aprendizagem profunda",
      "Medicina personalizada",
      "Transcriptómica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86511",
    "title": "Improving digital image correlation in the TopoSEM Software Package",
    "abstract": "TopoSEM is a software package with the aim of reconstructing a 3D surface topography of a microscopic samplefrom a set of 2D Scanning Electron Microscopy (SEM) images. TopoSEM is also able to produce a stability reporton the calibration of the SEM hardware based solely on output images.One of the key steps in both of these workflows is the use of a Digital Image Correlation (DIC) algorithm, ano-contact imaging technique, to measure full-field displacements of an input image. A novel DIC implementationfine-tuned for 3D reconstructions was originally developed in MATLAB to satisfy the feature requirement of thisproject. However, near real-time usability of the TopoSEM is paramount for its users, and the main barrier towardsthis goal is the under-performing DIC implementation.This dissertation work ported the original MATLAB implementation of TopoSEM to sequential C++ and itsperformance was further optimised: (i) to improve memory accesses, (ii) to explore the available vector exten sions in each core of current multiprocessor chips processors to perform computationally intensive operationson vectors and matrices of single and double-precision floating point values, and (iii) to additionally improve theexecution performance through parallelization on multi-core devices, by using multiple threads with a front wavepropagation scheduler.The initial MATLAB implementation took 3279.4 seconds to compute the full-field displacement of a 2576pixels by 2086 pixels image on a quad-core laptop. With all added improvements, the new parallel C++ versionon the same laptop lowered the execution time to 1.52 seconds, achieving an overall speedup of 2158.",
    "authors": [
      "Ferreira, José Filipe de Sousa Matos"
    ],
    "keywords": [
      "Digital image correlation",
      "Scanning electron microscope",
      "3D reconstruction",
      "Correlação de imagens digitais",
      "Microscópio eletrônico de varrimento",
      "Reconstrução 3D",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81299",
    "title": "Exploitation and annotation of Torulaspora delbrueckii genomes: comparison with biotechnological data",
    "abstract": "Nowadays, the most widely used yeast in wine, beer, and bread fermentations is Saccharomyces cerevisiae. However, in the past years, Torulaspora delbrueckii attracted interest due to its properties, from flavor and aroma-enhanced wine to the ability to be preserved longer in frozen dough. The main objective of this thesis was to explore T. delbrueckii genomes publicly available and the ones belonging to our project’s collection, exploring their genomic information and establishing its relationship with their origins and biotechnological applications.In the first phase, publicly available genomes of T. delbrueckii were explored, and their annotation was improved. EggNOG-mapper was used to perform functional annotation of the deduced T. delbrueckiicoding genes, offering insights into its biological significance, and revealing 24 clusters of orthologous groups (COG), gathered in three main functional categories: information storage and processing (28% of the proteins), cellular processing and signaling (27%) and metabolism (23%). Small intra-species variability was found when considering functional annotation of the four T. delbrueckii available genomes. A comparative study was also conducted between T. delbrueckii genome and those from 386 fungal species, revealing high homology with species of Zygotorulaspora and Zygosaccharomyces genera, but also with Lachancea and S. cerevisiae. Lastly, the phylogenetic placement of T. delbrueckii was assessed using the core homologues found across 204 common protein sequences of 386 fungal species and strains.In a second phase, the genome of fifty-four T. delbrueckii strains were sequenced and data was explored.The alignment, SNP statistics, annotation, among other steps, were attempted, for the first time, for those strains. PCA analysis was performed with those strains and the ones publicly available, to betterunderstand the connection between the strains’ technological groups. The present work represents a successful effort to increase and improve the annotation of T. delbrueckii’s genome. Overall, this work provides a starting point to unravel the diversity of potential biotechnological applications of T. delbrueckii.",
    "authors": [
      "Torre, Carolina Santiago Garrido Dias da"
    ],
    "keywords": [
      "Fermentation",
      "NGS",
      "Saccharomyces cerevisiae",
      "Torulaspora delbrueckii",
      "Winemaking",
      "Fermentação",
      "Produção de vinho"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92660",
    "title": "Edge-enabled multi-agent system architecture for multimodal machine learning",
    "abstract": "Public lighting, despite being one of the most important services a city must provide, it is also one of the main sources of energy consumption and consequently running costs. One way to drastically improve the efficiency of these systems is by using object detection in order to make each of the public lighting poles aware of their surroundings, giving them the ability to adapt to the environment. This is one of the main goals of an industry leader company's project in the area of public lighting, where this study is inserted. In this project, each public lighting pole is upgraded with a set of sensors: radar, microphone and camera(what we refer to as fog computing). By developing a multimodal machine learning model, the goal is to leverage the data from the different sensors to improve the object detection capabilities of traditional unimodal machine learning model. Additionally, the developed model will be deployed into an edge device that is also installed in the public lighting pole, due to data privacy concerns and network latency problems that would otherwise occur with traditional server-side approaches. This constrain raises the main question that this study will try to answer, which is how to develop a complex multimodal machine learning model for low-power efficient edge devices. In this study, a multi-agent architecture will be proposed, that authors can adapt to their own multimodal machine learning problems with edge devices. To prove the efficient of the proposed system, a proof of concept implementation will be carried out that involves the aforementioned sensors, as well as the You Only Look Once (YOLO) object detection model, with a feature-level data fusion approach. Finally, the implemented system will be deployed to an edge device, where the hardware performance will be tested and compared to similar work in the literature.",
    "authors": [
      "Coelho, Eduardo Benjamim Lopes"
    ],
    "keywords": [
      "Public lighting",
      "Object detection",
      "Smart cities",
      "Multimodal machine learning",
      "Fog and Edge computing",
      "Multi-agent systems",
      "YOLO model",
      "Sound classification",
      "Data fusion",
      "Iluminação pública",
      "Deteção de objetos",
      "Cidades inteligentes",
      "Aprendizagem automática multimodal",
      "Computação Fog e Edge",
      "Sistemas multi-agente",
      "Modelo YOLO",
      "Classificação de som",
      "Fusão de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83668",
    "title": "On the performance of strategic attribute grammars",
    "abstract": "Strategic programming is a powerful technique used in language processing to define functions that traverseabstract syntax trees. With strategies, the programmer only indicates the nodes of the tree where the work has tobe done, and the strategy used to traverse the whole tree and apply the function that works only on the definednodes.In Haskell, there are two libraries that implement strategies: Strafunski and an equivalent library developedby DI: Ztrategic. Beyond that, we also have the Kiama library which is implemented in the Scala programminglanguage. The Ztrategic library uses memorization in order to save work. Using memorization, the elimination ofall occurrences of \"bad smells\" in an abstract tree of a program is done only once!In this thesis, we present a detailed study of the performance of the Kiama, Ztrategic, and memoized Ztrategiclibraries, using different strategic problems and input languages.",
    "authors": [
      "Rodrigues, José Emanuel Silva"
    ],
    "keywords": [
      "Strategic programming",
      "Attribute grammars",
      "Zippers",
      "Ztrategic",
      "Programação estratégica",
      "Gramáticas de atributos"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27814",
    "title": "Generating automatically test cases based on models",
    "abstract": "I would like to thank the following people for their help and support over the course of thecompletion of this thesis.To João Saraiva, my supervisor, for his continued support and encouragement.To Luís Anjos and Nuno Vieira, my co-workers, that helped in everything I needed duringmy research time in Primavera.To my parents, Carlos and Isabel, that, even though in my life I may have not done everythingthe way they wanted and wished, were always there for my best interest, caring for me, mylife and my future. I feel eternal gratitude for the two of you.To my lovely sister, Catarina, for giving me company even when I didn’t want it, for lovingme unconditionally and for bringing a new cat into our household. Life with kittens isalways better than life without kittens!To my aunts, Nocas, Sissi and Patrícia, that I am sure have always believed in me.To my grandfather, that age only made of him a more interesting, funny and reliablecompany. To my grandma, for all the love and food filled of love that fed me during allthese years.To my girlfriend Ana, for all the love, patience and understanding, and for staying by myside.To all my friends, who make my life outside of research so enjoyable. Particularly, to myfriends Ricardo, Renato, Miguel, Pedro Nuno, Bruno and Sebastião, whose company hasbeen a constant for many years. To Zé and João Nuno, for the great time we have when weare together.",
    "authors": [
      "Veiga, Nuno Gil Correia Veloso da"
    ],
    "keywords": [
      "681.3.06",
      "658.0"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06",
      "658.0"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80770",
    "title": "Reflexão sobre a pessoalização jurídica dos robots artificialmente inteligentes",
    "abstract": "O objeto da nossa dissertação consiste na análise da adequação ou inadequação da atribuição de personalidade jurídica aos robots artificialmente inteligentes. No ordenamento jurídico português são atualmente admitidos dois tipos de pessoas jurídicas: as pessoas singulares e as pessoas coletivas. Poderá considerar-se a criação de um novo tipo de pessoas jurídicas?O florescimento da inteligência artificial, i.e., da disciplina que procura construir máquinas que atuem de forma semelhante ao ser humano, capazes de executar tarefas ou funções e tomar decisões com uma eficácia semelhante ou superior àquele, levantou dúvidas relativamente à adequação dogmática da tradicional dicotomia entre a personalidade singular e a personalidade coletiva. A criação de robots artificialmente inteligentes capazes de receber e utilizar dados que circulam em rede, bem como de aprender e utilizar experiências anteriores, contribui em larga escala para a pertinência de uma análise cuidada do tema. Tanto mais que a implementação de robots artificialmente inteligentes no quotidiano social e económico traz consigo uma panóplia de implicações éticas, as quais constituem desafios que cumpre teorizar e solucionar. A personalidade jurídica singular e a personalidade jurídica coletiva não assentam em considerandos axiológico-jurídicos e filosóficos iguais. A personalidade singular fundamenta-se no Homem, enquanto sujeito dotado de uma dignidade originária e inviolável, pelo que tem de ser reconhecida a todos os seres humanos, em qualquer circunstância (nunca a pessoa humana poderá ser objeto de um direito). Por sua vez, a personalidade coletiva assenta na necessidade de fornecer às pessoas singulares os instrumentos jurídicos adequados à prossecução dos seus propósitos ou objetivos. Determinados os fundamentos axiológico-jurídicos que subjazem a este instituto jurídico no ordenamento jurídico português, a atribuição de personalidade jurídica aos robots artificialmente inteligentes pressuporá a verificação de um desses considerandos. Haverá razões justificativas e legitimadoras da atribuição de personalidade jurídica aos robots artificialmente inteligentes? Qual é o entendimento perfilado pela Comissão Europeia na proposta de Regulamento publicada em 21 de abril de 2021? Estas são algumas das questões que nos propusemos tratar.",
    "authors": [
      "Fernandes, Rui Manuel Zilhão"
    ],
    "keywords": [
      "Inteligência artificial",
      "Robots artificialmente inteligentes",
      "Autonomia",
      "Ética",
      "Personalidade jurídica",
      "Artificial intelligence",
      "Artificially intelligent robots",
      "Autonomy",
      "Ethics",
      "Natural/legal personhood",
      "Ciências Sociais::Direito"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Sociais::Direito"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84101",
    "title": "Sistema de análise e previsão de consumo energético para uma cidade inteligente",
    "abstract": "Cidades Inteligentes é um conceito que ainda não se encontra muito bem estabelecido.Contudo, é de consenso comum que este consiste em fornecer a todos os seus cidadãosuma melhor qualidade de vida. Nestas cidades inteligentes encontramos múltiplos cená rios aplicacionais que podem ser explorados de forma a alcançar os objectivos necessários.Um exemplo destes cenários são os sistemas energéticos que, dada a evolução constante,sofrem uma grande sobrecarga. Consequentemente, é necessário recorrer a certas medidaspara aumentar a eficiência destes. Uma solução é a implementação de sistemas de análisede consumo de energia e de previsão de dados. Os dados são extraídos de fontes bastantediversas (sensores, câmaras e outros), dados estes que, posteriormente, são processados eexportados para data warehouses. Contudo, com a evolução tecnológica que se tem vindoa verificar, a quantidade de dados oportunos aumentou significativamente, bem como ascaracterísticas relativas à forma de como são recolhidos e tratados. Hoje em dia, a diver sidade destes dados é intensa, dependendo muito das circunstâncias operacionais e dossistemas envolvidos, o que gera vulgarmente cenários aplicacionais novos e estranhos paraos sistemas que usualmente estão envolvidos no seu tratamento. Neste sentido, existe anecessidade de inovar os processos de tratamento destes dados e, deste modo, aumentar aoperacionalidade e possibilidade de suportar uma cidade inteligente. Para tal, é necessárioimplementar processos de ETL e de previsão de dados para haver a capacidade de toma das de decisões de forma a manter uma cidade inteligente. Abordamos, nesta dissertação,os sistemas de análise de consumo de energia, visto serem um dos cenários aplicacionaismais explorados com a evolução tecnológica. Havendo a necessidade de aumentar a efi ciência desta área identificamos, planeamos e testamos algumas das medidas passíveis deimplementar para a previsão de dados futuros que permitam ajudar à tomada de decisão.Alcançando, por fim, a seleção de um algoritmo bastante preciso para a previsão destesdados.",
    "authors": [
      "Sousa, Bruno Alexandre Almeida"
    ],
    "keywords": [
      "Cidades inteligentes",
      "Previsão de dados",
      "Séries temporais",
      "Sistemas de análise de consumo energético",
      "Data forecasting",
      "Energy consumption analysis systems",
      "Smart cities",
      "Time series",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92629",
    "title": "Formalização de um protocolo Mesh para sistemas IoT em Alloy",
    "abstract": "Nos dias de hoje os dispositivos IoT fazem parte das nossas vidas, nomeadamente na integração das nossas casas, automação industrial ou monitorização de vários tipos de ambiente. Thread é um protocolo deredes mesh wireless que tem uma enorme eficiência energética, segurança, alcance, interoperabilidade,descentralização e com capacidade de regeneração, sendo por isso interessante para integrar em sistemas IoT. Dado isto, a validação e verificação dos requisitos do protocolo é extremamente importante e,por isso, nesta dissertação são propostas várias formalizações deste protocolo em Alloy. Posteriormenteforam realizadas várias verificações das suas propriedades mais relevantes, uma vez que, são estas quegarantem o correto funcionamento do algoritmo. Todas foram verificadas sem nenhum contra-exemploencontrado. Para além disso, esta dissertação também explora a relação entre o OpenThread Network Simulator, desenvolvido pela Google, e o modelo produzido em Alloy, com o objetivo de validar este último.Foram realizadas várias simulações e convertidas em instâncias executáveis nesse modelo. Isto permitiudetetar algumas incongruências entre ambos e corrigir o modelo por forma a ser capaz de executar todasas simulações realizadas.",
    "authors": [
      "Lourenço, Rafael Inácio"
    ],
    "keywords": [
      "OpenThread",
      "Thread",
      "Software verification",
      "Model checking",
      "Alloy",
      "Wireless Mesh Networks",
      "OTNS",
      "IoT",
      "Smart home",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66153",
    "title": "Assessment of microbial community interactions using different tools",
    "abstract": "Microbial communities participate in many biological processes, directly affecting itssurrounding environment. Thus, the study of a community’s behaviour and interactionsamong its members can be very useful in the biotechnology, environmental and humanhealth fields. Nevertheless, decoding the metabolic exchanges between microorganismsand community dynamics remains a challenge.Computational modelling methods have gained interest as a way to unravel the interactionsand behaviour. GSM models allow the prediction of an organism’s response tochanges in genetic and environmental conditions. Thus, the extension of such method to acommunity level can help decode a community’s phenotype.In this work, different GSM models and current bioinformatics tools were used to modelthe metabolism of different microbial communities. The different tools’ performances werecompared to assess which is currently the best method to perform an analysis on a communitylevel. Distinct case studies regarding microbial communities for which its interactionswere already known, were selected. To assess the tools’ performances, each tools outputwas compared to what was expected in theory.COBRA Toolbox's methods proved to be useful to build a community structure fromindividual GSM models, while pFBA and SteadyCom’s simulation methods can predictexchange between the organisms and the environment. Additionally, Dynamic Flux BalanceAnalysis (dFBA) approaches, such as DFBAlab and DyMMM, can successfully simulatemetabolite and biomass variation over time. Nevertheless, these methods are more limitedas they require specific organism information, which is not always available.Several GSM models are available for use. Nonetheless, their quality control has to gainattention as the simulations’ results are directly affected by the individual models accuracyto represent an organism’s metabolism. Thus, community model builders should carefullychose a GSM model, or combination of models before performing simulations.",
    "authors": [
      "Ribeiro, Catarina Moreira"
    ],
    "keywords": [
      "Microbial community",
      "Systems biology",
      "Genome-scale metabolic model",
      "Community modelling",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/85239",
    "title": "Predicting problems from Telecom installation processes",
    "abstract": "Improving customer experience is crucial in any industry, especially in telecommunications, wherecompetition is a constant factor. Today, all telecommunications companies rely on the massiveamount of data generated daily to get to know the customer, study their behavior, and create neweffective strategies for their business. The information collected may include network information,representing the status of hardware and software components in the network, and the client’s details and calls to support and customer assistance data, which describes problems, consumercomplaints, and all other problem-solving interactions. By combining these mountains of raw datawith data mining and machine learning techniques, companies can find solid patterns or systematicrelationships that represent valuable information, that is, generate knowledge.Within the most varied user experiences, the process of installing new services can be an eventthat raises doubts about their operation, degrade the user experience, or, in extreme cases, lead tomaintenance interventions. Therefore, the use of advanced predictive models that can predict suchoccurrences become vital. With this, the company can anticipate the cases that will be problematicand reduce the number of negative experiences.The main objective of this work is to create a predictive model that, through all the available datahistory, can predict which customers will contact the customer service with problems derived fromthe installation process and have a following maintenance intervention.After analyzing an imbalanced dataset with approximately 560K entries from a Portuguese telecommunications company, and resorting to the CRISP-DM methodology for modeling, the best resultswere found with LightGBM, which obtained an AUPRC of 0.11 and AUROC of 0.62. The best tradeoff between precision and recall was found with a threshold model of 0.43 in order to maximizerecall while still avoiding a large number of false negatives. The strategies explored in this work andthe challenges found may help the company understand which details should improve in its serviceprovision, and which data still need to be investigated in the future.",
    "authors": [
      "Costa, Diana Sofia Nogueira"
    ],
    "keywords": [
      "Customer",
      "Data Mining",
      "Installation",
      "Machine Learning",
      "Predict",
      "Service",
      "Telecommunications",
      "Cliente",
      "Instalação",
      "Previsão",
      "Serviço",
      "Telecomunicações",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23419",
    "title": "Automatização e otimização das análises multimodalidade de Ressonância Magnética Estrutural (Volumetria) e Tensor de Difusão",
    "abstract": "Cada vez mais a relação entre as tecnologias da informação e a saúde seestreitam. Concretamente, na neuroimagiologia, essa ligação tem vindo a tornar-se cada vezmais importante principalmente após o surgimento da imagem de Ressonância Magnética(MRI – Magnetic Ressonance Imaging).Com o desenvolvimento da tecnologia, para além das aquisições MRI convencionais, surgiramoutras técnicas como a aquisição de imagens de tensor de difusão (DTI – Diffusion TensorImaging) e da MRI funcional (fMRI). Estas técnicas permitem a obtenção de uma imageminterior do corpo. Um dos órgãos mais estudados com estas imagens é o cérebro, que é alvode vários estudos mas que devido à sua complexidade ainda é bastante desconhecido.Enquanto que com a MRI estrutural pode-se efetuar uma análise volumétrica às diferentesestruturas do cérebro, com a DTI é possível verificar a integridade da substância brancaatravés das fibras virtualmente criadas que representam o movimento das moléculas de água.Vários estudos referem os benefícios de uma análise multimodal com estas duas técnicas.Para tratamento e análise destas imagens é necessário uma gestão de várias aplicaçõesinformáticas que processam os dados e corregistam as imagens de forma automática. Um dosgrandes desafios consta, não só na utilização individual de cada ferramenta na qual é exigidoalgum conhecimento técnico, como na combinação das várias aplicações que apresentam osdados resultantes em diferentes formatos.Uma solução passa pela pesquisa e definição de fluxos de trabalho para que exista umaabordagem simples dos procedimentos a ter com as várias ferramentas e da sua combinaçãocom outras. No entanto, esta solução não impedirá o gasto de recursos de tempo e o trabalhomoroso de um estudo que contenha vários sujeitos.Assim, neste trabalho, para além de serem apresentados os vários fluxos de trabalho possíveispara análise multimodal, será exposto um módulo automatizado que será inserido numaaplicação de multimodalidade já existente: BrainCat.A presente dissertação apresenta um meio de facilitar as análises multimodais para que aqualidade quer a nível de investigação científica quer a nível dos diagnósticos clínicos aumente.",
    "authors": [
      "Maia, Liliana Filipa Costa"
    ],
    "keywords": [
      "BrainCat",
      "DTI",
      "MRI",
      "Multimodalidade",
      "Multimodality",
      "Volumetry",
      "681.3:61",
      "61:681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79548",
    "title": "Building an automated platform for the classification of peptides/proteins using machine learning",
    "abstract": "One of the challenging problems in bioinformatics is to computationally characterize sequences, structures and functions of proteins. Sequence-derived structural and physico-chemical properties of proteins have been used in the development of machine learning models in protein related problems. However, tools and platforms to calculate features and perform Machine learning (ML) with proteins are scarce and have their limitations in terms of effectiveness, user-friendliness and capacity. Here, a generic modular automated platform for the classification of proteins based on their physicochemical properties using different ML algorithms is proposed. The tool developed, as a Python package, facilitates the major tasks of ML and includes modules to read and alter sequences, calculate protein features, preprocess datasets, execute feature reduction and selection, perform clustering, train and optimize ML models and make predictions. As it is modular, the user retains the power to alter the code to fit specific needs. This platform was tested to predict membrane active anticancer and antimicrobial peptides and further used to explore viral fusion peptides. Membrane-interacting peptides play a crucial role in several biological processes. Fusion peptides are a subclass found in enveloped viruses, that are particularly relevant for membrane fusion. Determining what are the properties that characterize fusion peptides and distinguishing them from other proteins is a very relevant scientific question with important technological implications. Using three different datasets composed by well annotated sequences, different feature extraction techniques and feature selection methods (resulting in a total of over 20 datasets), seven ML models were trained and tested, using cross validation for error estimation and grid search for model selection. The different models, feature sets and feature selection techniques were compared. The best models obtained for distinct metric were then used to predict the location of a known fusion peptide in a protein sequence from the Dengue virus. Feature importances were also analysed. The models obtained will be useful in future research, also providing a biological insight of the distinctive physicochemical characteristics of fusion peptides. This work presents a freely available tool to perform ML-based protein classification and the first global analysis and prediction of viral fusion peptides using ML, reinforcing the usability and importance of ML in protein classification problems.",
    "authors": [
      "Sequeira, Ana Marta Fernandes Tavares"
    ],
    "keywords": [
      "Aprendizagem máquina",
      "Classificação de péptidos",
      "Péptidos de fusão viral",
      "Machine Learning",
      "Peptide Classification",
      "Viral Fusion Peptides",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59829",
    "title": "Analysis of brain connectivity in fMRI: a Deep Learning approach",
    "abstract": "The brain functional connectivity extracted from rs-fMRI has been used as a powerful tool to study the different networks in the brain. This neuronal network, found in normal condition, can be associated to different cognitive processes. The applicability of these networks in the future is promising, since is a greater technique to study the effects of several diseases or even treatments on normal brain functional connectivity. Firstly, this question should be addressed: are these networks possible to be described and to be used as features to classify a group or a particular subject?.In order to answer this question, it was settled the use of a Machine Learning method, which has been developed great advances in the recent years, due the good performances in the Deep Learning (DL) method. Therefore, it was created a workflow since the beginning, started with data acquisition until the application of DL methods and the process of creation and fine-tune of these models. In the end, several studies using the functional connectivity were done, namely the assessment of the brain functional connectivity to be used as a “fingerprint”. Additionally, it were performed some tests regarding the groups’ classification.After settled the correct approach and validate the DL framework, the “fingerprint” study showed a great improvement on impairment classification, even for simple models. We proved that rs-fMRI can be use in research field to identify singular brain patterns as well as the differences between the subjects, which could be applied as group differentiator in a population.",
    "authors": [
      "Ramalhosa, Ivo Miguel Marques"
    ],
    "keywords": [
      "Medical informatics",
      "Neuroimaging",
      "fMRI",
      "Functional connectivity",
      "Deep learning",
      "Informática médica",
      "Neuroimagem",
      "Conectividade funcional",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79810",
    "title": "Sistema de alerta rodoviário baseado em comunicações V2X",
    "abstract": "Quando falamos em utilizadores em risco num cenário rodoviário, vem-nos à cabeça os peões, ciclistas e possivelmente os motociclistas. Este tipo de utilizadores, particularmente se se tratarem de crianças ou idosos, são especialmente vulneráveis em cenários de pe-rigo quer por erro humano ou por agentes externos. O trabalho realizado no contexto desta dissertação tem o intuito de diminuir o número de fatalidades ocorridas em situações de perigo nas nossas estradas, e tem como objetivo providenciar um método auxiliar de indicação destas mesmas situações de perigo. O tema das comunicações entre veículos tem vindo a ser abordado há alguns anos, mas ainda há muito trabalho que pode ser desenvolvido nesta vertente. Aplicações capazes de comunicar diretamente com veículos circundantes podem aumentar a segurança rodoviária como um todo, não só dos peões como também dos condutores. A expansão da Internet aos veículos vem possibilitar esta abordagem, sendo que é esperado que futuramente todos os veículos produzidos estejam apetrechados com algum tipo de mecanismo de comunicação. A ideia aqui patente é a de alargar os alertas existentes em veículos mais atuais para os dispositivos móveis dos peões, ou em sentido contrário, comunicar a presença, posição ou até direção de um peão ao veículo. A principal dificuldade nesta abordagem continua a ser tecnológica, isto dado o facto de não existir uma tendência para tecnologia comum de ampla utilização. Atualmente os dis-positivos móveis utilizam tecnologias WiFi/ 4G e os veículos sobretudo DSRC (brevemente LTE-V). Esta dificuldade inicial poderá, contudo, ser ultrapassada com recurso a dispositi-vos multi-tecnologia, instalados por exemplo na berma da via pública, sendo estes capazes de agregar e redistribuir informação. Neste trabalho apresenta-se uma proposta para um sistema de alertas de segurança ro-doviária, de baixo custo, para condutores e utilizadores vulneráveis, baseado no uso de dispositivos móveis. A arquitetura do sistema é descrita, apresentado alternativas e jus-tificando as decisões tomadas. O sistema proposto foi implementado na totalidade num protótipo que foi posteriormente testado em cenários realistas com ajuda de um simula-dor construído para o efeito. Os resultados obtidos permitem constatar a sua viabilidade prática e o funcionamento de acordo com o esperado.",
    "authors": [
      "Miguel, João Rui de Sousa"
    ],
    "keywords": [
      "Comunicação",
      "Peão",
      "V2X",
      "Veículo",
      "Communication",
      "Pedestrian",
      "Vehicle",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80745",
    "title": "Distributed deep learning for sleep apnea detection on ECG signals",
    "abstract": "A huge amount of medical data is being generated each day, leaving the doctors unableto analyze such volume and make a good diagnosis for the patient. The emergence of BigData frameworks for data analysis leverages the automatic analysis of healthcare data in afaster and accurate manner, by scanning which information is relevant, and, consequently,detecting diseases in earlier stages.Nowadays, it is estimated that about 9% to 38% of the world’s population has sleep ap nea. Unawareness of the disease’s presence can lead to the development of cardiovasculardiseases, and consequently, death. The detection of sleep apnea syndrome through the tra ditional method, Polysomnography (PSG), becomes not only expensive but also inconvenientfor the patient. Therefore, systems based on Electrocardiogram (ECG) can improve the qua lity of a patient’s health by overcoming these inconveniences. This master thesis relies ondeep learning (DL) networks, such as convolutional and recurrent neural networks for sleepapnea detection. The computational complexity of these models depends on its size, typesof layers and data. This complexity also increases the computation time of the training taskleading to several hours spent training on a single machine. For this work, we proposea sleep apnea detection system based on ECGs, alongside with a distributed version of it,which parallelizes the training computation, reducing the overall learning time, while notcompromising the model performance.The results obtained for sleep apnea detection encourage the use of electrocardiogramsfor the detection of this disease. Our model achieved a value of 93% of sensitivity onthe Physionet database, being the highest value compared to other studies described inthe literature. Besides this, on the distributed environment it was accomplished similaroutput quality, reducing the training time by approximately 50%, from the centralized todistributed learning.The model was trained with the Sleep Heart Health Study (SHHS) data, achieving the highestresults compared to the work described in the literature that used the same dataset. Incomparison with the previous dataset, the model trained and tested with the SHHS wasnot able to attain a similar quality output. However, this corroborates the large diversity ofthe SHHS data. Moreover, when it was tested if this model could classify the Physionet data,it achieved promising results of 73,7%, 73,8%, 68,3% and 63,5% of accuracy, sensitivity, F1-score, and precision, respectively, which lead us to conclude that the SHHS trained modelcould be able to generalize to new data. In addition to this, on the distributed environment it was achieved equal output perfor mance for SHHS, reducing the training time by approximately 90%.",
    "authors": [
      "Machado, Ana Margarida da Silva"
    ],
    "keywords": [
      "Deep learning",
      "Sleep apnea",
      "Distributed deep learning",
      "Electrocardiogram",
      "Apneia do sono",
      "Deep learning distribuído",
      "Electrocardiograma",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47840",
    "title": "Especificação e desenvolvimento de um módulo de avaliação para um sistema de ensino inteligente",
    "abstract": "Hoje em dia, o recurso às novas tecnologias nas mais diferentes áreas doconhecimento tem sido evidente. A área da educação não foi, pois, exceção.Cada vez mais se tentam arranjar ferramentas tecnológicas que permitam aqualquer aluno ter acesso a um processo de aprendizagem mais simples eeficaz. Dentro desse leque variadíssimo de ferramentas encontramos os ITS(intelligent tutoring system). Genericamente, estes sistemas têm comoobjetivo fornecer instruções a alunos sem a intervenção direta de umprofessor. Para que isso possa acontecer, com sucesso, é necessário que estessistemas possuam uma base de conhecimento fiável, com conteúdosadequados em todas as vertentes de ensino que promovem. Nesta dissertaçãotivemos como base de trabalho a conceção e o desenvolvimento de ummódulo de avaliação para um sistema de ensino inteligente, com capacidadepara fazer a monitorização de todas as atividades desenvolvidas ao longo deum processo de aprendizagem ou de aferição de conhecimento. Com estamonitorização pretende-se fazer a criação de perfis de aprendizagem paratodos os alunos que utilizem o sistema, de forma a que se possa personalizaros processos de ensino, indo em contra às principais necessidades deaprendizagem dos alunos. Nesta dissertação descrevemos o trabalhorealizado no desenvolvimento do módulo de avaliação referido, dandoparticular atenção aos aspetos relacionados com a seleção e implementaçãodos mecanismos de profiling para a criação automática de perfis deaprendizagem, aos diferentes métodos de avaliação estudados, à preparaçãodos dados e serviços subjacentes ao seu armazenamento e processamento, àarquitetura do sistema de avaliação desenvolvido e, por fim, à demonstraçãodas funcionalidades implementadas.",
    "authors": [
      "Martins, Mónica Filipa Casanova"
    ],
    "keywords": [
      "Sistemas de apoio ao ensino",
      "Sistemas de ensino inteligentes",
      "Personalização de processos de aprendizagem",
      "Geração e manutenção de perfis",
      "Tutores artificiais",
      "Cartilha Maternal de João de Deus",
      "Learning support systems",
      "Intelligent tutoring systems",
      "Personalization of learning processes",
      "Profile maintenance",
      "Artificial tutors",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79966",
    "title": "Applying machine learning algorithms to medical knowledge",
    "abstract": "Achieving great and undeniable success in a great variety of industries and businesses has made the term Big Data very popular among the scientific community. Big Data (BD) refers to the ever fast-growing research area in Computer Science (CS) that comprises many work areas across the world. The healthcare sector is widely known to be highly proficient inthe production of big quantities of data. It can go from health information, such as thepatient’s blood pressure and cholesterol levels, to more private and sensitive data, such asthe medical procedures history or the report of ongoing diseases.The application of sophisticated techniques enables a profound and rigorous analysis ofdata, something a human cannot do in real-time. However, a machine is capable of rapidlycollect, group, storage and examine vast amounts of data and extract unknown and possi bly interesting knowledge from it. The algorithms used can discover hidden relationshipsbetween attributes that prove to be very useful for a corporation’s work. Buried structureswithin the produced data can also be detected by these techniques. Machine Learning (ML)methods can be adjusted and modelled to different input representations - this adaptabilityis one of the factors that contributes to its blooming prosperity.The main goal is to make predictions on data, by building utterly efficient models that canaccurately take in the data and thus predict a certain outcome. This is especially importantto the healthcare industry since it can considerably improve the lives of many patients.Everything from detecting a type of disease, predicting the chance of morbidity after ahospital stay, to aid in the decision making of treatment strategies are vital to patients aswell as to clinicians.Any improvement over established methods that have been previously studied, testedand published are an asset that will improve the patient’s satisfaction about the healthcareperformance in medical institutions. This can be achieved by refining those algorithms orimplementing new approaches that will make better predictions on the given data.The main objective of this dissertation is to propose ML approaches having acknowledged and evaluated the existent methods used in clinical data. In order to fulfill this goal,an analysis of the state of the art of medical knowledge repositories and scientific paperspublished related to the selected keywords selected was performed. In this line of work,it is crucial to understand, compare and discuss the results obtained to those previouslypublished. Thus, one of the goals is to suggest new ways of solving those problems andmeasuring them up against the existent ones.",
    "authors": [
      "Brito, Maria Ana de"
    ],
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Data mining",
      "Healthcare",
      "Knowledge discovery process",
      "Machine learning",
      "Conhecimento médico",
      "Inteligência artificial",
      "Mineração de dados",
      "Processo de descoberta de conhecimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59878",
    "title": "Definição de middleware ”cloud-based” para serviço a aplicações de monitorização de espaços físicos",
    "abstract": "Com a crescente utilização e adoção de dispositivos orientados à Internet of Things, especialmentedispositivos orientados ao espaço casa, crescem também o número de protocolos,APIs e frameworks desenvolvidos pelos fabricantes para complementar os seus dispositivos.Este elevado número de soluções, sem a adoção de um standard geral que facilita o desenvolvimentode aplicações para esta área, serve de motivação para esta dissertação.Com isto em mente, esta dissertação tem como objetivo o desenvolvimento de uma camadade middleware, que faz a ligação entre os mais diversos dispositivos e as aplicações clientes,abstraindo os dispositivos e as funcionalidades, facilitando imenso o desenvolvimentodestas aplicações. Este middleware também deverá oferecer algumas comodidades a nível deutilização, sob a forma de mecanismos de automação e definição de cenários.A dissertação irá expor o estado da tecnologia nesta área, identificando as mais valias etambém os possíveis problemas das mesmas. Também será revisto algum trabalho de investigaçãona área, de modo a entender os esforços já feitos. Feito o estudo sobre o estado dearte, iremos proceder ao desenho e conceção da arquitetura do middleware, passando para odesenvolvimento do mesmo. Por fim, será feito um caso de estudo, sobre a forma de umaaplicação mobile, que faz uso do middleware, demonstrando todas as suas funcionalidades ecapacidades de interoperabilidade.",
    "authors": [
      "Sousa, José Francisco Ferreira Alves de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/89640",
    "title": "Exploring the use of serious videogames in a robotic walker to improve ataxic gait",
    "abstract": "Ataxia is a neurological sign indicative of dysfunction in the cerebellum, a part of the brain that coordinatesmovement. The typical symptoms include lack of balance when walking or standing, loss of limb coordination,change in speech, and difficulty with fine motor tasks, strongly affecting the person’s daily activities. Cerebellarataxia can be inherited or caused by other disorders such as stroke, multiple sclerosis, or cerebral palsy. Whilethere is no known cure for inherited ataxia, the symptoms can be managed through intensive and personalisedrehabilitation, including physical, speech, and occupational therapy.Physical therapy, also known as conventional therapy, is a standard practice in the rehabilitation of patients withataxia. It focuses on performing different physical exercises repeatedly, where a therapist monitors the session andregulates the intensity. Although effective, the repetitiveness of conventional therapy can become monotonous,besides being a very time-consuming process, which can be cumbersome for the therapist. Without any additionalstimulation and decreasing motivation, patients may experience stagnation or even drop out of therapy.Robot-assisted Gait Training (RAGT) is being introduced in the rehabilitation of persons with motor disabilities.This technology allows personalised and intensity-adapted training, which could benefit the patient’s recovery.However, this therapy can also become monotonous, so there is a need for more interactive and appealingstrategies. A possible solution to this problem is the development of exergames (serious games) and theirinclusion in robotic-assisted therapy. This can become advantageous since the robotic devices integrate severalsensors that read the patient’s movement and can be used as controllers in the game, allowing a more immersiveexperience.Considering this, this dissertation proposes two serious videogames, which were integrated into a roboticwalker, the WALKit Smart Walker, intended for gait ataxia rehabilitation. The first game is cognitive, whose goal isto react to a certain shape or sound as quickly as possible. With this game, it is intended to study the influenceof dual tasking on motor rehabilitation. The second game is a dynamic one whose goal is to incite the patient’sbalance control. This game uses an algorithm for torso orientation estimation to control an avatar holding twobuckets full of water. The main goal is to give patients biofeedback regarding their postural balance and to exercisetheir balance with specific events, or minigames, that encourage them to lean in a way that causes weight transfer.Both games were validated with healthy volunteers, in terms of functionality and usability. The results allowed toconclude that both were fun to play and showed potential to aid rehabilitation. Moreover, the results emphasisedthe relevance of customisation, replayability, and aesthetics in serious games. Future work includes the thoroughvalidation of both serious games with patients with cerebellar ataxia, to assess their effectiveness in rehabilitationalong with WALKit Smart Walker.",
    "authors": [
      "Rodrigues, Alexandre Rzepecki"
    ],
    "keywords": [
      "Ataxia",
      "Balance",
      "Exergames",
      "Motivation",
      "Rehabilitation",
      "Serious games",
      "Equilíbrio",
      "Jogos sérios",
      "Motivação",
      "Reabilitação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92567",
    "title": "Concurrency hot spot optimisation in transactional memory",
    "abstract": "Database management systems have a long history of development and research, with systems like Post greSQL and languages like SQL already being well-established in the industry. Transactional memoryemerges as a new concurrency control mechanism for concurrent programming, inspired by ideas andconcepts from the database world. As is the case with database transactions, transactions in transac tional memory can (and will) conflict when multiple transactions try to modify the same data. This canlead to the appearance of hot spots in contended memory regions, quickly degrading the performance ofan application.In this dissertation, we propose new optimisation techniques for transactional memory hot spots,based on previous research on splitting techniques for numeric database records. We implement theoptimisations on an existing transactional memory system and measure their impact on performance,using custom-made and reference benchmarks.",
    "authors": [
      "Ribeiro, Rui André Santos"
    ],
    "keywords": [
      "Transactional memory",
      "Concurrent programming",
      "Hot spot optimisation",
      "Memória transacional",
      "Programação concorrente",
      "Otimização de hot spots",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94069",
    "title": "Fusão de dados aplicando modelos de machine learning",
    "abstract": "A educação e o sucesso académico são de grande relevância na sociedade atual, nomeadamente para o futuro profissional, económico e pessoal dos jovens. Tendo isto em conta, é de grande interesse e vital importância para as instituições de ensino poder prever a variação das notas dos alunos, principalmente alunos em risco de reprovação, uma vez que se podem alterar métodos de ensino e aplicar medidas corretivas e estratégias de intervenção para apoiar alunos de baixo desempenho, tendo em conta as suas necessidades. A introdução de Inteligência Artificial e de técnicas de Fusão de Dados nesta área pode ser muito interessante, podendo melhorar a eficiência na deteção de alunos em risco de insucesso escolar. Deste modo, esta dissertação visa o desenvolvimento de dois casos de estudo onde se pretende o desenvolvimento de algoritmos e procedimentos relativos à fusão e integração dos dados e a criação demodelos preditivos. No primeiro caso de estudo, é proposto desenvolver modelos de previsão para prever variações nas notas de alunos do ensino básico nas disciplinas de Português e Matemática do primeiro para o segundo período, através da implementação da técnica de early fusion. Como segundo caso de estudo desta dissertação, propôs-se o desenvolvimento de modelos preditivos para a previsão da variaçãodas notas de alunos do ensino secundário nas disciplinas de Português e Matemática do segundo para o terceiro período do 12º ano, havendo, neste processo, a implementação de duas técnicas de fusão de dados - early fusion e late fusion.Diante dos melhores modelos candidatos obtidos, comprovou-se que a fusão de dados obtém um bom desempenho na criação de modelos preditivos para a previsão da variação de notas, e que ambas as técnicas de fusão testadas são competentes, aparentando melhorar os resultados da previsão relativamente a modelos criados a partir dos conjuntos de dados de forma separada. Palavras-chave Fusão de dados, Machine Learning, Insucesso escolar, Early fusion, Late fusion, Edu cação, Inteligência Artificial",
    "authors": [
      "Teixeira, Renata Ferreira"
    ],
    "keywords": [
      "Fusão de dados",
      "Machine learning",
      "Insucesso escolar",
      "Early fusion",
      "Late fusion",
      "Educação",
      "Inteligência Artificial",
      "Data fusion",
      "School failure",
      "Academic performance",
      "Artificial intelligence",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86810",
    "title": "Deep learning generative models for novel enzyme design",
    "abstract": "Recent endeavours over the past few years have been applying generative Deep Learning (DL) models to generate novel proteins using an array of different approaches. Such initiatives represent a specially important development towards major contributions to the field of protein engineering. To contribute to this, various DL architectures can be applied to the different datasets to generate proteins with a particular set of properties. The field of DL applied to the generation of novel molecules has been presenting results that encourage further research on this subject. An increasing number of novel, computationally generated, molecules being synthesized with successful results creates grounds for stimulation of new endeavours and diversification of the current applications.The goal of the work presented in this dissertation is to apply different generative DL architectures to the design of novel protein sequences for a targeted set of optimized properties. The developed framework, termed GenProtEA, stands as the main contribution of this work. The framework envisages the implementation of generative DL architectures for the design of novel proteins and leverages the use sampling techniques and Evolutionary Computation to steer the generative process towards a specific set of properties. Evolutionary Algorithms (EAs) can be applied both to single and multi-objective optimization problems which in itself presents an added advantage. The optimization problems were designed considering the literature concerning protein design. The problems ranged from a simple maximization of the average hydrophobicity of the protein sequence to more complex problems such as minimizing two sets of events in a sequence or maximizing a probability of a protein being generated by a defined profile Hidden Markov Model (HMM). The results of the proposed case studies and the respective analysis accompany the framework in this endeavour.Two different generative DL architectures were deployed, trained, and evaluated, using loss and accuracy metrics to perform the analysis.: a Generative Adversarial Network (GAN) and a Variational Autoencoder (VAE). For the GAN architecture, new proteins are sampled varying the latent seed used in the generative process and then selecting the best candidates for each of the case studies. Besides following a same sampling approach to obtain new protein designs, the VAE latent space is explored using EAs. The results of this work show that the use of EAs in the optimization, steering the generative process, can produce the best results, allowing for more variability in the experiments designed and resulting in a much greater set of possibly functional novel proteins.",
    "authors": [
      "Martins, Miguel"
    ],
    "keywords": [
      "Deep learning",
      "Generative models",
      "Protein design",
      "Evolutionary algorithms",
      "Novel proteins",
      "Algoritmos evolucionários",
      "Design proteico",
      "Modelos generativos",
      "Desenho de novas proteínas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47726",
    "title": "Conceção de Arquitetura de interoperabilidade: o caso Caravela Seguros",
    "abstract": "Nos dias de hoje, existe cada vez mais a necessidade de interligação/integração entre múltiplos sistemas heterogéneos no que respeita a arquiteturas e tecnologias. Para que seja possível a comunicação entre os diversos sistemas recorre-se a serviços de middleware.Estes providenciam Programming Interfaces (APIs) com protocolos e padrões que todos entendem e através dos quais conseguem comunicar.Assim, também é possível fornecer soluções de interoperabilidade com sistemas já existentes e que à partida não estarão preparados para a heterogeneidade de sistemas e arquiteturas que entretanto surgiram.Desta forma, este tema de dissertação pretende conceber uma solução de middleware que possibilite a interoperabilidade com o sistema existente da Caravela Seguros, através da análise e conceção de uma arquitetura e posteriormente elaborando alguns serviços que provem a validade da solução encontrada. Assim, pretende-se obter um sistema que facilite a utilização dos sistemas da Caravela Seguros por parte das entidades que com ela se relacionem.",
    "authors": [
      "Sousa, Ana Zita Pinto de"
    ],
    "keywords": [
      "Interoperabilidade",
      "Middleware",
      "Sistemas Legacy",
      "Web services",
      "Interoperability",
      "Legacy Systems",
      "Web services",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79853",
    "title": "Applying data science and machine learning for psycho-demographic profiling of internet users",
    "abstract": "There always have been a huge interest in working with public data from online socialmedia users, with the exponential growth of social media usage, this interest and re searches on the area keep increasing.This thesis aims to address prediction and classification tasks on online social net work data. The goal is to predict psycho-demographic - personality and demographic -traits by doing text emotion analysis on social networks as Twitter and Facebook. Ourmain motivation was to raise awareness to what can be done with users’ social mediaor network information or usual behaviours on the web, such as from text analysiswe can trace their personality, know their tastes, how they behave and so on, and tospread the emotion-text relation on social networks subject, because it only started tobe studied recently and there’s so much data and information to do it.To perform these tasks mentioned above we carried an extensive review of literatureof previous works to define the state-of-art of the project and to learn and identify workstrategies. Almost all of the past researches, based their results on a vast sample ofusers and data, but because some frameworks and APIs were shutdown in recent years,such as MyPersonality from Facebook adding to some frameworks being paid for,resulted in a small sample of users’ data to analyze in our thesis which can prejudicethe results.We start by gathering data from Twitter and Facebook with users consent. On Twit ter we focused on tweets and retweets, on Facebook we focused on all of what theuser typed by using the DataSelfie plugin that stored all that data on a server thatcan be retrieved later. Our next step was to find emotions on their text data with thehelp of a lexicon that categorized words by eight different emotions, two of them wereput away because we focused only on the six major emotions - this is explained later- and we had to remove stopwords and apply stemming to all of the text and do aword-matching of every word of our data with every word from the lexicon. Afterthis, we asked our participants to fulfill a \"Big-Five\" personality questionnaire and toprovide us their age, so we added the Big-Five traits and age to each users individualdataset. We got their final versions, ready to apply machine-learning algorithms tofind correlations between emotions and personality or demographic attributes. Wefocused on practical and methodological aspects of the user attribute prediction task.We used many techniques and algorithms that we thought it were best fit for the datawe had and for the goal that we had to achieve.We gathered data in two datasets that we tested, one of them we called \"Mixed Lan guage Dataset\", contains all text entries from each user, and the other \"User Dataset\",contains one entry per user after we analyze every text entry for all users in order tohave a more general view on each one. For the first mentioned dataset we achievebest results with the decision trees algorithms, from 58% on the agreeableness trait,to 68% on the neuroticism trait. This dataset had a problem with the way data wasspread, so it was impossible to predict age and gender with efficiency. As for the lat ter, regarding demographic characteristics all of the classifiers had a good classifyingpercentage, from K-nearest’s 73% to Naive Bayes’ 95%. The most solid classifier forpersonality traits was the one using the CART decision tree algorithm, it ranged from50% on the openness trait to 76% on the agreeableness one. There were classifiers withterrible results, there were others that were a bit dull, and there were some that stoodout as we stated above. We had a small sample, and that was a problem as it wasn’tconsistent or solid in terms of data value and that can change our results, we believethat our results would be way better if we applied the same mechanisms to a muchbigger sample.Concluding, we demonstrate how we can predict personality or demographic traits- BigFive traits, age or gender - from studying emotions in text. As stated above, wehope this thesis will alert people for what can be done with their online information,we only focus on psycho-demographic profiling, but there are many other things thatcan be done.",
    "authors": [
      "Ribeiro, Hugo"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/20352",
    "title": "Serviço Centralizado de Estatísticas de Utilização de Repositórios",
    "abstract": "Nos dias de hoje presenciamos a uma mudança de paradigma no que dizrespeito aos repositórios institucionais. Passamos do repositório isoladono seu contexto institucional para os consórcios onde conjuntos de repositóriospartilham ideias, políticas e tecnologias, contribuindo para ocrescimento do conhecimento das comunidades em que estão inseridos.Podia falar-se da importância que os repositórios institucionais mantêmjunto da sua comunidade, mas com a mudança de paradigma torna-serelevante explorar os desafios de gestão que o novo contexto apresenta.Para gerir um consórcio é necessário possuir indicadores que auxiliemna tomada de decisão e que atendam às necessidades de informação queas entidades de fomento possuem no que diz respeito ao impacto dosinvestimentos em cultura, investigação, inovação e desenvolvimento.Esta dissertação apresenta o Serviço Centralizado de Estatísticas de Utilizaçãode Repositórios (SCEUR). Trata-se de um projeto inserido noâmbito da iniciativa Repositório Científico de Acesso Aberto de Portugal(RCAAP) que visa a construção de uma arquitetura que permita recolher,processar e apresentar de uma forma intuitiva dados estatísticosde utilização em repositórios institucionais e também auxiliar a partilhade dados estatísticos quer pela disponibilização de add-ons que facilitemessa tarefa no software usado pelas instituições quer pela disponibilizaçãode informação e recomendações variadas nesse contexto. São tambémapresentados projetos internacionais de referência neste contexto,normas existentes e tecnologias usadas para a implementação dos conceitossubjacentes.",
    "authors": [
      "Silva, Hélder de Jesus Almeida da"
    ],
    "keywords": [
      "SCEUR",
      "Estatísticas",
      "Statistics",
      "OAI-PMH",
      "ContextObjects",
      "Harvest",
      "KEEPS",
      "RCAAP",
      "02:681.3",
      "681.3:02"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "02:681.3",
      "681.3:02"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/62496",
    "title": "Aproximação digital das populações às suas instituições públicas",
    "abstract": "Com o crescimento significativo de utilizadores de dispositivos móveis, nos últimos anos,mais do que a necessidade apareceu a oportunidade de criar novas plataformas e serviçosdigitais que não só facilitam o quotidiano das pessoas, evitando deslocamentos, filas deespera e complicações desnecessárias, como tornam a comunicação das pessoas com asinstituições num processo mais rápido e cómodo. Como tal, estas alternativas estão gradualmentea complementar, em alguns casos mesmo a substituir, os métodos antigos.Esta dissertação propõe uma solução digital, na forma de uma aplicação móvel, para aproximaras populações às suas instituições públicas recorrendo a gamification, isto é, transformandoo processo de comunicação com uma Câmara Municipal ou Junta de Freguesia,por exemplo, recorrendo ao uso de elementos de jogos como conquistas, atribuição derecompensas, classificações, entre outras, com o objetivo de estimular a comunicação doutilizador com essas instituições permitindo que este possa ao mesmo tempo divertir-see competir com os outros, enquanto explora e ajuda a sua cidade. Para tal, além de umserviço de participação de ocorrências na cidade, os utilizadores têm à sua disposição todauma narrativa que os leva a completar missões, participar em eventos e conhecer gentenova bem como a conhecer verdadeiramente a sua própria cidade. Por sua vez, a instituiçãopública responsável pela manutenção e administração do sistema tem à sua disposição umaplataforma de administração para manter o conteúdo da aplicação atualizado para os seusutilizadores.No decorrer desta dissertação está documentado todo o processo de desenvolvimento daaplicação móvel e do servidor web, que a suporta, assim como todas as decisões tomadase as razões que as justificam. O documento contém, ainda, exemplos e explicações do funcionamentoda aplicação, considerações finais sobre o projeto e ideias para trabalho futuro.Nas considerações finais é feita uma comparação dos objetivos inicialmente traçados parao projeto e o resultado alcançado, provando que os mesmos foram cumpridos com sucesso.",
    "authors": [
      "Monteiro, Vasco F. S."
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92592",
    "title": "Deteção de veículos em parques de estacionamento através de machine learning",
    "abstract": "O tema principal abordado nesta dissertação é a classificação de imagens através de redes neuronais. O crescimento do estudo desta área da inteligência artificial permite a que, atualmente, os sistemas sejam mais eficientes. A forma mais ancestral da gestão dos parques de estacionamento passa pela colocação de sensores de movimento ou de presença nos lugares de estacionamento. Tal organização leva a grandes despesas na aquisição e manutenção do material. A implementação de um sistema de gestão centralizado e com recurso a métodos inteligentes, além de diminuir as despesas dos materiais, uma vez que uma câmara vem substituir os vários sensores, também leva a um maior conforto por parte dos utilizadores ao nível da simplicidade em encontrar uma vaga no parque de estacionamento.A metodologia desta dissertação passa por implementar várias redes neuronais e decidir qual é a que obtém um maior número de previsões realizadas com sucesso. Foram utilizados dois tipos de redes neuronais: as Multi-Layer Preceptron (MLP) e as Convolution Neural Networks (CNN). Todas as redes em estudo foram alimentadas com o dataset criado na plataforma CoppeliaSim, onde são criadas simulações de imagens captadas por uma câmara num parque de estacionamento.Os resultados dos testes realizados mostram que tanto as redes MLP como as redes CNN obtêm bons resultados no projeto implementado, comprovando que as imagens podem ser observadas como uma sequência de pixéis e, dessa forma, padronizadas.O presente trabalho constitui uma forte contribuição na crescente área de estudo das redes neuronais pois demonstra que, selecionando os parâmetros de rede adequados, é possível aumentar a sua eficiência. Mais ainda, as redes MLP conseguem apresentar melhores resultados comparativamente àsredes CNN, preparadas e desenhadas para a interpretação de imagens.",
    "authors": [
      "Coelho, Sara Daniela da Silva"
    ],
    "keywords": [
      "Machine learning",
      "Redes neuronais",
      "Classificação de imagens",
      "Otimizadores",
      "Neural networks",
      "Image classification",
      "Optimizers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82801",
    "title": "Modelação e previsão de decisões judiciais utilizando um repositório de sentenças",
    "abstract": "O ritmo da evolução tecnológica e a sua relação com o ser humano tem aumentado significativamente aolongo dos tempos, sendo que um dos ramos que mais impacto está a causar no quotidiano das pessoasé a inteligência artificial. A grande ascensão desta área é um fenómeno transversal a praticamente todosos setores da sociedade. Esta dissertação enquadra-se no setor da justiça.Ao longo dos anos, um dos principais problemas nos sistemas judiciais por todo o mundo é a mo rosidade na resolução dos processos judiciais. Tendo por base esta problemática, as entidades governa mentais adotam cada vez mais reformas na área da justiça com recurso à tecnologia, desejando sistemasjudiciais cada vez mais eficientes.Neste sentido, esta dissertação tem como objetivo o desenvolvimento de uma solução capaz de extrairconhecimento a partir de dados jurídicos portugueses. Esta solução é caracterizada por um conjunto demecanismos, com recurso a técnicas de inteligência artificial, que vai desde a extração de dados até àrealização de duas grandes experiências: análise de sentimentos e previsão da decisão. Estes mecanismospermitiram gerar diversas informações e conhecimento. Por um lado evidenciou-se a pouca relação entrea carga emocional dos textos dos juízes e a decisão, por outro destaca-se o desenvolvimento de modelosinteligentes capazes de prever a decisão com precisões de média de 76%, recorrendo ao conteúdo textual.Além disso, ao longo de todo o processo, também foram extraídas outras informações, como por exemplo,as palavras relacionadas com a procedência e improcedência de acórdãos, a legislação que é geralmentecitada em conjunto, entre outras. Em paralelo desenvolveu-se um protótipo de uma dashboard com aapresentação de informações e conhecimento de alto nível sobre os dados.",
    "authors": [
      "Rodrigues, Joel Soares"
    ],
    "keywords": [
      "Justiça",
      "Decisões judiciais",
      "Inteligência Artificial",
      "Processamento de linguagem natural",
      "Machine Learning",
      "Justice",
      "Judicial decisions",
      "Artificial Intelligence",
      "Natural language processing",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23574",
    "title": "Aquisição, tratamento, arquivo e difusão de exames de endoscopia",
    "abstract": "De entre os diversos tipos de exames de endoscopia, a esofagogastroduodenoscopia assume um papelpreponderante devido a ser o método ideal para examinar a mucosa do trato digestivo alto, bem comopara detetar inúmeras patologias gastrenterológicas. O resultado deste tipo de exames é, geralmente, umrelatório composto por um conjunto de frames capturados durante o exame, eventualmente acompanhadopor um vídeo. Hoje em dia, apenas as imagens juntamente com o relatório endoscópico, são arquivadas.O facto de o vídeo não ser arquivado pode conduzir a um incómodo no bem-estar do paciente, assimcomo a um acréscimo de custos e tempo despendido, pois frequentemente o mesmo é necessário pararevisão e validação da hipótese de diagnóstico, bem como para comparação de segmentos do vídeo comexames futuros. Mesmo nos casos em que a informação é arquivada, a falta de reutilização e partilha deinformação e vídeos entre entidades contribui, mais uma vez, para uma repetição desnecessária deexames.A existência de um arquivo de vídeos endoscópicos seria uma mais-valia, pois além de resolver osproblemas referidos ainda possibilitaria a sua utilização para fins de pesquisa e investigação, além dedisponibilizar exames para servirem como referência para estudo de casos similares.Neste trabalho é proposta uma solução abrangente para a aquisição, tratamento, arquivo e difusão deexames de endoscopia. O objetivo passa por disponibilizar um sistema capaz de gerir toda a informaçãoclínica e administrativa (incluindo conteúdo audiovisual) desde o seu processo de aquisição até aoprocesso de pesquisa de exames antigos, para comparação com novos casos. De forma a garantir acompatibilidade lexical da informação partilhada no sistema, foi utilizado um vocabulário endoscópicoestandardizado, o Minimal Standard Terminology (MST). Neste contexto foi planeado um dispositivo(MIVbox) orientado à aquisição do vídeo endoscópico, independentemente da câmara endoscópicautilizada. Toda a informação é armazenada de forma estruturada e normalizada, possibilitando a suareutilização e difusão. Para facilitar este processo de partilha, o vídeo sofre algumas etapas deprocessamento, de forma a ser obtido um vídeo reduzido e as respetivas características do conteúdo.Deste modo, a solução proposta contempla um sistema de anotação que habilita a pesquisa por conteúdo,servindo assim como uma ferramenta versátil para a investigação nesta área. Este sistema é ainda dotadode um módulo de streaming, no qual é transmitido, em tempo real, o exame endoscópico,disponibilizando um canal de comunicação com vídeo unidirecional e áudio bidirecional, permitindo que osprofissionais ausentes da sala do exame deem a sua opinião remotamente.",
    "authors": [
      "Laranjo, Isabel"
    ],
    "keywords": [
      "616-072.1",
      "616.3",
      "681.3:61",
      "61:681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616-072.1",
      "616.3",
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81330",
    "title": "Development of deep learning-based tools for the design of new compounds with desired biological activities",
    "abstract": "In the last few years, de novo molecular design has increasingly been using generative models, from theemergent field of Deep Learning (DL), to propose novel compounds that are likely to possess desiredproperties/activities, in areas such as drug discovery, materials sciences or biotechnology. A panoplyof deep generative models, such as Recurrent Neural Networks, Variational Autoencoders, AdversarialAutoencoders and Generative Adversarial Networks, can be trained on existing datasets, and provide forthe generation of novel compounds, typically with similar properties of interest. Additionally, differentoptimization strategies, including transfer learning, Bayesian optimization, reinforcement learning, andconditional generation, can be used to direct the generation process towards desired aims, regarding theirbiological activities, synthesis processes or chemical features. Various instances of experimental validationof these emerging methods have surfaced, with de novo generated molecules being synthesized andproving successful in in vitro, and even in vivo, assays. These successful practical realizations encouragefurther research into this blooming field.This dissertation aims to explore the application of generative DL to the de novo molecular design, witha focus on the targeted generation of new compounds. Two frameworks were developed to support thisendeavor and stand as the main contributions of this work. The first, termed DeepMolGen, standardizesthe implementation and usage of various generative DL architectures for molecular design. The second,termed EAMO, employs multi-objective evolutionary algorithms to navigate the latent space of autoencoderbased models, optimizing the generation of molecules with desired characteristics. These frameworks wereaccompanied with a systematic and critical review on deep generative models, the related optimizationmethods for targeted compound design, and their applications.Four state-of-the-art architectures were implemented, trained and evaluated under the DeepMolGenframework using a standard dataset and common metrics such as validity, uniqueness, novelty and theMOSES benchmark. The results showed that DeepMolGen was capable of performing the intended tasksand that most of the implemented models performed on par with their publications. Similarly, four casestudies from the literature were optimized with EAMO and the results compared to previous works. Theseexperiments showed that EAMO could control abstract chemical properties and is competitive with otherstate-of-the-art methods. Lastly, the three best performing models were combined with transfer learningand EAMO within a pipeline for the generation of sweeteners. The resulting set of 102 promising moleculeswas reviewed by expert chemists and the pipeline improved with their feedback. A second set of 99compounds was then generated and the preliminary observations pointed to significantly improved results.",
    "authors": [
      "Sousa, Tiago Filipe Escairo"
    ],
    "keywords": [
      "Deep Learning",
      "Generative models",
      "Molecular design",
      "Multi-objective evolutionary algorithms",
      "Novel sweeteners",
      "Algoritmos evolucionários multi-objectivo",
      "Desenho molecular",
      "Modelos generativos",
      "Novos adoçantes",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81331",
    "title": "eIDAS qualified trust services: serviço de preservação",
    "abstract": "De forma a uniformizar o mercado Europeu e conseguir mais confiança nas transaçõeseletrónicas (sic Considerando 2º do Regulamento eIDAS (2014)), a União Europeia publicouo Regulamento UE nº 910/2014 (Regulamento eIDAS (2014)), também conhecido comoRegulamento Eletronic Identification, Authentication and Trust Services (eIDAS). Este normativolegal pretende regular as assinaturas e selos electrónicos, a identificação eletrónica e osserviços de confiança dentro do Espaço Europeu. O objetivo deste regulamento é permitirtransações seguras e eficazes entre negócios, pessoas e as autoridades públicas.Para atingir o seu objetivo, o Regulamento eIDAS introduziu o conceito de serviçosde confiança qualificados. Os serviços de confiança qualificados permitem às assinaturaseletrónicas o efeito legal equivalente a uma assinatura manuscrita, quando baseadas numcertificado qualificado de assinatura eletrónica emitido por uma entidade que está integradana lista de confiança de um determinado Estado Membro. Estas assinaturas são intituladasde assinaturas eletrónicas qualificadas e são reconhecidas nos restantes Estados Membros.Tribunais (ou outros órgãos encarregados de procedimentos legais) não podem descartá-lascomo prova apenas porque são eletrónicas, têm de avaliá-las da mesma forma que fariamcom o seu equivalente em papel. (sic Artigo 25º do Regulamento eIDAS (2014))A necessidade de preservação de longo prazo de assinaturas eletrónicas é reconhecidano seio da União Europeia (UE). No Regulamento eIDAS, entre os serviços de confiançaqualificados introduzidos, encontra-se o serviço de preservação qualificado. Um serviçode preservação qualificado tem como objectivo preservar o estado de validade de umaassinatura eletrónica qualificada ao longo do tempo.Esta dissertação tem o seu foco no desenvolvimento de uma Prova de Conceito do serviçode confiança qualificado de preservação de assinaturas e selos eletrónicos qualificados, quese antecipa que comece a ser utilizado massivamente nos próximos anos.",
    "authors": [
      "Fernandes, João Manuel da Silva Gomes"
    ],
    "keywords": [
      "União Europeia",
      "Assinatura eletrónica qualificada",
      "Serviço preservação de longo termo",
      "eIDAS",
      "Validade das assinaturas eletrónicas",
      "Lista de confiança",
      "Europian Union",
      "Qualified electronic signature",
      "Long-term preservation service",
      "Eletronic signature validity",
      "Trust list",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80055",
    "title": "Retopology: a comprehensive study of current automation solutions from an artist’s workflow perspective",
    "abstract": "Topology (the density, organization and flow of a 3D mesh’s connectivity) constrains the suitability of a 3D model for any given purpose, be it surface showcasing through renders, use in real-time engines, posing or animation. While some of these use cases might not have very strict topology requirements, others may demand optimized polygon counts forperformance reasons, or even specific geometry distribution in order to take deformation directions into account.Many processes for creating 3D models such as sculpting try to make the user unaware ofthe inner workings of geometry, by providing flexible levels of surface detailing throughdynamic geometry allocation. The resulting models have a dense, unorganized topologythat is inefficient and unfit for most use cases, with the additional drawback of being hardto work with manually.Retopology is the process of providing a new topology to a model such as these, whilemaintaining the shape of its surface. It’s a technical and time-consuming process that clasheswith the rest of the artist’s workflow, which is mainly composed of creative processes.While there’s abundant research in this area focusing on polygon distribution qualitybased on surface shape, artists are still left with no options but to resort to manual workwhen it comes to deformation-optimized topology.This document exposes this disconnect, along with a proposed framework that attemptsto provide a more complete retopology solution for 3D artists. This framework combinestraditional mesh extraction algorithms with adapting manually-made meshes in a pipelinethat tries to understand the input on a higher level, in order to solve deficiencies that arepresent in current retopology tools.Our results are very positive, presenting an improvement over state of the art solutions,which could possibly steer discussion and research in this area to be more in line with theneeds of 3D artists.",
    "authors": [
      "Silva, António Pedro Carvalho Machado da"
    ],
    "keywords": [
      "Retopology",
      "Topology",
      "Remeshing",
      "Quad mesh extraction",
      "Surface registration",
      "3D modeling",
      "Computer graphics",
      "CG animation",
      "CG art",
      "Retopologia",
      "Topologia",
      "Extração de quad meshes",
      "Adaptação de superfícies",
      "Modelação 3D",
      "Computação gráfica",
      "Animação CG",
      "Arte CG",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83936",
    "title": "Anotação automática de textos para análise e identificação de conteúdo",
    "abstract": "Automatic text annotation systems are mechanisms that aim to provide assistance to users who need to extract and annotate relevant information in a given text. Usually, this type of system is developed for very specific application domains, in order to facilitate research processes on text content. The works of this dissertation will be developed based on the Tombo da Mitra, a codex that contains the inventory of the properties of the Archbishop’s Table of Braga, in the 17th century. The quantity and diversity of the elements referred to in the book are impressive, as it contains all the names and surnames, settlements, professions, types of land and buildings, among many other elements, which are very important for the study and learning of geography, culture, economy, architecture, religion and portuguese language of the 17th century. The annotation of these elements expressively shows their location in time and space, as well as their potential relationships, facilitating the study of the book and providing linguistic researchers, teachers and students with a valuable instrument to reach and reinforce knowledge about the book. In this dissertation, we present a tool specially designed for the annotation of documents in the Livro das Propriedades, allowing the management and listing of annotation tags and providing a clearer view of the content of the manuscript.",
    "authors": [
      "Santos, Tiago Miguel Fraga"
    ],
    "keywords": [
      "Annotation systems",
      "Automomatic tagging",
      "Data analysis",
      "Text mining",
      "Natural language processing",
      "Machine Learning",
      "Sistemas de anotação",
      "Tagging automático",
      "Análise de dados",
      "Text Mining",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66996",
    "title": "A mobile health application to assist professionals: a case study in a portuguese nursing home",
    "abstract": "Over the past few years, the world has been witnessing a huge demographic change: theaging population has been growing at an alarming rate. This problem has been a matterof concern for many countries since it has been posing several challenges to healthcaresystems worldwide. In Portugal, which is one of the countries with the largest aging population,this demographic change has led to several issues. In fact, in Portugal, the nursinghomes have been getting a higher demand, and health professionals are overloaded withwork. Furthermore, the fact that nursing homes still use paper to record information and toclinically manage their residents is another tremendous problem since this method is moreprone to errors and time-consuming.In this context, the present master’s dissertation emerged and consisted in the designand development of a mobile application for the health professionals, i.e. the nurses anddoctors, working in a Portuguese nursing home, more specifically in one of the nursinghomes of Santa Casa da Miseric´ordia de Vila Verde. This mobile application was developedto help the health professionals to clinically manage the residents and to assist them at thepoint-of-care, namely to schedule, perform, and record their daily tasks and to have accessand manipulate information.Additionally, the present dissertation also included the definition of clinical and performanceindicators to assist the decision-making process. It is important to mention that amobile solution was chosen since a hand-held device, which can be used anywhere andany time, is able to give access and store all the needed information at the point-of-care.Thereby, this project was developed in order for the nursing home to shift from thepaper-based to the computer-based management of data as well as to introduce technologicalimprovements in the facility, more specifically, Health Information and CommunicationTechnology. Thus, by taking advantage of the benefits provided by these improvements,the mobile application could help the health professionals to provide better care, namelyby reducing time-waste and errors, and, consequently, enhance elders’ quality of life. Furthermore,the solution could relieve some of the workload of the health professionals andhelp them make more informed and evidence-based decisions and, hence, improve thedecision-making process.",
    "authors": [
      "Esteves, Márcia Annie Araújo"
    ],
    "keywords": [
      "Business intelligence",
      "Elders",
      "Ethical issues in medicine",
      "Health information and communication technology",
      "Health professionals",
      "Mobile health",
      "Nursing home",
      "Idosos",
      "Lar de Idosos",
      "Problemas éticos na saúde",
      "Profissionais de saúde",
      "Tecnologia de informação e comunicação na saúde",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81339",
    "title": "Machine learning interpretability in a context of black box regression models",
    "abstract": "As máquinas têm demonstrado várias vantagens em comparação com os humanos, nomeadamente areproduzir e escalar tarefas, apresentando velocidade e precisão elevadas. Todavia, nem sempre é possívelcompreender o funcionamento dos seus algoritmos. Assim, a necessidade de explicar os resultados destestem vindo a crescer, levando ao aumento da relevância de ferramentas de explicabilidade, já que estaspossibilitam a redução das divergências entre a interpretação do modelo e o nível de raciocínio humano.O principal objetivo desta dissertação passou pelo desenvolvimento de uma técnica drill-down paraavaliar modelos de regressão caixa negra, considerando interações multivariável no âmbito dos preditores.Assim, propomos EDRs, uma combinação entre DRs e EDPs. De modo a facilitar a sua análise, foramimplementadas múltiplas formas de visualização: boxplots, histogramas e gráficos de densidade, exibindodistribuições completas, uma visualização em grafo para explorar interações entre preditores e tabelasde desempenho, comparando os quartis de cada distribuição com uma referência. Com base em pontosde corte e uma distribuição de referência, foi ainda efetuada uma extrapolação de contra-factos pararegressão.Aplicaram-se quatro algoritmos distintos a uma gama heterogénia de conjuntos de dados com o intuitode eliminar qualquer potencial enviesamento de modelo. Estas experiências mostraram que as EDRsapresentam vantagens em comparação com os EDPs. O número de gráficos a analisar foi reduzido, jáque apenas os subgrupos interessantes são apresentados. Além disso, podem ser detetadas interaçõescompostas por mais de três condições. Foi, também, considerado um caso de estudo, retratando umproblema de seleção de modelo. As EDRs mostraram-se cruciais para compreender como os modelosse comportam em relação a combinações específicas de dados e provar que o melhor modelo geral nemsempre é o melhor para certos subgrupos. Deste modo, as EDRs podem ser usadas para escolher ummodelo ou para gerar ensembles, usando os modelos com melhor desempenho para cada subgrupo.Apesar das vantagens comparativamente às ferramentas existentes, o uso das regras não esgota odomínio das variáveis, pois não se exibem todas as combinações possíveis, com até três condições. Nofuturo, pode ser proveitoso estudar uma discretização dos preditores numéricos guiada pelas regras, jáque esta etapa depende de técnicas externas. Meta-modelos também devem ser definidos para produzirensembles baseados no desempenho de cada subgrupo.",
    "authors": [
      "Pimentel, João Pedro Torres"
    ],
    "keywords": [
      "Aprendizagem máquina",
      "Desempenho",
      "Interpretabilidade",
      "Regressão",
      "Interpretability",
      "Machine learning",
      "Performance",
      "Regression"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79981",
    "title": "Sistema de apoio à decisão baseado em regras para pricing",
    "abstract": "Desde os primórdios dos tempos que o ser humano procura optimizar e automatizar todosos processos que se apresentam como morosos e repetitivos. O processo de pricing deprodutos nos retalhistas é um sistema complexo e que consome tempo ao processo internodos mesmos. Este projeto de investigação tem como objectivo o desenvolvimento de umaplataforma que seja capaz de optimizar e automatizar o processo de pricing de produtos,tendo como base um modelo baseado em múltiplas regras.Com o aumento do volume de vendas online, surgem, no âmbito do processo de pricing,diversos problemas relativos aos processos associados à expansão de uma plataforma deE-commerce. Aparece assim, a necessidade de um sistema capaz de responder a estes problemas e auxiliar na eficácia, rapidez e tomada de decisão de quem lida diariamente comesta questão complexa. Assim, propõe-se o desenvolvimento de um sistema que deveráser capaz de auxiliar a tomada de decisão na definição do preço de venda dos produtoscomercializados em qualquer plataforma de E-commerce.A solução que é desenvolvida ao longo deste projeto de investigação, terá que ser passível de gerir, em tempo real, o processo de pricing de um retalhista, bem como auxiliarna decisão da definição de preços. O foco deste projeto de investigação será dado a sistemas de apoio à decisão orientados a modelos, uma vez que é de extrema importância aversatilidade e a adaptação do sistema a múltiplos contextos e variáveis.Como tal, e de forma a responder às questões de investigação que orientam este projetode investigação, estrutura-se o conteúdo em quatro capítulos fundamentais: o Estado daArte, a Metodologia de investigação e Ferramentas de desenvolvimento, o Desenvolvimentodo trabalho e as Conclusões.Durante o capítulo dedicado ao Estado da Arte abordam-se definições e conceitos essenciais ao capítulo de Desenvolvimento deste projeto, tal como o conceito de Sistemas de Apoioà Decisão, a definição do conceito de Motores de Regras e de Algoritmos de Inferência.Para estruturar a forma como se irá conduzir este projeto de investigação, no capítulo deMetodologia e Ferramentas de Desenvolvimento, apresenta-se a metodologia de investigação e as ferramentas de desenvolvimento aplicadas neste estudo, tal como o ambiente noqual a solução final foi desenvolvida.O capítulo de Desenvolvimento define-se pela exposição da investigação e lógica aplicadano desenvolvimento de um Sistema de apoio à Decisão para o Processo de pricing. Porúltimo, o capítulo em que se expõem as conclusões deste projeto de investigação, tem comoobjectivo analisar os princípios teóricos que servem de base a provas de conceito, seguindo-se pela exposição da análise SWOT. O desenvolvimento desta análise é enquadrado nametodologia de investigação definida inicialmente, Design Research, avaliando a soluçãodesenvolvida de modo a perceber se os requisitos iniciais foram cumpridos.Assim, e de forma a concluir esta investigação, e relacionar todos os conceitos abordadose tecnologias utilizadas, as questões de investigação são respondidas de forma a expor aviabilidade da solução apresentada.",
    "authors": [
      "Parente, Nelson Arieira"
    ],
    "keywords": [
      "E-commerce",
      "Sistemas de apoio à decisão",
      "Arquitetura de software",
      "Motores de regras",
      "Padrões de desenho",
      "Decision support systems",
      "Software architecture",
      "Rule engines",
      "Design patterns",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60043",
    "title": "Study, selection and evaluation of an IoT platform for data collection and analysis for medical sensors",
    "abstract": "Every day, huge amounts of data are generated in the healthcare environments from severalsources, such as medical sensors, EMRs, pharmacy and medical imaging. All of thisdata provides a great opportunity for big data applications to discover and understand patternsor associations between data, in order to support medical decision-making processes.Big data technologies carry several benefits for the healthcare sector, including preventivecare, better diagnosis, personalized treatment to each patient and even reduce medical costs.However, the storage and management of big data presents a challenge that traditional database management systems can not fulfill. On the contrary, NoSQL databases are distributedand horizontally scalable data stores, representing a suitable solution for handling big data.Most of medical data is generated from sensor embedded devices. The concept of IoT,in the healthcare environment, enables the connection and communication of those devicesand other available resources over the Internet, to perform or help in healthcare activitiessuch as diagnosing, monitoring or even surgeries. IoT technologies applied to the healthcaresector aim to improve the access and quality of care for every patient, as well as toreduce medical costs.This master thesis presents the integration of both big data and IoT concepts, by developingan IoT platform designed for data collection and analysis for medical sensors. Forthat purpose, an open source platform, Kaa, was deployed with both HBase and Cassandraas NoSQL database solutions. Furthermore, a big data processing engine, Spark, was alsoimplemented on the system.From the results obtained by executing several performance experiments, it is possibleto conclude that the developed platform is suitable for implementation on an healthcareenvironment, where huge amounts of data are rapidly generated. The results also made itpossible to perform a comparison between the performance of the platform with Cassandraand HBase, showing that the last one presents slightly better results in terms of the averageresponse time.",
    "authors": [
      "Rei, João Pedro Nóbrega"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47826",
    "title": "Análise e conceção de um sistema para rastreabilidade em contexto industrial",
    "abstract": "Apesar da evolução observada nas últimas décadas na utilização de Sistemas de Informação(SI) na gestão de matérias primas (MPs) e informação ao longo dos vários elos de umacadeia de abastecimento, a rastreabilidade de MPs em tempo real é ainda muito limitadadevido à falta de integração dos SI com tecnologias de identificação automática.A fábrica Bosch Car Multimedia de Braga (Bosch BrgP) não é indiferente a este problemae recorre com grande frequência a fornecedores que utilizam rotas dispendiosas em termosde tempo, custo e esforço de operacionalização, o que levanta diversos desafios relativamenteaos processos logísticos (planeamento, receção, gestão e abastecimento de MPs), ouseja, de todo o fluxo de MP desde os fornecedores até às linhas de produção.Atualmente, a Bosch BrgP enfrenta problemas que decorrem da dificuldade em localizar,em tempo real e com elevado grau de precisão, todas as MPs (envolvidas) nos diferentes fluxosda logística interna (na cadeia de abastecimento interna), o que dificulta o planeamentodas necessidades de cliente. Deste modo, a Bosch BrgP tem a necessidade permanente deter acesso a informação relativa à localização da MP.É então pretendido otimizar a logística interna da Bosch BrgP, tornando-a mais competitivano mercado e diminuir os desvios de stock originados pela dificuldade em localizaras MPs. De forma a resolver os problemas de visibilidade e rastreabilidade de MPs desdea sua receção até ao seu consumo, esta dissertação analisa e concebe uma solução pararastrear o fluxo das MPs em todo o processo interno responsável pela gestão da cadeia deabastecimento interna, que recorre a tecnologia Radio-Frequency IDentification (RFID).Os objetivos são alcançados através do desenvolvimento de um sistema de identificaçãoe localização de MPs, que garante uma maior visibilidade dos fluxos de MPs, contribuindopara a redução do tempo de abastecimento às linhas de produção. Para além disso, osistema deve garantir que toda a movimentação/localização da MP em curso na cadeia deabastecimento interna é rastreável e efetuada de modo automático, em tempo real e comelevado grau de precisão, contribuindo para a disponibilização dessa informação sempreque solicitada.Esta dissertação contribui para esse esforço de desenvolvimento com atividades de análisee conceção que se concretizam através do levantamento de requisitos (funcionais e nãofuncionais), da conceção da arquitetura de hardware e software, da definição dos pontos deinteroperabilidade com os sistemas legados e da caracterização do novo fluxo de rastreabilidade.Estas atividades são documentadas através de diversos esquemas e diagramas (deuse-cases e de atividades) que seguem a notação Unified Modeling Language (UML).",
    "authors": [
      "Gomes, Nuno André Barbosa"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23534",
    "title": "Sistema inteligente para a simulação do posicionamento e movimento de pessoas ou equipamentos dentro de um hospital",
    "abstract": "Os orgãos de gestão dos hospitais optam cada vez mais pela implementaçãode sistemas de monitorização baseados na tecnologia RFID, tendo em vistasobretudo a redução de custos associados a perda e roubo de equipamentos,o aumento da segurança de pacientes e pro ssionais e a deteção do acesso depessoal a zonas não autorizadas. Testes e estudos de otimização, nomeadamenteem relação a configuração dos sensores na arquitetura da rede RFID,devem ser realizados para aumentar o desempenho destes sistemas. A simulação surge neste contexto como uma importante ferramenta de apoio,uma vez que permite que os estudos sejam endereçados a partir de um ambientecomputacional, evitando as desvantagens inerentes à realização de testesno próprio hospital. Recentemente, o estudo da questão de conservação deenergia, em redes de sensores de tracking de objetos, tem atraído muita atenção. A previsão de trajetórias pode ser utilizada para determinar oconjunto de sensores que num determinado momento devem ser desativadospara reduzir consumos energéticos e, consequentemente, aumentar o tempode vida do sistema.O objetivo deste trabalho é integrar as duas temáticas referidas - simulação e previsão - no desenvolvimento de um sistema inteligente capazde simular e prever a trajetória de uma entidade numa área preenchida comsensores. A área escolhida para a simulação consiste no piso pediátrico deum hospital no Norte de Portugal, onde um sistema RFID de monitorizaçãose encontra atualmente implementado para a monitorização de pacientes.A análise do sistema desenvolvido é realizada através do estudo de cenários,onde estatísticas obtidas em diferentes condi cões de simulação são analisadassob diversos prismas. Os resultados provam que o sistema proposto é capazde simular com sucesso o movimento de uma ou mais entidades num ambientehospitalar real, e de estimar localizações com uma precisão média de62% para um modelo de simulação que tem em consideração a existência devariabilidade e aleatoriedade no movimento. A previsão é realizada atravésda aplicação de um algoritmo de Data Mining, denominado SK-Means, aohistórico de percursos de cada entidade, e permite a obtenção de padrões de movimento específicos e distintos para cada uma delas. As localizações previstas são apresentadas ao utilizador em tempo real, de forma dinâmica.Pretende-se que no futuro o sistema possa ser implementado no hospitalpara permitir a visualização em tempo real dos pacientes.",
    "authors": [
      "Salgado, Cátia Matos"
    ],
    "keywords": [
      "61:621.398",
      "621.398:61",
      "614"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "61:621.398",
      "621.398:61",
      "614"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84086",
    "title": "Post generator for social media based on emotions and personality",
    "abstract": "Social networks are currently one of the main sources of fake-news dissemination. On the other hand, fact-checking agencies, which emerged with the aim of solving the problem of fake-news, find it difficult to spread their content on social media. The engagement of fake-news is considerably greater than that of fact-checking. This dissertation contributes to defining a set of heuristics applicable to fact-checking posts in mi-croblog environments, in order to increase their reach and engagement with the reader. The systematic review, inspired by the guidelines defined by Kitchenham, made it possible to identify the main strategies used in the dissemination of fake-news and fact-checkings on social media. The results showed that the dissemination and viralization of posts depends on two aspects: the dissemination strategy and the engagement strategy created with the reader. The first is outside the scope of study in this document. Regarding the second, the systematic review showed that the inclusion of emotions and personality in social media posts is an efficient strategy to improve reader engagement. Furthermore, engagement seems to depend on the type of language and elements that make up the post. Since we are working in the context of fake-news in which ethical limits are frequently tested and extrapolated, it is important to reinforce that the defined approach is ethically valid. In this sense, the course of the dissertation continued with the design and development of a Post Generator Algorithm based on emotions and personality capable of increasing the engagement of fact-checking posts. The algorithm was tested in an experiment carried out with twenty participants, where patterns were searched between the posts obtained by the algorithm and those created by the fact-checking journal Snopes and the interactions achieved in each of them. This experiment, carried out in a lab environment, proved the hypothesis that emotions and personality in Microblog posts increase user engagement with fact-checking posts.",
    "authors": [
      "Barbosa, Maria Laura de Araújo"
    ],
    "keywords": [
      "Natural language processing",
      "Post generator",
      "Social networks",
      "Twitter",
      "Fake-news",
      "Fact-checking",
      "Emotions",
      "Personality",
      "Engagement",
      "Processamento de linguagem natural",
      "Gerador de posts",
      "Redes sociais",
      "Emoções",
      "Personalidade",
      "Envolvimento"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83670",
    "title": "Online-SoBA: text analysis to study social behaviors",
    "abstract": "This document reports a Master’s Project that fits in the 5th year of the Integrated Master’s inInformatics Engineering from the Universidade do Minho. This Master work, Online-SoBA(Online Social Behavior Analysis), aims at developing a semi-automatic system capable ofanalyzing short texts that correspond to comments written in reaction to ’posts’ in Portuguesesocial media (social networks or online newspapers), in order to identify behaviors thatcharacterize social opinions about a given topic along a given period of time. To meet thechallenge posed, these short texts are analyzed and the information is extracted according toschemes that describe the phenomena to be studied, that is, sentence structures in naturallanguage that characterize the referred behaviors. In order to achieve this goal, a pipelinewas implemented to process the texts contained in the NetLang Corpus, and later a webplatform was developed to present the generated results to the end user, as well as to allowa fluid navigation over them. The report introduces the system architecture designed toattain the objectives, discusses the implementation decisions and the development steps, andshows the interface created for a fruitful knowledge extraction. Before closing the documentwith the conclusions, an experiment conducted is described and the results are analyzed toassess the SoBA from the end user perspective.",
    "authors": [
      "Mendes, Paulo Jorge"
    ],
    "keywords": [
      "Natural language processing",
      "Social behavior",
      "Text analysis",
      "Sentiment analysis",
      "POS Tagging",
      "Processamento de linguagem natural",
      "Comportamento social",
      "Análise de texto",
      "Analise de sentimentos"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46680",
    "title": "Plataforma computacional para apoio à decisão em neonatologia e pediatria",
    "abstract": "O erro humano está frequentemente associado ao calculo manual de dosagem demedicamentos, sendo a sua incidência maior na população pediátrica do que napopulação adulta. Desta forma, esta dissertação tem como objetivo a finalização eimplementação de uma plataforma de apoio à decisão médica de modo a reduziresse erro.A codificação de episódios de internamento é de extrema importância para agestão financeira hospitalar, uma vez que esta é utilizada para o calculo de financiamentohospitalar através da utilização de GDH. Neste sentido, foi desenvolvidauma plataforma de apoio à gestão financeira que permite auxiliar na codificaçãode episódios de internamento de qualquer unidade da instituição de saúde, mesmodas unidades de Neonatologia e Pediatria.O desenvolvimento de ambas as plataformas foi acompanhado por um médicoresponsável, tendo sido testadas e realizadas as devidas alterações. Encontrandosea primeira na fase final de testes e a segunda em funcionamento em ambientehospitalar. Por fim foram lançados dois questionários direcionados à avaliação decada plataforma.",
    "authors": [
      "Coimbra, Ana Cecília Sousa da Rocha"
    ],
    "keywords": [
      "Ciências Médicas::Outras Ciências Médicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Médicas::Outras Ciências Médicas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56105",
    "title": "Inferring epidemiology and microevolution of Mycobacterium tuberculosis strains from deep-sequencing data of patient samples",
    "abstract": "Tuberculosis, caused by the intracellular pathogen Mycobacterium tuberculosis is an infectious disease that remains a global public health problem where approximately one-third of the world population have been at least in contact and is latently infected with.Whole genome sequencing has revolutionized the investigation of mycobacterial genomes. The application of this technology has provided innovative understandings into the evolution of the Mycobacterium tuberculosis due to recent studies reporting conflicting findings on its genomic stability, particularly during the evolution of drug resistance in modern lineages.To address this question we focused on understanding the genotypic and epidemiological factors that influence the spread and fitness of this bacterium by analyzing deep –sequencing data of 85 patient samples from Central Asia. Samples were part of a larger study of 399 clinical isolates of newly diagnosed patients with pulmonary TB collected between 2012 and 2013 at the NCTLD in Tbilisi, Georgia.All the samples were mapped against H37Rv strain. We focused on single-nucleotide polymorphisms to reconstruct models for molecular evolution, using Maximum Likelihood and Bayesian Inference methods. 84% of our population belongs to the Beijing lineage, associated with the massive spread of multidrug-resistant strains. Relationship between mutations on rpoB and rpoC were associated with drug resistance to rifampicin and mutations on pncA region also demonstrated to be related with drug resistance to pyrazinamide.Furthermore we found that the amount of variation accumulated within a patient can be as high as that observed between patients along, what we assume to be, a chain of transmission. Intrapatient diversity was found in all of the follow up patients.Our study adds new data to the understandings of the variability among Mycobacterium tuberculosis strains in an intra and interpatient microevolution scenario.",
    "authors": [
      "Barbosa, Bárbara Andreia Andrade"
    ],
    "keywords": [
      "Tuberculosis",
      "Whole genome sequencing",
      "Microevolution",
      "Phylogeny",
      "Multidrug resistance",
      "Tuberculose",
      "Microevolução",
      "Filogenia",
      "Multirresistência a antibióticos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/49391",
    "title": "On the improvement of sleep onset latency detection and sleep-wake classification using cardiorespiratory features",
    "abstract": "This document describes an investigation performed at Philips Research (Eindhoven, TheNetherlands) which aimed at improving the performance of Philips current sleep/wake classificationmethods using portable devices based on unobtrusive cardiorespiratory signalmodalities. Particularly, this research focused on improving the detection of the Sleep OnsetLatency (SOL) parameter.Using a data set with recordings of healthy subjects, several alternative classification modelswere built, evaluated and compared to the current classifier.It was found that the performance of the current classifier, regarding SOL detection, decreaseswith increasing SOL, leading to an underestimation of this parameter, and possiblyundervaluation of symptoms of sleep disruptions or even sleep disorders, during medicaldiagnosis.The main issue associated to this fault is that the current classifier is trained with examplesfrom the entire night and therefore, for subjects with extended SOL periods, fails to capturethe characteristics of wake before the initiation of sleep.In this report a new method of distinguishing sleep from wake, to be applied with recordingsof subjects with SOL over 30 minutes, is proposed. The new method comprises two steps:one specially dedicated to identify wake before the initiation of sleep (and more accuratelydetect the moment of Sleep Onset (SO)), and the other one to distinguish wake after SOL.Hence, it requires the use of two classifiers which differ regarding techniques for featureselection and are trained with examples of different periods of the night recordings.",
    "authors": [
      "Azevedo, Cármina Augusta Pereira"
    ],
    "keywords": [
      "Machine learning",
      "Sleep staging",
      "Sleep/wake detection",
      "Sleep classification",
      "Sleep monitoring",
      "Sleep onset latency",
      "Cardiorespiratory features",
      "Actigraphy",
      "Aprendizagem-maquina",
      "Classificação de sono",
      "Detecção sleep/wake",
      "Latência de sono",
      "Features cardiorrespiratórias",
      "Monitorização do sono",
      "Actigrafia",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79815",
    "title": "Connectionist systems for image processing and anomaly detection",
    "abstract": "A Inteligência Artificial (IA) e a Ciência de Dados estão cada vez mais presentes no nosso quotidiano e osbenefícios que trouxeram para a sociedade nos últimos anos são notáveis. O sucesso da IA foi impulsionadopela capacidade adaptativa que as máquinas adquiriram e está estreitamente relacionada com a sua habilidade para aprender. Os sistemas conexionistas, apresentados na forma de Redes Neurais Artificiais (RNAs), que se inspiram no sistema nervoso humano, são um dos mais importantes modelos que permitem a aprendizagem. Estes são utilizados em diversas áreas, como em problemas de previsão ou classificação, apresentando resultados cada vez mais satisfatórios. Uma das áreas em que esta tecnologia se tem destacado é a Visão Computacional (Computer Vision (CV)), permitindo, por exemplo, a localização de objetos em imagens e a sua correta identificação. A Deteção de Anomalias (Anomaly Detection (AD)) é outro campo onde as RNAs vêm surgindo como uma das tecnologias para a resolução de problemas. Em cada área são utilizadas diferentesarquiteturas de acordo com o tipo de dados e o problema a resolver. Combinando o processamento de imagense a deteção de anomalias, verifica-se uma convergência de metodologias que utilizam módulos convolucionaisem arquiteturas dedicadas a AD. O objetivo principal desta dissertação é estudar as técnicas existentes nestesdomínios, desenvolvendo diferentes arquiteturas e modelos, aplicando-as a casos práticos de forma a compararos resultados obtidos em cada abordagem. O caso prático principal consiste na monitorização de pavimentosrodoviários por meio de imagens para a identificação automática de áreas degradadas. Para isso, dois protótipos de software são propostos para recolher e visualizar os dados adquiridos. O estudo de arquiteturas deRNAs para o diagnóstico da condição do asfalto por meio de imagens é o foco central no processo científicoapresentado. Os métodos de Machine Learning (ML) utilizados incluem classificadores binários, Autoencoders(AEs) e Variational Autoencoders (VAEs). Para os dois últimos modelos, práticas supervisionadas e não supervisionadas são também comparadas, comprovando a sua utilidade em cenários onde não há dados rotuladosdisponíveis. Usando o modelo VAE num ambiente supervisionado, este apresenta uma excelente distinção entreáreas de pavimentação em boas condições e degradadas. Quando não existem dados rotulados disponíveis, amelhor opção é utilizar o modelo AE, utilizando a distribuição de semelhanças das reconstruções para calcular othreshold de separação, atingindo accuracy e precision superiores a 94%). O processo completo de desenvolvimento mostra que é possível construir uma solução alternativa para diminuir os custos de operação em relaçãoaos sistemas comerciais existentes e melhorar a usabilidade quando comparada às soluções tradicionais. Adicionalmente, dois estudos demonstram a versatilidade dos sistemas conexionistas na resolução de problemas,nomeadamente no projeto de estruturas mecânicas, possibilitando a modelação de campos de deslocamento epressão em placas reforçadas; e na utilização de AD para identificar locais de aglomeração de pessoas atravésde técnicas de crowdsensing.",
    "authors": [
      "Gomes, Luís Filipe Fernandes"
    ],
    "keywords": [
      "Sistemas conexionistas",
      "Redes neuronais artificiais",
      "Deep learning",
      "Ciência de dados",
      "Visão computacional",
      "Deteção de anomalias",
      "Autoencoders",
      "Variational autoencoders",
      "Monitorização automática de pavimentos",
      "Connectionist systems",
      "Artificial neural networks",
      "Data science",
      "Computer vision",
      "Anomaly detection",
      "Automatic pavement monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79840",
    "title": "A gestão e otimização de preço no retalho especializado: motores de regras versus solvers",
    "abstract": "The continuous social and economic development has led, over time, to an increase inconsumption, as well as a greater demand from the consumer for what he buys. In thissense retailers have the need to respond to these challenges and explore new opportunities.Naturally, the selling price of a product assumes a fundamental role in the purchasedecision, and in that way the retailers must carefully analyze and define the best price foreach product, based on several factors, such as: perceived value of the product, positioningof the product, the company strategy, competition.Faced with all these challenges, the use of Information Systems is essential for retailers sothat it can support them in the pricing decision. These information systems are becomingincreasingly complex, including demand forecasts, and making recommendations based onbalanced buying patterns due by the economic evolution of markets.In a first phase the ideia was to make a study on two main price recommendation systems:Rules Motors and Price Optimization. As the objective of the dissertation is to change thealgorithm of Regular Price Optimization of the software tool Profimetrics, part of the studywas conducted according to the methodology of the tool. After an analysis of the company’scurrent algorithm, the changes were made to perfect it. Subsequently, we used the casestudy methodology, in the application of the algorithm developed to a retail company.Through this case study it was possible to make a brief diagnosis in order to comparethe current algorithm of the company with the developed algorithm.",
    "authors": [
      "Marciel, Cecília Catarina Domingues"
    ],
    "keywords": [
      "Retalho",
      "Preço",
      "Motores de regras",
      "Solvers",
      "Otimização de preço",
      "Fore-cast",
      "Retail",
      "Price",
      "Rule engines",
      "Price optimisation",
      "Forecast",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56111",
    "title": "Reconstruction of the genome-scale metabolic model of Nitrosomonas europaea",
    "abstract": "Nitrogen is one of the four most common elements in any cell and thus, it is neededto sustain all kinds of life, making the nitrogen cycle crucial to life on Earth. However humanactivities have doubled the transfer of the reactive nitrogen into the biosphere, largelythrough the excessive use of fertilizers. This lead to eutrophication of aquatic systems, anegative ecosystem response usually associated with reduction of the biodiversity in it.This work is set to improve the removal technique of reactive nitrogen by transformingit into non-reactive nitrogen - through Nitrosomonas europaea, an essential and ubiquitousbacteria in the nitrogen cycle. By using it in wastewater treatment plants, it is possible toovercome a limiting step of this transformation, which ultimately helps to stop eutrophication.N. europaea is the most studied ammonia-oxidizing bacteria to date and has variouspathways that involve different compounds of nitrogen, making it metabolically versatileand, therefore, suitable for wastewater treatments. In this work, it was reconstructed agenome-scale metabolic model of N. europaea, using merlin (a specialized software for thistask), to allow performing in silico simulations with different environmental conditions,providing knowledge of its underlying metabolic fluxes.This reconstruction was made through computational means (including several iterativesteps such as automatic and manual annotation of the genome, curation of themetabolic pathways, among others), was validated through laboratorial means (by growingthe organism in a chemostat and quantifying the compounds of its biomass), and wassupported by literature in many cases.This validation was represented by the accuracy of the model (a comparison betweenthe in vivo with the in silico data), and was equal to 98, 36 %.Now, with a metabolic model of the organism, a guided approach may be developedto optimize the conversion of ammonia into nitrite, to be later metabolized by other organismsto produce molecular diatomic nitrogen (inactive nitrogen), thus providing a solutionto eutrophication.",
    "authors": [
      "Raposo, Pedro Miguel Brígida"
    ],
    "keywords": [
      "Nitrosomonas europaea",
      "Genome-scale metabolic model",
      "Nitrogen",
      "Eutrophication",
      "Engenharia e Tecnologia::Biotecnologia Ambiental"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Biotecnologia Ambiental"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34155",
    "title": "Exploration and application of machine learning algorithms to functional connectivity data",
    "abstract": "Methods for the study of the functional connectivity in the brain have seen severaldevelopments over the last years, however not yet in a fully realized manner. Machine learning andcomplex network analysis are two promising techniques that together can help the process of betterexploring functional connectivity for future clinical applications.Machine learning and pattern recognition algorithms are helpful for mining vast amountsof neural data with increasing precision of measures and also for detecting signals from anoverwhelming noise component (Lemm, Blankertz, Dickhaus, & Müller, 2011). Complex networkanalysis, a subset of graph theory, is an approach that allows the quantitative assessment ofnetwork properties such as functional segregation, integration, resilience, and centrality (Rubinov& Sporns, 2010). These properties can be fed into classification algorithms as features. This is anew and complex approach that has no standard procedures defined, so the aim of this work is toexplore the use of fMRI-derived complex network measures combined with machine learningalgorithms in a clinical dataset.In order to do so, a set of classifiers is implemented on a feature dataset built with brainregional volumes and topological network measures that, in turn, were constructed based onfunctional connectivity data extracted from a resting-state functional MRI study. The set of classifiersincludes the nearest neighbor, support vector machine, linear discriminant analysis and decisiontree methods. A set of feature selection methods was also implemented before the classificationtasks. Every possible combination of feature selection methods and classifiers was implementedand the performance was evaluated by a cross-validation procedure.Although the results achieved weren’t exceptionally good, the present work generatedknowledge on how to implement this recent approach and allowed the conclusion that, for mostcases, feature selection improves the performance of the classifier. The results also showed thatthe decision tree algorithm produces relatively good results without being associated with a featureselection method and that the SVM classifier, together with RFE feature selection method, producedresults on the same level as other work done with a similar approach.",
    "authors": [
      "Veloso, Telma Alves"
    ],
    "keywords": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "61:681.3",
      "681.3:61",
      "616-073",
      "612.8"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80301",
    "title": "Visual Semantic Embedding Model based on DeViSE for medical imaging",
    "abstract": "During the last decades, artificial intelligence algorithms have been evolving to the point that they can achieve some amazing results like, identify and navigate roads, identify fraudulent transactions, personalize crops to individual conditions, discover new consumer trends, predict personalized health outcomes, optimize merchandising strategies, predict maintenance, optimize pricing and scheduling in real-time, diagnose diseases, among many others.However, although it can do all of that, it needs all the data to be correctly label, in other words, it can not, for example, diagnose a disease, such as a stroke, if it does not know what a stroke is, so if the algorithm has never been trained to identify strokes a new algorithm has to be created or the current one has to be retrained, similar issues happen in the other examples.This work focuses on this problem and tries to solve it by using a related in a high dimensional vector space, called semantic space, where the knowledge from known classes can be transferred to unknown classes.",
    "authors": [
      "Diogo, Ludgero da Silva"
    ],
    "keywords": [
      "Machine Learning",
      "DeViSE",
      "Medical Imaging",
      "Semantic Model",
      "Aprendizagem Máquina",
      "Imagem Médica",
      "Modelo Semântico",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91564",
    "title": "Classification and clustering using swap test as distance metric",
    "abstract": "This master’s thesis explores the advantages of using a quantum-based distance metric in a MachineLearning (ML) algorithm. It compares the performance of such a hybrid algorithm with an entirely classicalalgorithm. Quantum Machine Learning (QML) has been growing in recent years. Some studies suggest thatQML may even provide a polynomial speed-up for data categorization compared to traditional ML. However,analyzing the benefits is not straightforward, as QML algorithms often rely on abstract, oracle (black-box)models that frequently rely on Quantum Random Access Memory (QRAM). Furthermore, loading classicaldata onto quantum registers limits the applicability of QML, imposing a bottleneck. We used the SwapTest to measure the overlap between two quantum states to achieve our objective. Then we replaced theclassical distance metric in a distance-based machine learning algorithm with the quantum-based distancemetric. Our research showed that the Swap Test could be used as a distance metric in classical algorithms,despite the fact that the results obtained are not better than the classical metrics. In the final discussion,we present some ways that can improve the obtained results.",
    "authors": [
      "Sousa, Tomás Rodrigues Alves de"
    ],
    "keywords": [
      "Quantum machine learning",
      "Machine learning",
      "Classification",
      "Clustering",
      "Swap test",
      "Distance metric",
      "Metrica de distancia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27858",
    "title": "Dashboarding: projeto e implementação de painéis analíticos",
    "abstract": "Na atualidade, graças às elevadas capacidades computacionais e gráficas existentes, é possíveldotar os sistemas de processamento analítico com ferramentas de visualização e manipulação deinformação muito atrativas e de fácil utilização, em particular quando utilizamos para issodashboards. Os dashboards tornam a interação com a informação proveniente de um sistema deprocessamento analítico mais interativa e eficaz, muito graças à modularidade inerente aos seuscomponentes gráficos e à sua qualidade inata de representar a informação graficamente. Amodularidade também é uma característica importante uma vez que permite modificar o sistemautilizando apenas cliques do rato, enquanto que, por sua vez, a representação gráfica dainformação facilita a sua análise e interiorização (Few, 2006a). Estas qualidades, entre outras,fazem com que os dashboards sejam uma ferramenta fulcral na análise da informação e nosuporte à tomada de decisão no seio de uma empresa, tendo sempre em mente que o sucesso deuma empresa está dependente da capacidade que os seus responsáveis e funcionários têm detomar decisões acertadas em tempo útil. Em geral, os dashboards podem ser utilizados paramonitorizar o desempenho de uma empresa, tanto a nível global como a nível individual, definirestratégias de marketing, analisar tendências, entre outros. Nesta dissertação pretendeu-seinvestigar a utilização de dashboards em sistemas de processamento analítico, abordando desde oseu desenho até à sua implementação e exploração prática. Complementarmente, de forma ademonstrar a utilidade e vantagens desse tipo de instrumentos, procedeu-se à implementação deum sistema piloto, incorporando na sua estrutura uma coleção de dashboards providos demecanismos de auto-adaptabilidade aos requisitos dos utilizadores.",
    "authors": [
      "Barros, Rui Miguel Pereira da Costa"
    ],
    "keywords": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47729",
    "title": "A Data Mining approach towards effective Dengue outbreak prediction in Seremban, Malaysia",
    "abstract": "In Malaysia, the incidence rate of Dengue Fever and Dengue Haemorrhagic Fever has reached thelevel of epidemic, and its numbers keep growing. In the last few years, a big effort has been putinto developing methods for predicting dengue outbreaks. However, the path for undertakingeffectively those predictions, and therefore save Human lives, is still a very long one. Thisdissertation work focused on the use of Data Mining techniques, for discovering hidden patterns ondata obtained by crossing information related to patients infected with dengue in Malaysia andmeteorological data coming from the areas where those patients got infected.",
    "authors": [
      "Coutinho, Celso Filipe Nogueira"
    ],
    "keywords": [
      "Clustering",
      "Cross-industry standard process for Data Mining",
      "Data Mining",
      "Decision Trees",
      "Dengue Fever",
      "Knowledge discovery in databases",
      "Árvores de Decisão",
      "Descoberta de conhecimento em bases de dados",
      "Febre de Dengue",
      "Mineração de dados",
      "Processo padrão inter-indústrias para mineração de dados",
      "Segmentação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80297",
    "title": "Learning to play DCSS with Deep Reinforcement Learning",
    "abstract": "DCSS is a roguelike game in which the player must explore and find artifacts. In every stepof the game, there are decisions to make, and the complexity of the game resides in the vastamount of options available to the player at any given time. The commands can be dividedinto classes such as movement, combat, inventory management and usage, spell casting,and divine abilities.We aim to implement an intelligent bot that will be able to play the game. To do so, wewill use DRL.DRL is where deep learning and RL meets. It uses the same principles of RL, to learn toperform a task, receiving rewards for every action made. The difference is that the actionwe will perform is chosen by a DNN, and therefore, we call it DRLPyTorch[11] will be as the framework used to implement a NN that will be able to findthe solution for every small decision that can be made during gameplay. If all of thosedecisions are merged, an intelligent bot should be able to play with some degree of success.The aim of the intelligent bot is to learn a task, which in this case, is playing the game,without programming any real behavior.",
    "authors": [
      "Gonçalves, André Almeida"
    ],
    "keywords": [
      "Dungeon Crawler Stone Soup",
      "Inteligência Artificial",
      "Aprendizagem por Reforço",
      "Deep Reinforcement Learning",
      "Artificial Inteligence",
      "Reinforcement Learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/57383",
    "title": "Development of integrated models of hepatocyte cells",
    "abstract": "Metabolism acts a machinery by maintaining the functionality of the cell in responseto several perturbations, keeping a balance in the levels of crucial metabolites and cellcomponents and producing energy by breaking down certain compounds. A better understandingof these mechanisms cannot be restricted to the knowledge of the function ofspecific tissues or cell types, it also requires knowledge about their interactions.The human liver has a high number of physiological functions related to the metabolism,such as the production of the bile, hormones and vitamins. The hepatocytes have a majorimpact in human metabolism, being the most metabolically active cell types in humans.Malfunction on the metabolism of this type of cells is related to several diseases, likehepatitis, cirrhosis or non-alcoholic fatty liver disease (NAFLD), where the last one isconsidered a manifestation of obesity.A particular pathway has been associated not only with obesity, but also with cancerand type 2 diabetes, the mechanistic TOR (mTOR) pathway. Signalling of this pathwayhas an effect on most of cellular functions and regulates growth and proliferation. It hasbeen shown that alterations in this pathway can lead to fat accumulation in the liver ofobese people. A better understanding of this complex pathway may help researchers tounveil more information on how this pathway works and how it can help in the treatmentof several diseases.The increase of high-throughput data, due to the advances in sequencing and otherexperimental techniques, allowed us to better understand the molecular characteristics of the cell. A useful tool to process all this information are Genome-scale metabolicmodels (GSMMs). A GSMM is a list of mass-balanced reactions, which can be related tocellular compartments, like the cytoplasm. Given high-throughput data, GSMMs can beutilized for the simulation of the metabolism of a certain cell type through a constraintbasedmodelling framework. There are several algorithms/ tools to create tissue-specificmetabolic models (based on a generic human model, such as Recon2) including tINIT,MBA or mCADRE.Although all these methods still face a number of issues, the generated models cansimulate human tissues and can be a good starting point for a better understanding ofcomplex diseases. An important limitation of these models is the fact that they onlyrepresent the metabolic layer of the cells, while for models to be able to support accuratesimulations, a number of other important sub-systems (e.g. regulation, signalling) shouldalso be taken into account. This models (Integrative models) combine the informationand material flow of the three previous mentioned sub-systems, delivering a more robusttool with more predictive strength.",
    "authors": [
      "Ferreira, Jorge Miguel Lourenço"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82557",
    "title": "Virtual network function development for NG-PON Access Network Architecture",
    "abstract": "The access to Internet services on a large scale, high throughput and low latency has grownat a very high pace over time, with a growing demand for media content and applicationsincreasingly oriented towards data consumption. This fact about the use of data at the edgeof the network requires the Central Offices (CO) of telecommunication providers, to be pre pared to absorb these demands. COs generally offer data from various access methods, suchas Passive Optical Network (PON) technologies, mobile networks, copper wired and oth ers. For each of these technologies there may be different manufacturers that support onlytheir respective hardware and software solutions, although they all share different networkresources and have management, configuration and monitoring tools (Fault, Configuration,Accounting, Performance, and Security management - FCAPS) similar, but being distinct andisolated from each other, which produces huge investment in Capital Expenditure (CAPEX)and Operational Expenditure (OPEX) and can cause barriers to innovation. Such panora mas forced the development of more flexible, scalable solutions that share platforms and net work architectures that can meet this need and enable the evolution of networks. It is thenproposed the architecture of Software-Defined Network (SDN) which has in its proposal toabstract the control plane from the data plane, in addition to the virtualization of several Net work Function Virtualization (NFV). The SDN architecture allows APIs and protocols suchas Openflow, NETCONF / YANG, RESTCONF, gRPC and others to be used so that there iscommunication between the various hardware and software elements that compose the net work and consume network resources, such as services AAA, DHCP, routing, orchestration,management or various applications that may exist in this context.This work then aims at the development of a virtualized network function, namely a VNFin the context of network security to be integrated as a component of an architecture guidedby the SDN paradigm applied to broadband networks, and also adherent to the architectureOB-BAA promoted by the Broadband Forum. Such OB-BAA architecture fits into the initia tive to modernize the Information Technology (IT) components of broadband networks, morespecifically the Central Offices. With such development, it was intended to explore the con cepts of network security, such as the IEEE 802.1X protocol applied in NG-PON networksfor authentication and authorization of new network equipment. To achieve this goal, thedevelopment of the applications was based on the Golang language combined with gRPCprogrammable interfaces for communication between the various elements of the architec ture. Network emulators were initially used, and then the components were ”containerized”and inserted in the Docker and Kubernetes virtualization frameworks. Finally, performancemetrics were analyzed in the usage tests, namely computational resource usage metrics (CPU,memory and network I/O), in addition to the execution time of several processes performedby the developed applications.",
    "authors": [
      "Araújo, Igor Virgílio Aquino Martins de"
    ],
    "keywords": [
      "Passive optical network",
      "Central office",
      "Network function virtualization",
      "Virtual network function",
      "Software-defined network",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80579",
    "title": "Aplicação de MQTT em ambiente industrial, trazer indústrias legadas para a I4.0",
    "abstract": "Taking into account the characteristics of industrial environments, sometimes resistant toinnovation so as not to harm productive factors, technologies that are outdated are oftenadopted. In addition, there is a clear need and desire for obtaining data related to the pro duction process in order to optimize it and, therefore, obtaining a better performance of theindustrial equipment. Complementary, there is the need for transferring information aboutrefined concepts among multiple departments to assist tasks such as process engineeringand order management.The purpose of this dissertation is to build a platform developed by Setlevel, calledCoreflux, where the building modules are capable of providing not so up-to-date technologieswith the capabilities to integrate Industry 4.0 concepts. Based on the concept of Internet ofThings, the work aims at providing obsolete equipment and technologies with the capacity tobehave as newer equipment, while maintaining the same benefit that these older technologiesoffer to the industrial sector, such as reliability and availability.",
    "authors": [
      "Silva, João Manuel ferreira da"
    ],
    "keywords": [
      "Industrial internet of things",
      "Internet of things",
      "Industry 4.0",
      "Manufacturing",
      "Coreflux",
      "Sensor network",
      "Computer communication",
      "Machine to machine",
      "Big Data",
      "Data gathering",
      "Manufacturing process",
      "Manufacturing optimization",
      "Automation",
      "Internet das coisas industrial",
      "Internet das coisas",
      "Indústria 4.0",
      "Manufatura",
      "Rede de sensores",
      "Comunicação por computador",
      "Máquina a máquina",
      "Obtenção de dados",
      "Processo industrial",
      "Otimização industrial",
      "Automação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28305",
    "title": "MDA SMART: uma ferramenta multiplataforma baseada em modelos",
    "abstract": "Atualmente, o maior desafio no desenvolvimento de software é referente à a portabilidade das aplicações para as várias plataformas disponíveis, especialmente pela crescente heterogeneidade nos componentes de hardware, de middleware e de software base.O desenho de modelos abstratos de software é uma das formas mais elegantes e eficientes para solucionar este desafio. A Model-Driven Software Engineering (MDSE) ́é uma metodologia de desenvolvimento em que os modelos são chave em todo o ciclo de vida do projeto, desde a captura de requisitos, passando pelas fases de modelação e desenvolvimento, e por fim nos processos de teste e instalação.O objetivo primário desta dissertação foca-se na construção de uma ferramenta, o MDA SMART, capaz de interpretar modelos abstratos de software, parametrizáveis, e de gerar automaticamente código fonte para várias plataformas. A ferramenta, caracterizada por uma arquitetura robusta e extensível, é idealizada para permitir a manipulação de modelosde forma ágil, para ser modular o suficiente para integrar novos perfis meta-modelo e para escalar eficientemente para novas plataformas.O MDA SMART resulta da articulação de uma Domain-Specific Language (DSL) para a gestão dos meta-modelos e consequentes processos de transformação. Na utilização da DSL são obtidos processos de transformação rigorosos, com elevado desempenho e que visam maximizar a consistência e portabilidade dos modelos através de medidas ajustadas a destoarem a heterogeneidade entre as plataformas. Adicionalmente, a ferramenta visa compatibilizar os modelos de lógica de negócio com os referentes às interfaces gráficas que, conjugados, vão permitir a obtenção de modelos e código fonte com alto nível de consistência e completude.",
    "authors": [
      "Costa, Rogério Araújo"
    ],
    "keywords": [
      "Model-driven software engineering",
      "Domain-specific language",
      "Transformação de modelos;",
      "Geração automática de código fonte",
      "Geração de código portável",
      "Cross-platform generation",
      "Cross-platform code",
      "681.3.06"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79849",
    "title": "Plataforma inteligente de apoio à decisão médica no transplante de órgãos",
    "abstract": "A alocação adequada de órgãos para transplantação é crítica e crucial. No entanto, o número de órgãos a ser doados não é suficiente dada a quantidade de pacientes em lista de espera. Assim, a determinação do maior número possível de potenciais dadores, de forma eficiente e eficaz torna-se essencial e pode contribuir para melhorar a taxa de sucesso de transplantação de órgãos.Ao longo dos últimos anos, a utilização de Tecnologias de Informação (TIs) e de ferramentas computacionais em vários setores económicos, incluindo o setor da saúde, cresceu exponencialmente, já que têm potencial para transformar e melhorar a prestação de cuidados de saúde. Assim, e aliando a necessidade da eficiência na descoberta de potenciais dadores com a emergência das TIs na saúde, surge a necessidade de uma plataforma Web de apoio à decisão clínica. O objetivo desta plataforma é automatizar o processo de descoberta de informaçãoútil e acionável, através da utilização de tecnologias como Business Intelligence(BI) e Data Mining (DM), ajudando na tomada de decisão clínica diária. Assim, esta éresponsável pela recolha, gestão, armazenamento e sinalização de potenciais dadores.No âmbito deste projeto de dissertação, foi redesenhada e otimizada a plataforma Web Organite, atualmente implementada no Centro Hospitalar do Porto (CHP). Envolveu transformações tanto no design da interface do utilizador, como no modo como a informação está organizada na plataforma, de forma a melhorar a experiência do utilizador e a interação com os dados clínicos. Foi ainda desenvolvida uma metodologia, com base em técnicas de Data Mining, para construir um modelo preditivo que avalia quais os pacientes que dão entrada no hospital que têm maior probabilidade em serpotenciais dadores de órgãos. O objetivo é tornar mais simples e eficaz o processo de identificação de potenciais dadores, contribuindo positivamente na tomada de decisão do Gabinete de Coordenação de Colheita e Transplantação (GCCT), e impactando na redução da lista de doentes que aguarda um transplante.",
    "authors": [
      "Reis, Rita Soares"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79369",
    "title": "Plataforma de registo clínico para medicina física e reabilitação",
    "abstract": "Clinical records are a fundamental component of the correct treatment and follow-up of patients. The management of patient flow is complex and in the current pandemic times, the work organization must be increasingly focused and oriented towards simple and effective records. Communication among the different departments is fundamental and must ensure that data and information flows without breaks. The provision of health care to patients is a vital importance in a society and the more advanced the medicine is the better is the treatment. The impact of technologies drastically benefits the treatment chosen by the doctor and the respective diagnoses, so the more information the doctor has regarding of the state of health and all kinds of interventions of his patients, the more prepared he will be to face and the more appropriate the methods will be used for each one. The main objective of this dissertation is to study the applicability of tools to develop a web platform to be used in the management of information to health professionals. An organic platform that can be molded to various situations and that are capable to supporting clinical records where each health professional is able to visualize the tasks according to their specialty, such as physiotherapy and rehabilitation professionals. A web platform that allows any professional in the area, a simple registration and access, offering a practical history of patients with all the necessary information and all the interventions they have done.",
    "authors": [
      "Domente, Alexandru"
    ],
    "keywords": [
      "Health information systems",
      "Clinical Records",
      "Electronic Health Records",
      "Interoperability",
      "Physical Medicine",
      "Sistemas de informação em saúde",
      "Registos clínicos",
      "Registos de saúde eletrónicos",
      "Interoperabilidade",
      "Medicina Física",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27794",
    "title": "Processamento e análise de dados de Ressonância Magnética funcional (RMf) em repouso e tensor de difusão (DTI)",
    "abstract": "Os estudos de conectividade cerebral têm-se tornado bastante relevantes no meiocientífico e clinico. No entanto, estes estudos não são fáceis de realizar, uma vez queas aplicações existentes foram desenvolvidas por investigadores para as suasnecessidades, não sendo apropriadas para um profissional de saúde. Assim, foi criadaa aplicação BrainCAT que permite que os estudos de conectividade cerebral deRessonância Magnética funcional e tensor de difusão sejam realizados de formaintuitiva e que seja facilmente manuseada por um utilizador com poucosconhecimentos na área.Assim, o objetivo desta dissertação centra-se na aplicação BrainCAT. Assim, estadissertação pode ser dividida em cinco parte: teste da aplicação com um númeroelevado de casos, levantamento de falhas e de formas de melhorar a aplicação,implementação das melhorias através da programação em objective-C, testarnovamente a aplicação para os mesmos casos e, por fim, comparar os resultados dasduas análises.As melhorias implementadas baseiam-se em formas de tornar a aplicação maisintuitiva e de melhorar os resultados. A nível de melhorias de resultados centrou-se naetapa da normalização, uma vez que esta era a que apresentava mais problemas.Assim, entre outras implementações, foram adicionadas etapas como a remoção dopescoço, a verificação da extração do crânio na imagem estrutural e a normalizaçãonão linear.Relativamente à comparação entre resultados, verificou-se que com a extração dopescoço da imagem estrutural, a etapa seguinte de extração do crânio melhorou. Averificação do resultado desta imagem, facilita o utilizador e previne que casos nãosejam vistos. A normalização linear apresentou resultados melhores, contudoapresenta uma maior sensibilidade a artefactos ou erros de pré-processamento.",
    "authors": [
      "Lopes, Vanda Carolina da Silva"
    ],
    "keywords": [
      "681.3:61",
      "61:681.3",
      "616-073",
      "616.8"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3",
      "616-073",
      "616.8"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94070",
    "title": "Análise e previsão de comportamento com base no perfil do utilizador usando algoritmos de machine learning",
    "abstract": "The decision to drop out of school is one that carries profound implications, not only for the individual involved but also for the wider society. This complex issue intersects with various socioeconomic, cultural, and personal factors, leading to a range of long-term effects that extend far beyond the immediate impli cations of leaving school. For individual consequences, dropping out of school often results in reduced employment opportunities and earning potential. Without the foundational education provided by a high school diploma or equivalent, individuals may find themselves limited to lower-wage jobs with fewer bene fits and job security. This, in turn, can lead to a lifetime of financial instability and limited social mobility. The repercussions of school dropouts ripple through society. Economically, a lower-educated workforce can mean reduced productivity and innovation, impacting national competitiveness and economic growth.Societally, there can be an increase in dependency on welfare systems and public assistance, as indivi duals without adequate education may struggle to support themselves and their families. Education is closely linked to health outcomes. Individuals who leave school prematurely are at a greater risk of en gaging in unhealthy behaviors such as substance abuse and may have limited access to healthcare and health information. This not only affects their personal health and well-being but also places a burden on public health systems. High dropout rates often disproportionately affect marginalized communities, exacerbating existing inequalities. This perpetuates a cycle of poverty and limited educational attainment,impacting generations. Education plays a critical role in fostering civic awareness and participation. In dividuals who drop out of school may be less likely to engage in civic activities, vote, or participate in community organizations, leading to a less engaged and informed citizenry. In a global context, the edu cational attainment of a population is a key factor in a country’s ability to compete and cooperate on theinternational stage. Higher dropout rates can impede a nation’s development and its ability to address global challenges effectively. Evaluating user profiles plays a crucial role in anticipating risk situations. Therefore, the focus of this master thesis is to process data using machine learning and deep learning methods. The aim is to develop detailed profiles that include information on the emotional state of individuals, identification of risk alerts predictions of potential warning situations, and strategies to be adopted. This master thesis aims to use these advanced technologies to create an effective risk prediction and response system. So, it’s present a study on the classification of middle and secondary school students based on their interactions with a bot and their school’s management system, in order to identify risk behaviors and predict success or failure in school. Through the use of machine learning techniques, the answers given by students will be analyzed to extract relevant characteristics and predict their math and portuguese grades. The results of this study will provide insight into the potential use of student data from this age group as a means of identifying at-risk behaviors and predicting student success in school, and will inform thedevelopment of more effective interventions and support for at-risk students.",
    "authors": [
      "Lacerda, Maria Beatriz Araújo"
    ],
    "keywords": [
      "Machine learning",
      "Educational data mining",
      "Learning analytics",
      "Classification",
      "School success",
      "Classificação",
      "Sucesso escolar",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92700",
    "title": "Leap motion and artificial intelligence for surgical navigation: automatic hand gestures recognition",
    "abstract": "Globalmente, 310 milhões de cirurgias são realizadas a cada ano e existe uma probabilidade de 2% a 5% de infeções do local cirúrgico. Problemas relacionados com a esterilização de equipamentos sãouma das razões. Estas infeções impactam negativamente a saúde física e mental do paciente comprometendo a sua qualidade de vida. As cirurgias assistidas por computador estão a ajudar os cirurgiões a realizar operações mais seguras e a permitir aos pacientes menos tempo de recuperação.No entanto, este meio de interação geralmente depende de dispositivos de contato físico, como rato e teclado, que expõem a sala de cirúrgica a condições assépticas. O Leap Motion ultrapassa o problema dos dispositivos físicos uma vez que não precisa de nenhum tipo de interação física.Esta dissertação tem como objetivo conceber, desenvolver e validar uma abordagem de interação homem-computador intuitiva e sem contacto, baseada no reconhecimento automático de gestos manuais através do Leap Motion, seguindo uma conceção centrado no utilizador. Para tal, foi primeiramente realizado um protocolo junto dos utilizadores finais para determinar que gestos eram mais intuitivos. Posteriormente, foram criados dois grandes datasets (um de imagens da mão e um com características da mão) para alimentar modelos de inteligência artificial que pudessem reconhecer os gestos manuais de qualquer pessoa. O melhor modelo desenvolvido, com 96.25% de precisão nos dados de teste, foi baseado no algoritmo Support Vector Machine e foi, de seguida, integrado na ferramenta de reconhecimento de gestos manuais que através das previsões do modelo, executa a respetiva ação no ecrã, removendo a necessidade de periféricos com contacto físico.A partir de uma validação preliminar realizada junto de voluntários da Universidade do Minho e uma validação clínica realizada junto de cirurgiões do hospital Trofa Saúde Braga Centro, verificou-se que os utilizadores demoram mais tempo a realizar o mesmo conjunto de tarefas com a ferramenta de deteção de gestos manuais do que com o uso tradicional do rato. Contudo, foi possível observar que há uma curva de aprendizagem da ferramenta e que estes tempos diminuem com a experiência. Por fim, o System Usability Scale, que é um teste padronizado de avaliação de usabilidade, revelou que a aplicação desenvolvida atinge um resultado de 76.67 ± 9.86, porém há uma perceção de usabilidade maior na validação preliminar do que na validação clínica (67.5 ± 6.37). Através de uma última questão aberta pôde-se ainda perceber que a sensibilidade do cursor é o que precisa de mais atenção e constitui o ponto principal do trabalho futuro, juntamente com melhorias na interface gráfica.",
    "authors": [
      "Magalhães, Bruno Miguel Fernandes"
    ],
    "keywords": [
      "Cirurgia assistida por computador",
      "Inteligência Artificial",
      "Interação humano-computador",
      "Leap motion",
      "Reconhecimento de gestos manuais",
      "Artificial Intelligence",
      "Computer-assisted surgery",
      "Hand gestures recognition",
      "Human-computer interaction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27897",
    "title": "Analysing call graphs for software architecture quality profiling",
    "abstract": "Risk assessment is an important topic for financial institution nowadays, especially in the context of loan applications or loan requests and credit scoring. Some of these institutions have already implemented their own custom credit scoring systems to evaluate their clients’ risk supporting the loan application decision with this indicator. In fact, the information gathered by financial institutions constitutes a valuable source of data for the creation of information assets from which credit scoring mechanisms may be developed.Historically, most financial institutions support their decision mechanisms on regression algorithms, however, these algorithms are no longer considered the state of the art on decision algorithms. This fact has led to the interest on the research of new types of learning algorithms from machine learning able to deal with the credit scoring problem.The work presented in this dissertation has as an objective the evaluation of state of the art algorithms for credit decision proposing new optimization to improve their performance. In parallel, a suggestion system on credit scoring is also proposed in order to allow the perception of how algorithm produce decisions on clients’ loan applications, provide clients with a source of research on how to improve their chances of being granted with a loan and also develop client profiles that suit specific credit conditions and credit purposes.At last, all the components studied and developed are combined on a platform able to deal with the problem of credit scoring through an experts system implemented upon a multi-agent system. The use of multi-agent systems to solve complex problems in today’s world is not a new approach. Nevertheless, there has been a growing interest in using its properties in conjunction with machine learning and data mining techniques in order to build efficient systems. The work presented aims to demonstrate the viability and utility of this type of systems for the credit scoring problem.",
    "authors": [
      "Couto, Luís Diogo Monteiro Duarte"
    ],
    "keywords": [
      "681.3.06"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84474",
    "title": "Programming language complexity analysis and its impact on Checkmarx activities",
    "abstract": "Tools for Programming Languages processing, like Static Analysers (for instance, a StaticApplication Security Testing (SAST) tool, one of Checkmarx’s main products), must beadapted to cope with a given input when the source programming language changes.Complexity of the programming language is one of the key factors that deeply impact thetime of giving support to it.This Master’s Project aims at proposing an approach for assessing language complexity,measuring, at a first stage, the complexity of its underlying context-free grammar (CFG).From the analysis of concrete case studies, factors have been identified that make thesupport process more time-consuming, in particular in the stages of language recognitionand in the transformation to an abstract syntax tree (AST). In this sense, at a second stage, aset of language features is analysed in order to take into account the referred factors thatalso impact on the language processing.The main objective of the Master’s work here reported is to help development teams toimprove the estimation of time and effort needed to adapt the SAST Tool in order to copewith a new programming language.In this dissertation a tool is proposed, that allows for the evaluation of the complexity of alanguage based on a set of metrics to classify the complexity of its grammar, along with a setof language properties. The tool compares the new language complexity so far determinedwith previously supported languages, to predict the effort to process the new language.",
    "authors": [
      "Pinto, Gonçalo Rodrigues"
    ],
    "keywords": [
      "Complexity",
      "Grammar",
      "Language-based-tool",
      "Programming language",
      "Static code analysis",
      "Complexidade",
      "Gramática",
      "Ferramenta baseada em linguagens",
      "Linguagem de programação",
      "Análise de código estático",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93070",
    "title": "Propriedades elétricas em heteroestruturas multiferróicas",
    "abstract": "Os filmes finos multiferróicos magnetoelétricos, que apresentam um acoplamento entre a ordem magnética e elétrica, têm atraído recentemente bastante interesse científico e tecnológico. A possibilidade de controlar a magnetização (polarização) por meio de um campo elétrico (magnético) permite estudar as interações electro-magnéticas à nanoescala, criar nanoestruturas multifuncionais e alargar a gama de potenciais aplicações. Nesse âmbito, combinando um material piezoelétrico com um material magnetostritivo, as interações mecânicas entre as fases induzem um acoplamento magnetoelétrico. Esse acoplamento entre a fase piezoelétrica e a fase magnetostritiva dá origem a diversas aplicações em microeletrónica, por exemplo, memórias de múltiplos estágios ou sensores. Neste contexto, foram produzidas compósitos magnetoelétricos constituídas por 0.85[0.6Ba(Zr0.2Ti0.8)O3 − 0.4(Ba0.7Ca0.3)TiO3] − 0.15SrTiO3 (0.85BCZT-0.15STO), um bom piezo/ferroelétrico, livre de chumbo, e CoFe2O4 (CFO), um material ferromagnético com alta magnetostrição. A técnica utilizada para a deposição das heteroestruturas em bicamada foi a ablação laser (PLD). Neste trabalho, foi a estudada a influência da espessura do CFO em dispositivos do tipo Pt(111)/BCZT-STO/CFO/Au, e posteriormente, a influência do substrato no dispositivo Nb:STO(001)/BCZT-STO/CFO/Au, comparando com os resultados obtidos para as estruturas anteriores. Após a sua produção, estes dispositivos foram caraterizados a nível estrutural por difração de raios-X com incidência rasante (GIXRD), a nível morfológica por microscopia eletrónica de varrimento (SEM) e a nível da sua composição química por espetroscopia de dispersão de energia (EDS). Posteriormente foi realizada a caracterização ferroelétrica e dielétrica. Os resultados obtidos por SEM e GIXRD mostram que as camadas de BCZT-STO e CFO nos dispositivos de Pt/BCZT-STO/CFO/Au são policristalinas e apresentam crescimento colunar. Já para a estrutura Nb:STO/BCZT-STO/CFO/Au, verificou-se o crescimento epitaxial dos filmes de BCZT-STO e CFO que constituem a bicamada. As medidas de GIXRD confirmam a formação da fase tetragonal do BCZT-STO e a estrutura espinela cúbica do CFO. Os ciclos P-E, obtidos para todos os dispositivos, permitem concluir que compósito apresenta comportamento ferroelétrico. As propriedades dielétricas dos filmes foram estudadas pela medição da capacidade (C) e das perdas dielétricas (tan 𝛿) em função da frequência e temperatura. Verifica-se que BCZT-STO apresenta um comportamento relaxor. Através do modelo de Havriliak-Negami e incluindo a contribuição da condutividade, modelizou-se e ajustou-se a constante dielétrica imaginária, para obter os tempos de relaxação e energias de ativação, de maneira a saber os principais mecanismos de condução presentes nestas estruturas.",
    "authors": [
      "Moreira, João Filipe Martins"
    ],
    "keywords": [
      "Ablação laser",
      "BCZT/CFO",
      "Compósitos magnetoelétricos",
      "Havriliak-Negami",
      "Magnetoeletricidade",
      "Laser ablation",
      "Magnetoelectric composites",
      "Magnetoelectricity",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94031",
    "title": "Melhoria da segurança do e-mail através de DKIM, SPF e DMARC",
    "abstract": "As ameaças cibernéticas constituem um grande problema para as empresas. Desde phishing a ran somware, são distribuídas principalmente através de correio eletrónico, através do qual os atacantesfazem-se passar por fontes confiáveis, convencendo deste modo a vítima a clicar no link e/ou anexomalicioso. Este problema agrava-se com a prática de spear-phishing, especialmente prevalente no con texto empresarial, que personaliza o e-mail com informações específicas relevantes para o(s) alvo(s), oque aumenta a eficácia do ataque.Os objetivos do trabalho realizado foram a pesquisa e o desenvolvimento de ferramentas que permitamidentificar estes e-mails maliciosos. As soluções propostas tiram partido de protocolos como o DKIM, oSPF e o DMARC para ajudar a classificar a confiabilidade de um e-mail suspeito. Dado um e-mail, oobjetivo é obter o resultado para cada um destes protocolos, que pode ter sido calculado por outrosservidores de correio eletrónico antes de chegar ao destinatário ou é calculado após a sua receção.Ambos os tipos de validações foram desenvolvidas e ficaram operacionais, com a exceção da validação“metódica” do DKIM (feita in house), devido ao seu posicionamento limitado pela arquitetura imposta.Os testes confirmaram o bom funcionamento e resiliência das soluções contra e-mails provenientes dedomínios diferentes.",
    "authors": [
      "Carneiro, Luís Miguel Rocha"
    ],
    "keywords": [
      "E-mail",
      "Análise",
      "DKIM",
      "SPF",
      "DMARC",
      "Analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66524",
    "title": "Do Registo de Saúde Eletrónico à administração de medicamentos assistida",
    "abstract": "Ao longo dos últimos anos, tem-se assistido a um crescente avanço ao nível das Tecnologiasda Informação (TI) e o caso da aplicabilidade das TI à área da saúde não éexcepção, dando origem ao que se designa de Tecnologias de Informação da Saúde(TIS). Assim, com o decorrer do tempo e dos ditos avanços, surgiram novas ferramentas,tecnologias e Sistemas de Informação Hospitalar (SIH) com o intuito de melhorara qualidade da prestação dos serviços das instituições de saúde e, do lado do utente,com o objetivo de proporcionar um acesso cada vez mais eficiente aos cuidados desaúde. Um dos avanços mais significativos na prossecução da interoperabilidade entresistemas e na centralização da informação é o Registo de Saúde Eletrónico (RSE). Estesistema integra dados do utente provenientes de várias fontes, tornando-se um ativoválido no que diz respeito ao suporte da decisão clínica. Paralelamente a isso, permiteainda o acesso a aplicações para a realização de processos operacionais, tais como aprescrição de medicamentos e exames de forma eletrónica.Partindo destes pressupostos, foi então estudado o estado atual dos avanços destesSIH em Portugal, por forma a perceber de que forma seria possível, com os recursosexistentes atualmente, munir o utente de melhores e mais informações acerca da suasaúde. Por isso, o principal objetivo deste projeto de dissertação é desenhar e desenvolveruma aplicação móvel capaz de apoiar o utente no cumprimento das suas obrigaçõesde saúde, sejam elas consequência de eventos numa determinada instituição ou mesmoa toma de medicamentos prescritos. Para além disso, é também pretendido que, paraalém do possível apoio conseguido através da aplicação criada, se consiga ainda criaruma comunidade de auxílio ao utente, através da criação de um agregado. A principalmotivação é, portanto, uma melhoria na qualidade da saúde do utente, através de umacompanhamento monitorizado e o mais individualizado possível.Metodologicamente, partiu-se de uma análise completa aos dados provenientes doPortal do Serviço Nacional de Saúde (SNS) e de outras instituições de saúde, com ointuito de contornar a inexistência de uma API e conseguir extrair e tratar os dados e,posteriormente, carregá-los na Base de Dados (BD) que alimenta a aplicação. Ultrapassada essa dificuldade, comprova-se então a possibilidade de agregar toda a informaçãode um mesmo utente numa só aplicação, com a devida autenticação.",
    "authors": [
      "Dias, Inês São José Simões"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84128",
    "title": "Energy debt: applying technical debt to energy consumption",
    "abstract": "The purpose of this dissertation is to analyse the impact of certain practices in long termpower usage and expand on the concept of Technical Debt by introducing this aspect ofenergy consumption, dubbing the resulting notion as Energy Debt.This dissertation presents energy debt as a range of excess of energy required to executecode. It holds a minimum and maximum cost which depends on a set of factors duringruntime.We analyse existing research regarding energy consumption to compile a detailed set ofenergy smells and the expected energy savings when they are eliminated via refactoring.Then, we present the debt model that computes excessive energy expenditure of a softwaresystem. This debt model is based on the number and variety of occurrences of energy smellspresent on the software’s source code.Lastly, we’ve developed a tool which we dubbed E-Debitum, which extends the Sonar Qube framework to detect energy smells and compute energy debt.",
    "authors": [
      "Maia, Daniel Fernandes Veiga"
    ],
    "keywords": [
      "Green software",
      "Technical debt",
      "Code smells",
      "Refactoring",
      "Energy debt",
      "Code analysis",
      "Software systems engineering",
      "Débito técnico",
      "Débito energético",
      "Análise de código",
      "Engenharia de sistemas de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81333",
    "title": "New generation of interoperable artefacts in medical informatics",
    "abstract": "A healthcare institution produces huge amounts of data on a daily basis. These data come from n sourcesand therefore have m different formats. As it is evident, this heterogeneity of information sources can be a hugeobstacle, since increases the need for this information to be available and shared among the various informationsystems. The exchange of clinical information between Health Information System (HIS) is crucial for the effectiveprovision of care, significantly improving the performance of the institutions.In the healthcare industry, the ability of different information and software systems to communicate and sharedata, as well as use the data exchanged, is called interoperability. This knowledge can be exchanged aroundthe healthcare ecosystem thanks to the use of standards and data sharing models, regardless of the applicationbeing used. However, the lack of interoperability remains a concern. The Agency for the Integration, Diffusionand Archive (AIDA) proposes to achieve levels of interoperability never before implemented. For this purpose,web services are used for the processing, dissemination, and archiving of clinical information.In the context of this master’s dissertation, the aim is to develop and explore new information technologyartefacts to help the administrative and accounting teams of the Hospital da Santa Casa da Misericórdia deVila Verde. This solution aims to fill the existing gaps between the hospital and the Assistência na Doençaaos Servidores do Estado (ADSE). The first gap occurs in the dentistry speciality, where it is not possible toverify the patient’s ADSE status to perform some dental medicine act under co-payment. The second gapoccurs in the invoicing process through ADSE in real-time, in order to perform some medical appointmentsand/or acts resulting from them since the hospital’s accounting team cannot verify if a patient complies with theADSE requirements. The gaps identified can make ADSE refuse to reimburse the hospital for the medical actsperformed, which makes the administrative and accounting work unpleasant. In this situation, the hospital willhave to find a solution that suits the patient and the ADSE. Very often, the hospital has to bear the costs.In order to overpass these problems with the validation and invoicing process of medical acts through ADSE,this project consists of two web applications that enable the hospital’s information systems to interoperate withthe ADSE web service.",
    "authors": [
      "Nunes, Filipe André Martins"
    ],
    "keywords": [
      "Health Information Systems",
      "Interoperability",
      "AIDA",
      "Web Services",
      "Medical Acts",
      "ADSE",
      "Sistemas de Informação na Saúde",
      "Interoperabilidade",
      "Atos médicos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56069",
    "title": "Power aware scheduler for heterogeneous environments",
    "abstract": "Efficient or green computing is becoming a key issue in current programming techniques,going beyond high performance computing, by simultaneously considering issues such asenergy or power consumption. In heterogeneous environments, where different processorsand accelerators co-processors may coexist, there is a real opportunity to reduce the overallenergy consumption of the system by using scheduling decisions in run-time that can havea good and quick response to changes in the different components.Current tools to aid the development of efficient applications lack yet these run-time facilities.This motivated the development of a new framework with a power-aware schedulerfor heterogeneous environments, PASH-Frame, whose prototype is the key object of thisdissertation. This work extended previous performance-based scheduling work to includerun-time power-aware features, adding tools to measure power consumption at each deviceand using different scheduling decisions to get the best outcome according to pre-definedtargets by the end user.To evaluate the overall behaviour of PASH-Frame, several tests were performed: 1000SAXPY tasks with vector sizes varying from 16 thousand elements to 256 thousand; 200SGEMM tasks with matrices varying from 64 thousand elements to 16 million and finallya test that combines the two previous ones. Results show that the scheduling algorithmimplemented in the framework can achieve good results in some cases, in spite of not beingable to make some critical decisions when it comes to energy consumption reduction likeforcing a component to idle to save energy.",
    "authors": [
      "Magalhães, João Paulo Fontoura Moutinho"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64257",
    "title": "Extending the BiYacc framework with ambiguous grammars",
    "abstract": "Contrarily to most conventional programming languages where certain symbols are used soas to create non-ambiguous grammars, most recent programming languages allow ambiguity.This results in the necessity for a generic parser that can deal with this ambiguity withoutloss of performance.Currently, there is a GLR parser generator written in Haskell, integrated in the BiYaccsystem, developed by Departamento de Informática (DI), Universidade do Minho (UM), Portugalin collaboration with the National Institute of Informatics, Japan. In this thesis, this necessityfor a generic parser is attacked by developing disambiguation filters for this system whichimprove its performance, as well as by implementing various known optimizations to thisparser generator. Finally, performance tests are used to measure the results of the developedwork.",
    "authors": [
      "Macedo, José Nuno Castro de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/49442",
    "title": "Monitorização de dados sensoriais biomédicos através de SNMP",
    "abstract": "A população mundial está a envelhecer, sendo que, na Europa, 5% da população temmais de 80 anos e estima-se que este número venha a triplicar nos próximos 20 anos. Estaevolução traz consigo um novo conjunto de desafios sociais e económicos, nomeadamente noâmbito da prestação de cuidados médicos.Uma das vertentes mais importantes da prestação de cuidados médicos é amonitorização de pacientes. Os sensores biomédicos atuais são dispendiosos e funcionamcom sistemas computacionais protegidos pelos fabricantes, com tecnologias próprias nãopartilhadas. Assim, torna-se útil a definição dum novo paradigma capaz de diminuirrelevantemente os custos de aquisição, instalação e manutenção desses sistemas demonitorização.Nesta dissertação apresenta-se um modelo genérico de monitorização biomédicacapaz de ser implementado em unidades hospitalares, visando a recolha de dados sensoriaisbiomédicos, o seu pré-processamento e eventual integração numa base de dados global (numacloud local, por exemplo). A arquitetura permite a monitorização automática de pacientes devariados serviços de saúde com a menor intervenção humana possível, independentemente dotipo de sensor utilizado, com baixo custo de implementação e de aplicação universal.O sistema desenvolvido utiliza mecanismos normalizados para a representação dainformação a monitorizar assim como para a comunicação entre as entidades da arquitetura, eque são baseados nas tecnologias amplamente utilizadas para a gestão de redes Internet.Nomeadamente, foram criadas definições para novas bases de dados específicas paramonitorização e configuração de sensores biomédicos utilizando o paradigma dasManagement Information Bases. Além disso, o protocolo de comunicação entre as entidadesda arquitetura proposta é o Simple Network Management Protocol (SNMP).Como prova de conceito foi implementado, com sucesso, um protótipo que ilustra aarquitetura proposta, incluindo o hardware dum sensor biomédico básico de baixo custo e osoftware dum agente SNMP e duma simples aplicação biomédica capaz gerar alertas emsituações clinicamente pre-definidas por uma equipa médica.",
    "authors": [
      "Antão, Sónia Carolina de Carvalho Silva"
    ],
    "keywords": [
      "Sensores biomédicos",
      "Monitorização biomédica",
      "Simple network management protocol",
      "Management information base",
      "Health sensors",
      "Health monitoring",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56060",
    "title": "A library of user interface widgets prototypes for car dashboards",
    "abstract": "Ensuring the good usability and user experience of software systems is invaluable, and forthat, following a standardized usability engineering process is fundamental. Prototypingplays a crucial role in this process, enabling the proper validation of the usability guidelinesbefore reaching the actual implementation phase. This dissertation focuses on the constructionof a JavaScript widgets library to ease the process of prototyping user interfaces. Thislibrary will later be incorporated in the prototyping software tool PVSio-Web.",
    "authors": [
      "Pacheco, Henrique Jorge Caldas"
    ],
    "keywords": [
      "PVSio-Web",
      "Prototyping",
      "User interfaces",
      "Usability",
      "Prototipagem",
      "Interface",
      "Usabilidade",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/67485",
    "title": "Dispositivo de realidade virtual para melhoria da marcha em pacientes com a doença de Parkinson",
    "abstract": "In recent years there have been many improvements to medical procedures, involving theuse of augmented reality technology to provide new innovative approaches to difficult tasksthat are often required of the patients, requiring less physical exertion from the to achievethe same results or simply looking at the problem in a new perspective. Virtual realitytechnology has the capability of creating an interactive, motivating environment in whichpractice intensity and feedback can be manipulated to create individualised treatments toretrain movement.Currently there is a very large amount of people suffering from minor to severe functionallimitations, impairments such as loss of range of motion, decreased reacting times, disorderedmovement organisation, and impaired force generation create deficits in motor controlthat effect the personss capacity for independent living and economic self-sufficiency.The use of augmented reality is starting to be used in more medical scenario’s and in thetreatment of many diseases generally co-related with motor difficulties or recovery treatments.One of the diseases that has been looked more prominently for augmented reality developmentis the Parkinson’s disease which causes its patients to suffer severe gait constrictionand whose generalised gait treatments didn’t produce a significant improvement in the patientsgait without the use of heavy medication.One other important detail to take notice is that the Parkinsons disease causes the patientto abruptly enter a freezing state without any kind of warning which can lead the patientto fall and severally harm itself depending on the situation at hand.The objective of this thesis is to explore the possibilities of the use of augmented reality inan attempt to improve gait in patients suffering from Parkinson’s disease. For this purposemany augmented reality glasses were analysed selecting the best one in terms of affordability,comfort and utility. The application developed has the objective of improving thepatients gait by displaying an augmented reality supper- imposed path for the patient tofollow matching auditory cues with each of the patients steps and also helping the patientof he suddenly finds himself affected by a ”freezing” episode.",
    "authors": [
      "Dias, João"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83934",
    "title": "Monitoring architecture for services and servers in healthcare environment",
    "abstract": "Information systems are continuously evolving in nature and complexity. Infrastructure concerns such as availability, efficiency, and disaster recovery have been some of the most important drivers regarding how infrastructure is planned and executed. As these mechanisms evolved, so did the underlying foundation for their functioning — Information Technology (IT) infrastructure monitoring. Inside the healthcare environment, it is important to discuss IT infrastructure monitoring and disaster prevention and recovery since availability and communication are vital for the proper functioning of healthcare units, whether acting in isolation or on a network. When acting on a network, it is especially important to be able to easily monitor and observe each unit from a single point of access so that actions can be swiftly taken when there is a problem. Considering the wide range of available solutions and heterogeneous nature of IT infrastructure, even within the healthcare industry, the majority of the solutions either focus too much on a particular problem of some industry, healthcare or not, or are too generic and can’t fulfill the needs of an increasingly connected and interdependent healthcare industry. This Dissertation proposes a web and microservices-based IT infrastructure monitoring backend solution with a multi-site and multi-organization scheme at its core that is designed to be scalable, easily deployed and integrated with existing tools, and simple to further extend and improve. This solution has two main components, one server which is the central point of the solution, the guardian server, and the other one, which is the local client to be installed on each organization’s infrastructure. The produced backend solution was tested and validated in two healthcare organizations which provided useful feedback and a positive answer to the usefulness of a monitoring solution, such as the one developed in this Dissertation, in improving the efficiency and reliability of the organizations’ IT infrastructure and, therefore, their healthcare services. A formal evaluation of the solution was also carried out with a combination of a Strengths, Weaknesses, Opportunities and Threats (SWOT) analysis and a risk assessment report, both mechanisms providing useful insights on the strengths and limitations of the solution, as well as possible improvement points.",
    "authors": [
      "Ramos, Vasco António Lopes"
    ],
    "keywords": [
      "IT infrastructure monitoring",
      "Microservices",
      "Healthcare",
      "Backend architecture",
      "Containerization",
      "Monitorização de infraestruturas IT",
      "Micro-serviços",
      "Ambiente hospitalar",
      "Arquitetura de backend",
      "Containerização",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84184",
    "title": "Automatic parameter tuning using reinforcement learning",
    "abstract": "Every major Database Management System (DBMS) and most components in a distributedsystem in use today, closed or open source, comprise a set of configuration parameters whichhave substantial influence over the performance of the system. The correct configuration andtuning of these parameters often leads to a performance level that is orders of magnitudegreater than that achieved by default configurations.The number of parameters tends to increase as new versions are released. Moreover, theoptimal values for these parameters vary with the environment, namely the workload towhich the system is being subjected to, and the physical characteristics of the hardware it isrunning on.It is common to delegate the responsibility of parameter tuning to a system administrator.The problem with this approach is that it requires both extensive prior experience withthe specific system and workload at hand, and a large amount of the administrator’s time.Moreover, variables may establish extensive and non-trivial correlations between them thatare very difficult to identify and tune.This dissertation introduces an automated and dynamic approach to parameter tuningusing a reinforcement learning approach, while also adopting the use of deep neuralnetworks to tackle the fact that complex relations between variables may exist.Two use cases were implemented to showcase our approach, in the context of a distributeddatabase. One where we adjust tuning variables specific to each replica and another wherewe adjust the shard configuration of the cluster (i.e. what shard is allocated to what replica).The reinforcement learning agents act at the middleware level, where all replication logic isheld. The performance was measured in terms of the reward achieved by those agents aswell as the values for the individual performance metrics that make up that reward. Forthe use case that concerns individual replica configurations, a maximum gain in reward of105.41% was observed in one of the replicas as well as a maximum gain of 484.31% in one ofthe individual performance metrics. In the second scenario, of shard reallocation, we sawimprovements in reward value up to 28.72% and of up to 69.92% for individual metrics.",
    "authors": [
      "Ferreira, Luís Manuel Meruje"
    ],
    "keywords": [
      "Reinforcement learning",
      "Distributed databases",
      "Middleware",
      "Optimization",
      "Machine learning",
      "Aprendizagem por reforço",
      "Bases de dados distribuídas",
      "Otimização",
      "Aprendizagem máquina",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86222",
    "title": "Real-time implementation of 3D LiDAR point cloud semantic segmentation in an FPGA",
    "abstract": "In the last few years, the automotive industry has relied heavily on deep learning applications for perception solutions. With data-heavy sensors, such as LiDAR, becoming a standard, the task of developing low-power and real-time applications has become increasingly more challenging. To obtain the maximum computational efficiency, no longer can one focus solely on the software aspect of such applications, while disregarding the underlying hardware. In this thesis, a hardware-software co-design approach is used to implement an inference application leveraging the SqueezeSegV3, a LiDAR-based convolutional neural network, on the Versal ACAP VCK190FPGA. Automotive requirements carefully drive the development of the proposed solution, with real-time performance and low power consumption being the target metrics. A first experiment validates the suitability of Xilinx’s Vitis-AI tool for the deployment of deep convolutional neural networks on FPGAs. Both the ResNet-18 and SqueezeNet neural networks are deployed to the Zynq UltraScale+ MPSoC ZCU104 and Versal ACAP VCK190 FPGAs. The results show that both networks achieve far more than the real-time requirements while consuming low power. Compared to an NVIDIA RTX 3090 GPU, the performance per watt during both network’s inference is 12x and 47.8x higher and 15.1x and 26.6x higher respectively for the Zynq UltraScale+ MPSoC ZCU104 and the Versal ACAP VCK190 FPGA. These results are obtained with no drop in accuracy in the quantization step. A second experiment builds upon the results of the first by deploying a real-time application containing the SqueezeSegV3 model using the Semantic-KITTI dataset. A framerate of 11 Hz is achieved with a peak power consumption of 78 Watts. The quantization step results in a minimal accuracy and IoU degradation of 0.7 and 1.5 points respectively. A smaller version of the same model is also deployed achieving a framerate of 19 Hz and a peak power consumption of 76 Watts. The application performs semantic segmentation over all the point cloud with a field of view of 360°.",
    "authors": [
      "Delgado, Pedro Paulo Fontes"
    ],
    "keywords": [
      "LiDAR",
      "Deep learning",
      "FPGA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84085",
    "title": "Aplicação mobile para gestão e telemonitorização de hipertensos e diabéticos",
    "abstract": "Currently, the chronic diseases of hypertension and diabetes have a huge prevalence not only in Portugal butalso in Europe, which leads to the need to develop mechanisms that allow greater control of the health condition of chronic patients with hypertension or diabetes. With this need to increase management control, mobile applications are presented as a solution to improve the management and monitoring of the health condition of chronically ill patients. Due to their mobility and connectivity with medical devices, they are increasingly positioned as the most complete answer that users can turn to better manage their health condition. However, existing applications are mostly manual control or are dependent on a specific medical device. It was in this context that Altice Labs proposed this dissertation.Thus, the purpose of this dissertation is to develop a mobile application for the management and monitoring ofhypertensive and diabetic patients, in order to allow greater efficiency in the management and telemonitoring of the chronic diseases of hypertension and diabetes. The application must be integrated with the SmartAL platform by Altice Labs and allow the user to easily and intuitively record vital signs and daily activity, as well as food. After carrying out studies on hypertension and diabetes and carrying out an extensive analysis of the market for mobile applications, the application was developed according to the proposed objectives, having been validated through a pilot test with an end user. In the conclusion, a reflection is carried out on the work carried out, the results obtained and the future work.",
    "authors": [
      "Gomes, Pedro Miguel Queirós"
    ],
    "keywords": [
      "Hypertension",
      "Diabetes",
      "Management",
      "Monitoring",
      "Application",
      "Hipertensão",
      "Diabetes",
      "Gestão",
      "Monitorização",
      "Aplicação"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59826",
    "title": "Realidade aumentada em aplicações móveis para planeamento cirúrgico ortopédico",
    "abstract": "Cada vez mais as necessidades e os requisitos do paciente acompanham os desenvolvimentos tecnológicos na área da cirurgia médica. Tal acontece para que estes obtenham uma intervenção o mais eficiente e seguro possível por parte dos serviços de saúde e dos seus profissionais. No entanto, hoje em dia ainda é difícil implementar e operar diferentes tipos de tecnologias em ambientes médicos devido às desvantagens que estes podem trazer para os seus utilizadores e por todo o processo de aprendizagem que estas requerem. Numa primeira abordagem, este trabalho tem como objetivo esclarecer conceitos e reunir algumas soluções existentes para a resolução destes problemas, assim como as respetivas tecnologias utilizadas pelas mesmas. Posteriormente é elaborado e apresentado um conceito e protótipo de uma aplicação móvel de planeamento cirúrgico ortopédico que implementa tecnologias de Realidade Aumentada. A solução proposta pretende ajudar o cirurgião desde a fase de planeamento até a própria fase de intervenção cirúrgica. Para além de alguns exemplos e da apresentação do trabalho realizado para a solução, é também descrito o processo de implementação e a arquitetura do sistema. Tendo em conta o protótipo desenvolvido, são discutidas as vantagens da sua utilização num contexto cirúrgico e levantados alguns pontos de interesse futuro a serem estudados e implementados para a sua melhoria.",
    "authors": [
      "Inácio, José Alberto Mestre Conceição"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79360",
    "title": "Plataforma autónoma de recolha e análise de infraestruturas tecnológicas",
    "abstract": "The growth of technological infrastructures in companies and institutions, along with thelack of specialized human resources to manage them effectively, sometimes creates misinformationabout the actual status of these infrastructures, making them unstable or evenunsafe.In that context, the need arises to create an autonomous and analytical information collectionsystem. This system should be flexible and extensible enough to adapt to the variousconfigurations of equipments and software available, capable to produce a report with asmuch information as possible, as well as some recommendations to improve infraestructureoverall status, using only open source technology.This dissertation arises from this need and aims to design and implement this platform.",
    "authors": [
      "Mota, Nelson Duarte Cardoso da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91573",
    "title": "Development of a web clinical management application",
    "abstract": "The time of doing all the work manually is passing by as the influence of developingtechnology is increasing. Most of the tasks done in a business can be automated by software.Because of that, the growing demand for this kind of technology is making IT companiesdevelop any software that is required.This report - the dissertation that describes a thesis in Informatics Engineering - coverssome of these technologies, focusing on the clinic area. The development of a clinic webapplication was proposed by Wintouch to help clinic businesses boost their productivity andorganization. This Master’s work, herein reported, began with the research of the state of theart, studying what the market is like, and analyzing what are the drawbacks of the existingsimilar applications. The lessons learned at that stage were relevant to design a new webapplication that can stand out above competitors. The design of the application’s architectureis discussed below along with the technologies used to best fit the application to reach theobjectives proposed and meet the desired requirements. The report presents a detailedaccount of the outcomes of the development process, encompassing both backend andfrontend implementations. Notable features and functionalities are thoroughly documented,alongside a reflection of the challenges encountered during the development journey.",
    "authors": [
      "Cerqueira, Rúben Correia"
    ],
    "keywords": [
      "Software engineering",
      "Web application",
      "Clinic software",
      "Software development",
      "Engenharia de software",
      "Aplicação web",
      "Software para clínicas",
      "Desenvolvimento de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80035",
    "title": "Gestão e controlo de dispositivos IoT através da interação com assistentes digitais",
    "abstract": "A crescente adoção de assistentes digitais faz com que os casos de uso para os quais estes são utilizados sejam cada vez mais abrangentes. Isto aliado ao facto dos dispositivos IoT estarem cada vez mais acessíveis, leva a que comece a ser comum os utilizadores controlaremdiversos dos seus dispositivos através dos assistentes digitais.O presente documento retrata a dissertação com o tema Gestão e controlo de dispositivos IoT através a interação com assistentes digitais. O principal foco desta passa pela investigaçãoda gestão de múltiplos dispositivos de IoT, inseridos no mesmo ambiente e que possam ser geridos/controlados através de uma plataforma de interação por voz, neste caso um assistente digital.Embora já existam soluções disponíveis para desenvolvimento de aplicações neste sentido, estas são muito recentes carecendo tanto ao nível do número de funcionalidades como da flexibilidade oferecida para sua utilização.Esta investigação resulta numa proposta de arquitetura para aplicações deste domínio eda implementação de um sistema que permita fazer a gestão e controlo de um conjunto de dispositivos inteligentes inseridos no contexto de uma smart home. A arquitetura destesistema e das aplicações que o constituem dever ao possibilitar a inclusão de novas funcionalidades inexistentes nos assistentes digitais atuais e a implementação de algumas destas funcionalidades devera ser apresentada como casos de estudo ao longo do presente documento.",
    "authors": [
      "Peixoto, Júlio Dinis Sá"
    ],
    "keywords": [
      "Arquitetura de aplicações",
      "Assistentes digitais",
      "Dispositivos IoT",
      "Google assistant",
      "Applications architecture",
      "Digital assistants",
      "IoT devices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79920",
    "title": "Refactoring Java monoliths into executable microservice-based applications",
    "abstract": "Nos últimos anos assistiu-se a uma mudança drástica na forma como o software é desenvolvido. Osprojectos de software de grande escala estão a ser construídos através de uma composição flexível depequenos componentes possivelmente escritos em diferentes linguagens de programação e com os processos de deploy independentes – as chamadas aplicações baseadas em microsserviços. Isto tem sidomotivado pelos desafios associados ao desenvolvimento, manutenção e evolução de grandes sistemasde software, mas também pelo aparecimento da cloud e pela facilidade que trouxe em termos de escalabilidade horizontal, reutilização e flexibilidade na propriedade e no deploy. O crescimento dramático dapopularidade dos microsserviços levou várias empresas a aplicar grandes refactorings aos seus sistemasde software. Contudo, esta é uma tarefa desafiante que pode demorar vários meses ou mesmo anos.Esta dissertação propõe uma metodologia capaz de transformar automaticamente aplicações desenvolvidas em Java sob uma arquitetura monolítica em aplicações baseadas em microserviços. A metodologia proposta é direccionada para as aplicações que tiram partido da técnica ORM para relacionar classescom as entidades da base de dados, através de anotações no código fonte. A nossa abordagem recebecomo input o código fonte e uma proposta de microsserviços, e aplica técnicas de refactoring às classespara tornar cada microsserviço independente. Esta metodologia cria uma API para cada chamada de métodos de classes que se encontram noutros serviços, e as entidades da base de dados também sofremrefactoring para serem incluídas no serviço correspondente. A metodologia proposta foi implementadaatravés da construção de uma ferramenta que suporta o refactoring de aplicações desenvolvidas em JavaSpring e que utilizam as anotações da JPA para o mapeamento entre as classes e as entidades.Realizou-se um análise quantitativa e qualitativa em 120 projetos open-source aleatoriamente recolhidos do GitHub. Na avaliação quantitativa procurou-se perceber a aplicabilidade da metodologia e naanalise qualitativa, através da execução de testes unitários, procurou-se avaliar se aplicação original e aaplicação baseada em microserviços gerada são funcionalmente equivalentes.Os resultados são promissores sendo a metodologia capaz de realizar o refactoring em 69% dosprojetos, sendo o resultado da execução dos testes unitários igual em ambas as versões dos projetos.",
    "authors": [
      "Freitas, Francisco José Oliveira"
    ],
    "keywords": [
      "Arquitetura de microsserviços",
      "Aplicações baseadas em microsserviços",
      "Decomposição de monólitos",
      "Java",
      "Refactoring",
      "Microservice architecture",
      "Microservice-based applications",
      "Monolithic decomposition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81327",
    "title": "Aplicação de técnicas de gamificação em sistemas educacionais",
    "abstract": "Em contexto educativo e numa sociedade em permanente mudança, há uma necessidade premente de adaptaras metodologias pedagógicas, no sentido de adequar os processos de ensino e de aprendizagem às característicasdos alunos, implementando diferentes abordagens, que permitam captar a evolução do aluno e incrementara sua atenção, motivação e empenho nas matérias a estudar, como forma de promover o sucesso da aprendizagem.Durante os últimos anos várias iniciativas de investigação e de aplicação têm sido desenvolvidas com oobjetivo de integrar técnicas e modelos de Gamificação no domínio dos sistemas de ensino e de aprendizagem,com vista ao desenvolvimento da motivação e desenvolvimento dos alunos nas mais variadas áreas do conhecimento.Nesta dissertação demonstramos a aplicação de técnicas de Gamificação em sistemas educacionais,através da incorporação de elementos de jogos nas suas várias vertentes, convencidos que estas permitem contribuirpositivamente para o desenvolvimento dos processos de aprendizagem e, em particular, para o aumentoda concentração e interesse dos alunos nesses sistemas. Para esse fim, utilizámos um sistema específico deavaliação de conhecimento, com o objetivo de combater a diminuição da motivação e potenciar o uso e a exploraçãodesse sistema. Tendo isso presente, começamos por analisar os benefícios que a Gamificação podeter em contextos educativos, analisando aspetos que permitam aumentar a motivação dos alunos, melhorar oseu processo de aquisição de conhecimento e, consequentemente, o seu sucesso académico. Posteriormente,concebemos um modelo de gamificação específico e fizemos a sua implementação no sistema de avaliaçãoreferido recorrendo a linguagens como a Python, a JavaScript e a HTML, entre outras.",
    "authors": [
      "Cunha, Miguel Afonso Machado da"
    ],
    "keywords": [
      "Gamificação",
      "Plataformas educacionais",
      "Sistemas de avaliação de conhecimento",
      "Técnicas",
      "Mecânicas e dinâmicas de jogo",
      "Aprendizagem",
      "Motivação",
      "Gamification",
      "Educational platforms",
      "Knowledge assessment systems",
      "Techniques",
      "Mechanics and game dynamics",
      "Learning",
      "Motivation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84378",
    "title": "Development of a fraud detection microservice platform",
    "abstract": "O mundo ciber-físico deixou de optar por verificações manuais e promoveu a adoção desistemas mais eficientes e fiavéis para detetar transações fraudulentas. Estes sistemas visamotimizar e também melhorar a forma como estas transações são validadas.Para atingir estes objetivos, foram criados ou adaptados modelos de aprendizagemautomática para realizar estas tarefas. São cuidadosamente testados e desenvolvidos paraatender às necessidades dos utilizadores para garantir que não se envolvem em negóciosfraudulentos e para evitar tentativas maliciosas de roubar ou fazer qualquer dano aoutilizador final.Nos últimos anos, o DTx tem vindo a desenvolver um sistema capaz de hospedar este tipode algoritmos e disponibilizá-los para sistemas de produções em ambientes ciber-físicos.No início deste trabalho, a DTx propôs conceber e criar uma plataforma que pudesse serimplementada num ambiente em nuvem e também capaz de acolher um módulo de IA queesteja qualificado para prever entradas de Churn em extratos de telecomunicações.Nesta dissertação, o grande objetivo foi criar uma plataforma baseada numa arquiteturade microserviços, de forma a fornecer uma solução para os requisitos especificados peloDTx e torná-la uma solução simples, mas eficiente.De forma abrangente, esta dissertação começa por expor um estudo profundo sobreo estado atual da arte dos sistemas ciber-físicos, ambientes em nuvem, algoritmos deaprendizagem automática e plataformas que podem acolher este tipo de sistemas. Emseguida, apresenta-se então as especificações do sistema, a forma como foi implementado,os seus diversos serviços e, finalmente, uma análise dos resultados onde é possível ver quemaior parte dos requisitos foram atingidos.",
    "authors": [
      "Afonso, Carlos Manuel Marques"
    ],
    "keywords": [
      "Microserviços",
      "Cliente-servidor",
      "Machine learning",
      "Detetação de fraudes",
      "Classificação",
      "Microservices",
      "Monolithic",
      "Fraud detection",
      "Classification",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81297",
    "title": "Network anomaly detection using adversarial Deep Learning",
    "abstract": "Computer networks security is becoming an important and challenging topic. In particular, onecurrently witnesses increasingly complex attacks which are also bound to become more and moresophisticated with the advent of artificial intelligence technologies.Intrusion detection systems are a crucial component in network security. However, the limitednumber of publicly available network datasets and their poor traffic variety and attack diversity are amajor stumbling block in the proper development of these systems.In order to overcome such difficulties and therefore maximise the detection of anomalies in thenetwork, it is proposed the use of Adversarial Deep Learning techniques to increase the amount andvariety of existing data and, simultaneously, to improve the learning ability of the classification modelsused for anomaly detection.This master’s dissertation main goal is the development of a system that proves capable of improving the detection of anomalies in the network through the use of Adversarial Deep Learning techniques,in particular, Generative Adversarial Networks. With this in mind, firstly, a state-of-the-art analysis anda review of existing solutions were addressed. Subsequently, efforts were made to build a modular solution to learn from imbalanced datasets with applications not only in the field of anomaly detection inthe network, but also in all areas affected by imbalanced data problems. Finally, it was demonstratedthe feasibility of the developed system with its application to a network flow dataset.",
    "authors": [
      "Valente, Maria Elisa Maciel"
    ],
    "keywords": [
      "Network security",
      "Anomaly detection",
      "Deep Learning",
      "Generative adversarial networks",
      "Segurança das redes de computadores",
      "Deteção de anomalias",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82800",
    "title": "Serviço de acesso remoto a coleções musicais em servidores privados",
    "abstract": "Streaming content has completely changed the way we consume it. The arising of theInternet, and streaming services, has allowed people to access a big collection of musicwithout buying it, just by subscribing to a service or to use it for free with the consumptionof publicity.But, and for those who have a local collection? They can’t use these platforms unless theservice allows the user to buy the content, store it in a local environment, play it there, andthe user could play it remote by streaming.The use of clouds appears to be a good solution for this group of people. Unfortenelly,when the collection is really big, the additional costs of storage of the cloud could be a bigproblem. In addition, normally, the user interface of these services isn’t enjoyable, i.e., isn’tuser-friendly.So, a hybrid system could be the ideal, a streaming service with cloud services that allowstoring the private collection. This is the best for the user but will force them to store all thecollection or at minimum a part of the collection which they want to access, in an externalsource, with a strict organizational structure. For a new user, this couldn’t be a problem butis for a user who already has a big collection.This dissertation has proposed a remote access service to these private collections so thatwith just a simple gadget (smartphone, pc, ...) the user can access the content stored on theirprivate server anywhere on the planet, without change the original location of the collection.This service was built using only normalized and open-source technologies.Based on the proposed architecture, was also developed a prototype with the mainfunction of the system, like the possibility to play music from the private collection ona smartphone Android. The conclusion of the tests made to the prototype was that thisalternative solution could be very good for the people who want that their collection remainsin one place, and, at the same time, can play it remotely.",
    "authors": [
      "Oliveira, Jorge Miguel da Silva"
    ],
    "keywords": [
      "Mobile application",
      "Music",
      "Remote server",
      "Streaming",
      "Aplicação móvel",
      "Música",
      "Servidor privado",
      "Streaming",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59939",
    "title": "A feasibility study on the use of smartphone sensors for development of Advanced Driver Assistance Systems",
    "abstract": "Technological evolution is impacting several industries, e.g., by allowing them to deliver higher levelsof functionality. The automotive industry is an example of how technology is supporting the developmentof new solutions in vehicle safety and comfort.Advanced Driver Assistance Systems (ADAS) are cases of solutions that evolved significantly inrecent years. This is possible not only due to the progress of electronic solutions but also becauseof higher quality in software. The smartphone is an example of this evolution with a broad rangeof applicability since these devices have been used to develop ADAS, making them an interestingcost-effective platform to develop such systems.Previous research has shown smartphones’ ability to output sensors data with the necessary qualityfor a broad number of applications with special focus in inertial sensors. However, such studiestend to be difficult to reproduce or lack the desired detail levels of their experimental methods. Concernsabout how good are smartphone sensors and their use to develop ADAS emerge when readingexisting literature, particularly, how the context of collecting data is controlled and which variablesimpact the collection process.In order to assess the feasibility of using smartphones as sensing devices, questions arise on howdifferent parts of the collection setup affect the quality of data collected. Motivated by those questions,a study considering four different hypotheses is proposed to assess the impact of a controlled set ofvariables, namely: brands of inertial sensors, car mounts, sensor sampling rates, and vehicles. A setof controlled experiments is performed to assess the impact of each variable in the collection processof inertial sensors, more precisely the vertical acceleration.To perform the experiments, three special-purpose tools were developed. Smartphones used inthe experiments feature an application to collect and export their sensors data. A researcher of anexperiment operates another smartphone application to annotate road anomalies found while driving.A desktop application automates the computation and statistical validation of the vertical accelerationcorrelation from different setups.Dynamic Time Warping was used to compute the correlation coefficient of vertical accelerationas measured by different devices. Results show a baseline correlation coefficient of 0.892 with astandard configuration of software and hardware. When one of the independent variables is changed,the resulting coefficients range from 0.827 to 0.848.Randomization tests were executed to statistically validate experiments results, making use of aRandom Shuffle algorithm on surrogate data. Such tests rejected all four proposed null hypothesesregarding dissimilarities on vertical acceleration sensed by different setups.From the controlled experiment a deeper understanding of the variables influencing data collectionwith smartphones was obtained. Results showed that varying the inertial sensors, car mounts, ratesof sampling, or vehicles had a low impact on vertical acceleration sensed by smartphones. This isa good indicator that smartphones can be used to develop ADAS without the need to standardizeevery part of the collection setup. Thus, it possible to foresee the deployment of a system to a wideraudience by taking advantage of existing equipment.",
    "authors": [
      "Santos, Nuno Miguel Teixeira dos"
    ],
    "keywords": [
      "Advanced driver assistance systems",
      "Smartphones",
      "Inertial sensors",
      "Vertical acceleration",
      "Correlation coefficient",
      "Dynamic time warping",
      "Sistemas avançados de assistência ao condutor (Advanced driver assistance systems)",
      "Sensores inerciais",
      "Aceleração vertical",
      "Coeficiente de correlação",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47725",
    "title": "Correct translation of imperative programs to single assignment form",
    "abstract": "A common practice in compiler design is to have an intermediate representation of the source codein Static Single-Assignment (SSA) form in order to simplify the code optimization process and makeit more efficient. Generally, one says that an imperative program is in SSA form if each variable isassigned exactly once.In this thesis we study the central ideas of SSA-programs in the context of a simple imperative languageincluding jump instructions. The focus of this work is the proof of correctness of a translationfrom programs of the source imperative language into the SSA format. In particular, we formally introducethe syntax and the semantics of the source imperative language (GL) and the SSA language; wedefine and implement a function that translates from source imperative programs into SSA-programs;we develop an alternative operational semantics, in order to be able to relate the execution of a sourceprogram and of its SSA translation; we prove soundness and completeness results for the translation,relatively to the alternative operational semantics, and from these results we prove correctness of thetranslation relatively to the initial small-step semantics.",
    "authors": [
      "Azevedo, Marta Vasconcelos Castro"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80865",
    "title": "Análise através da curva ROC: que ferramentas utilizar?",
    "abstract": "ROC (Receiver Operating Characteristic) curve is a statistic tool that allows the evaluationof the accuracy of a classification system. These curves are drawn on a two-dimensionalgraph, with the ordinate representing the true positive fraction or sensitivity and the abscissarepresenting the false positive fraction or 1-specificity. The index that evaluates the accuracyof these graphs is represented by the area under the curve (AUC) where the larger that areais the bigger the test performance is.Its first appearance dates to the year of 1950. Nevertheless, computationally , the firstsoftware only appeared around 1993 and since then several tools have been made availablefor its analysis. Regarding the theoretical part of the subject, there is a vast bibliographyexisting which introduces all the necessary concepts to analyze a ROC curve visuallyand statistically. However, only a few of those documents discuss the evaluation and thecomparison of software that attain these same curves, consisting of old works in whichthe vast majority corresponds to software that when compared to the current scenario areoutdated or fell out of use.The R software environment with a programming language mainly for statistical use iscurrently one of the best tools to perform the ROC analysis. The variety of packages in thiswork environment make it an interesting study product, which allows us to take advantageof the different features in different the packages or enjoy the same features but by differentmeans and formats. Like R there are several tools that can perform this same analysis, as isthe case of STATA software, which receives regular updates that have been improving thistool recurrently. With the versatility of allowing us to work from a command line or throughmenus predefined by the software itself, it makes it a very accessible and convenient tool toexplore.The R language is also related to the package called shiny, which can create browserapplications through its own commands, making it possible to transpose the differentcommands of packages R into a single application. Due to the wide variety of ROC packagesin R, it is interesting to link them to shiny. Therefore, a library in the application format wasdesigned to group the different packages on the same browser page. The result of this isROSY application available on https://pquintasbcl.shinyapps.io/ROSY/.Due to the increasing use of ROC analysis in different systems, it is essential to explorethe best computational methods to process it in a correct way. Therefore, in this work theresearch and selection of different software/tools to perform this type of analysis is done,based on the different existing bibliographic documents in order to compare them and create a checklist, which will allow us to visualize the fundamental characteristics present in eachsoftware analyzed.",
    "authors": [
      "Quintas, José Pedro dos Santos"
    ],
    "keywords": [
      "ROC curves",
      "Statistic",
      "Software",
      "R",
      "Shiny",
      "Package",
      "Checklist",
      "Curvas ROC",
      "Estatística",
      "Software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64181",
    "title": "Business intelligence: indicadores de desempenho num bloco operatório",
    "abstract": "A predominância de um mundo orientado pelas Tecnologias de Informação (TI) é umanoção que está bem presente no quotidiano, desde a mais ínfima tarefa rotineira ao dia-adianum posto de trabalho. De todas as atividades onde as TI incidem, destaca-se nestetrabalho a aplicação das mesmas numa área cujo foco é de extrema importância – a saúde.O setor da saúde, tal como outras áreas organizacionais, não é uma exceção à influênciadas tecnologias e tornou-se uma área que integra a informação como um bem fundamentalpara seu o bom funcionamento.O crescente aumento do volume de dados de registos eletrónicos e de fontes diversascom que os profissionais de saúde lidam todos os dias originou uma nova necessidade –a transformação desses dados em informação para extração de conhecimento. Consequentemente,a dificuldade do processamento de tamanho volume de informação potenciou oaparecimento dos sistemas de Business Intelligence (BI), capazes de lidar com a quantidadede dados armazenada e cujo objetivo passa por apresentar informação sob a forma de conhecimentopara suportar o processo de tomada de decisão.A grande motivação para a implementação de sistemas de BI surgiu da possibilidadede conceção de uma forma de disponibilizar a informação de forma rápida, eficaz e visualmenteapelativa cuja interpretação seja algo intuitiva. A informação relevante podeser disponibilizada em diversos formatos, como por exemplo num Dashboard – técnica devisualização interativa crucial na análise da informação e no suporte à decisão.O pressuposto principal desta dissertação é evidenciar que na modalidade cirúrgica deuma unidade hospitalar é também possível transmitir informação que permita auxiliar osprofissionais de saúde na gestão de um bloco operatório. Deste modo, foram construídosindicadores de diversas categorias que poderão ser relevantes face às possíveis necessidadeshospitalares.Na seleção da tecnologia a utilizar para o desenvolvimento da plataforma de BI optou-sepelo Power BI, ferramenta de Business Intelligence bastante intuitiva e que permite a partilhados elementos visuais que vão influenciar a leitura do profissional de saúde responsávelpela gestão da unidade cirúrgica.Após o desenvolvimento dos Dashboards, pode-se afirmar que o resultado foi satisfatório,uma vez que foram criados indicadores de desempenho que permitem perceber a importânciade um sistema de BI para a gestão mais eficiente de uma unidade cirúrgica.",
    "authors": [
      "Leal, Sara Rafael Marinho"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92620",
    "title": "Vision System Hardware: simulation and test automation",
    "abstract": "This dissertation addresses the challenge of efficiently developing and testing software in hardware-dependent systems, with a specific focus on simulating hardware used in the Vision Software project of Smartex. Smartex is a company that applies AI solutions to the textile industry, particularly to the quality inspection cycle in fabric production.The primary motivation behind this research is to overcome delays in the software development life cycle caused by the software having physical hardware dependencies, whose unavailability can injure the software development life cycle. To achieve this, this dissertation aims to develop software capable of simulating the Vision Software’s hardware components and provide an application programming interface (API) for seamless interaction with the simulation software. The research objectives encompass several key aspects. Firstly, the development of simulation software to faithfully simulate the hardware components of the Vision System. Secondly, the creation of an automated test pipeline that leverages the simulation software to enhance Smartex’s Quality Assurance processes for the Vision Software. Additionally, the reliability and effectiveness of the hardware simulationsolution will be thoroughly tested. Lastly, the impact of this work on Smartex’s software life cycle, specifically for the Embedded Systems and Quality Assurance teams, will be carefully measured and evaluated. This dissertation offers a comprehensive approach to hardware simulation through the use of software as well as automated software testing, effectively addressing a critical challenge in modern software development. The achieved solution significantly improved the overall software life cycle for the EmbeddedSystems and Quality Assurance teams at Smartex, enabling the testing and development of the Vision Software without the need for physical hardware dependencies.",
    "authors": [
      "Baixo, Ivo Alexandre Pereira"
    ],
    "keywords": [
      "Hardware-dependent systems",
      "Software simulation",
      "Software testing",
      "Software development",
      "API",
      "AI",
      "Industry 4.0",
      "Sistemas dependentes de hardware",
      "Simulação por software",
      "Teste de software",
      "Desenvolvimento de software",
      "IA",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79808",
    "title": "Synthetic data approach for traffic sign recognition",
    "abstract": "Currently, Advanced Driver Assistance Systems (ADAS) have been gradually increasing theirpresence in everyday life, thanks in part to its ability to recognize several distinct typesof objects in the road, namely, traffic signs. These systems employ Convolutional NeuralNetworks (CNNs), a type of classification algorithms that relies on an enormous amount ofdata in order to be effective. Current traffic sign datasets suffer from a scarcity of samplesdue to the necessity of compiling and labeling them manually. Such task is highly resourceand time consuming. Thus, researches resort to other mechanisms to deal with this problem,such as increasing the architectural complexity of the neural networks or performing dataaugmentation.This work addresses the data shortage issue by exploring the feasibility of developing asynthetic dataset. Such set would not require gathering and labelling manually thousandsof real word traffic sign images, requiring only easily collectable information and no humanintervention.The only data required is a set of templates for each sign given that a particular sign mayhave more than one template. This is required to cope with outdated pictograms that arestill present in streets and roads.We apply several colour and geometric processing methods to the templates aiming toachieve a look similar to real signs, from the CNN point of view. One of such methods isthe usage of Perlin noise to both simulate shadows and avoid the clean and homogeneouslook that templates have.Two use cases for synthetic data usage are presented: considering the synthetic datasetas a standalone training set, and merging synthetic data with real samples when real datais available. The first option provided results that not only clearly surpass any previousattempt on using synthetic data for traffic sign recognition, but are also encouraginglyplacing the accuracies obtained close to state-of-the-art results, with much simpler networks.The second approach provided results on three distinct test datasets that consistently beatstate-of-the-art results, either in accuracy or in simplicity of the network.",
    "authors": [
      "Silva, Diogo Lopes da"
    ],
    "keywords": [
      "Synthetic data",
      "Traffic sign recognition",
      "Convolutional neural networks",
      "European traffic signs",
      "Dados sintéticos",
      "Reconhecimento de sinais de trânsito",
      "Redes neuronais convolucionais",
      "Sinais de trânsito europeus",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82524",
    "title": "Flexible molecular alignment: an industrial case study on quantum algorithmic techniques",
    "abstract": "Flexible molecular alignment is a complex and challenging problem in the area of Medic inal Chemistry. The current approach to this problem does not test all possible alignments,but makes a previous analysis of all the variables and chooses the ones with potentiallygreater impact in the posterior alignment. This procedure can lead to wrong ”best align ments” since not every data is considered.Quantum computation, due to its natural parallelism, may improve algorithmic solutionsfor this kind of problems because it may test and/or simulate all possible solutions in anexecution cycle.As a case study proposed by BIAL and in collaboration with IBM, the main goal ofthis dissertation was to study and create quantum algorithms able to refactor the problemof molecular alignment in the new setting of quantum computation. Additionally, thecomparison between both classical and quantum solutions was defined as a subsequentgoal.During this dissertation and due to its complexity, in order to produce a practical solu tion to this problem, we resorted to a manageable number of conformations per molecule,revisited the classical solution and elaborated a corresponding quantum algorithm. Suchalgorithm was then tested in both a quantum simulator and a real device.Despite the privileged collaboration with IBM, the quantum simulations were not pro duced in viable time, making them impractical for industry applications. Nonetheless, tak ing in consideration the current point of development of quantum hardware, the suggestedsolutions still has potential for the future.",
    "authors": [
      "Oliveira, Marta Sofia Saraiva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92560",
    "title": "Kamailio-KSIPADP: support for cloud and kubernetes environments and evolution towards SaaS",
    "abstract": "The traditional application format perfectly suited the necessities of the software industry a few yearsago, but today’s needs often call for distributed applications, based on microservices, and containerized.This approach is advantageous at several levels: better usage of available resources, automatic deploy ment with integrated scaling and fault tolerance, as well as the inherent security granted by the isolationfrom the host’s operating system.Further, in order to avoid the acquisition and maintenance costs associated with the physical equipment,companies are now migrating their workloads to the Cloud, which can be further supported through amicroservice based architecture, thus capitalizing on the benefits of both.Microservice based architectures lead to an effort to rethink current applications, so that they can bemigrated to a containerized solution and deployed in a cloud environment. One case is the applicationKSIPADP, developed by Altice Labs, and the subject case of this dissertation. It is based on Kamailioand acts as a Session Initiation Protocol (SIP) Server with several components. Over the course of thisdissertation, KSIPADP is first migrated into a containerized Docker environment, followed by a singleinstance deployment in Kubernetes, and finally into a scalable Kubernetes implementation that integratesadditional services like RTPEngine or a Media Server. Every aspect of the process that led to the project’ssuccessful completion is detailed over the course of the dissertation, alongside all the tests that wereperformed in order to validate the solution’s correct functionality.",
    "authors": [
      "António, Pedro Miguel da Silva Paiva"
    ],
    "keywords": [
      "Kamailio",
      "Containers",
      "Virtualization",
      "Docker",
      "Kubernetes",
      "VoIP",
      "SIP",
      "Cloud",
      "Microservices",
      "RTP",
      "Virtualização",
      "Microsserviços",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23403",
    "title": "Monitorização e prevenção de falhas em sistemas baseados em Agentes",
    "abstract": "Os avanços tecnológicos verificados nos dia de hoje assim como a quantidadede informação e comunicação que lhes estão associados, atribuem umpapel de grande importância aos sistemas de monitorização. É no seio destaevolução, que a competição existente entre os vários setores de mercado nãoestão acessíveis a erros e falhas, principalmente ao nível dos equipamentostecnológicos. Assim, neste contexto de intolerância, assiste-se a uma proliferaçãoe a uma utilização cada vez maior de sistemas de monitorização eprevenção.Mais importante que monitorizar é prevenir. Ter a capacidade de evitaruma falha, e permitir a resolução de algum problema atempadamente é umamais valia para o desempenho, atribuindo fiabilidade e qualidade ao serviço.A gestão e a verificação de equipamentos, bem como de processos associadosaos mesmos permitem um maior controlo e domínio dum sistema.Assim esta dissertação tem como objetivo principal a implementação dumsistema para monitorizar a atividade de um ou mais sistemas multi-agente,com capacidade para intervir e avisar o administrador do sistema quandoocorre um problema.O sistema construído, que assenta numa estrutura formada por três unidadesdistintas, unidade de análise, unidade de processamento e unidade deinterface com o utilizador, permitem a implementação de processos para trocae integração de informação, exercendo assim uma comunicação fundamentalentre sistemas e utilizadores . É um sistema direcionado principalmenteà gestão e monitorização do desempenho de diferentes equipamentos assimcomo dos processos em execução nos mesmos. Este trabalho foi desenvolvido em colaboração com um Hospital no Norte do País e o ambiente escolhidopara a implementação da plataforma foi unicamente laboratorial.A concretização deste projeto esteve dividida em quatro fases: Início,Pesquisas, Construção da Aplicação e Escrita da Dissertação. Na primeirafase, o Início, foi feito o levantamento dos principais requisitos para o sistema,definição dos objetivos, e elaboração de um plano de trabalho. Na segundafase, Pesquisas, procedeu-se ao levantamento de informação sobre os conceitosteóricos relacionados com o tema em causa, como artigos científicos quedão suporte ao tema, entre outros trabalhos já publicados na mesma área.No final desta etapa já se encontravam definidas as ideias base da aplicaçãoa construir de acordo com as necessidades. Na terceira fase deste projeto, aConstrução englobou a modelação e implementação da plataforma de monitorização,de acordo com as especificações definidas anteriormente. A quartaetapa englobou a escrita da dissertação, o que incluiu um enquadramentodos conceitos teóricos em função da aplicação desenvolvida.",
    "authors": [
      "Vieira, Nuno Manuel Rodrigues"
    ],
    "keywords": [
      "Monitorização",
      "Prevenção",
      "Falhas",
      "Multi-agente",
      "Monitoring",
      "Preventing",
      "Fault management",
      "Multi-agent",
      "681.3:61",
      "61:681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79896",
    "title": "Integrating post-quantum cryptography (NTRU) in the TLS protocol",
    "abstract": "We aim to integrate new “suites”, using post-quantum authentication and encryption tech niques, in the TLS protocol. Namely, this project is dedicated to integrating algorithmsbelonging to the NTRU family of cryptossystems in the OpenSSL library and in the Pythonpackage “Cryptography”.Even though all the algorithms included in this project have already been imple mented as part of their submissions to the NIST Post-Quantum Standartization project,currently there doesn’t seem to exist a way to perform prototyping and testing of these cryp tossystems in real-life use cases, and it would be interesting to create such tools.We also aim to test if these algorithms could be further optimized for speed andefficiency by comparing the reference implementations (submited to NIST and publicly avail able) with our own implementations that perform some required mathematical operations ina very efficient manner (by using specialized number theory libraries).",
    "authors": [
      "Fontes, Afonso Pires"
    ],
    "keywords": [
      "Cryptography",
      "Post-quantum",
      "NTRU",
      "Criptografia",
      "Pós-quântica",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92815",
    "title": "GLM’s em Data Science como ferramenta na seleção de fatores de risco: um exemplo de previsão de delirium",
    "abstract": "Globalmente, 25% da população sofre de distúrbios mentais, sendo possível implementar metodologiasque possibilitem a deteção e previsão numa fase mais precoce. Concretamente, o delirium é uma disfunção neuropsiquiátrica aguda, prevalente em doentes admitidos em contexto hospitalar de internamentoe terapia intensiva. Sendo uma manifestação multifatorial é normalmente subdiagnosticada e negligenciada. O delirium pode ser categorizado, de acordo com o perfil de atividade motora, em hipoativo ehiperativo. Neste contexto, surge o tema da dissertação que visa desenvolver uma aplicação capaz deprever a ocorrência de delirium e dos seus subtipos, com base na metodologia dos GLMs.Os modelos de regressão logística multinomial são frequentemente implementados para identificar asvariáveis mais contributivas, dado que permitem modelar a relação entre os preditores e uma variável dependente multicategórica. As etapas que precedem a implementação do algoritmo dizem respeito ao pré-processamento dos dados. No decorrer do processo de modelação, aplicou-se o ADASYN para gerar amostras sintéticas devido ao desbalanceamento das classes da variável dependente. Posteriormente, foi realizada a seleção de variáveis recorrendo a diversas técnicas, sendo que o método Elastic Net com um alphade 0,1 foi o que demonstrou um melhor desempenho. Para tal, este modelo foi implementado na aplicação disponível em https://alexandra-coelho.shinyapps.io/Delirium_detection/.Para o subtipo hipoativo, permitiu a seleção de 27 variáveis, tendo obtido uma AUC-PR de 0,307 euma AUC-ROC de 0,691. As variáveis mais contributivas incluem o período de internamento em dias, oalcoolismo, os analgésicos, os cardiotónicos, assim como, o grupo de diagnóstico referente à toxicidade edrogas. Relativamente ao subtipo hiperativo, o modelo determinou 29 variáveis relevantes, onde obteveum valor de AUC-PR de 0,074 e de 0,531 para a AUC-ROC. Das variáveis mais impactantes destacam-se a PCR, a idade, a pO2, os critérios SIRS e o local de proveniência no SU, nomeadamente, o UDC1.Especula-se que os baixos valores associados essencialmente ao subtipo hiperativo são devidos à baixarepresentatividade desta categoria. Apesar deste modelo preditivo ainda poder ser melhorado, assume-secomo uma ferramenta útil para os profissionais de saúde aquando o diagnóstico do delirium no SU.",
    "authors": [
      "Coelho, Alexandra Moreira"
    ],
    "keywords": [
      "Delirium",
      "GLM",
      "Regressão logística multinomial",
      "Multinomial logistic regression",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81332",
    "title": "DIDs, Claims, Credentials e Blockchains (Self-sovereign Identity)",
    "abstract": "The high growth in the use of digital identities creates the need to develop mechanismsthat can protect the personal data of each individual. The way identity is treated todayprevents each of us from being able to control our personal information. This is due to thecentralized architecture in which the personal data are inserted, that is, all these data are kepttogether and controlled by the entities responsible for providing the most varied services,which is wrong since the identity belongs to the person and thus it must be responsiblefor controlling that identity. Centralized identity management brings within itself severalproblems, whether intentional (that is, data correlation for profiling) or unintentional (thatis, data breach).To face this problem, multiple entities across the world are developing decentralizedidentity managment systems based on a self-sovereign identity architecture where eachindividual is responsible for managing and storing a set of credentials, each with parts oftheir personal information. A self-sovereign identity architecture allows users to provide onlysmall parts of their personal information or even to omit any type of personal identification,using cryptographic techniques like selective disclosure and zero-knowledge proofs, whichallows them to have more control over their privacy.Taking into account the current problems of digital identity, this dissertation aims toexplore the state of the art and develop a proof of concept, through the implementation ofa system based on self-sovereign identity, which is able to cover the use cases for digitalidentity. Thus, this document shows the architecture implemented, with a blockchain,responsible for the storage of all public data, and a user agent, responsible for facilitating allinteractions of the various users with the developed system.The proof of concept developed allows not only to validate that it is possible to correctmany of the problems associated with centralized identity management, but also to explorenew cryptographic strategies in order to improve the way each of us manages our ownidentity.",
    "authors": [
      "Peixoto, Ricardo Jorge Marques"
    ],
    "keywords": [
      "Decentralized identifiers",
      "Digital identity",
      "Distributed ledger technology",
      "Privacy",
      "Self-sovereign identity",
      "Verifiable credentials",
      "Credenciais verificáveis",
      "Identidade auto-soberana",
      "Identidade digital",
      "Identificadores descentralizados",
      "Privacidade",
      "Tecnologia de registo distribuído",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94066",
    "title": "Implementação de biblioteca criptográfica em Jasmin",
    "abstract": "O processo de implementação de algoritmos criptográficos exige uma grande preocupação com a eficiên cia do código desenvolvida, mas sem descurar os aspetos de correção e segurança. Uma implementaçãoque apresente bom desempenho, mas não dê garantias totais da sua correção pode ser vulnerável aataques, enquanto que uma outra implementação que garanta a segurança a custo da eficiência podecomprometer a usabilidade ou até ser descartada em contextos com recursos computacionais limitados.A framework Jasmin pretende conciliar essa dicotomia, possibilitando desenvolver software criptográficode elevado desempenho e altos níveis de confiabilidade, através da combinação de construções de altoe baixo nível.No projeto apresentado nesta dissertação recorre-se à linguagem Jasmin para codificar dois algor timos criptográficos largamente utilizados: o algoritmo de cifragem autenticada com dados associadosAES-GCM e o algoritmo de assinatura digital baseado na curva elíptica P-384. Ao longo do mesmo tam bém são implementadas outras primitivas criptográficas, como o algoritmo de cifra de bloco AES-CTRe a função de hash SHA-384, sempre tendo em consideração os conceitos de segurança inerentes aodesenvolvimento de software criptográfico, tais como a política constant-time.",
    "authors": [
      "Sousa, Luís Enes"
    ],
    "keywords": [
      "Criptografia",
      "Jasmin",
      "AES-GCM",
      "ECDSA",
      "Curva P-384",
      "Cryptography",
      "P-384 Curve",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79964",
    "title": "Development of an algorithm for counting vehicles and pedestrians based on video",
    "abstract": "The population density in the urban environment has increased significantly, consequentlyincreasing the number of vehicles and people on the public road. Possible monitoring ofthis flow allows better problem management, and the enhancement of solutions in a smartcity context, solutions that promote regular traffic in a city.This work presents a solution for counting vehicles and people in a video to use thesolution developed in cities of Portugal. The solution combines deep learning techniquesand traditional computer vision techniques, combining object detection, classification, ob ject tracking, and fingerprint concepts. For each concept is presented the state of the arttechniques and techniques used in similar problems.To choose the best fingerprint methods, a comparative study of different techniques wasproduced. With a dataset of vehicle and people images, the following techniques were con sidered: Fourier Transform, Scale Invariant Feature Transform (SIFT), Color Co-occurrenceHistogram (CCoH), and Autoencoders, of which CCoH showed better results.The solution pipeline consists of the YOLOv3 algorithm for the object detection part, hav ing the algorithm a convolutional neuronal network for object classification; Kalman Filterfor object tracking was chosen in conjunction with the CCoH technique for object finger print. The pipeline ends with the matching of the newly detected objects with previouslydetected objects, using the Hungarian algorithm for this correspondence.In order to extract features using the defined pipeline, a python library has been devel oped, allowing visualization of its operation and easy integration with video sources (videofiles and cameras). Object counting, area definition, line intersection, heatmap’s, and objectcollision are examples of features that can be obtained by the library.As a global solution, a web application was developed, including a frontend application,a backend, a relational database, and a service to perform video processing with the helpof the developed library. The web application is in use and in a production environment.",
    "authors": [
      "Matos, Miguel"
    ],
    "keywords": [
      "Computer vision",
      "Deep learning",
      "Object detection",
      "Object fingerprint",
      "Object tracking",
      "Detecção de objectos",
      "Monitorização de objectos",
      "Visão por computador",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59734",
    "title": "Development of an adaptable multicast overlay network",
    "abstract": "Multicast is a group communication paradigm created in order to reduce, as much aspossible, the amount of data generated to the network. However, limited deployment of IPMulticast protocols has motivated an interest in alternative approaches which implement asimilar process of Multicast at an application-level (using solely end-systems and not therouters). In this context, different methodologies are presented, entitled Application-LayerMulticast or Overlay Multicast, which may vary in the way they operate.This dissertation’s objective is to develop and experiment a prototype of an overlay multicastsystem. This system should be easily configurable and adaptable in order to assumedifferent strategies when establishing the multicast distribution tree. It is also expectedto explore and integrate collaborative mechanisms between the overlay network and theInternet Service Providers (ISP).With the presented context, the first step to take is an investigation on the state of theart, where technologies relevant to this work will be presented. After this initial step, thedeveloped system’s architecture will be described, one which enables different ways ofbuilding and maintaining the multicast distribution tree. The envisioned system can operateindependently, integrating mechanisms where the distribution tree relies solely on peerdecisions, which will be firstly addressed. Then, this work will move on to collaborativemechanisms between the overlay’s management (the central node) and the Internet ServiceProviders. Based on the proposed system architecture, several mechanisms are explored,not only focusing on alternative ways to build distribution trees, but also mechanismsallowing for some traffic engineering objectives involving the Internet Service Providers.Using the CORE network emulator, all the proposed mechanisms are tested, and resultsare analyzed to corroborate the system’s correct operation.",
    "authors": [
      "Sampaio, André Filipe da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27791",
    "title": "Sistema de notificação de eventos adversos em ambiente hospitalar",
    "abstract": "O tema da gestão do risco começou a ser debatido aquando da publicação do relatório doInstituto de Medicina, em 2000 “To Err Is Human: Building a Safer Health System”, quecaptou a atenção da população mundial, ao revelar que 44000 a 98000 pessoas morrem nosEstados Unidos da América como consequência direta de erros nos cuidados de saúde. É apartir da última década que este tema passa a ser de extrema importância, e se começam atomar medidas para o prevenir. As organizações de saúde voltaram as suas políticas para agestão do risco, contudo, tem-se notado que estas têm grandes dificuldades a aprender com oerro. De forma a colmatar este problema é necessário registar todos os eventos adversos. Nomínimo esses registos servem para identificar perigos, e minimizar a ocorrência do mesmoevento.O objetivo deste trabalho é o estudo e a concepção de um sistema de notificação e tratamentode eventos adversos integrável em sistemas de informação hospitalar existentes. Além disso,dá a conhecer a importância que os sistemas de notificação de eventos adversos têm nasociedade atual, ao fornecer um estado de arte desta temática, e apresentando sistemas jáutilizados, e com benefícios já reconhecidos.A avaliação do sistema desenvolvido foi efetuada através da sua integração na Intranet detodos os hospitais do Grupo Trofa Saúde, que no futuro se irá refletir como um indicador dequalidade dos cuidados prestados nesta organização de saúde.",
    "authors": [
      "Martins, Renata Reis Pires"
    ],
    "keywords": [
      "681.3:61",
      "61:681.3",
      "614.253.83"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:61",
      "61:681.3",
      "614.253.83"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79851",
    "title": "Using container-based virtualization on web apps production environment: dipcode development cycle",
    "abstract": "With the fast evolution of the internet over the last years, the top priority on softwaredevelopment has shifted from what? to when?. Reduced time-to-market is now thecompetitive edge that all companies strive for.The usage of container-based virtualization technologies keep the multiple environmentswhere a development team works similar enough, that their work is made easier when devel oping and testing new features, which in turn results in a significantly faster delivery. Thenature of this tecnhology also brings numerous advantages when it comes to management,monitoring and maintaining resources, allowing for an ease of adjustment, based on theclient needs.Throughout this dissertation is presented an extended base of knowledge about containertechnologies, especially Docker, as well as what are the basic techniques to use when buildingan application inside such infrastructure, from the writing of the Dockerfile to the adaptationof the multiple pipelines responsible to deploy the application.",
    "authors": [
      "Guimarães, Luís Miguel Pinheiro"
    ],
    "keywords": [
      "Docker",
      "Docker swarm",
      "Kubernetes",
      "Orchestration",
      "Containers",
      "Containerization",
      "Services",
      "Build",
      "Deploy",
      "Web applications",
      "Orquestração",
      "Containerização",
      "Serviços",
      "Build",
      "Deploy",
      "Aplicações web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79792",
    "title": "Probabilistic logic programming for cancer genomics",
    "abstract": "Over the past years, research on cancer genomics has been boosted by the advances in high throughput sequencing technologies. The Cancer Genome Atlas (TCGA) project is an effort to map the genomic alterations possibly associated with specific types of tumours and aims to improve the prevention, diagnosis and treatment of cancer. The generation of large and heterogeneous datasets, as a result of TCGA and other similar projects, creates the need to use advanced bioinformatics and computational tools for the analysis of cancer genomic data.Despite different bioinformatics frameworks have been established in order to explore and perform comprehensive analysis of cancer datasets, the area of logic and probabilistic logic programming has not been sufficiently explored in the analysis of cancer data.The main goal of this thesis was to explore Problog – a probabilistic logic programming (PLP) language – to encode interactions on heterogeneous cancer genomics datasets that may lead to new insights. To accomplish this objective, our work consisted in the elaboration of a python program and a Problog framework. The used datasets involved stomach cancer genomic data.The python program – ProceOmics – aimed to process and format cancer genomic data so it could be used by Problog programs. The Problog framework – Problog Knowledge Base (KB) –intended to codify the data previously processed by ProceOmics. To evaluate the consistency of the developed framework and explore possible relations between the different types of genomic data, queries were formulated to the Problog KB.Thus, this thesis provides a tool that establishes a link between the genomic data contained in public databases with probabilistic logic programs. We hope this work may help to overcome futureefforts to use PLP on genomic data analysis.",
    "authors": [
      "Fernandes, João Pedro Alves"
    ],
    "keywords": [
      "Cancer genomics",
      "Exploration",
      "Problog",
      "Stomach cancer",
      "TCGA",
      "Data processing",
      "Estudos genómicos",
      "Exploração",
      "Cancro do Estômago",
      "Processamento de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92690",
    "title": "Monitorização da qualidade da água numa estação de tratamento de águas residuais: uma abordagem baseada em Machine Learning",
    "abstract": "A monitorização da qualidade da água é uma tarefa fundamental que tem de ser incluída em qualquerprocesso operacional de uma Estação de Tratamento de Águas Residuais (ETAR), pois permite verificarse os efluentes, quando descarregados no meio ambiente, cumprem os valores padrão definidos por lei.Se estes valores não forem monitorizados com eficácia, a poluição da água continua a intensificar-se e,consequentemente, a água torna-se cada vez mais escassa. Devido ao problema da poluição da água,tem-se verificado um aumento na escassez da água, em Portugal, durante as últimas décadas.A introdução de abordagens de Machine Learning (ML) neste tipo de operações pode vir a serbastante importante, devido à sua capacidade de melhorar a eficiência da monitorização da qualidade daágua e da previsão das substâncias da mesma. Uma das principais características que esta abordagemoferece é a interpretação de padrões e tendências nos dados, que não são facilmente identificáveis poroutras técnicas, e ainda a interpretação de relações não lineares nos dados.Deste modo, este trabalho visa a implementação de modelos de ML baseados em ConvolutionalNeural Networks (CNN), Long Short-Term Memory (LSTM), Transformer, e Transformer-LSTM,para a previsão dos valores da condutividade e do caudal, no afluente duma ETAR, de modo a apoiara monitorização da qualidade da água, e a celeridade do processo de tomada de decisão neste tipo deinfraestruturas. Diante dos melhores modelos candidatos obtidos, verificou-se que os modelos baseadosem Transformer alcançaram os melhores resultados na previsão da condutividade, nas duas abordagensconsideradas (multivariate e univariate), enquanto que os modelos baseados em CNN alcançaram omelhor desempenho na previsão do caudal, também nas duas abordagens supramencionadas.",
    "authors": [
      "Pereira, João Paulo Ribeiro"
    ],
    "keywords": [
      "Deep Learning",
      "Estação de tratamento de águas residuais",
      "Machine Learning",
      "Séries temporais",
      "Sustentabilidade",
      "Sustainability",
      "Time series",
      "Wastewater treatment plant",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83611",
    "title": "Development of a classification algorithm for vehicle impacts: an anomaly detection approach",
    "abstract": "In the past decade, Machine Learning has been heavily applied to automobile industry solutions, the mostpromising being development of autonomous vehicles.New mobility services are available today as alternatives to owning a car, like ride hailing and carsharing.High costs associated with the maintenance of the vehicle and the reduced rate of vehicle usethroughout the day are some of the factors for the popularity of these services.Car-sharing is self-service mode of transport that provides its members with access to a fleet of vehiclesparked in various locations throughout a city.Damages are expected to happen when vehicles are used and the required repair implies costs to fleetoperators. Systems able to detect these damages will promote a better use of these vehicles by vehicleusers.Vehicle damages result from impacts with other objects, for instance, other vehicles or structures ofany kind and these impacts inflict deformations to the vehicle exterior structure. Most of these impactscan be perceived or detected by the forces involved as result of the impact.Anomaly Detection is a technique applicable in a variety of domains, such as intrusion detection, frauddetection, event detection in sensor network or detection ecosystem disturbances.The objective of this thesis is the study and development of a semi-supervised intelligent system for detectionand classification of vehicle impacts with an Anomaly Detection approach, using the accelerometerdata, and following a strategy that would allow exploring a Machine Learning cycle.This thesis was developed under an internship in the company Bosch Car Multimedia S.A, located inBraga.",
    "authors": [
      "Pereira, Rita Alexandra Ferreira"
    ],
    "keywords": [
      "Accelerometer",
      "Anomaly detection",
      "Impacts",
      "Semi-supervised learning",
      "Acelerómetro",
      "Aprendizagem semi-supervisionada",
      "Deteção de anomalias",
      "Impactos",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28562",
    "title": "Comparison of software development methodologies based on the SWEBOK",
    "abstract": "We are facing a period where software projects have a huge dimension involvingsmall resources, high risk and a wide range of available approaches. In thisscenario the Software Development Methodologies (SDMs) can prove to be auseful ally, but very dangerous and even fatal if misused. The big issue aroundthis matter is how to choose the appropriated SDM that ts a speci c project.In the given scope, this dissertation describes a framework for comparing SDMsdelivering a set of procedures that should be followed when the choice of anSDM is made. The dissertation approaches the framework by applying it to agroup of SDMs that were selected by their popularity and signi cance. Thisexercise is done to prove the concept of the framework and to provide a basecomparison, with each chosen SDM, that can, and should, be extended by thosewho choose to use the framework.The classi cation is achieved by de ning a scale that goes from total satisfactionto no satisfaction, with an intermediate level of partial satisfaction, that is appliedto a set of keys. These keys are based in SWEBOK (Software EngineeringBody Of Knowledge) that describes and explains the di erent Knowledge Areas(KA) stating their common issues and best practices. To explain the framework,the dissertation analyzes each KA and evaluates the selected SDMs byassessing how their approach complies with SWEBOK's knowledge areas, usingthe previous stated scale.The framework delivered can be enriched by its user who should provide weightsto each KA regarding the project in which the SDM will be used and previousexperiences",
    "authors": [
      "Simão, Elísio Maciel"
    ],
    "keywords": [
      "681.3.06"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27886",
    "title": "Conceção e linhas de orientação para o desevolvimento de aplicações em HTML5 : o case study Primavera BSS",
    "abstract": "Nos últimos anos temos assistido a grandes evoluções nas aplicações web,proporcionando uma interação cada vez mais apelativa, tanto no seu aspeto como na suausabilidade. Com estas evoluções foi surgindo o conceito de Rich Internet Applications(RIA). As RIA são aplicações web que tem características e funcionalidades que eramusualmente desempenhadas por software para desktop dada a sua complexidade. Oconceito passa por transferir processamento para o cliente (browser), permitindo umainteração mais próxima com o utilizador, respostas mais rápidas e uma menorsobrecarga sobre o lado do servidor. Esta ideia permite uma abordagem aodesenvolvimento de aplicações empresariais no contexto web, sendo possível manter acapacidade de processamento das aplicações desktop tirando partido do melhor domundo web e permitindo chegar mais facilmente aos clientes.A PRIMAVERA, como interessada neste tipo de aplicações, já desenvolveu aFramework Athena assente nas tecnologias Microsoft Silverlight e WCF RIA Services,mas dado que a evolução do Microsoft Silverlight foi descontinuada por parte daMicrosoft, existe a necessidade de procurar uma alternativa. O HTML5 surge comoprincipal alternativa, mas é preciso perceber se este pode efetivamente constituir-secomo tal.Assim, esta dissertação será inicialmente focada na análise comparativa entre astecnologias Microsoft Silverlight e HTML5, passando depois pela implementação deprotótipos, que no final, permitam à PRIMAVERA tomar uma decisão sobre asubstituição da tecnologia.O caso de estudo será uma aplicação desenvolvida em HTML5, JavaScript eCSS, que poderá ser instalada em dispositivos móveis com qualquer sistema operativo.Estas aplicações, denominadas aplicações híbridas, implicam também uma elevadacomplexidade no lado do cliente e envolvem vários dos conceitos relacionados com odesenvolvimento de RIA em HTML5.",
    "authors": [
      "Costa, Vítor Nuno Rodrigues"
    ],
    "keywords": [
      "681.3.06",
      "681.324"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.06",
      "681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64530",
    "title": "Space colonisation based procedural road generation",
    "abstract": "Procedural generation of content has been studied for quite some time and it is increasinglyrelevant in scientific areas and in video-game and film industries. Procedural road layoutgeneration has been traditionally approached using L-Systems, with some works exploringalternative avenues. Although originally conceived for biological systems modelling, theadequacy of L-Systems as a base for road generation has been demonstrated in severalworks.In this context, this work presents an alternative approach for procedural road layoutgeneration that is also inspired by plant generation algorithms: space colonisation.In particular, this work uses the concept of attraction points introduced in space colonisationas its base to produce road layouts, both in urban and inter-city environments. As willbe shown, the usage of attraction points provides an intuitive way to parameterise a roadlayout. The original Space Colonization Algorithm (SCA) generates a tree like structure,but in this work, the extensions made aim to fully generate a inter-connected road network.As most previous methods the method has two phases. A first phase generates whatis mostly a tree structure growing from user defined road segments. The second phaseperforms the inter connectivity among the roads created in the first phase.The original SCA parameters such as the killradius help to control the capillarity of theroad layout, the number of attraction points used by each segment will dictate its relevanceestablishing a road hierarchy naturally dependent on the distribution of the attractionpoints on the terrain. An angle control allows the creation of grid like or more organicroad layouts.The distribution of the attraction points in the terrain can be conditioned by boundarymaps, containing parks, sea, rivers, and other forbidden areas. Population density maps canbe used to supply an explicit probabilistic distribution to the attraction points. Flow-fieldscan be used to dictate the flow of the road layout. Elevation maps provide an additionalrestriction regarding the steepness of the roads.The tests were executed within a graphic toolbox developed simultaneously. The resultsare exported to a geographical information file format, GeoJSON, and then maps are renderedusing a geospatial visualisation and processing framework called Mapnik.For the most part, parameter settings were intuitively reflected on the road layout andthis method can be seen as a first step towards fully exploring the usage of attraction pointsin the context of road layout.",
    "authors": [
      "Fernandes, Gabriel Dias"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88584",
    "title": "Geração de lojas online orientada para uma framework – funcionalidade de gestão de utilizadores",
    "abstract": "O desenvolvimento de todo o código necessário para uma loja online é um trabalho demorado e complexo. Todas as lojas online têm diferentes exigências e condições, no entanto, existem pontos comuns entre vários tipos de loja. Supondo a possibilidade de extrair esses aspetos comuns, seria possível a criação de um esqueleto de código com todos os componentes básicos, sobre os quais um programador poderia acrescentar e moldar a loja online, reduzindo assim o tempo necessário para a desenvolver. A elaboração desta dissertação começou pela criação de uma loja online que suportasse um negócio fictício de venda de bicicletas e produtos relacionados. Este passo serviu para identificar e avaliar quais seriam os parâmetros necessários especificar, e em que sentido fariam diferença na construção de um website de vendas bem como, distinguir os componentes comuns e as suas relações.A presente dissertação tem como propósito principal o desenvolvimento de uma ferramenta para a criação de lojas online. Esta ferramenta pretende ser usada por programadores para produzir o código padrão que é comum a qualquer implementação de uma solução de loja online. Através da recolha de parâmetros sobre a loja online a ser criada, são construídos os ficheiros necessários para a implementação do site.A aplicação concebida focou-se na utilização de apenas alguns dos componentes básicos, nomeadamente, “Utilizadores” e “Categorias” no back-end, “Registo e Login” e “Perfil” no front-end, mostrando assim ser possível usar a framework desenvolvida para uma implementação parcial de uma loja online. A abordagem modular permite expandir a aplicação, criando e adicionando outros componentes à estrutura parcial existente.",
    "authors": [
      "Teles, Tiago Martins"
    ],
    "keywords": [
      "Comércio eletrónico",
      "Plataforma E-Commerce",
      "Loja online",
      "E-Commerce",
      "E-Commerce platform",
      "Online store",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94067",
    "title": "Ecossistema da Carteira de Identidade Europeia (EUDIW): componente ’Person Identification Data (PID) provider’",
    "abstract": "No seguimento do regulamento Europeu de Identificação Electrónica (eIDAS), tem vindo a ser promovida a Carteira de Identidade Digital Europeia (European Digital Identification Wallet - EUDIW). Trata-se de um mecanismo de identificação desmaterializado que oferece garantias de segurança e privacidade aos cidadãos, e que se pretende interoperável nos vários estados membros. Esta dissertação aborda aemissão de Personal Identification Data (PID) para diferentes países no contexto da EUDIW, e ainda umaemissão da versão digital da carta de condução automóvel (mobile Driver License — mDL). O estudo compreende uma análise de soluções semelhantes disponibilizadas em Portugal, assim como uma análise dos standards e regulamentos que se enquadram no âmbito desses sistemas. Foca-se depois no desenho e implementatção do protótipo para a componente de Personal Identification Data Provider e da mDL.",
    "authors": [
      "Silva, Rolando José Soares"
    ],
    "keywords": [
      "PID",
      "mDL",
      "Carteira digital",
      "EUDIW",
      "Identidade digital",
      "Identidade móvel",
      "CMD",
      "Digital wallet",
      "Digital Identity",
      "Mobile Identity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55842",
    "title": "Development of web-based tools for metabolomics data analysis and mining",
    "abstract": "The recent advances in metabolomics experimental techniques have provided novel approachesfor many research issues in the biological fields. Indeed, the ability to identifyand quantify numerous compounds in biological samples provides significant advances infunctional genomics, biomarker identification, sample characterization or drug discoveryand development. To take full advantage of these data advanced bioinformatics methodsfor data analysis and mining have been required.A number of methods and tools for metabolomics data analysis have been put forwardrecently, being one of the major limitations still faced the lack of integrated frameworks forextracting relevant knowledge from these data and being able to integrate these data withprevious biochemical knowledge. Also, the lack of reproducibility in many data analysesor data mining processes is a strong obstacle for biological discovery.In recent work from the host group, specmine, a metabolomics and spectral data analysis/mining framework, in the form of a package for the R system, has been developed toaddress some of these issues.In this thesis, an integrated web-based platform for metabolomics data analysis and mining,named WebSpecmine, was designed and developed, based on the specmine package, thusproviding an easier and friendly user interface. This website provides means for analysingmetabolomics data from different formats, including tasks such as pre-processing, univariateand multivariate analysis and metabolite identification. This web-based platform wasdeveloped collaboratively and, therefore, this work focused mainly in data from nuclearmagnetic ressonance and mass spectrometry.Also, the package faced some limitations regarding types of analysis not yet provided,such as metabolite identification for other data formats besides Mass Spectrometry coupledto Liquid Chromatography. Therefore, the extension of the metabolite identification featurewas addressed, by implementing such analysis for Nuclear Magnetic Ressonance data inthe specmine package, as well as making it available in the website.The website was validated by applying it to reproduce the pipelines from previous studiesthat made use of the specmine package. Furthermore, a case study involving bananapeels and the analysis of their characteristics and potential made use of the newly createdwebsite to further validate its functionality. All the analyses here executed were stored andare available in the web application, as public projects.",
    "authors": [
      "Cardoso, Sara Manso de Sousa"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81066",
    "title": "Large-scale, controlled growth of two-dimensional materials by chemical vapor deposition",
    "abstract": "In this work, the optimization of two atmospheric pressure chemical vapor deposition systems was carried out in order to grow two different bidimensional materials, namely hBN and MoSe2. The first is an insulator with a structure similar to graphene and it is seen as an optimal candidate for several applications, in particular photonics and optoelectronics. The latter is a prominent semiconductor belonging to the family of two-dimensional transition metal dichalcogenides which demonstrated outstanding optoelectronic properties, such as thickness-dependent photoluminescence, combined with lightweight and flexibility. Several deposition parameters were investigated in parallel with an extensive characterization methodology carried out by optical microscopy, Raman spectroscopy, atomic-force microscopy, scanning electron microscopy, x-ray photoelectron spectroscopy, energy dispersive x-ray spectroscopy and transmission electron microscopy. As a final result, reliable experimental procedures have been established which lead to the growth of few-layer polycrystalline hBN films (up to 20cm2) and μm-sized single crystals of monolayer MoSe2. Selected samples were tested in experimental devices. Thefluorescent properties of 2D hBN films were probed to quantify its performance as a single-photon emission source at room temperature. A sensing device based on 2D MoSe2 was assembled to investigate the optical response of the material to various degree of tensile strain.",
    "authors": [
      "Fernandes, João Henrique de Castro"
    ],
    "keywords": [
      "Chemical vapor deposition",
      "Hexagonal boron nitride",
      "Molybdenum diselenide",
      "Two-dimensional materials",
      "Deposição química de vapores",
      "Diseleneto de molibdénio",
      "Materiais bidimensionais",
      "Nitreto de boro hexagonal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64157",
    "title": "Automated computer-aided design of cranial implants: a deep learning approach",
    "abstract": "Over the past decade, there has been an increase in the number of surgeries for reconstructionof cranial defects with implants. Computer-aided Design (CAD) software enablesthe design of patient-specific cranial implants, therefore increasing the reliability of thereconstruction, but there is often a lack of appropriate software. Either the software is expensiveto acquire which limits its availability or it is not user-friendly for clinicians andtherefore very time-consuming to use. This thesis proposes a Deep Learning (DL) approachtowards the automated CAD of cranial implants, allowing the design process to be lessuser-dependent and even less time-consuming.The problem of reconstructing a cranial defect, which is essentially filling in a hole in a skull,was posed as a 3D shape completion task and, to solve it, a Volumetric Convolutional DenoisingAutoencoder (DAE) was implemented using the open-source DL framework PyTorch.In order to train the autoencoder, a large amount of 3D skull models was required, andthese were obtained by processing an open-access dataset of Magnetic Resonance Imaging(MRI) brain scans. The 3D skull models were represented as binary voxel occupancy gridsand experiments were carried out for different voxel resolutions (303, 603 and 1203). Foreach experiment, the autoencoder was evaluated in terms of quantitative and qualitative 3Dshape completion performance. The obtained results showed that the implemented VolumetricDAE is able to perform shape completion on 3D models of defected skulls, allowingfor an efficient and automatic reconstruction of cranial defects with a single forward passof the trained model. Even though the current computational resources impose limitationsin the resolution of the 3D skull models, the results presented in this thesis make it possibleto conclude that DL can be considered a promising approach towards the automatedreconstruction of cranial defects.",
    "authors": [
      "Morais, Ana Rita André"
    ],
    "keywords": [
      "Cranial implants",
      "Medical imaging",
      "Computer-aided design (CAD)",
      "Deep learning",
      "3D Shape completion",
      "Denoising autoencoders",
      "Implantes cranianos",
      "Imagem médica",
      "Desenho assistido por computador",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79989",
    "title": "Performance tuning to determine electronic properties of materials with Quantum Espresso",
    "abstract": "Desde os anos 70, a teoria do funcional da densidade tem sido uma das técnicas maisutilizadas em física quântica para resolver a equação de Schrödinger, para determinar aspropriedades eletrónicas dos materiais, usando funções de onda eletrónica e a energia decada eletrão. O método de resolução do campo autoconsistente (sigla em inglês SCF) é umprocesso iterativo que calcula a densidade dos eletrões a partir de funções de onda. Estescálculos com a equação de Schrödinger são realizados múltiplas vezes de forma sucessivaaté se atingir a convergência autoconsistente.O SCF neste processo iterativo é atualmente calculado em pacotes de software dedicado,como o Quantum ESPRESSO (QE), um produto open-source em Fortran 90, para cálculo daestrutura eletrónica dos materiais. Sendo estes cálculos computacionalmente intensivos, asua execução paralela permite melhorar o desempenho do processo de cálculo. O QuantumESPRESSO (QE) suporta paralelismo em ambiente de memória distribuída, com messagepassing interface (MPI) e, mais recentemente, em memória partilhada, com OpenMP.A presente dissertação apresenta várias propostas de instalação e configuração desta fer ramenta. Estas propostas sugerem diferentes estratégias de paralelismo tendo em vistaobter melhorias de desempenho deste tipo de simulações, comparativamente a um estudoanterior realizado nas mesmas condições de experimentação. Para o presente estudo foramutilizados processadores multicore e many-core do cluster SeARCH. Estes testes exploraramo impacto de versões multiprocesso com múltiplos fios de execução por processo, introduzi das em versões mais recentes do QE com desenvolvimento de paralelizações híbridas.Através de diferentes casos de teste, diferentes instalações e parâmetros configuráveis(número de pools) este trabalho explorou e procurou obter um ambiente de execução quemelhor favorecia o desempenho de simulações do tungsten diselenide (WSe2) no clusterSeARCH. Os resultados obtidos nestes testes, onde se aconselham certas configurações ese desaconselham outras, destinam-se a ajudar as comunidades de Física a encontrar umambiente de execução afinado em termos de desempenho, para o caso concreto deste tipode simulações.",
    "authors": [
      "Caldas, Sérgio Manuel Rodrigues"
    ],
    "keywords": [
      "Biblioteca ELPA",
      "Computação paralela",
      "Disselénio de tungsténio",
      "Eficiência computacional",
      "Intel KNL",
      "Quantum Espress",
      "Computational efficiency",
      "ELPA-library",
      "Parallel computing",
      "Tungsten diselenide",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84131",
    "title": "Programmable environments for quick orchestration of deployments",
    "abstract": "Com o surgimento da computação em cloud, tem havido uma crescente adoção da containerisação(containerisation) e orquestração de containers para o desenvolvimento de software. As empresas queaderem a práticas de ContinuousIntegration/ContinuousDelivery(CI/CD)e microserviços beneficiammuito da adoção destas tecnologias, pois os containerstem permitido o aprovisionamento mais rápido eautomatizado de aplicações, melhorando a sua escalabilidade e capacidade de tolerância a faltas.A Feedzai é uma empresa que usa algoritmos de machine learning para combater a fraude financeira,usando um sistema distribuído complexo constituído por múltiplas tecnologias. Escrever configuraçõespara ambientes de teste nestas condições é frequentemente um desafio para o engenheiro de testes,especialmente se feito manualmente, resultando num maior custo em horas-humano necessárias paradesempenhar esta tarefa. Quando se trata de ambientes de teste, estas configurações dependem muito datopologia requerida pelo teste, o que resulta num potencialmente grande e crescente número de ficheirosde configuração para gerir. É obrigatório resolver este problema cedo, de forma a antecipar uma stacktecnológica difícil de gerir à medida que os casos de teste que terão de ser cobertos crescem. Esteproblema foi alvo de várias tentativas de solução por parte de engenheiros na Feedzai, mas as soluçõesresultantes provaram, com o passar do tempo, ser insuficientes, resolvendo apenas parte do problema.Esta dissertação apresenta a arquitetura e principais decisões de implementação do ProgrammableEnvironmentsforQuickOrchestrationofDeployments(Pequod), uma framework que se propõe a resolver o problema supramencionado ao permitir ao programador lançar um ambiente composto por uma stack tecnológica arbitrária usando uma qualquer tecnologia de containerisação/orquestração escolhida pelomesmo. Com esta ferramenta, o programador apenas escolhe quais os componentes que serão lançados e descreve as dependências entre os mesmos; a lógica de configurar estes componentes usando atecnologia escolhida é executada pelo Pequod, sem que o programador tenha de ficar familiarizado comesta. O desenho da Domain-SpecificLanguage(DSL)que permite ao programador definir o ambiente deforma transparente é também discutido aqui.O presente documento apresenta também uma avaliação das capacidades desta framework usando doisprodutos distintos da Feedzai. Os resultados desta avaliação revelaram que esta nova framework está emconformidade com os objetivos delineados de início, resolvendo os problemas que as soluções antecessoras não resolviam.",
    "authors": [
      "Poleri, Maria Helena Ribeiro"
    ],
    "keywords": [
      "Automação",
      "Containerisação",
      "Integração contínua",
      "Orquestração de containers",
      "Automation",
      "Container orchestration",
      "Containerisation",
      "Continuous integration",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36779",
    "title": "Automatic detection of architectural violations in evolutionary systems",
    "abstract": "Software applications evolve over the years at a cost: their architecture modularity tends to be degraded. This happens mainly because software application maintenance often leads to architectural degradation. In this context, software architects need to elaborate strategies for detecting architectural degradation symptoms and thus maintaining the software architectural quality. The elaborations of these strategies often rely on tools with domain-specific languages (DSLs), which help them to specify software architecture rules. These tools also enforce the adherence of these rules in the evolving program. However, their adoption in mainstream software development is largely dependent on the usability of the language. Unfortunately, it is also often hard to identify their usability strengths and weaknesses early, as there is no guidance on how to objectively reveal them. Usability is a multi-faceted quality characteristic, which is challenging to quantify before a DSL is actually used by its stakeholders. There is even less support and experience on how to quantitatively evaluate the usability of DSLs used in software maintenance tasks. To this end in this dissertation, a usability measurement framework was developed based on the Cognitive Dimensions of Notations (CDN). The framework was evaluated both qualitatively and quantitatively using two textual DSLs for architecture rules in the context of two evolving object-oriented systems. The results suggested that the proposed metrics were useful: (1) to early identify the DSL usability limitations to be addressed, (2) to reveal specific features of the DSLs favoring software maintenance tasks, and (3) to successfully analyze eight usability dimensions that are critical in many DSLs. However, along with these results this evaluation also revealed that this kind of tools lack support for communication among the stakeholders, creating a gap in the software development. To solve this problem we proposed heuristics for tools that use DSLs for detecting architecture degradation symptoms. These heuristics will permit the exchange of information between the stakeholders, thereby, also increasing the tool usability. Finally, we chose TamDera as the tool to implement these heuristics in our study domain. Therefore, we implemented in the new version of TamDera the communication support for the stakeholders by using a new architecture and a new environment with the developed heuristics.",
    "authors": [
      "Albuquerque, Diego de Lara e"
    ],
    "keywords": [
      "DSL",
      "Architectural degradation",
      "Code anomalies",
      "Usability",
      "CDN framework",
      "Metrics",
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79939",
    "title": "Characterization of genetic variants in 70 portuguese individuals",
    "abstract": "A análise genómica das populações tem contribuído significativamente para o aumento do número de SNVs descritos em bases de dados. Estudos populacionais prévios têm contribuído com 18 a 57% novas variantes. A nova informação genética é particularmente relevante enquanto referência para propósitos clínicos. Iniciativas à escala global como o 1000 Genomes Project (1kG) incluem populações Ibéricas, contudo, nenhum indivíduo Português foi incluído no mesmo grupo. Tanto quanto se sabe, nenhum indivíduo Português foi incluído no projeto gnomAD, o maior conjunto de dados genómicos atual. Acreditamos que uma coleção de informação genómica referente à população Portuguesa poderia trazer grandes benefícios ao diagnóstico molecular em pacientes Portugueses. As alterações genéticas detetadas em 70 indivíduos Portugueses foram inseridas em uma base de dados não-relacional. A informação publicada pelos projetos 1kG e gnomAD para cada alteração incluída nas mesmas foi adicionada à referida base de dados. Frequências alélicas reportadas para sete populações incluídas na base de dados do gnomAD, cinco populações do 1kG e 5 subpopulações Europeias do mesmo projeto foram comparadas contra os valores calculados para os nossos dados. As diferenças das distribuições alélicas foram testadas com o Fisher’s Exact test. Os p-values obtidos foram corrigidos de acordo com a sua False Discovery Rate (FDR). Os exomas de indivíduos Portugueses analisados continham 224,155 alterações genéticas filtradas de acordo com critérios de qualidade definidos no presente estudo. Aproximadamente 16,4% das variantes não se encontravam descritas nas bases de dados dos projetos 1kG e gnomAD. Os resultados obtidos endossam evidências, previamente descritas na literatura, de uma correlação entre as diferenças genéticas das populações comparadas em relação à população Portuguesa e a distância geográfica das mesmas a Portugal. Diferenças significativas entre distribuições alélicas da população estudada e outras subpopulações Europeias foram encontradas para 7,284 alterações genéticas distribuídas por 2,571 genes. Os resultados obtidos sugerem a existência de marcadores genéticos populacionais e podem motivar futuros estudos com vista a detetar marcadores genéticos específicos da população Portuguesa. O estudo apresentado representa uma contribuição significativa para, não só enriquecer iniciativas genómicas de grande escala, mas também para estabelecer uma referência auxiliar para análises genéticas a doentes Portugueses.",
    "authors": [
      "Martins, Daniel Eduardo Fernandes"
    ],
    "keywords": [
      "Genómica",
      "Alterações genéticas",
      "Exomas",
      "Distribuição alélica",
      "População",
      "Genomics",
      "Variants",
      "Exomes",
      "Allele distribution",
      "Population",
      "Ciências Naturais::Outras Ciências Naturais"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Outras Ciências Naturais"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79918",
    "title": "Sistema de apoio à decisão clínica baseado em regras utilizando o modelo openEHR",
    "abstract": "Atualmente os Sistemas de Apoio à Decisão Clínica (SADC) são um componente essencial para qualquer sistema informático clínico. A sua correta integração nos sistemas existentes permite uma maioreficiência na aquisição e disponibilização de dados importantes e relativos a cada paciente. Deste modo, éimperativo que um SADC garanta a interoperabilidade semântica, isto é, a capacidade de comunicar comoutras plataformas sem que hajam perdas de significado no seu conteúdo. É necessário também garantirque estes sistemas sejam de fácil integração, de modo a possibilitar a agregação da maior quantidadede dados clínicos possível dos diferentes sistemas existentes nas instituições.Com isto, surge então a necessidade de optar por padrões que garantam a interoperabilidade semântica deste sistemas e que facultem meios para a sua rápida integração. Neste sentido, emerge oopenEHR, um padrão e-health aberto que oferece os meios necessários para garantir as propriedadessupracitadas anteriormente. Adicionalmente, mas não menos importante, este apresenta um módulo deapoio à decisão que permite codificar a lógica de decisão presente nos planos e guidelines clínicos.São várias as definições existentes para o conceito de guideline, no entanto, pode-se assumir, paraefeitos de simplificação, o guideline como sendo um conjunto de declarações que visam guiar o profissional de saúde na tomada de decisão no seu dia-a-dia. Este tipo de sistema é relevante pois visa minimizarpossíveis erros de teor humano que podem facilmente ser cometidos por parte dos profissionais que estão sujeitos diariamente a vários factores como turnos extensos, cansaço, entre outros e que em últimainstância podem compremeter a tomada de uma decisão acertada.Assim, a presente dissertação tem como finalidade desenvolver um Sistema de Apoio à DecisãoClínica, baseado em regras e no modelo openEHR, que permite a criação, gestão e execução de guidelinesclínicos. Para além disso, foi-se mais além e desenvolveu-se uma interface gráfica que faculta ao utilizadorum meio simples, intuitivo e de agradável visualização.",
    "authors": [
      "Silva, Sarah Tifany da"
    ],
    "keywords": [
      "Sistemas de apoio à decisão clínica",
      "openEHR",
      "Interoperabilidade semântica",
      "Motor de inferência",
      "Módulo de apoio à decisão",
      "Guidelines",
      "Interoperabilidade semântica",
      "Registos eletrónicos de saúde",
      "Clinical decision support system",
      "Semantic interoperability",
      "Inference engine",
      "Decision support module",
      "Electronic health records",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34139",
    "title": "Usabilidade e aceitação de tecnologia : estudo de casos em sistemas de informação hospitalar",
    "abstract": "O aumento exponencial da informação médica e a procura pela otimização da prestação de cuidados de saúde, conduziu à introdução das tecnologiasde informação na área da saúde. Não existe qualquer dúvida de que a introdução do Processo Clínico Eletrónico (PCE) na área da saúde é uma maisvalia para a qualidade dos cuidados prestados aos utentes. Para além depromover uma maior qualidade dos serviços prestados, a implementação eintegração deste tipo de sistemas em ambiente hospitalar, aumenta a segurança dos utentes e reduz os custos. No entanto, a adoção e aceitação desistemas de PCE na área médica não se tem revelado uma tarefa fácil. Umdos principais fatores apontados para o fracasso da adoção destes sistemasestá associado ao seu baixo nível de usabilidade.Dentro desta problemática, a presente dissertação pretende avaliar a usabilidadee a aceitação dos sistemas de informação em ambiente hospitalar quegarantem o registo eletrónico da informação relativa aos seus utentes. Paraalém disto, é realizada também uma avaliação global das funcionalidadesinerentes ao PCE implementado no Centro Hospitalar do Alto Ave (CHAA),com o intuito de alcançar um ambiente hospitalar livre de papel.",
    "authors": [
      "Novo, Ana Cristina Gomes Ribeiro"
    ],
    "keywords": [
      "614:681.3",
      "681.3:614",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "614:681.3",
      "681.3:614"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81298",
    "title": "E-anamnesis: a clinical observation electronic platform for emergency departments",
    "abstract": "One of the reasons for the increased number of visits to emergency departments is theprimary health care inability to handle urgent needs and provide all the health servicesneeded to assess complex conditions. A significant amount of these visits are due to theabnormal flow of patients whose clinical condition is of low severity and could ideally beresolved with self-care and primary health care.The crowding in emergency departments causes operational and logistical problems andhas undesirable consequences for patients, health professionals and hospitals. Delays intreatment interventions and increased mortality, medical errors and waiting times are just aphew examples of critical consequences that can occur, resulting in a significant barrier tothe quality of health care delivery.With the advances in technology, several institutions have found in self-service an alternativefor the patient’s collection of health information autonomously. These devices can be usedby low clinical severity patients (with the blue, green or yellow bracelets from Manchestertriage) to reduce waiting time in the emergency departments.This dissertation proposes a technological solution to improve both the time and qualityof the anamnesis procedure performed by medical staff in the emergency department. Theintroduction of a self-service kiosk in the emergency department waiting room will make itpossible to quickly and intuitively collect the patient’s past medical history, usual medication,main complaint symptoms and vital signs. Subsequently, this data will be made availableto the physician before each clinical observation. The hypothesis considered is that byproviding a selective, structured and uniform anamnesis information’s presentation of eachpatient, medical staff observation can proceed much faster and accurately, focusing on theconfirmation of the most relevant aspects. The primary purpose of this solution is to reducethe period of clinical observation and thus improve the response capacity of the emergencydepartment with the same resources.",
    "authors": [
      "Santos, Fátima Jacinta Pereira dos"
    ],
    "keywords": [
      "Primary Health Care",
      "Emergency department",
      "Crowding",
      "Health kiosk",
      "Self-Service Kios",
      "Cuidados de Saúde Primários",
      "Serviço de Urgência",
      "Lotação hospitalar",
      "Quiosque de saúde",
      "Quiosque de auto-atendimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82129",
    "title": "Characterizing and revealing biomarkers on patients with Cerebral Amyloid Angiopathy using artificial intelligence",
    "abstract": "Cerebral Amyloid Angiopathy is a cerebrovascular disorder resulting from the deposition of an amyloidogenic protein in small and medium sized cortical and leptomeningeal vessels. A primary cause of spontaneous intracerebral haemorrhages, it manifests predominantly in the elder population. Although CAA is a common neuropathological finding on itself, it is also known to frequently occur in conjunction with Alzheimer’s disease, being sometimes misdiagnosed. Currently, CAA diagnosis is generally conducted by post-mortem examination or, in live patients by the examination of an evacuated hematoma or brain biopsy samples, which are typically unavailable. Therefore, a reliable and non-invasive method for diagnosing CAA would facilitate the clinical decision making and accelerate the clinical intervention.The main goal of this dissertation is to study the application of Machine Learning (ML) to reveal possible biomarkers to aid the diagnosis and early medical intervention, and better understand the disease. Therefore, three scenarios were tested: Classification of four neurodegenerative diseases with annotation data obtained from visual rating scores, age and gender; Classification of the diseases with radiomic data derived from the patient’s MRI; and a combination of the previous experiments. The results show that the application of Artificial intelligence in the medical field brings advantages to support the physicians in the decision making process and, at some point, make a correct prediction of the disease label.Although the results are satisfactory, there are still improvements to be done. For instance, image segmentation of cerebral lesions or brain regions and additional clinical information of the patients would be of value.",
    "authors": [
      "Silva, Fátima Solange Lima Rezende da"
    ],
    "keywords": [
      "Machine learning",
      "CAA",
      "Medical imaging",
      "MRI",
      "Biomarkers",
      "Artificial intelligence",
      "AAC",
      "Imagiologia médica",
      "Imagem por ressonância magnética",
      "Biomarcadores",
      "Inteligência artificial",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86803",
    "title": "Emotion recognition: recognition of emotions through voice",
    "abstract": "As the years go by, the interaction between humans and machines seems to gain more and more importancefor many different reasons, whether it's taken into consideration personal or commercial use. On a timewhere technology is reaching many parts of our lives, it's important to keep thriving for a healthy progressand help not only to improve but also to maintain the benefits that everyone gets from it. This relationshipcan be tackled through many points, but here the focus will be on the mind.Emotions are still a mystery. The concept itself brings up serious questions because of its complex nature.Till the date, scientists still struggle to understand it, so it's crucial to pave the right path for the growth ontechnology on the aid of such topic. There is some consensus on a few indicators that provide importantinsights on mental state, like words used, facial expressions, voice.The context of this work is on the use of voice and, based on the field of Automatic Speech EmotionRecognition, it is proposed a full pipeline of work with a wide scope by resorting to sound capture andsignal processing software, to learning and classifying through algorithms belonging on the Semi SupervisedLearning paradigm and visualization techniques for interpretation of results. For the classification of thesamples,using a semi-supervised approach with Neural Networks represents an important setting to tryalleviating the dependency of human labelling of emotions, a task that has proven to be challenging and,in many cases, highly subjective, not to mention expensive. It is intended to rely mostly on empiric resultsmore than theoretical concepts due to the complexity of the human emotions concept and its inherentuncertainty, but never to disregard prior knowledge on the matter.",
    "authors": [
      "Andrade, Guilherme Marques"
    ],
    "keywords": [
      "Automatic speech emotion recognition",
      "Semi supervised learning",
      "Human emotion",
      "Unlabeled dataset",
      "Aprendizagem semi supervisionada",
      "Emoção humana",
      "Dados sem anotação",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/85581",
    "title": "Estudo sobre a importância dos princípios e padrões das arquiteturas orientadas a microsserviços",
    "abstract": "Com o atual crescimento do desenvolvimento de software e aumento da procura para solução de diversos problemas, começa-se a verificar a incapacidade de certas abordagens arquiteturais para lidarem com alguns dos desafios atuais. Efetivamente, alguns destes desafios estão relacionados com o aumento da adesão das pessoas à tecnologia, personalização das aplicações, implementação rápida de novas funcionalidades, crescimento e complexidade das aplicações, otimização da produtividade das equipas de desenvolvimento, adequação das melhores tecnologias, entre outros. Deste modo, as arquiteturas orientadas a microsserviços resolvem alguns destes problemas atuais. Este projeto de dissertação tem como objetivo estudar as arquiteturas orientadas a microsserviços, os seus princípios, padrões e testar a aplicabilidade dos mesmos a um caso de estudo do mundo real, um sistema E commerce, com a finalidade de resolver alguns dos problemas e desafios desta abordagem arquitetural. As categorias de padrões de arquiteturas orientadas a microsserviços estudadas são: Decomposição, Manutenção de Dados, Mensagens Transacionais, APIs Externas, Descoberta de Serviços e Segurança. Dos padrões estudados e implementados, os que tiveram resultados interessantes foram o Saga e CQRS, devido a resolverem problemas relacionados com a manutenção dos dados, que se torna complexa com a característica distribuída deste tipo de arquiteturas. A avaliação da aplicabilidade destes padrões ao caso de estudo, faz-seem comparação do desenho e implementação, com o estudo do estado de arte, através dos pontos positivos e negativos, bem como são efetuados testes à aplicação desenvolvida para conclusão de resultados. Por fim, foram efetuados testes de carga, para verificar a capacidade de escalabilidade, mas principalmente o impacto que as decisões arquiteturais têm na performance e disponibilidade das funcionalidades.",
    "authors": [
      "Pereira, José André Martins"
    ],
    "keywords": [
      "Arquiteturas de software",
      "CQRS",
      "Microsserviços",
      "Padrões de desenho",
      "Saga",
      "Software architecture",
      "Microservices",
      "Design patterns",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60009",
    "title": "Distribuição e sincronização de áudio digital em redes sem fios",
    "abstract": "Como em todas as áreas tecnológicas, também os utilizadores de sistemas áudio procuram soluções o mais práticas, convenientes e eficazes possível. As tecnologias sem ﬁos têm vindo, gradualmente, a substituir as soluções com ﬁos (ou cabladas) nos sistemas de distribuição de áudio, tanto, digital como analógico. O mercado está repleto de soluções que cumprem este propósito em variados graus de usabilidade e capacidades técnicas, sendo que o maior desaﬁo é colocado pelo problema da sincronização na distribuição individual e independente de canais áudio no formato digital. As soluções atuais são quase exclusivamente versões comerciais, utilizando algoritmos e tecnologias proprietárias relativamente complexas e, como tal, não gratuitas. Este aspeto tende a atrasar o desenvolvimento de novos sistemas. Esta dissertação documenta o projeto que teve como principal objetivo provar que é possível construir um sistema de distribuição independente e simultânea de canais áudio digitais, alcançando níveis ótimos de sincronização usando algoritmos e mecanismos alternativos sobre tecnologias bem conhecidas de rede local sem ﬁos. Assim, este relatório apresenta a investigação elaborada sobre tecnologias universais e gratuitas, que facilitem o desenvolvimento de um novo mecanismo de livre acesso para a distribuição digital de áudio sobre redes locais IEEE 802.11. Esse mecanismo e os algoritmos associados são discutidos em detalhe, sendo depois apresentados os passos da implementação e teste desse mecanismo num sistema protótipo. O protótipo desenvolvido oferece um nível excelente de sincronização dos canais áudio em todos os dispositivos reprodutores do sistema, mesmo quando sujeito a condições de stress numa rede sobrecarregada. A eﬁcácia do sistema foi medida analiticamente e não foi detetada nenhuma deterioração signiﬁcativa na sincronização dos canais áudio nos sistemas reprodutores ﬁnais. Também nos testes empíricos de audição os utilizadores não conseguiram detetar qualquer nível de dessincronização. Espera-se que a tecnologia desenvolvida, implementada e testada, permita o aparecimento futuro de novas soluções gratuitas, ou comercialmente mais competitivas, mas que poderão rivalizar com as soluções atuais em termos das capacidades técnicas de distribuição de áudio digital, sobre redes locais de utilização genérica e comum.",
    "authors": [
      "Gylytskyy, Oleksii"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/77512",
    "title": "Software development for monitoring and supporting intralogistics vehicles: From a laboratory prototype to an industrial prototype",
    "abstract": "Technological progress and advancements are constant and natural. The extreme impactand the daily use of everything related to technology on everyone’s day to day life is irrefutable.The inclusion of robots and automated vehicles and machines is increasing becauseof the advantages linked to their use.This reality led to the beginning of a project, Smart Autonomous Mobile Units (SAMU),included in the iFactory program which comes from a partnership between Bosch CarMultimedia (CM), and University of Minho. The project consists of the automation ofspecific industrial vehicles used in the movement of materials and finished products thatsupport the intralogistics processes.This dissertation presents the software process behind the transformation of a LaboratoryPrototype to an Industrial Prototype. A laboratory prototype was the result of the previousstages of development since this prototype was created and tested only in a controlledenvironment. For the next step of the SAMU project, the Proof of Concept, it is mandatoryto increase the maturity level of this prototype. The PoC will be performed on a realindustrial environment having a lot more adversities than the prior tests setting whereeverything was under control. On the Bosch Braga plant the autonomous vehicles have tointeract with manually-driven vehicles as well. For this reason, the necessity of creatinga system that could coordinate the allocation of logistic services to each vehicle had toemerge.A new solution was structured and developed following the basics of software designand architecture to modify and complement the former system with all the functions it isrequired to have. The final solution for the central platform of the industrial prototype iscomposed of seven microservices and each of them have a different purpose and businesslogic behind. The improvement of the central platform led to the development of newfunctionalities, such as the organization of the vehicles’ fleet and the visualization of thevehicles dynamically moving across their area of operation.The created solution was proven, by the results, to be functioning properly and as intended.The use of microservices reinforces their advantages for this specific context, sincethe deployment becomes easier and their maintenance is more straightforward. Anotherimportant aspect is that the addition of new functionalities is also a more effortless process.Considering that the previous solution was being developed with microservices the preservationof this type of architecture was the right answer for the continuation of development.",
    "authors": [
      "Fernandes, Ana Esmeralda Alves"
    ],
    "keywords": [
      "prototype",
      "industry",
      "autonomous vehicles",
      "user interfaces",
      "software development",
      "software design",
      "software architecture",
      "microservices",
      "logistics",
      "protótipo",
      "indústria",
      "veículos autónomos",
      "interfaces gráficos",
      "desenvolvimento de software",
      "design de software",
      "arquitetura de software",
      "microsserviços, logística"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27856",
    "title": "Mobile application to support pediatric medical practice",
    "abstract": "The objective of this project is to develop a mobile application in order to aid pediatricians performing theirwork. The necessity of this application was initially identified by a pediatrician working in Santo AntónioHospital of Oporto, after also verifying the interest of some of his coworkers. The bibliography also statessome situations where mobile applications may be helpful, such as: errors in the administration of drugs orthe difficulty pediatricians face in performing needed mathematical operations.It is made a review of pediatric applications, mobile mostly, in order to know what kinds of applications arealready available for pediatricians. It is presented the analysis of 5 distinct applications, from medicalcalculators for emergency situations to decision support systems that given a set of clinical characteristicsit is provided a list of diagnosis to consider.Following it is done a study of requirements elicitation and prioritization. Its objective is to know thetechniques and tools already studied in the bibliography, as well as to identify the most appropriate onesfor this project. Several elicitation and prioritization techniques were used in this project. It is also used atool to register the requirements.In order to develop a mobile application that may run on the majority of smartphones in the market, it ismade an analysis of the smartphone operating systems market share, as well as of market shareprojections for the next few years. After identifying the target operating systems for the app it is made astudy of the mobile cross-platform development frameworks. The framework choice considered the elicitedrequirements and the operating systems with the greatest market share.After a learning period of the involved technologies, the pediatric app is developed using the gatheredrequirements and following the results of the requirements prioritization. The development of theapplication was always followed by a pediatrician, and as a result the application was tested and refinedduring that time. Finally, the application is released as well as a questionnaire to evaluate it.",
    "authors": [
      "Oliveira, Rui Emanuel Barros"
    ],
    "keywords": [
      "681.324",
      "616-053.2"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.324",
      "616-053.2"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92720",
    "title": "Geração de imagens de faces com alta resolução utilizando variational autoencoders",
    "abstract": "Atualmente, a Inteligência Artificial, e em especial a subárea da aprendizagem automática eprofunda, é alvo de enorme interesse por parte das comunidades académica e empresarial.Das áreas aplicacionais da aprendizagem automática, a par com o processamento de línguanatural, a visão por computador é aquela que tem suscitado mais interesse e gerado maisresultados científicos. A geração de imagens é um dos problemas que se enquadra na visãopor computador.Com esta dissertação pretende-se estudar os modelos geradores de imagens, e de entreas alternativas para atacar este problema, o foco do trabalho são os modelos VariationalAutoencoders. Na fase inicial da dissertação é feito um levantamento bibliográfico do estadoda arte do tema do trabalho, visando adquirir os conhecimentos necessários para concretizara parte experimental.Os conhecimentos adquiridos na fase de levantamento bibliográfico foram aplicados nafase seguinte, onde se desenvolveu, treinou e avaliou modelos capazes de gerar imagensnovas com faces humanas. O foco foi a arquitetura Vector Quantized Variational Autoencoder(VQ-VAE), auxiliada por um modelo autorregressivo PixelCNN. No entanto, foram tambémexplorados outros modelos geradores, tendo em mente complementar o estudo em causa, econsequentemente, poder tirar conclusões mais abrangentes.Após a implementação dos modelos, foi possível concluir que dentro de todos os modelostestados o VQ-VAE apresentou o melhor desempenho, quer seja a nível qualitativo atravésda inspeção visual das faces geradas, quer seja a nível quantitativo com a aplicação damétrica Frechet Inception Distance. Além do VQ-VAE, o outro modelo que se destacou foi oVector Quantized Generative Adversarial Network, comprovando assim o potencial da aplicaçãoda quantização de vetores nos modelos geradores.",
    "authors": [
      "Gonçalves, Tiago Rico"
    ],
    "keywords": [
      "Modelo gerador de imagens",
      "Variational autoencoder",
      "VQ-VAE",
      "PixelCNN",
      "VQ-GAN",
      "Image generation model",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84350",
    "title": "Real-time rendering of particle based fluid simulations",
    "abstract": "Particle based methods have been been increasing in popularity in fluid simulations as of late due to the increasesseen in computational power in the form of various many core devices like graphics-cards (GPUs) or high-corecount CPUs.The increasing popularity of particle based fluid simulations resulted in a need for higher fidelity renders whichportray the particle data in a cohesive substance rather than isolated particles. Rendering the particle data inreal-time however is a difficult problem due to the fact that the simulations tend to be computationally intensiveand, as a result, the rendering has to deal with computational constraints that make it difficult to achieve visualquality while, at the same time, keeping the interactive aspect of the simulation.Currently there are various approaches to rendering particle data which thread a balance between visual qual-ity and performance. These techniques can vary widely by having entirely different algorithms/approaches andhaving different objectives, some of them focusing on having the best looking surfaces, while others pretendingto achieve the shortest render times, or even trying to have a low memory footprint.Computational constraints have kept certain techniques from being usable in the interactive realm. This con-straints have, however, been loosening up due to hardware advances which has, in turn, been lowering the gapbetween high-quality renders and interactive rendering and thus opening the door to new rendering approaches.This thesis aims to explore the implementation of real-time fluid rendering in conjunction with a preexistingparticle-based simulation. The thesis will be divided in two halves with the first half covering a screen-spaceimplementation, a family of techniques which aims to render the fluid surface while having a low computationalcost, while the second half explores a volumetric render implementation based on a voxel grid while leveragingthe computational power of a modern GPU.The screen-space implementation is able to represent the frontal surface of the fluid, and also implements theextraction of a back surface. This enables it to enhance its visual fidelity at a low computational cost. This workalso compares different approaches used in the process of smoothing these surfaces, which enable a highersurface cohesion while tackling the preservation of the fluid’s edges.The volumetric implementation uses a voxel grid to represent the fluid, enabling it to render multiple refractionsthus achieving a more realistic render. The volumetric implementations is also able to represent occluded fluidfeatures such as air bubbles by leveraging the GPU memory, and a more accurate colouring of the fluid.",
    "authors": [
      "Fernandes, José Carlos Pereira"
    ],
    "keywords": [
      "Voxel",
      "Screen-space",
      "Smoothing",
      "Reflection",
      "Refraction",
      "Ray-casting",
      "Fluid",
      "Particles",
      "Suavização",
      "Reflexões",
      "Refrações",
      "Fluidos",
      "Partículas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79361",
    "title": "Enhance Smart Home capabilities by learning from usage patterns and IoT devices",
    "abstract": "Domotics represent a field of consumer electronics related primarily to home automation.Although smart environments have existed for decades and even for longer in the imaginaryof people and sci-fi, the new age of Internet of Things (IoT) and the low-cost Do ItYourself (DIY) electronics/maker market has brought smart homes and the understandingof domotics closer to everyone.In recent years, the interest in the fields of domotics and IoT has increased. This recent academicand industrial interest has contributed to the evolution of the Smart Home concept,being more and more appealing to our civilization. This has prompted new commercial solutionson the market, which are discovering ways to expand and increase their own valueby integrating new features, especially from the IoT market.The proliferation of IoT devices leads to new sources of information. In addition, the relationbetween the environment occupant and the environment is responsible for addingvalue to this data. Afterwards, the data can then be used in a meaningful way, by machinelearning algorithms to learn usage patterns. Furthermore, this data may or may beunrelated to the data incoming from sensors.This Masters Project will focus on learning strategies to allow Smart Homes to becomeintelligent, in a sense that they anticipate needs and actions, thus enabling extended featuresto the home automation system. The user should be able to give feedback on his ”feeling”regarding the decision making and suggestions, possible by our system implementation.",
    "authors": [
      "Mirra, José Alexandre da Silva"
    ],
    "keywords": [
      "Internet of Things",
      "Machine Learning",
      "Data Mining",
      "Domotics",
      "Smart Home",
      "Smart Environments",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84362",
    "title": "Automated driving: an approach to automated guided vehicles in a controlled environment",
    "abstract": "The growing need for process optimisation is one of the main drivers of technologicalmodernisation in companies. Process automation has been one of the main initiatives inthis modernisation task. In many scenarios, the movement of any kind of vehicles or othermoving objects requires human hands.In this context, this Master’s Thesis, included in the fifth year of the Integrated Masterin Informatics Engineering in the University of Minho, reports the work developed in abusiness context with the help of Deloitte Technologys S.A. that welcomed the student in acurricular internship program. The aim is to develop an implementation of of a simulatorof automated guided vehicles in a controlled environment, which can reveal itself as aninitiative for the modernisation of the logistic process of moving mobile objects or vehicles.In order to develop the Master’s Project, a work methodology was outlined. This methodology is supported on four crucial stages to reach the proposed objectives: documentary,proposal, pre-development and development. The documentary analysis was made todevelop a better understanding of the context and to study other developed projects andtools that could be necessary for the proposal and development. In the solution proposal westarted by dividing the problem in several components, in a kind of \"divide and conquer\"method. During this process, a conceptual map was developed, which will be analysed inthe document and will guide us to the first Conceptual Architecture Diagram. The next stepis through the analysis of the Conceptual Architecture Diagram to study which modules willbe out of the scope of what is necessary for the full functioning of the Master project, takingcertain components as assumptions and substituting some modules by other solutions, sothe development can be faster however in a way to not neglect the required functionality forthe full operation of the solution. After these steps, the development of the solution willfollow, putting into practice all the knowledge acquired in the previous stages.The Master’s Project culminated in five micro-services: Vehicle, Controller, Map, Vehicle/Sessions Micro-Service and Alert; and a web application that allows the user to checkwith a GUI data about vehicles, sessions, alerts and the map of the controlled environment.These micro-services and the web application working simultaneously allow having a vehiclesimulator automatically driven in a controlled environment.",
    "authors": [
      "Lopes, Luís Francisco Mendes"
    ],
    "keywords": [
      "Automated guided vehicles",
      "Automated driving",
      "Path calculation",
      "Controlled environment",
      "Impact prediction",
      "Veículos automaticamente conduzidos",
      "Condução automatizada",
      "Cálculo de rota",
      "Ambiente controlado",
      "Previsão de impacto",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80563",
    "title": "Application of semantic segmentation through data acquired from sensors",
    "abstract": "Today, AI is very important in our lives as its used all around us without our knowledge. From simple things such as personal assistants like Alexa and Siri, and advertising algorithms focusing on our tastes -Netflix on the recommendation of movies or, even more common, the presentation of advertising basedon our search history -, to robots and to smart houses, cities or even vehicles. The presence of AI isincreasing and even if we are still far away from our ’General AI’ ideology, a machine capable of anythingautonomously, each day we get closer.In the last decade multiple applications of AI have been through breakthroughs. For example, the firstimplementations of autonomous vehicles were introduced by Tesla and other companies. A number ofdiscoveries must have been made to achieve this revolution of AI performance and, among them, is two ofthe most important developments: Object Detection and Semantic Segmentation, closely related to eachother. These are responsible for understanding the environment so the machine can take actions, beingthe latter an improvement of the first in terms of sensibility error associated to each entity detected as wellas being able to detect its corresponding type, in a pixel level. These machines require more and moredata to analyse, having many types of sensors in order to collect information, such as radars, cameras,LiDAR, among others.This work falls in the study of the use of Semantic Segmentation techniques and its application oncategorising data from image related sensors in order to explain its breakthroughs and challenges, as wellas improving and overcoming such obstacles. Data will consist mainly of scans from outdoor/self-drivingcars POV (KITTI360) with the ability to be used with other types of data such as indoor scans (COCO), toexplain both road and more day-to-day images semantic compositions, applied on a state-of-art solution.Consecutively we will perform a process of optimisation in order to reduce computation costs. Currentlythe works of DeepLab (with the research of deeplabv3[1]) have achieved a high success on SemanticSegmentation overcoming previous problems such as handling component boundaries with more refinedlines while keeping it fairly easy to run on more less powerful machines, being the start point for this work.",
    "authors": [
      "Monteiro, Filipe Pimenta Oliveira"
    ],
    "keywords": [
      "Image segmentation",
      "Semantic segmentation",
      "Deep learning",
      "Self-driving",
      "Automotive security",
      "Segmentação semântica de imagens",
      "Veículos autónomos",
      "Redes neuronais convolucionais",
      "Segurança automóvel",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/65343",
    "title": "The influence of emotions on human computer interaction",
    "abstract": "Emotion is an essential part of what means to be human, but it is still disregarded bymost technical fields as something not to be taken into account in scientific or engineeringprojects. However, the understanding of emotion as an aspect of decision making processesand of modelling of human behaviour is essential in order to create a better connection betweenhumans and their tools and machines. This dissertation focuses on the measurementof emotion of users through the use of non-intrusive methods, like measuring inputs andreactions to stimuli, along with the creation of a tool that measures the emotional changescaused by visual output created by the tool itself. Usage of the tool in a test environmentand the subsequent analysis of the data obtained will allow for conclusions about the effectivenessof the method, and if it is possible to apply it to future studies on human emotionsby investigators in the fields of psychology and computation.",
    "authors": [
      "Teixeira, André Francisco Soares Carvalho Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56114",
    "title": "HLA Binding Intelligence (HABIT): an integrated web-server for generation and advanced interpretation of peptide-HLA binding predictions",
    "abstract": "Human T cells are essential in the control of pathogen infections, cancer and autoimmunediseases. Immune responses mediated by T cells are aimed at specific peptides, designatedT cell epitopes, that are recognized when bound to Human leukocyte antigen (HLA)molecules. The HLA genes are remarkably polymorphic in the human population drivinga broad and fine-tuned capacity of their encoded proteins to bind a wide array of peptidesequences. Amino acid variants in epitopes might impact the HLA-peptide interaction andconsequently the level and type of generated T cell responses. Having the tools to effectivelypredict and measure the impact of amino acid variants in HLA binding will be ofgreat value for the future of personalized host directed therapies.Multiple algorithms based on Machine learning (ML) have been implemented to estimateHLA-peptide binding. However, there is still no tool capable of performing integratedanalyses, namely comparing wild type and mutant sequences by predicting all overlappingpeptides including amino acid positions of interest. This requires that researchers haveprogramming skills to analyse prediction data for HLA peptide binding and to extractpotentially meaningful conclusions from that data.The main objective of this thesis was to design and implement a web server, called HLABinding Intelligence (HABIT), that automates HLA binding prediction and advanced interpretationof the impact of peptide variants in inducing adaptive immune responses. HABITintegrates the best overall predictors for peptide-HLA binding (class I and II), NetMHCpanand NetMHCIIpan, respectively. The application features an intuitive and user-friendly interfaceavailable online for academic users, including comparative studies of wild type andmutant sequences, statistical tests and calculations of human population coverage. The performanceof the web server was tested with a case study representative of tuberculosis, theR233L mutation found in the acetyl-/propionyl-CoA carboxylase beta chain (AccD2) protein. Theresults obtained through the interactive graphical interface allowed the automated identificationof the most promising peptide sequences to be used in the design of T cell-mediatedimmunotherapy approaches.Overall, HABIT provides a quick and easy way to answer major questions of rationalimmunotherapy studies, namely “What is the influence of an amino acid variants on HLAbinding?”, “Do wild type and mutant epitopes show statistically significant differences inpredicted HLA binding properties?” and “What is the predicted population coverage of theepitopes?”. Thus, HABIT constitutes a promising and reliable advance in the discovery ofmolecular determinants that influence the variation in T cell mediated immune responses.",
    "authors": [
      "Martins, Joana dos Santos"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28357",
    "title": "Digital signage : aplicação baseada em eventos usando API Google Calendar",
    "abstract": "O uso de painéis de digital signage ou displays (ou, em português, painéis digitais) está cada vez mais a ser implementado nos locais públicos e semipúblicos, uma vez que é uma forma actual de apresentar, de um modo dinâmico, informação, publicidade e conteúdos de entretenimento. Desta forma, as pessoas expostas a esta tecnologia passam a ter um acesso mais rápido e actualizado à informação sobre tudo o que as rodeia.Os ecrãs dos displays têm evoluído no sentido da melhoria da sua qualidade. Além disso, a descida dos preços torna esta tecnologia bastante mais acessível para ser aplicada em vários sectores, nomeadamente na indústria, no comércio, na educação, na saúde, etc., substituindo os meios tradicionais publicitários e informativos, baseados em papel.Uma rede de painéis digitais permite a actualização dos seus conteúdos de forma remota, com base num servidor central que controla toda a informação apresentada na rede, enquanto que nos meios tradicionais a actualização de conteúdos é muito mais cara, demorada e de difícil gestão.Como as pessoas olham pouco tempo para os painéis digitais, exige-se um esforço muito grande no estudo do local onde estes são instalados e na modelação dos conteúdos a serem apresentados, de forma a que o ambiente se torne o mais atractivo possível e, deste modo, se promova a atenção de quem passa pelos painéis. Também existem displays que usam tecnologias mais sofisticadas, permitindo a adaptação automática dos conteúdos apresentados em função do contexto envolvente, fazendo com que a informação seja mais direccionada ao público, sem a necessidade de controlo humano.Dado o crescente sucesso, à escala planetária, da digital signage, têm surgindo cada vez mais soluções de software aplicadas nesta vertente. Por conseguinte, o tema principal desta dissertação incidirá sobre o desenvolvimento de uma aplicação web para painéis de digital signage, sugerida pela empresa Ubisign, com o objectivo de permitir configurar visualizações com informação sobre eventos provenientes do Google Calendar.Uma vez que a promoção de eventos geralmente exige custos elevados de design e produção, é necessário minimizá-los no contexto das redes de digital signage. Para tal, existem determinadas ferramentas on-line de gestão de eventos, como o Google Calendar, com a função de permitir que o utilizador especifique eventos que vão ocorrer e efectue o seu escalonamento com base em calendários. Do ponto de vista de um developer, estas ferramentas evitam a necessidade de implementar outros softwares de gestão de eventos, permitindo também desenvolver outras aplicações que comuniquem com estas ferramentas, através de APIs apropriadas e bem documentadas.Para tirar partido disto, a aplicação a desenvolver terá de recorrer à API Google Calendar para disponibilizar, de forma personalizada, informação sobre eventos que estejam planeados para um dado sítio com uma rede de digital signage instalada e, para esse efeito, terá de ser integrada no serviço Ubisign.com.",
    "authors": [
      "Branco, Pedro Miguel Pereira Tomé"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23504",
    "title": "Extração de conhecimento nas listas de espera para consulta e cirurgia",
    "abstract": "O desenvolvimento industrial, a constante inovação e a necessidade de melhoriacontínua por parte das organizações originou um crescimento exponencial do volumede dados armazenados existindo, por isso, uma maior quantidade de informação relacionadacom cada instituição. Cada vez mais as instituições dependem do tipo de dadosque armazenam e acumulam.Atualmente, uma organização tem de fazer uma gestão eficiente das suas bases dedados de modo a extrair o máximo de conhecimento possível para apoiar no processode tomada de decisão e assim garantir competitividade no mundo dos negócios e dosmercados. A necessidade de implementação de um sistema de apoio à decisão no seiode uma instituição fez emergir o conceito de Business Intelligence: processo responsávelpela transformação dos dados em informação útil e organizada, e a subsequenteconversão dessa informação em conhecimento valioso para a tomada de decisão.A área da saúde é bastante susceptível, tanto a nível clínico como administrativo, àqualidade e rapidez das decisões tomadas, uma vez que estas decisões colocam sempreem causa a vida humana. Assim, é de extrema importância a utilização de sistemas deapoio à decisão nas unidades de saúde.O propósito deste projeto prende-se, essencialmente, com a exploração e aplicaçãode uma ferramenta de BI aplicada no contexto da saúde. Por outro lado, pretende-setambém avaliar a aplicabilidade de uma ferramenta open source em sistemas complexose integrados como os das instituições hospitalares. Neste sentido, a ferramentaexplorada e avaliada foi o Pentaho. Foi realizada uma monitorização e simulação dosdados clínicos relativos às listas de espera para consulta e cirurgia e à ocupação dassalas do bloco operatório de um hospital no norte de Portugal.Verificou-se que o Pentaho, enquanto ferramenta open source, é inteiramente capazde ser implementada e integrada numa instituição hospitalar, com a potencialidadede uma ferramenta proprietária. Sendo assim conclui-se que o Pentaho é uma ferramentade BI bastante eficiente, capaz de apresentar soluções válidas e atrativas para aresolução de problemas e o para suporte à tomada de decisões.",
    "authors": [
      "Oliveira, Olívia Raquel Ferreira"
    ],
    "keywords": [
      "614:681.3",
      "681.3:614"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "614:681.3",
      "681.3:614"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83043",
    "title": "Construção de uma plataforma de e-commerce: uma abordagem baseada numa arquitetura de microsserviços",
    "abstract": "As arquiteturas monolíticas estão, em grande parte, presentes na maioria das plataformas de e-commerce, o queleva a um processo de modificação mais complicado e entregas demoradas ao cliente, uma vez que não estápreparada para trabalho em paralelo.A arquitetura de microsserviços veio proporcionar outra forma de desenvolvimento destas plataformas, permitindoo trabalho em simultâneo por diferentes equipas, produzindo novas entregas para o cliente de forma maisacelerada e segura. Todavia, esta possui alguns desafios e complexidades, o que leva muitas vezes à escolha deuma arquitetura monolítica para o desenvolvimento da aplicação.A maioria das aplicações não são imutáveis, pois mesmo estando entregues ao cliente são sujeitas amodificações. Esta necessidade de modificar a aplicação leva a preocupações acerca da rapidez com que asnovas funcionalidades são entregues ao cliente. É preciso tomar decisões no início do desenvolvimento sobreque arquitetura seguir, de modo a tomar a decisão mais vantajosa. No caso de aplicações monolíticas a mudançapara uma arquitetura de microsserviços facilita este aspeto, bem como muitos outros. Contudo, esta separaçãopode-se tornar quase impossível se o monolítico não for bem preparado para uma eventual futura mudança.Uma das maiores dificuldades numa migração de um monolítico para microsserviços, relaciona-se coma definição do que deve ser cada microsserviço e na comunicação entre estes. A migração deve partir daidentificação de partes do código que possam ser isoladas sem ter muito impacto no resto do código. Com odesenho de um diagrama de packages é possível obter uma visão sobre a estrutura do sistema, percebendo quecomponentes são mais fáceis e mais difíceis de extrair. Deve-se começar por extrair aqueles que contém menosdependências, adquirindo as vantagens de uma migração incremental que permite que sejam reduzidos os errosefetuados, porém, pode haver situações em que se queira extrair um componente com mais dependências.É necessário compreender o porquê da migração para uma arquitetura de microsserviços. Esta decisãonão deve ser tomada apenas porque a arquitetura de microsserviços está em voga, mas sim por razõesfundamentadas. Dentro destas razões encontra-se a rapidez com que as mudanças são efetuadas e colocadasem produção, pois é mais fácil realizar modificações e voltar a instalar os microsserviços sem que toda a aplicaçãotenha que reiniciar. Isto permite uma melhor estruturação da equipa, possibilitando que várias equipas possamtrabalhar em simultâneo para a mesma aplicação, não prejudicando em nada outros microsserviço. Outra razãoé a necessidade de escalar os microsserviços independentemente, providenciando maior robustez, pois a falhade um serviço não leva à falha de toda a aplicação ou então pela escolha de tecnologia, podendo-se implementaros microsserviço com a tecnologia que seja mais adequada e eficiente.",
    "authors": [
      "Correia, Luís Rafael Barbosa"
    ],
    "keywords": [
      "Arquitetura de microsserviços",
      "Arquitetura monolítica",
      "Arquiteturas de software",
      "Padrões de microsserviços",
      "Microservice architecture",
      "Microservice patterns",
      "Monolithic architecture",
      "Software architectures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82797",
    "title": "Aprendizagem automática aplicada à deteção de vulnerabilidades",
    "abstract": "The HTTP protocol is a stateless protocol, that means, each request made by the useris an independent request, there is no notion of state. So, to add it to the applicationswe need an additional tool to implement this notion of state. For this, cookies are used,allowing the websites to identify the authenticated users. A cookie is a file stored in thecustomer’s browser and sent together with HTTP requests, allowing the website to recognizethe customer and send a response corresponding to the request made.This dissertation aims to strengthen the protection of data associated with authenticationsessions through the identification and analysis of authentication cookies using machinelearning techniques. If web applications are vulnerable to malicious attacks, such as BrokenAuthentication or XSS (Cross-Site Scripting), attackers can gain access to the informationstored in the cookie. Using this information they can steal the user’s session, being able toauthenticate themselves in the web application to obtain access to data/services.Using machine learning techniques, we can identify within a set composed of severaltypes of cookies, which cookies are associated with authentication. The objective is therecognition of this type of cookies, since this is the one that needs greater security, takingcare in case the attacker even gaining access to this file, there is no possibility of decipheringthe information that puts the users session at risk.In addition to the classification of cookies, the detection and analysis of the encoding usedwill be carried out. The tool will then be integrated into the security testing software, BurpSuite, working as an extension in order to facilitate and reduce the time necessary for a QAanalyst to spend checking cookies.",
    "authors": [
      "Ribeiro, Paulo Filipe Silva"
    ],
    "keywords": [
      "Authentication",
      "Burp suite",
      "Cookie detection",
      "Machine Learning",
      "Security",
      "Autenticação",
      "Deteção de cookies",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59828",
    "title": "Community based repository for georeferenced traffic signs",
    "abstract": "In traffic environments, road signs have a key role to control, warn, and command or prohibitthe driver of certain actions. Traffic sign maintenance is essential to prevent negativeevents. In order for these traffic signs to play the role they were designed for, periodic onsiteinspections are essential and followed out to determine if signs are in good conditionand visible, both during the day and night. However, periodic inspections are time and costconsuming.Another issue is related to the drivers’ awareness to the traffic signs on the road. Manyfactors, both internal and external to the driver, may potentially contribute to him missing asign. Given the purpose of this dissertation, we will focus primarily on the external factorssuch as the sign being damaged or occluded, or distractions caused by the many gadgetsinside the vehicle. Due to all these extraneous influences, a traffic sign recognition systemmay help the driver to respect these signs and increase significantly their safety, as well asthe others around them.Some high-end vehicles already have such a warning system, at least for danger signs.However, drivers with these vehicles represent a small fraction of the total driving force.This dissertation aims at bringing such a system to a much broader audience.Smartphones are one of the most used devices by society today, mostly due to the manyfunctionalities they provide in day to day life and their relative accessible monetary value.The increased computational power and cameras’ quality improvement of these devicesover the years make them good candidates to support the access to this kind of technologyto all. In other words, smartphones of this day and age have the necessary resources to beused as instruments for sign recognition.Hence, we propose a dual purpose community based approach. On the one hand, eachdriver can use his mobile device to detect, recognize and geolocate traffic signs, contributingto the traffic sign central repository. Detection is performed using Cascade Classifiers,while a Convolutional Neural Network supports the recognition phase. The repository,based on the information received from the clients, can be used to provide sign statusreports and to enable more direct and timely inspection instead of relying on prescheduledglobal inspections. On the other hand, drivers would have access to the database of trafficsigns, therefore being able to receive real-time notifications regarding traffic signs such asspeed limit signs, school proximity, or road construction signs. Hence, allowing the systemto perform its function even if the recognition phase is not active when used in a lowcomputational power device.",
    "authors": [
      "Novais, Hélder Manuel Pereira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/77351",
    "title": "Persistência dinâmica de dados para soluções de business intelligence",
    "abstract": "Existem hoje em dia diversas soluções de Business Intelligence no mercado que permitem aanálise de informação de forma intuitiva, permitindo o acesso a utilizadores de negócio dasmais diversas áreas. Estas soluções vieram assim tornar o processo de análise de informaçãoágil, permitindo que esteja presente em mais processos de negócio, executados por utilizadoresnão especialistas. Estes processos de negócio exigem por vezes a manipulação de dadosmanualmente. A tarefa de manipulação manual de dados provenientes de diversas fontesé um processo complexo, onde há a necessidade de proceder à implementação de queries,pipelines customizados, entre outras tarefas específicas. Estas tarefas estão assim fora doalcance de profissionais sem conhecimento técnico de programação e desenvolvimentode software. Existe assim a necessidade de construção de uma ferramenta que permita ainteração e manipulação de dados sem conhecimento técnico que seja transversal a diferentescontextos com a possibilidade destes também terem diferenças ao nível de tecnologias debase de dados.O Tableau, ferramenta de visualização e interação com dados provenientes de diversasfontes, anunciou que iria disponibilizar uma api para aceder e interagir com a informaçãopresente nos dashboards, possibilitando assim o desenvolvimento de software por terceiros.Por conseguinte, surgiu então a oportunidade de criar um produto que no formato deplugin tivesse como objetivo preencher a lacuna de manipular e interagir com os dadospresentes numa base de dados sem estar constrangido a um cenário ou tecnologia específica.Desta maneira conseguimos expandir o conjunto de ações disponíveis aos utilizadores paraalém das atuais que estão de momento restritas apenas à visualização e interação com dadosprovenientes de diversas fontes, sem qualquer possibilidade de alteração e preservação denova informação.Este documento relata o plano de trabalhos sobre a manipulação de dados e estruturas, emmúltiplos contextos com a possibilidade de alteração de tecnologia de base de dados. Tendoisto em conta, está estipulado o desenvolvimento de um produto de um motor dinâmico demanipulação de queries dinâmico, que permita a interação por parte dos utilizadores comdiferentes contextos e que secundariamente seja de fácil integração em diferentes ferramentasde visualização através de interfaces gráficas.",
    "authors": [
      "Luís, Marcos de Morais"
    ],
    "keywords": [
      "Business intelligence",
      "Tableau",
      "Manipulação de dados",
      "Manipulação de estruturas",
      "Interação com dados",
      "Plugin",
      "Data manipulation",
      "Structures manipulation",
      "Data interaction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84563",
    "title": "Arquitetura de micro-serviços e o caso da Framework Lithium da PRIMAVERA BSS",
    "abstract": "A PRIMAVERA BSS é uma empresa tecnológica portuguesa ao serviço da gestão empresarial. Empresapioneira, em Portugal, a desenvolver soluções de gestão para Windows. A nível tecnológico, a PRIMAVERABSS disponibiliza um vasto leque de serviços, nomeadamente softwares de gestão.O desenvolvimento dos micro-serviços na empresa assenta numa framework proprietária — a frameworkLithium. Esta assegura uma arquitetura comum e padrões de desenho aplicando práticas deModel-driven Development (MDD).Os micro-serviços são cada vez mais uma arquitetura muito utilizada na indústria. Grandes empresas,adotaram esta arquitetura e muitas outras seguem a tendência, ao migrar as suas aplicações para estaarquitetura. Contudo, existe ainda uma dificuldade em construir um sistema neste estilo muito devidoà falta de informação ou conhecimento acerca dos padrões disponíveis, e por isso mesmo, este estiloarquitetural necessita de ser amplamente estudado.Posto isto, um dos objetivos desta dissertação é suprir esta lacuna, através do estudo de elementosimportantes, que devem ser considerados durante o desenvolvimento de aplicações/sistemas baseadosna arquitetura de micro-serviços. Outro objetivo passa por estudar a framework acima referida bem comorespetivas alternativas.Em suma, para além de se ter procedido à pesquisa e estudo de arquiteturas concorrentes, bem comode frameworks alternativas à Lithium, também foi desenvolvida uma aplicação baseada na arquitetura dosmicro-serviços.",
    "authors": [
      "Gonçalves, Fábio Daniel de Sá"
    ],
    "keywords": [
      "Arquiteturas de software",
      "Micro-serviços",
      "Open banking services",
      "Padrões",
      "Frameworks",
      "Lithium",
      "Software architectures",
      "Microservices",
      "Patterns",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83003",
    "title": "Incorporação de um sistema analítico numa plataforma computacional de avaliação",
    "abstract": "It is now possible to prove that technology has proven to be a strong ally in the mostdiverse areas, from economics and management to health or banking. Education is therefore no exception. The insertion of technology and software that provide students witheducational and motivational support for their learning has been a major challenge. It isbased on this lack of support that a project called “Leonardo” emerged at the Universityof Minho. This project considers the development of educational software, which aimsto provide students with a supervised learning method that will allow students to improve their knowledge and also observe their results. All of this will allow you to drawlessons that support your progress in a given area of study. This dissertation proposes,therefore, the development of a data warehousing system, which allows the collectionof information about users and their interactions with the referred system. In this way,it will be possible to obtain a multidimensional data analysis platform, which allows tomonitor the student’s state of knowledge over time, thus allowing him / her to evaluatehis / her progress, in a given field of study, during the interaction with a student. the system.",
    "authors": [
      "Oliveira, Bárbara Nadine Freitas"
    ],
    "keywords": [
      "Educational software",
      "Educational systems",
      "Data warehousing",
      "Multidimensional Data Systems",
      "Data analysis",
      "Data visualization",
      "Dashboarding",
      "Software educativo",
      "Sistemas educacionais",
      "Sistemas multidimensionais de dados",
      "Análise de dados",
      "Visualização de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92791",
    "title": "Integração de soluções Cloud Oracle Retail com uma plataforma de integração low code",
    "abstract": "Nesta dissertação, é explorada a interseção vital entre tecnologia, integração de sistemas e necessidades em constante evolução no setor do retalho. O estudo começou com uma análise aprofundada dos requisitos tecnológicos e operacionais do retalho moderno, destacando-se a importância crítica da integração eficiente de dados para acompanhar as expectativas dos clientes.Os objetivos centrais deste trabalho foram investigar, identificar e implementar uma framework LowCode capaz de integrar soluções Oracle Retail Cloud com aplicações externas, com o intuito de simplificar, acelerar e reduzir os custos associados a esse processo. Para alcançar esses objetivos, a pesquisa envolveu uma revisão exaustiva das tecnologias existentes, a seleção criteriosa das ferramentas de mercado mais adequadas e a análise cuidadosa das estratégias de segurança relacionadas com integração em cloud. A conclusão bem-sucedida deste trabalho culminou com a evolução do Retail Consult Integration Broker para atender aos requisitos de computação em nuvem e de baixo código, uma solução adaptável e robusta que atendeu às metas definidas. Durante o processo de desenvolvimento, ajustes foram feitos para garantir uma solução ainda mais refinada. Esta framework não apenas cumpriu os objetivos iniciais, mas também representou uma contribuição significativa para a modernização do setor do retalho. Além de fornecer uma solução técnica, esta dissertação ressalta a importância estratégica de adotar abordagens Low Code e de computação em nuvem no contexto das operações retalhistas. A combinação dessas tecnologias oferece uma resposta eficaz às constantes mudanças do mercado, proporcionando às empresas a flexibilidade necessária para inovar, adaptar-se rapidamente e atender às expectativas dos clientes num ambiente tecnológico em constante evolução.",
    "authors": [
      "Gomes, José Miguel Ferreira"
    ],
    "keywords": [
      "Oracle Retail",
      "Integração Low Code",
      "Adaptadores spring",
      "Computação em nuvem",
      "Transformação digital",
      "Low Code Integration",
      "Spring adapters",
      "Cloud computing",
      "Digital transformation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27835",
    "title": "Melhorar a usabilidade de aplicações web : mais resultados com menos esforço",
    "abstract": "A contratação eletrónica é apresentada como uma obrigação a ser cumprida pelos diversos estados, no contexto da união europeia. Até ao ano 2016, todas as contratações públicas deverão ser executadas com total transparência, segurança e fiabilidade, através da utilização de plataformas web de contratação devidamente certificadas. A VORTALnext> posiciona-se como um dos líderes desta área em Portugal, oferecendo aos seus clientes, uma plataforma transversal e poderosa de contratação. Apesar da qualidade existente na plataforma VORTALnext>, existem diversas lacunas de usabilidade que são objeto de estudo nesta dissertação.A usabilidade apresenta-se como um ponto crítico para a aceitação da plataforma por parte dos seus clientes. Entrevistas, heurísticas, questionários e testes foram as ferramentas utilizadas para análise dosreais problemas dos clientes, tendo sido concluído que o dashboard é um dos fatores de aceitação mais relevantes.Tendo como base os resultados obtidos e como foco o dashboard, foram desenhadas diversas aproximações, tanto da arquitetura de informação, como posicionamento e organização dos elementos no ecrã. Utilizaram-se cores para evidenciar as ações mais importantes, números para representar volume de trabalho/negócio, alertas visuais como aviso/lembrete, listas priorizadas e “one-click actions”, conseguindo assim minimizar as falhas de usabilidade detetadas através deste trabalho.Através desta estratégia, um novo dashboard foi desenhado para apresentar apenas o necessário, quando necessário e de uma forma verdadeiramente útil e agradável para o utilizador.",
    "authors": [
      "Silva, Fábio Samuel Coelho da"
    ],
    "keywords": [
      "Avaliação de usabilidade",
      "Contratação eletrónica",
      "Web",
      "Arquitetura de informação",
      "Dashboard",
      "Usability evaluation",
      "E-procurement",
      "Information architecture",
      "Évaluation de l'utilisabilité",
      "Architecture de l'information",
      "681.324:347.4",
      "347.4:681.324"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.324:347.4",
      "347.4:681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92659",
    "title": "Muon tomography: application of image reconstruction algorithms within the LouMu project",
    "abstract": "Muon tomography or muography is an imaging technique that allows non-invasive observation of the in terior of large and dense structures such as pyramids and volcanoes. Muography resorts to cosmic-raymuons, subproduct particles of the interaction of cosmic rays with the Earth’s atmosphere, and to muontelescopes, devices capable of detecting these particles and their trajectory. Depending on the use givento this instrument, the technique subdivides into scattering muography and transmission muography. Thelatter works similarly to radiography and outputs two-dimensional projections of the structures in the fieldof view of the muon telescope. To obtain their 3D image it is necessary to apply image reconstructionalgorithms to the 2D projections. In 2019, LIP, the Laboratory of Instrumentation and Experimental Par ticle Physics, constructed a muon telescope and initiated the first transmission muography experimentin Portugal under the collaboration LouMu. As a starting point, the telescope acquired data at its con struction site, the Department of Physics of the University of Coimbra, and efforts are being made to usethis same data to reconstruct three-dimensional images of the building. This dissertation arrives in thatcontext, as the work presented here concerns how the 2D projections taken at the Department of Physicsof the University of Coimbra were obtained and how they are being used to derive 3D reconstructions ofthe building while resorting to image reconstruction algorithms. Regarding the 2D results, it is explainedhow simulated and experimental images muography images were obtained. It is concluded that, althoughthe two compare well on a coarse-grained scale, some disparities still need to be addressed in futureanalysis, namely to be able to perform 3D reconstruction. In that sense, the development and testingof image reconstruction algorithms to retrieve 3D images from the 2D projections was performed only insimulation. The applicability of two methods, the back-projection and the SART iterative algorithm, to thecase study of the Department of Physics of the University of Coimbra was analyzed and a third algorithm,analytical inversion, is currently being developed and tested under the same conditions. Out of the firsttwo approaches, only the iterative algorithm resulted in successful reconstructions while using the muontelescope of the LouMu collaboration.",
    "authors": [
      "Duarte, Magda Mendes"
    ],
    "keywords": [
      "Muon",
      "Tomography",
      "Muography",
      "Imagiology",
      "Reconstruction",
      "Muão",
      "Tomografia",
      "Muografia",
      "Imagiologia",
      "Reconstrução",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/62543",
    "title": "Previsão na área farmacológica: modelos estatísticos vs Deep Learning",
    "abstract": "Hoje em dia, a tomada de decisões de forma rápida e eficaz é essencial nas organizações de saúde. Neste sentido, surgem os Sistemas de Apoio à Decisão, as plataformas de Business Intelligence e os Sistemas de Tratamento de Dados. De forma a apoiar a decisão no âmbito farmacêutico surgem plataformas de previsão, as quais pretendem auxiliar ao máximo a tomada de decisão por parte dos prestadores de saúde. No âmbito desta dissertação, foi realizado um projeto com o objetivo de extrair conhecimento de forma automatizada a partir de informações passadas e traduzi-las de forma a desenvolver um sistema de previsão de vendas para a área farmacêutica.Tradicionalmente, na área da previsão, é comum a utilização de modelos estatísticos, no entanto é interessante perceber se o Deep Learning consegue acompanhar os resultados obtidos através destes modelos. Para o efeito, foi elaborado um estudo comparativo entre modelos de previsibilidade, conseguidos através de modelos estatísticos e conexionistas. Para os primeiros fez-se uso de funções de modelação disponíveis em librarias da linguagem de programação R e no segundo foram aplicadas redes neuronais recorrentes, nomeadamente as Long Short Term Memory, através de bibliotecas disponíveis em Python para construção de um modelo deep learning. As metodologias desenvolvidas através dos diferentes modelos de previsibilidade foram aplicadas a três casos de estudo, cada um associado a um conjunto de dados diferente. Assim, tornou-se possível analisar o comportamento dos modelos desenvolvidos quando aplicados a conjuntos de dados distintos.Por último, foram apresentados os resultados obtidos para os três casos de estudo, referentes à aplicação de ambas as práticas, e feita uma comparação das mesmas. Foi verificado o sucesso da utilização de algoritmos de Deep Learning na área da previsão, obtendo melhores resultados que aqueles conseguidos através dos tradicionais modelos de previsão estatísticos. Este trabalho permitiu perceber o potencial que o deep learning apresenta, sendo no entanto necessário mais trabalho futuro para dar enfâse a esta afirmação.",
    "authors": [
      "Ferreira, Raquel Marques"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64128",
    "title": "Desenvolvimento de um programa para comparação de curvas ROC: para amostras independentes e amostras relacionadas",
    "abstract": "A análise ROC (Receiver Operating Characteristic) tem vindo a ganhar muita popularidade,principalmente na área da medicina, dado que é uma ferramenta útil para avaliar e especificarproblemas no desempenho de um indicador de diagnóstico.A área abaixo da curva ROC (AUC) é um indicador que pode ser utilizado para comparaçãode duas ou mais curvas ROC.Este trabalho, surgiu da necessidade de existência de softwares que permitem o cálculodas medidas necessárias para comparação de sistemas com base nas curvas ROC. Existemvários softwares que efetuam o cálculo de medidas associadas à análise ROC, no entantoapresentam algumas lacunas, nomeadamente no que diz respeito à comparação paraamostras independentes com diferentes dimensões e na comparação de duas curvas ROCquando estas se intersetam.Neste trabalho é apresentado uma nova aplicação que se designa por CERCUS. Esta foidesenvolvida usando a linguagem de programação JAVA e destaca-se pela possibilidade decomparar duas ou mais curvas ROC.Este programa tem como principal intuito o cálculo de várias estimativas ROC, usandoos diferentes métodos sugeridos no desenrolar do trabalho e fazer a comparação de curvasROC, mesmo que haja interseção, quer para amostras independentes ou amostras emparelhadas.Permite ainda, a representação no plano unitário da curva ROC empírica e a áreaentre as curvas.",
    "authors": [
      "Moreira, Augusto Daniel Teixeira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79925",
    "title": "Nutritional management and recommendations for hospital users and medical inpatients",
    "abstract": "Nutrition is fundamental to human well-being and health, especially when applied topatients who need special health care. In these cases, it is crucial that each patient hasadequate nutrition to meet their needs, in order to accelerate their recovery process.Recommender systems make it possible to offer suggestions to users, adapted to theirpreferences and to previously obtained information about them. Food recommender systemsare recommender systems applied to nutrition and diet. They are usually implementedfeeding plans recommendation platforms based on food and the person using it.In this sense, the existing gap in the use of these recommendation systems applied tonutrition in health care is notorious. This is mainly due to the difficulty in associating thenutritional value of each food with the needs of patients.The main objective of this project is to fill the existing void, through the development andimplementation of a platform that will allow the planning of meals taking into account thenutritional plan of the food and the specific needs associated with the users of the Vila VerdeSocial Canteen.The use of machine learning algorithms will allow us to identify how the connectionbetween food and patient requirements can be made, making this task possible, which iscomplex due to the wide domain associated with it.This platform will be used for the generation of kitchen meal plans, which shall beproduced using the algorithms developed after a bibliographic study and an investigation ofthe existing work, in order to understand how they can be implemented and which are themost adequate to the nutritional recommendations system.",
    "authors": [
      "Rodrigues, Pedro Miguel de Mata"
    ],
    "keywords": [
      "Machine learning",
      "Meal plan",
      "Recommendation systems",
      "Nutrition systems",
      "Planeamento de refeições",
      "Sistemas de recomendação",
      "Sistemas de nutrição",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84566",
    "title": "Development and analysis of mathematical models to study metabolic constraints and capacities in different photosynthetic types",
    "abstract": "Climate change and a growing human population necessitate improved crop adaptability andyield. Improving photosynthesis is one promising route to boosting plant productivity. Photosynthesisis hampered by the dual activity of its main CO2-fixing enzyme ribulose-1,5-bisphosphatecarboxylase/oxygenase (Rubisco). The enzyme side-reacts with O2, leading to the production ofa toxic byproduct, which must be expensively recycled through the photorespiratory pathway.Rubisco’s oxygenation rate depends on the CO2 : O2 ratio and increases under high temperatures.In C3 plants, which make up 90% of the known plant species, this phenomenon candecrease photosynthetic efficiency by an estimated fourth. C4 plants have evolved a carbon-concentration mechanism that suppresses photorespiration by spatially separating initial carbonfixation and re-fixation by Rubisco. Initial carbon fixation occurs in the mesophyll cells, whiledecarboxylation and carbon fixation by Rubisco occurs in the bundle sheath cell and releasespyruvate or phosphoenolpyruvate which then moves back to the mesophyll cells for the nextcycle. To successfully engineer C4 metabolism in C3 plants, it is important to obtain a quantitativeunderstanding of both the energetics and distribution of metabolic fluxes of this metaboliccycle. Here, we tackle this question by analysing a large-scale metabolic model, consisting ofmesophyll and bundle sheath cells connected through the exchange of cytosolic metabolites.We parameterized the model for the main C4 crop maize (Zea mays) by using biochemical andanatomical constraints derived from the literature. These constraints also enable the modelto correctly predict the appearance of the C4 cycle, different C4 subtypes and decarboxylationenzyme co-activity. Accounting for the volumetric ratio between the two cell types leads tomore accurate predictions of C2 photosynthesis, a triose phosphate-3-phosphoglycerate shuttlebetween the cell types, mesophyll-specific nitrate reduction, choice of decarboxylation enzyme,the ratio of ATP production between the cell types, cell type-specific cyclic or linear electrontransport activity and biomass production. Thus, our modelling approach can guide biologicalengineering strategies to implement C4 photosynthesis into other plant systems to ultimatelyimprove crop productivity.",
    "authors": [
      "Machado, Tiago Moreira"
    ],
    "keywords": [
      "Anatomical constraints",
      "C4 photosynthesis",
      "Constraint-based modelling",
      "Zea mays",
      "Restrições anatómicas",
      "Fotossíntese C4",
      "Modelação baseada em restrições",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92788",
    "title": "Non-volatile memory devices based on ferroelectric oxide thin films",
    "abstract": "Information storage is a paramount challenge in the current century driven by the need to scale down memory cells and lower their operating voltages while ensuring high-speed and non-volatile characteristics. Over the last century, ferroelectric materials have emerged as promising candidates for the development of non-volatile memories where two electrical switching states can be written and retained for a long time. The use of conventional ferroelectrics (perovskite oxides) for memory applications has been intensively studied for decades. However, they are not CMOS compatible and are limited by the high growth temperature. Recently, the discovery of ferroelectricity in binary oxides, such as zirconium oxide, ZrO2, or hafnium oxide, HfO2, added new advantages and functionalities in ferroelectrics-based memory devices.This thesis explores the potential of using ferroelectric HfO2- and ZrO2-based materials for non volatile memory applications. In the first stage, it was investigated the impact of annealing temperature on the ferroelectric properties of (HfxZr1-x)O2 films x = 0, 0.3 and 0.5 in a Pt/(HfxZr1-x)O2/W capacitor structure. It was found that an annealing at 680 ºC is the optimal choice for improving ferroelectric properties in terms of thermal budget resulting in a remanent polarization (𝑃𝑟) of 9.2 µC/cm2for the (Hf0.3Zr0.7)O2 composition. Moreover, the device showed a stable performance up to 1.8x106cycles. In a second research study, a La0.7Sr0.3MnO3 (LSMO)/HfO2/W stack was grown on a Nb:SrTiO3 substrate. The 3 nm-thick epitaxially grown HfO2 layer was found to crystallize in the polar rhombohedral phase. Although no evidence of ferroelectric properties was found, a bipolar interfacial resistive switching behaviour was reported in the fabricated device. It is suggested that this RS behaviour is explained by phase transitions at the LSMO/HfO2 interface caused by a reversible migration of oxygen vacancies. The device showed a memory window of almost 10 and a non-volatility retention of at least 100 seconds.",
    "authors": [
      "Silva, Nuno Manuel Estrócio e"
    ],
    "keywords": [
      "Ferroelectricity",
      "Non-centrosymmetric phases",
      "Non-volatile memory",
      "Thin film",
      "Fases não-centrossimétricas",
      "Ferroeletricidade",
      "Filme fino",
      "Memória não volátil",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27812",
    "title": "Análise e conceção de uma framework de reporting genérica e parametrizável",
    "abstract": "Actualmente as aplicações PRIMAVERA incluem componentes de reporting que exigemdemasiado esforço de implementação e de manutenção, na medida em que todo o seudesenvolvimento é manual, repetitivo e assente em tecnologia desactualizada. Estes componentes de reporting são baseados nas soluções Crystal Reports, sendo necessária a construção/desenho em tempo de desenvolvimento de todos os relatórios que são pretendidos para um determinado produto. Cada um destes relatórios tem o seu desenho próprio, a suas próprias características e configurações, não existindo qualquer forma de partilhar determinadas propriedades que possam ser comuns aos vários relatórios. Por norma pretende-se que todos os relatórios de um produto tenham um aspecto uniforme, como porexemplo o layout ou fonte utilizada para determinados campos (por exemplo o título do relatório). Significa isto que é necessário na construção de cada um dos relatórios replicar todas estas características que são comuns, o que exige um esforço significativo e pode ser propício ao erro quando as regras de desenho de relatórios não estão bem definidas no início do desenvolvimento. Este problema torna-se mais evidente quando por exemplo num produto com um elevado número de relatórios se pretende fazer uma alteração numa destascaracterísticas comuns. A simples alteração do tipo de fonte do título do relatório acaba por ser um processo bastante dispendioso, uma vez que é necessário editar todos os relatórios individualmente. Esta dissertação surgiu da necessidade de desenvolver um novo componente de reporting que possa responder às limitações actuais. No âmbito do projecto PRIMAVERA ATHENA, está inserida a Framework de Reporting, cuja finalidade é dar suporte à criação, geração e apresentação de relatórios nos produtos desenvolvidos sobre a Framework ATHENA. Um dos principais objectivos da Framework de Reporting é a geração automática de relatórios a partir dos modelos das aplicações, acabando assim com todo o processo manual de criação de relatórios.",
    "authors": [
      "Sá, Igor Gonçalo Gomes de"
    ],
    "keywords": [
      "681.3.06",
      "658.0"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.06",
      "658.0"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55836",
    "title": "Deploying time-based sampling techniques in Software-Defined Networking",
    "abstract": "Today’s computer networks face demanding challenges with the proliferation of services andapplications requiring constant access, low latency and high throughput from network infrastructures.The increase in the demand for this type of services requires continuous analysis anda network topology capable of adapting to the dynamic nature of applications, in order toovercome challenges such as performance, security and flexibility.Software-Defined Networking (SDN) emerge as a solution to meet these challenges byusing a network control plane, dissociated from the data plane, able to have a global view ofthe topology and act when required, depending on the variation in infrastructure congestion.Decisions involving different activities, such as network management and performanceevaluation, rely on information about the state of the network that in traditional networks involvesa substantial amount of data. Traffic sampling is essential in order to provide valuablestatistical data to applications and enable appropriate control and monitoring decisions to bemade.In this context, this work proposes the application of time-based sampling techniques in aSDN environment to provide network statistics at the controller level, taking into account theunderlying need to establish a balance between the reliability of the data collected and thecomputational burden involved in the sampling process.The results obtained emphasize that it is possible to apply these sampling techniques byusing OpenFlow Group Mod messages, although packet losses can occur on the switch duringperiods of network congestion.",
    "authors": [
      "Teixeira, David Rodrigues"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84396",
    "title": "The role of an API gateway in a microservice architecture",
    "abstract": "Nowadays, with the development of bigger and more complex applications, the architecturalparadigm for application development is changing from a more traditional Monolithicapproach to an architectural style called Microservices. In this more recent, and increasinglypopular, style of developing applications, a tool that has also become increasingly morepopular is API Gateways. In this thesis I explored these and a few other concepts on variousexamples, recording my experience, with the intent to create a guide on how to moreefficiently implement these tools on to your own projects, facilitating the usually long andarduous process of researching, learning, and implementing new technologies into yourwork.",
    "authors": [
      "Parente, Pedro Dias"
    ],
    "keywords": [
      "Microservices",
      "Microserviços",
      "API Gateway",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80091",
    "title": "A machine learning approach to The Big Five Personality Test",
    "abstract": "One of the most accurate personality assessments available is the Goldberg’s ’The BigFive Personality Test’, which measures the five OCEAN dimensions: Openness, Conscientiousness, Extraversion, Agreeableness and Neuroticism. This assessment is performed bypresenting a total of forty adjectives requesting the subject to rate each word using a scaleof 1 to 9 indicating whether it accurately (9) describes herself or not (1). Nonetheless, scientific research has shown that this test may, accurately, suggest personality traits such asaggressive reactions, work performance, fitness on specific expertise areas and also mental illnesses. However, one big disadvantage of this test, it simply takes too much timeto perform, which can result on undesirable measurements. Indeed, several developmentshave been done in order to reduce the required effort to perform this test, an example isThe Mini Marker Test by Saucier. This study aims to propose a viable shorter alternative tothis by applying machine learning techniques, i.e., although measurement precision may bereduced, is it possible to build a much shorter version losing as little precision as possibleby just requiring the subject to select the adjectives that characterise him the most?For this study, it was developed a platform to collect data, requesting both the subject torate each adjective but also to select those he most identifies with. With this, the availabledata contains both ratings and the selections of the words that most characterise the subject.Three different machine learning architectures are developed and tested. Both regressionand classification approaches are considered. The main input for these architectures arethe words selected by each evaluated subject. Data collected by this work showed to beinsufficient, requiring the use of data augmentation techniques. For this, different versionsare proposed, one including the use of frequent itemset mining techniques. The proposedmachine learning architectures shown a very high precision, with an RMSE of around 7%.The results show the proposed solutions to be able to perform a shorter version of thistest with a minimum precision loss. It was also possible to define a list of common setsof selected words. Further research can be performed mainly on two different streamlines,i.e., strength the data collection process and develop an even shorter version of this test.",
    "authors": [
      "Perdigão, Miguel Campos Calafate Carneiro"
    ],
    "keywords": [
      "Big five",
      "Data augmentation",
      "Data science",
      "Machine learning",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60003",
    "title": "An efficient software tool to segment slice and view electron tomograms",
    "abstract": "Segmentation is a key method to extract useful information in Electron Tomography.Manual segmentation is the most commonly used method, but it is subject to user bias andthe process is slow. The lack of adequate automated processes, due to the high complexityand to the low signal-to-noise ratio of these tomograms, provided the main challengesfor this dissertation: to develop a software tool to efficiently handle electron tomograms,including a novel 3D segmentation algorithm.Tomograms can be seen as a stack of 2D images; operations on tomograms usually lead tocomputationally intense tasks. This is due to the large amount of involved data and to thestrided and random memory access patterns. These characteristics represent serious problemson novel computing systems, which rely on complex memory hierarchy architecturesto hide memory access latency time.A software tool with a user-friendly interface — TomSeg — was designed, implementedand tested with experimental datasets, built with sequences of Scanning Electron Microscopyimages obtained using a Slice and View technique. This tool lets users align, crop, segmentand export electron tomograms, using computationally efficient processes. TomSeg takes advantageof the most usual architectures of modern compute servers, namely based on multicoreand many-core CPU devices, exploring vector and parallel programming techniques;it also explores the available GPU-devices to speedup critical code functions. Validationand performance results on a compute server are presented together with the performanceimprovements obtained during the implementation and test phases.TomSeg is an open-source tool for Unix and Windows that can be easily extended withnew algorithms to efficiently handle generic tomograms.",
    "authors": [
      "Sousa, Paulo Rafael da Costa e"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88586",
    "title": "Towards a typed linear algebra formal semantics for spreadsheets",
    "abstract": "This master dissertation addresses the problem of spreadsheet errors by using typed linear algebra inspreadsheet design. The study builds on previous efforts to solve this issue and presents an approach toimprove the quality and reliability of spreadsheet systems.The outcome of this study shows that the adoption of a typed linear algebra approach in spreadsheetdesign can significantly reduce the risk of errors and improve the reliability of spreadsheet-based systems.The tool developed in this dissertation allows users to derive spreadsheet models in Haskell from formalspecifications, which are then translated into a particular spreadsheet format. This process helps toensure the accuracy and consistency of the generated spreadsheets, as it is based on precise and well typed specifications. Additionally, the use of typed linear algebra in the semantics of spreadsheet functionsand constructions such as e.g. running totals provides a solid foundation for the correctness. Overall, theresults of this study demonstrate the effectiveness of the typed linear algebra approach in improving thequality and reliability of spreadsheet systems.",
    "authors": [
      "Azevedo, Rui Filipe Brito"
    ],
    "keywords": [
      "Formal methods",
      "Master’s dissertation",
      "Spreadsheets",
      "Typed linear algebra of programming",
      "Álgebra linear tipada da programação",
      "Dissertação de mestrado",
      "Folhas de cálculo",
      "Métodos formais",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81388",
    "title": "A portabilidade de dados de saúde dentro e fora da União Europeia - desafios jurídicos e técnicos no âmbito da proteção de dados",
    "abstract": "Constituído por três capítulos, este trabalho teve como objetivo oferecer um ponto de vista técnico jurídico sobre o quadro de normalização atual do exercício do direito de portabilidade de dados de saúde, em um contexto de dentro e fora da União Europeia. Para o efeito, foram considerados os processos e medidas em prestação de cuidados de saúde e as respetivas necessidades operacionais, extraindo das mesmas o que implicaria na necessidade de um intercâmbio de dados de saúde, dentro e fora da União Europeia, considerando a proteção especial inerente aos mesmos e considerando alguns conceitos jurídicos diversos, mas com alguma intercessão, como o direito de acesso e de portabilidade. Foram observadas as medidas de interoperabilidade a nível nacional, continental Europeu e os desafios a nível transcontinental na saúde, tendo em conta a matéria de arquitetura de segurança das redes e sistemas de informação relativos a dados pessoais face ao exercício do direito de portabilidade e o caráter normalizador global que a União Europeia tem assumido em matéria deinteroperabilidade para o exercício do direito de portabilidade de dados ao abrigo do RGPD.Foram estudadas e buscadas soluções recorrendo à lógica de Soft-law, para alcançar uma normalização que não enfrentasse as limitações de aplicação geográfica aplicáveis às normas vinculadas a um ordenamento jurídico – Hard-Law no setor da saúde, buscando perceber se um standard de boas práticas para a portabilidade neste seguimento seria viável, aflorando ao final as soluções encontradas e as dificuldades pendentes de solução.",
    "authors": [
      "Ladeia, Yuri Rodrigues"
    ],
    "keywords": [
      "Quadro regulatório intercontinental para a portabilidade de dados",
      "Cuidados de saúde transfronteiriços",
      "Os Direitos dos titulares de dados",
      "Regulamento Geral sobre a Proteção de Dados",
      "Privacidade e Proteção de Dados",
      "Intercontinental regulatory framework for data portability",
      "Cross-border healthcare",
      "The rights of data subjects",
      "General Data Protection Regulation",
      "Privacy and Data Protection",
      "Ciências Sociais::Direito"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Sociais::Direito"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81212",
    "title": "Uma nova plataforma para auxiliar o processo de comunicação entre encarregados de educação, profissionais de educação e especialistas em pediatria",
    "abstract": "As urgências hospitalares têm por objetivo responder a emergências que surjam, seja dentro ou fora de horas e, por ser um serviço onde qualquer tipo de caso pode surgir e, sendo cada caso um caso, é extremamente importante garantir a obtenção do melhor atendimento.Isto aplica-se ainda mais quando se refere à Ala de Pediatria. Estes por receberem crianças de todo o tipo de idades têm de conseguir responder às diferentes necessidades e dificuldades, mas grande parte desse trabalho passa pelos pais/encarregados de educação que têm a função de informar os enfermeiros, ou médicos, dos pré-cuidados que tenham sido administrados, ou sintomas anteriores.Deste modo, esta dissertação apresenta a problemática, razões e solução encontrada, ao definir objetivos e ao estabelecer metas para lá chegar. É apresentado um breve estado de arte com as pesquisas e revisão de literatura mais relevantes na área do problema e identificação de soluções comuns. Ainda são apresentadas as metodologias de investigação que serão utilizadas, são ela Design Science Research e Proof of Concept, e as tecnologias a ser utilizadas, que passam por React para o frontend, Node JS para o backend e MySQL para a Base de Dados.Para terminar é demonstrado os artefactos criados, desde o levantamento de requisitos, passando pela arquitetura de software e terminando no Desenho e Desenvolvimento, todos estes foram ao longo da sua execução avaliados e feitos teste de usabilidade e viabilidade.",
    "authors": [
      "Sousa, Sandra Teixeira Marques de"
    ],
    "keywords": [
      "Aplicação",
      "Comunicação",
      "Doenças",
      "Informação",
      "Medicação",
      "Plataforma",
      "Application",
      "Communication",
      "Diseases",
      "Information",
      "Medication",
      "Platform",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83650",
    "title": "Aplicação de técnicas de IA na deteção de irregularidades em contratos públicos",
    "abstract": "Com o passar dos anos, acompanhado pela evolução da tecnologia, existe um aumento acentuado nonúmero de dados acumulados no setor de contratação pública em Portugal. Este aumento abre caminhopara a possibilidade de tirar proveito desses dados, com recurso à utilização de técnicas emergentes deinteligência artificial, de modo a melhorar o funcionamento do processo de feitura de contratos públicosem Portugal.No panorama da contratação pública, um dos grandes problemas que afetam a qualidade dos contratoscelebrados é a existência de irregularidades não detetadas aquando da celebração dos contratos, essasirregularidades geram assim contratos que não cumprem as regras definidas para os contratos e quepodem comprometer a qualidade dos bens e serviços providenciados pelo Estado.Portanto, esta dissertação visa recolher um dataset de contratos públicos a partir do portal disponi bilizado pelo governo português, processar e analisar os dados recolhidos, codificar as regras do Códigodos Contratos Públicos num sistema de regras, investigar e utilizar técnicas de Inteligência Artificial demodo a desenvolver um pipeline com a finalidade de encontrar padrões de suspeição de conluio e detetarirregularidades, e, por fim, conceber modelos de Machine Learning para prever os valores futuros dasdespesas de cada entidade ou região.Para sustentar o trabalho desenvolvido foram analisadas e relatadas na dissertação algumas implemen tações existentes de técnicas de inteligência artificial em contratação pública, juntamente com algumasabordagens de deteção de fraudes, assim como foram analisados diversos paradigmas e algoritmos deML.Por fim é demonstrado de que forma os modelos de ML foram concebidos e otimizados, e é feita aanálise de resultados dos modelos criados.A investigação e experimentação realizada abre perspetivas para o futuro da aplicação de soluçõesde Inteligência Artificial na área da contratação pública.",
    "authors": [
      "Sousa, Tiago Dias de"
    ],
    "keywords": [
      "Machine Learning",
      "Data Mining",
      "Contratação Pública",
      "Extração de Conhecimento",
      "Public Procurement",
      "Knowledge Extraction"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92707",
    "title": "Development of fashion products using 3D digital technology to enhance the presentation of fashion collections - case studies",
    "abstract": "Esta dissertação de mestrado examina a transformação dinâmica da indústria da moda, desde as práticas tradicionais até a combinação de tecnologias digitais avançadas. O foco principal está na reinvenção da moda através da adoção de programas digitais inovadores em 3D, que são cada vez mais proeminentes no cenário da moda atual.Esta pesquisa apresenta uma análise abrangente no papel do design de moda, com vertente no 3D, na indústria da moda contemporânea, particularmente no desenvolvimento e apresentação de coleções de moda. O contraste entre as apresentações físicas tradicionais de moda e a abordagem moderna é orientada para a tecnologia que destaca os benefícios da tecnologia 3D no aumento da acessibilidade, da relação custo-benefício e da liberdade criativa.O estudo aborda o design 3D que integra sistemas de simulação virtual para oferecer um novo upgrade na apresentação de coleções de moda no processo de desenvolvimento de design e na vertente comercial. A autenticidade da representação material e a criação de gémeos digitais realistas também são exploradas, com foco em plataformas que melhoram a fidelidade visual dos ativos digitais.A pesquisa inclui uma revisão ampliada da literatura e um estudo das marcas de moda que adotaram esta inovação tecnológica, selecionando 3 casos de estudos proeminentes para análise detalhada. O estudo sugere que para que as empresas de moda prosperem nas vendas e cativar novos clientes, é imperativo o investimento estratégico na tecnologia 3D e na integração digital. Esses avanços são fundamentais para promover uma indústria da moda inclusiva, sustentável e inovadora.Concluindo, esta dissertação serve como um recurso essencial para profissionais e entusiastas da moda, oferecendo insights sobre o potencial transformador da tecnologia digital 3D no futuro das apresentações de moda.",
    "authors": [
      "Oleiro, Sara Luísa Ribeiro"
    ],
    "keywords": [
      "Simulação virtual vestuário",
      "Design de moda",
      "Coleções de moda",
      "Tecnologia digital 3D",
      "3D prototipagem virtual",
      "Virtual garment simulation",
      "Fashion design",
      "Fashion collections",
      "3D digital technology",
      "3D virtual prototyping",
      "Humanidades::Outras Humanidades"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Humanidades::Outras Humanidades"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/62611",
    "title": "REM sleep: a new hypothesis for its structure and further HRV characterization",
    "abstract": "The understanding of the Rapid Eye Movement (REM) sleep structure is verylimited. The REM stage presents periods with distinct characteristics, suggestingthat it should be divided into sub-stages. At least two sub-stages are said to existduring REM, the tonic and phasic periods, mainly separated by the presence andabsence of rapid eye movements.The main objective of this thesis was to evaluate the existence of patternsduring the REM stage, based on the signals from the polysomnography. Thisevaluation focused on characterizing the phasic and tonic periods, through the developmentof algorithms. Moreover, this study was extended using unsupervisedlearning to analyze the existence of other REM sub-stages. Finally, this investigationwas completed with the evaluation of the heart rate variability during theREM period, taking into consideration the REM sub-stages.This study corroborated the characteristics described in the literature for thephasic and tonic period from the REM stage. The limitations showed by the initialapproach were surpassed with the application of a clustering technique. Thisresulted in a division of the REM period in 4 sub-stages, each presenting a specificpattern in the characteristics of the electroencephalography (EEG), electrooculography(EOG), and electromyography (EMG).The study of the heart rate variability during REM showed specific patternsbetween the changes in the heart rate variability and the control of the AutonomicNervous System (ANS) during this period and each REM sub-stage. Moreover,the structure of the REM sleep changed in a specific pattern with the advance ofthe REM cycle, while there was a decrease of the heart rate and an increase in theparasympathetic activity.In this thesis, we presented a new hypothesis for the REM structure, whichcan be applied in future studies, allowing to be consolidated and possibly contributingto a better understanding of the REM stage. The findings emphasizedthe need to consider the REM stage a non-homogeneous stage. As for the studyof the heart rate variability, this investigation led to a possible explanation for thevariations occurring during REM sleep, bringing new knowledge that can benefitthe unobtrusive methods for sleep monitoring.",
    "authors": [
      "Mateus, Pedro da Costa"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/25677",
    "title": "Impacto da utilização de técnicas de amostragem na caracterização de fluxos de tráfego",
    "abstract": "The constant development of the Internet and underlying transmission technologies,together with the increasing popularity of provided services, such as multimediaapplications and applications using P2P technologies, are contributing to the continuousgrowth of the network traffic in volume and diversity.To be able to handle such amount of data while assuring the quality and operationof provided services, traffic-measuring tools are required to implement mechanismsthat scale and have a minimum interference on the normal network behaviour.One of the most common solutions for this purpose involves the implementationof measurement techniques based on traffic sampling. These techniques aim toprovide accurate estimations of traffic behaviour and characteristics by processingfractions of the original network traffic.Another fundamental area of traffic monitoring concerns the traffic classificationand characterization, as it supports important tasks such as resource allocation,planning and management, security and quality of service. Attending to this, addedup to the mentioned growth in traffic volumes, it is likely that traffic classificationand characterization will be increasingly supported by traffic sampling mechanisms.This work aims to study the impact of traffic sampling mechanisms on the accuracyof traffic flows characterization. This was carried out through the applicationof classical and adaptive traffic sampling techniques to real traffic traces, which werecaptured in different real scenarios and opened to public access. The resulting sampleddata is then organized under the form of flow records, which are then classifiedaccording to their transport and application protocols.The performance of the distinct traffic sampling techniques in enabling a correctcharacterization of traffic flows was then assessed taking into account multiple metricsapplied to the flow records of sampled traffic, compared to the metrics of thefull original traffic.",
    "authors": [
      "Martins, David Esteves Magalhães"
    ],
    "keywords": [
      "621.39"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "621.39"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80271",
    "title": "Aplicação de normas clínicas em anatomia patológica usando o SNOMED",
    "abstract": "Nos dias de hoje, os Sistemas de Informação Hospitalar assumem-se como uma ferramenta indispensável para a prestação de cuidados de saúde, uma vez que permitem o aumento da qualidade e da eficiência quer na prática clínica, quer na gestão hospitalar. A interoperabilidade emerge, assim, como uma necessidade, uma vez que a enorme diversidade de sistemas torna difícil a troca e a partilha de informação clínica. Além disso, a elevada quantidade de sistemas não articulados faz com que seja muito mais provável a existência de dados repetidos e contraditórios, razão pela qual se torna ainda mais imperativa a utilização de normas e terminologias que conduzam à uniformização do registo clínico. Neste contexto, a presente dissertação descreve a criação de um modelo relacional de dados, que serve de base a uma aplicação de classificação de termos médicos segundo o Systematized Nomenclature of Medicine Clinical Terms (SNOMED CT), no âmbito da Anatomia Patológica. A base de dados construída assenta na estrutura original do SNOMED, e foi obtida através da criação de subsets que contêm os conceitos de interesse para o referido setor.",
    "authors": [
      "Domingues, Andréa"
    ],
    "keywords": [
      "SNOMED CT",
      "Terminologias clínicas",
      "Base de dados",
      "Anatomia patológica",
      "Interoperabilidade",
      "Clinical terminologies",
      "Database",
      "Pathological anatomy",
      "Interoperability",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/87253",
    "title": "Implantação e avaliação da plataforma Open Source MANO (OSM)",
    "abstract": "A virtualização de funções de rede (NFV, Network Function Virtualization) é uma das principais tecnologias impulsionadoras da quinta geração de redes móveis 5G,cujo objetivo é separar as funções de rede (NFs, Network Functions) do hardware. As NFs são virtualizadas sobre hardware comum, o que traz mais flexibilidade e escalabilidade às redes. Esse paradigma tem como principal desafio a integração detecnologias emergentes e o apoio aos casos de uso que o 5G deverá suportar. Neste trabalho, fez-se a implantação e avaliação do desempenho do Open SourceMANO (OSM) nas versões 7, 8 e 9. Em termos de métricas funcionais, foram avaliadas as percentagens de resource footprint da máquina virtual onde o OSM foi instalado, bem como as utilizadas pelas funções de rede virtual (VNFs, Virtual Network Functions) no OpenStack, o gestor de rede virtual (VIM, Virtual Infrastructure Manager), utilizado. Quanto às métricas operacionais, foi medido o atraso no processo de on-boarding (OPD, On-boarding Process Delay), ou seja, o tempo necessário para que a imagem de uma VNF inicialize, e o atraso no processo de implantação (DPD,Deployment Process Delay), que é o tempo necessário para que uma VNF seja configurada. Além disso, foi feita uma comparação do OSM com outras plataformasde gestão e orquestração (MANO, Management and Orchestration) de codigo aberto em relação às suas especificações técnicas.Para a implantação da plataforma OSM foi criado um ambiente virtualizadoadequado à realização do experimento, onde foram instaladas as três versões do OSM(7, 8 e 9) e o OpenStack, versão MicroStack, sendo que cada versão do OSM foi integrada com o OpenStack para realização do experimento operacional. Foi possível realizar os experimentos para todas as versões. Dos experimentos para a medição das métricas operacionais OPD e DPD, verificou-se, de forma geral, que as versões 7 e 8 do OSM apresentam desempenho semelhante e melhor que o da versão 9. Tambémse verificou que o valor do OPD tem uma tendência crescente com a complexidade daVNF e com o incremento sucessivo de VNFs na infraestrutura de virtualização de redes(NFVI, Network Functions Virtualization Infrastructure).",
    "authors": [
      "Moniz, Rosana Mafalda Vieira"
    ],
    "keywords": [
      "Avaliação de desempenho",
      "OSM",
      "NFV",
      "VNF",
      "MANO",
      "OpenStack",
      "Benchmarking",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27923",
    "title": "MINHA: avalição realista de aplicações distribuídas num ambiente centralizado",
    "abstract": "Nos últimos anos os sistemas distribuídos têm sofrido um crescimento exponencial. Estes sistemas, normalmente implementados na plataforma Java, são compostos por um vasto conjunto de componentes de middleware, os quais desempenham várias tarefas de comunicação e de coordenação. Esta tendência influencia a modelação e a arquitetura de novas aplicações cada vez mais complexas obrigando a um enorme esforço e a um custo elevado na avaliação do seu desempenho. A concorrência e a sua distribuição, bem como o facto de muitos problemas só se manifestarem pela grande escala em si, não permite que a sua avaliação seja feita com recurso a simples ferramentas que não tenham em conta estas características. Avaliação realista e controlada de aplicações distribuídas é ainda hoje muito difícil de alcançar, especialmente em cenários de larga escala. Modelos de simulação pura podem ser uma solução para este problema, mas criar modelos abstratos a partir de implementações reais nem sempre é possível ou mesmo desejável, sobretudo na fase de desenvolvimento na qual ainda podem não existir todos os componentes ou a sua funcionalidade estar incompleta. Para colmatar esta falha, nesta dissertação é apresentada o Minha, uma plataforma que permite uma avaliação realista das aplicações através da combinação de modelos abstratos de simulação e implementações reais num ambiente centralizado. Esta plataforma combina a execução de código real sob análise, com modelos de simulação do ambiente envolvente, isto é, da rede e da aplicação. Este sistema permite reproduzir as condições de um sistema em grande escala e através da manipulação de bytecode Java, suporta componentes de middleware inalterados. A utilidade deste sistema é demonstrada aplicando-o ao WS4D, uma pilha que cumpre a especificação Device Profile for Web Services.",
    "authors": [
      "Bordalo, João"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27864",
    "title": "Suporte à interoperabilidade entre o Automation Studio e Sistemas SCADA : tradução de sinópticos de XAML para SVG",
    "abstract": "Em 2008 é posto em marcha um projecto de inovação e desenvolvimento denominado por InPact entre a EFACEC, a EDP Distribuição e a Universidade do Minho. O objectivo deste projecto é fornecer um conjunto de ferramentas de engenharia para programação dos sistemas de protecção, automação e controlo de sistemas de energia. Pretendia-se que as ferramentas suportassem a gestão completa dos sistemas da EFACEC, baseadas nas normas internacionais IEC 61850, IEC 61131-3, IEC 61499 e IEC 60870-5. Nesse âmbito, a EFACEC criou o Automation Studio, um ambiente de desenvolvimento integrado, desenvolvido em linguagem C# da framework .Net da Microsoft, sendo as ferramentas integradas nesse ambiente como plugins. Entre as ferramentas desenvolvidas, conta-se um editor de sinópticos. Tendo este sido desenvolvido em C# .Net, utiliza XAML para a descrição dos diagramas. No entanto, dos equipamentos produzidos pela EFACEC, mais concretamente, a plataforma para automação e controlo de sistemas de energia e gestor de sistemas SCADA UC 500, utiliza SVG para a visualização e interacção com os sinópticos. Assim, embora o editor permita criar os sinópticos para a plataforma UC 500, as linguagens utilizadas não são compatíveis. Para ultrapassar estes problemas de interoperabilidade, entre o editor e a plataforma, surgiu a necessidade de desenvolver um compilador XAML para SVG.O objectivo do trabalho, desenvolvido no âmbito desta dissertação, foi então o desenvolvimento do referido compilador de XAML para SVG. Este deveria ser integrável no ambiente de edição do Automation Studio para, desta forma, permitir a configuração de diversos equipamentos da EFACEC, em particular da plataforma UC 500, a partir desse ambiente de desenvolvimento integrado. Após várias fases de testes e de melhoramentos, o compilador foi definitivamente integrado no editor Automation Studio na sua versão 2.0 e seguintes. O resultado positivo deste projecto é visível pela utilização actual em dois exemplos reais, um na subestação de Ermesinde e outro no Bahrain, ambos apresentados neste documento.",
    "authors": [
      "Silva, Nuno Miguel Milhases da"
    ],
    "keywords": [
      "XAML",
      "WPF",
      "SVG",
      "SCADA",
      "EFACEC",
      "XAML",
      "WPF",
      "SVG",
      "SCADA",
      "EFACEC"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64121",
    "title": "Development of a scoring system to assess potential biomarkers for atrial fibrillation",
    "abstract": "Atrial fibrillation affects millions of individuals worldwide, posing a major threat topublic health due to the variety of comorbidities that constitute by-products of the disease.In light of this epidemic, new means of diagnosis, prognosis and therapy are pressing.Biomarkers, particularly protein markers, are important tools in this process but lackvalidation, which is essential before clinical translation. Several appraisal benchmarkshave been developed to determine the relative potential of biomarkers, but these presentmultiple limitations.We developed a bioinformatic-oriented scoring function aimed at weighing theimportance of proteins and mitigating the limitations of the currently known scores. Aftertaking an extensive literature search and mining a massive volume of reports, data wasorganized into several subsets, according to the sample major characteristic and atrialfibrillation type. A mathematical scoring function was proposed, based on the consensusof studies supporting the protein-disease association (incoherence), median of thereported fold-changes and importance of each study according to the number of diseasedindividuals, and applied to each subset in the form of an algorithm implemented in Python3.5.The developed ranking method performed well regarding both the degree of alterationand the inconsistency parameters. Our results portray a set of proteins with the highestbiomarker potential (highest scores) for atrial fibrillation. We also selected the top fivepotential biomarkers for atrial fibrillation in general and for each type of disease. Themain biological functions in which they are involved were retrieved for comparison withthe state of the art. Alterations in the expression levels of proteins involved in either ofthese functions seem to agree with AF’s pathophysiology and clinical presentation,showing the effectiveness of the developed algorithm.Overall, the developed pipeline seems to improve the processes of biomarker rankingand selection for a target disease, allowing a leap towards clinical translation.",
    "authors": [
      "Magalhães, Beatriz Teixeira de"
    ],
    "keywords": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/57310",
    "title": "Analysis of the effects of the radiation therapy in cancer treatment by medical image processing",
    "abstract": "This dissertation begins with a brief introduction, where the main objective,segmentation of an organ/structure into sub-segments and performinga radiation analysis after a radiotherapy treatment. Then, there is a shortintroduction to what cancer is, as well as some of the most relevant to thisdissertation. After, the implementation of the various interfaces created, forthe identification of the contours of the structures and for the final objective.Finally, the results obtained from these interfaces, where its possibleto observe the fulfillment of the proposed objectives. Finally the conclusionsobtained and the validation by a Doctor of the field, and proposals for futurework, namely the segmentation of more complex structures, such as rectumand sigma-colon.",
    "authors": [
      "Catarina, César Fernando Vivo Ferreira de"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36563",
    "title": "Handle default data with case-based reasoning: an approach to solve problem reports",
    "abstract": "On a business context, it is responsibility of the Software Product Support Team analyze and solve, if necessary, problems that may arise on software products. Sometimes, the reported problems are not a real defect, i.e., sometimes the client does not have a full understanding about all features of the software product. The team must evaluate and analyze all the Problem Reports that arrive every day. As products are spread across different customers, it is normal to have Problem Reports that are very similar to others that have already been solved for other clients and/or by another member of the Support Team. This dissertation proposes the development of a system that is able to analyze a Problem Report and then provide past problems that are similar to the one being analyzed. An artificial intelligence technique, named Case-Based Reasoning, will be used to achieve such goals. Existent Case-Based Reasoning systems are neither complete nor adaptable to specific domains since the effort to adapt either the reasoning process or the knowledge representation mechanism, to a new domain, is too high. To address such drawbacks, a generic reasoning component will be designed and developed. This dissertation introduces a new approach to the typical Case-Based Reasoning cycle where is possible to handle default, unknown and incomplete data.",
    "authors": [
      "Fernandes, Bruno Filipe Martins"
    ],
    "keywords": [
      "Similarity analysis",
      "Case-based reasoning",
      "Intelligent systems",
      "Degree-of-confidence",
      "Análise de similaridade",
      "Raciocínio baseado em casos",
      "Sistemas inteligentes",
      "Grau-de-confiança",
      "681.3"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92796",
    "title": "Semi-supervised object detection: a pipeline workflow proof of concept",
    "abstract": "Object detection in computer vision plays a pivotal role in applications such as autonomous vehicles, surveillancesystems and medical imaging. However, a consistent challenge faced by the field is the scarcity of labeled datarequired for training robust object detection models. The manual annotation process for objects within imagesis labor-intensive and expensive, constraining access to large-scale annotated datasets, which in turn hindersprogress in object detection research and applications.This work aims to respond to the pressing issue of data scarcity by exploring a series of automated learningmechanisms, harnessing pre-existing image classification datasets to craft custom-labeled datasets tailored forobject detection. In its foundation, is the streamlining of the transformation of image classification datasets intoaccurately annotated object detection datasets by generating bounding box annotations and object labels. Thisdata synthesis strategy serves the subsequent stages of model training, where object detection models are trainedon the newly generated datasets without the need for labor-intensive manual annotations. The significance ofthis work resides in its potential to reduce the time and financial costs associated with manual labeling. Thiscost-effectiveness holds immense importance for organizations reliant on the practical application of objectdetection, particular so in more niche areas, potentially breaking down entry barriers for smaller enterprises.The validity of the methodology is put to the test within the domain of fruit detection, where annotated datanotably sparse. Performance is assessed using common metrics, including mean average precision, precision,and recall, with pre-existing annotations from the DeepFruits dataset.In this work, we start by exploring a fully autonomous semi-supervised pipeline-based workflow which allowedto replicate the labeling of 45% of a subset of DeepFruits. This strategy’s performance, which provided an objectdetection precision of approximately 69%, was then improved by introducing a manual filtering step for the removalof false positive detections, thus necessitating some amount of human interaction. However this, in turn, increasedthe new pipeline’s precision to 88%, allowing it to correctly recognize 60% of DeepFruits’ objects. Finally, weexplored the possibility of converting this manual filtering stage to an automatic verification layer supported by anunderlying convolutional neural network that partitions the detected objects into desirable or undesirable objects.Here, we evaluated how the application of such image classifier would have improved the previous pipeline, whichnecessitated manual filtering. Applying it would have allowed for the removal of 81% of all accumulated falsepositives.",
    "authors": [
      "Sousa, Bruno Alexandre Dias Novais de"
    ],
    "keywords": [
      "Deep learning",
      "Semi-supervised learning",
      "Computer vision",
      "Object detection",
      "Image classification",
      "Fruit detection",
      "Image synthesis",
      "Aprendizagem profunda",
      "Aprendizagem semi-supervisionada",
      "Deteção de objetos",
      "Classificação de imagem",
      "Deteção de fruta",
      "Síntese de imagem",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79951",
    "title": "Encaminhamento de tráfego em redes SDN (Software-Defined Networking)",
    "abstract": "Through the exponential growth of the internet service usage comes the inevitability ofthe creation of a tool that can both be useful and versatile in the management of all of thecreated traffic volume.From this necessity arises the concept of Software-Defined Networking (SDN), that aimsto offer a set of protocols and technologies capable of easing the management and theefficiency of maintenance of the various network infrastructures that require its usage.This work aims for the initial comparison of the multiple capacities of several SDN controllers existent in the market, specifying their programming languages and characteristics.After such comparison follows the selection of one of the SDN controllers, in order fora prototype of a traffic forwarding solution to be developed, as to get the most out of thechosen controller’s characteristics and of the global SDN approaches.That same developed solution should allow for multiple characteristics, highlighting thepossibility of its capacity to deal with traffic forwarding under specific parameters. Initially,the solution should incorporate the Dijkstra Algorithm to calculate the shortest paths andinject its results into the network, as it also should immediately converge after link failure.Then, it should also be conducive to events that occur in real time, it should allow for link orrouter protection in the infrastructure, effectively react to link failure, react to different loadlevels in the network, flow traffic forwarding through specific routes, topology multiplexingthrough different virtual networks, among others. That way, the developed prototype maycome up as an alternative SDN approach to certain well-known interior routing protocols,such as Routing Information Protocol (RIP) and Open Shortest Path First (OSPF).",
    "authors": [
      "Pereira, Luís Gonçalo Epifânio"
    ],
    "keywords": [
      "SDN",
      "Encaminhamento",
      "Tráfego",
      "Dados",
      "Protocolos",
      "Networking",
      "Gestão de fluxo",
      "Infraestrutura",
      "Forwarding",
      "Traffic",
      "Data",
      "Protocols",
      "Networking",
      "Flow management",
      "Infrastructure",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84391",
    "title": "Algoritmos de aprendizagem automática para previsão de AVC",
    "abstract": "O Acidente Vascular Cerebral (AVC) foi, em 2020, a segunda principal causa de morte no mundo e primeira no que toca a incapacidade. Com a motivação de contribuir para ajudar a reduzir os números que são alarmantes e continuam a crescer, surge este projeto, do qual se pretende que resultem modelos que possam tentar prever se um indivíduo irá, ou não, ser vítima deste problema e descobrir quais assuas características ou dados clínicos que mais influenciam esta previsão, pois, segundo a Sociedade Portuguesa de Medicina Interna (SPMI), 80% dos casos podem ser prevenidos[1]. Para o efeito, o projeto a desenvolver incluirá uma recolha e tratamento de datasets que organizem dados clínicos de vários pacientes e a incidência desta problemática, um estudo acerca das técnicas e algoritmos de Machine Learning mais adequados aos modelos a desenvolver, sendo depois aplicados através de modelos de Data Mining (DM), dando uso a ferramentas como Weka e RapidMiner, para indução dos modelos de previsão, assim como algoritmos em linguagens como Python e R, conjugando, assim, os factos de que \"o setor da saúde é rico em informação, e o Data Mining está a tornar-se uma necessidade\"[2]. Finalmente, estes modelos serão testados, validados e comparados, do qual resulta esta dissertação.",
    "authors": [
      "Costa, Eduardo João Gomes Teixeira da"
    ],
    "keywords": [
      "Mineração de dados",
      "Aprendizagem automática",
      "Acidente Vascular Cerebral",
      "Previsão",
      "Data mining",
      "Machine learning",
      "Stroke",
      "Prediction",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84394",
    "title": "Autonomous optimization for a transactional middleware",
    "abstract": "In the last few years, data management engines have become increasingly modular, separating some of itsmain layers, such as data storage and transactional management. The exposure of the transactional manage ment component brings new challenges, in particular its correct configuration and tuning when running differentworkloads. In this sense, this dissertation focuses on the autonomous optimization of a particular transactionalmiddleware, pH1, while keeping in mind the tuning of other similar systems.It is becoming more and more important to develop algorithms that can automatically optimize these systemswhose performance is heavily dependent on a proper configuration. The use of machine learning techniques forsimilar problems (database knob tuning) has become common in the literature [1, 2, 3, 4], especially in a blackbox perspective where it does not have visibility over particular details of the system.Usually, these systems are located in realistic online environments, where workloads can change at differenttimes. Even though there are numerous research projects for automatic knob tuning, these projects have notentirely addressed this problem and are mostly developed for offline training when the workloads remain static.We propose OPAL as the component that when executing transactional workloads is able to dynamically adjustits configurations in an online environment with a continuous space. Our approach allows for online changes anduses reinforcement learning as a starting point taking into consideration tuning algorithms in continuous spaces,as is the case of DDPG [5].",
    "authors": [
      "Marques, Susana Vitória Sá Silva"
    ],
    "keywords": [
      "Reinforcement learning",
      "Actor-critic methods",
      "Online environments",
      "Black box system",
      "Aprendizagem por reforço",
      "Métodos ator-crítico",
      "Ambientes online",
      "Black box vision",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/85168",
    "title": "Development of a deep learning-based algorithm to predict pneumonia cases fram chest X-ray images",
    "abstract": "Interstitial lung diseases (ILD) are defined as a set of more than 200 pulmonary disorders. Among these, the ones broadly termed as pneumonia represent a major cause of morbidity and mortality in the world. The chest radiograph (CXR) was the first x-ray based lung imaging technique to emerge and is still widely used as a diagnostic method for pneumonia and other lung diseases. However, correct interpretation of CXR requires analysis by experts and stays vulnerable to errors and observer-related variation. To counteract these problems, artificial intelligence (Al) methods have been applied for the automated analysis of CXR and other medical images. The deep learning (DL) branch of AI and in the particular the methods based on convolutional neural networks (CNN), recently obtained impressive results in these tasks. This dissertation presents a DL approach to classify pneumonia from medical CXR image datasets. Two different models based on the development of CNN were trained from a preprocessed dataset of CXR images obtained from 8562 individuals classified as normal (n=7214) or with pneumonia (n=1348) (Dataset XP1’). Model 1 applied a normal cross entropy loss function, and model 2 an alternative loss function aiming at counteracting the unbalance in normal/pneumonia class frequency. For performance enhancing both models underwent a hyper optimization procedure. The optimized model 1 and 2 were tested on a test set from PI'. To better understand the predictability and generalization potential we then tested both models on an unrelated test set of 624 images (Dataset XP2). Interestingly, model 1 obtained better performance when tested on XP2 than in XP1', scoring an accuracy of 85%, recall of 93% and precision of 85% for the detection of the pneumonia class. The higher homogeneity present on dataset XP2 compared with dataset XP1' could be a plausible justification. As for model 2, it correctly predicted more pneumonia cases an test set XP1' than model 1. However, on test set XP2 the results were poor, predicting most cases as pneumonia and scoring a recall value of only 26% for the pneumonia class. Testing the DL models on unseen data is a relevant but not always performed validation. Overall, the higher accuracy, recall and precision levels of model 1 in XP2 suggests it has a higher potential to be applied for real-word application although its performance should be further improved and evaluated. This work opened promising new lines of research for the future development of a high-performance CNN-based automated method to classify CXR and assist in the diagnostic of pneumonia.",
    "authors": [
      "Carlos, Nuno Rafael Boto"
    ],
    "keywords": [
      "Interstitial lung diseases",
      "Pneumonia",
      "Chest radiographs",
      "Artificial intelligence",
      "Deep learning",
      "Convolutional neural networks",
      "Doenças intersticiais pulmonares",
      "Radiografias torácicas",
      "Inteligência artificial",
      "Redes neuronais convolucionais",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79991",
    "title": "Mecanismos de raciocínio para sistemas de avaliação de conhecimento",
    "abstract": "Sendo certo que o recurso à tecnologia no ensino é cada vez mais notório, a utilização de sistemas informáticos de tutoria continua aquém do seu potencial, ainda que seja um tema abordado há já algumas décadas. Assim, surgiu a iniciativa Leonardo e o respetivo desenvolvimento de uma ferramenta computacional para sistemas de avaliação de conhecimento, com vista a ser aplicada, pelo menos, no suporte de processos de avaliação de alunos, na Universidade do Minho. De entre os módulos que caracterizam estes agentes de software, no contexto desta dissertação, destacam se a base de conhecimento, o mecanismo de raciocínio e o modelo do estudante. Dado que o esforço maior recai em habilitar os tutores artificiais à adaptação, em tempo real, da avaliação ao nível de conhecimento atual dos alunos, surge a necessidade de desenvolvimento de um mecanismo de raciocínio, que seja capaz de determinar, criteriosamente, o que deve ser apresentado de seguida num dado momento avaliativo. O trabalho desta dissertação focou-se na conceção e implementação de um sistema de avaliação baseado em conhecimento para o sistema Leonardo, com a capacidade de ajustar de forma dinâmica, à medida da perícia e conhecimento dos estudantes alvos do processo de avaliação, o seu comportamento, acompanhando de perto a evolução do processo de aprendizagem dos estudantes. Essencialmente, neste trabalho implementou-se a “máquina” de raciocínio para o sistema Leonardo poder sustentar de forma efetiva a avaliação de estudantes ao longo do tempo, numa ou mais áreas do conhecimento.",
    "authors": [
      "Coelho, João da Cunha"
    ],
    "keywords": [
      "Sistemas de avaliação de conhecimento",
      "Motor de inferência",
      "Tutor artificial",
      "Processo de avaliação",
      "Mecanismo de raciocínio",
      "Regra de produção",
      "Base de conhecimento",
      "Knowledge evaluation systems",
      "Inference engine",
      "Artificial tutor",
      "Assessment process",
      "Reasoning mechanism",
      "Production rule",
      "Knowledge base",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84076",
    "title": "Where@UM: onde é a sala da minha próxima aula?: o problema do posicionamento",
    "abstract": "Com a evolução natural da humanidade há uma preocupação crescente com o aumento das infraestruturas, por forma a acompanhar o aumento populacional. Essas infraestruturas assumem dimensõesde grande escala, o que torna a tarefa de navegação e posicionamento no seu interior complexa.A presente investigação, enquadrada no projeto Where@UM, procura desenvolver uma aplicaçãomóvel que auxilie a navegação dos pedestres, no interior dos campi da Universidade do Minho.Esta dissertação procura responder aos domínios da segurança e posicionamento indoor. Para talfoi desenvolvido um sistema de segurança robusto, garantindo questões de privacidade e de controlo deacesso a alguns serviços. Foi concebida uma aplicação móvel que faz uso deste sistema de segurança,a qual disponibiliza estratégias para que, de forma colaborativa, os utilizadores desta possam ter umcontributo ativo na criação e manutenção do mapa de rádio do sistema.Com objetivo de testagem da aplicação desenvolveu-se uma campanha de recolha de dados com 8participantes, por forma a testar a credibilidade e funcionalidade do sistema concebido. Foram encontrados dados que suportam a qualidade do sistema desenvolvido, com base na análise quantitativa dos dados recolhidos bem como por meio da opinião de satisfação dos participantes.",
    "authors": [
      "Pinheiro, Lázaro Donato Martins"
    ],
    "keywords": [
      "Aplicação móvel",
      "Segurança",
      "Posicionamento indoor",
      "Navegação indoor",
      "Leitura do ambiente rádio",
      "Mapa de rádio",
      "Fingerprints",
      "Mobile application",
      "Security",
      "Indoor positioning",
      "Indoor navigation",
      "Radio environment scan",
      "Radio map",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84475",
    "title": "LazyFS: a file system for assessing applications data durability",
    "abstract": "A atual era digital depende de dados numa perspetiva de grande escala e as organizações requeremsistemas de armazenamento que funcionem corretamente sob falhas. Por exemplo, falhas de energiapodem levar à perda de dados em aplicações cujos ficheiros ainda estão armazenados em memória,isto é, num meio volátil. Evitar estes cenários de perda de dados constitui um grande desafio, umavez que exige que os programadores apliquem primitivas de sincronização (fsync()) que garantem adurabilidade dos dados, a custo de uma potencial diminuição do desempenho das aplicações.As ferramentas de injeção de faltas permitem ajudar os programadores com testes automáticos econsequente validação das suas políticas de durabilidade de dados. No entanto, as abordagens atuaispara sistemas de ficheiros focam-se: (1) na manipulação direta de hardware; ou (2) em erros internosde implementação do sistema de ficheiros, e não na interação da aplicação com o mesmo. Além disso,estas ferramentas são limitadas quanto à informação disponibilizada ao programador, de forma a estecompreender a causa efetiva que levou à perda de dados reportada.Para resolver estes desafios, esta dissertação propõe o LazyFS, um sistema de ficheiros que simulaa perda de dados utilizando uma abordagem de injeção de faltas em software reprodutível e automática.Este sistema tem uma cache dedicada que gere os dados das aplicações e a sua sincronização parauma camada persistente. A pedido, o LazyFS pode limpar todos os dados que não foram previamentesincronizados, fornecendo também aos programadores informações relevantes sobre os dados em riscode serem perdidos com potenciais falhas de energia.O desempenho e validação da correção do protótipo demonstra que a nossa solução consegue avaliara durabilidade dos dados de aplicações, sem adicionar uma sobrecarga significativa à sua execuçãonormal. Foram também reproduzidas quatro anomalias em diferentes bases de dados e o protótipo já seencontra integrado na ferramenta de injeção de faltas Jepsen. Atualmente, o LazyFS está a ser usado,juntamente com o Jepsen, para avaliar sistemas de bases de dados em produção, como o PerconaMySQL Server, MongoDB e o etcd. Adicionalmente, foi descoberta uma possível violação de coerênciano etcd, que está a ser estudada pela sua equipa de desenvolvimento.",
    "authors": [
      "Azevedo, João Pedro Rodrigues"
    ],
    "keywords": [
      "Injeção de faltas",
      "Perda de dados",
      "Reprodutibilidade",
      "Sistema de ficheiros",
      "Data loss",
      "Fault injection",
      "File system",
      "Reproducibility",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92716",
    "title": "A deep-learning approach to detect scratches in vehicles",
    "abstract": "A deteção de danos na estrutura externa de veículos representa um desafio para os fornecedoresde serviços de viaturas de aluguer, especialmente os serviços mais recentes de mobilidade em que ainspeção de danos não é realizada no final de cada aluguer. À medida que estes serviços se tornam maispopulares e que mais pessoas deixam de ter necessidade de possuir um carro pessoal, espera-se queas empresas procurem formas de facilitar este tipo de inspeção de danos. Como tal, o objetivo principaldesta dissertação é desenvolver uma solução que os fabricantes de automóveis poderiam potencialmenteadotar. Esta solução envolve a criação de um algoritmo para detetar riscos em carros usando dados deáudio obtidos a partir de microfones.Este estudo explora a utilização de Aprendizagem Profunda na análise de imagens, por exemploespectrogramas, que são representativas de eventos de riscos. Para tal serão estudados e implementadosmétodos de transformação do sinal de áudio em imagens referentes a uma representação espectral doáudio e avaliar a capacidade de um algoritmo de Aprendizagem Profunda aprender a identificar este tipode eventos. O algoritmo é treinado tendo em conta a enorme variedade de sons que podem ser captadospelo microfone, considerando os infinitos ambientes possíveis a que um carro pode estar sujeito, devidoa uma condução diária.",
    "authors": [
      "Soares, André Rodrigues"
    ],
    "keywords": [
      "Inteligência Artificial",
      "Algoritmos",
      "Aprendizagem automática",
      "Aprendizagem profunda",
      "Redes neuronais",
      "Artificial Intelligence",
      "Machine learning",
      "Deep learning",
      "Algorithms",
      "Damage detection",
      "Neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86732",
    "title": "Integração de mecanismos de raciocínio adaptativo em sistemas de avaliação",
    "abstract": "Um sistema de raciocínio pode ser caracterizado como um agregado de componentes de software que realizam em conjunto processos de tomada de decisão complexos. Este tipo de sistema está bastante ligado a umadas áreas de trabalho mais mediáticas atualmente, a Inteligência Artificial. Algumas iniciativas de desenvolvimento dentro desta área tendem a incorporar este tipo de ferramenta em sistemas de avaliação, mais concretamente em tutores inteligentes, com o intuito de ajudar os estudantes no seu processo de aprendizagem. Nesta dissertação apresenta-se a conceção e a implementação de um conjunto de mecanismos de raciocínio baseado em casos e baseado em regras. Estes dois tipos de mecanismos foram idealizados para integrar o atual módulo de avaliação do sistema Leonardo, uma plataforma que complementa o estudo presencial dos alunos da Universidade do Minho. Os novos mecanismos, em particular os de raciocínio baseados em casos, complementam o processo de avaliação do sistema Leonardo aumentando as suas capacidades de raciocínio aquando da realização dos processos de avaliação estendendo as sessões de Quizz. Quanto aos mecanismos baseados em regras, estes representam uma importante camada entre o módulo de avaliação e a interface do tutor do sistema, visto que não permite apresentar questões de escolha múltipla na interface que não estejam de acordo com critérios estabelecidos por peritos. Nesta dissertação veremos como tais mecanismos foram fundamentados, desenvolvidos e integrados no sistema Leonardo.",
    "authors": [
      "Leite, Jaime Ricardo Faria"
    ],
    "keywords": [
      "Sistemas de eLearning",
      "Sistemas de avaliação",
      "Raciocínio baseado em casos",
      "Raciocínio baseado em regras",
      "Motor de raciocínio",
      "eLearning Systems",
      "Evaluation systems",
      "Case-based reasoning",
      "Rule-based reasoning",
      "Reasoning engine",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79821",
    "title": "Improving the efficiency of the energy-split tool to compute the energy of very large molecular systems",
    "abstract": "The Energy-Split tool receives as input pieces of a very large molecular system and computesall intra and inter-molecular energies, separately calculating the energies of each fragmentand then the total energy of the molecule. It takes into account the connectivity informationamong atoms in a molecule to compute (i) the energy of all terms involving atoms covalentlybonded, namely bonds, angles, dihedral angles, and improper angles, and (ii) Coulomband the Van der Waals energies, that are independent of the atom’s connections, whichhave to be computed for every atom in the system. The required operations to obtain thetotal energy of a large molecule are computationally intensive, which require an efficienthigh-performance computing approach to obtain results in an acceptable time slot.The original Energy-Split Tcl code was thoroughly analyzed to be ported to a parallel andmore efficient C++ version. New data structures were defined with data locality features, totake advantage of the advanced features present in current laptop or server systems. Theseinclude the vector extensions to the scalar processors, an efficient on-chip memory hierarchy,and the inherent parallelism in multicore devices. To improve the Energy-Split’s sequentialvariant a parallel version was developed using auxiliary libraries. Both implementationswere tested on different multicore devices and optimized to take the most advantage of thefeatures in high performance computing.Significant results by applying professional performance engineering approaches, namely(i) by identifying the data values that can be represented as Boolean variables (such asvariables used in auxiliar data structures on the traversal algorithm that computes theEuclidean distance between atoms), leading to significant performance improvements due tothe reduced memory bottleneck (over 10 times faster), and (ii) using an adequate compressformat (CSR) to represent and operate on sparse matrices (namely matrices with Euclideandistances between atoms pairs, since all distances further the cut-off distance (user defined)are considered as zero, and these are the majority of values).After the first code optimizations, the performance of the sequential version was improvedby around 100 times when compared to the original version on a dual-socket server. Theparallel version improved up to 24 times, depending on the molecules tested, on the sameserver. The overall picture shows that the Energy-Split code is highly scalable, obtainingbetter results with larger molecule files, even when the atom’s arrangement influences thealgorithm’s performance.",
    "authors": [
      "Pereira, Sara Alexandra da Silva"
    ],
    "keywords": [
      "Energy-split",
      "Intramolecular energy",
      "Development framework",
      "HPC",
      "Energia intramolecular",
      "Framework de desenvolvimento",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84079",
    "title": "Conception and evaluation of data augmentation techniques for tabular data",
    "abstract": "Imbalanced learning and small-sized datasets are present in Machine Learning problems, even with the increased data availability provided by recent developments. The performance of learning algorithmsin the presence of unbalanced data and significant class distribution skews is known as the imbalancedlearning problem. The models’ performance on such problems can drastically decrease for certain classeswith an uneven distribution, because the models do not learn the distributive features of the data andpresent accuracy too favorable for a specific set of classes of data. This can have negative consequenceswhen talking about cancer detection, for example, since the model may identify poorly unhealthy patients.Hence, Data Augmentation techniques are usually conceived to evaluate how models would behave in nondata-scarce environments, generating synthetic data similar to real data. By applying those techniques,the amount of available data can be increased, balancing the class distributions. However, there is nostandardized Data Augmentation process that can be applied to every domain of tabular data. Therefore,this dissertation aims to identify which characteristics of a dataset provide a better performance whensynthesizing samples by a data augmentation technique in a tabular data environment. Moreover, if thedata augmentation algorithm synthesizes more real samples, it is expected to increase the classifier’sperformance as well. Our results demonstrate that datasets whose features are mainly categorical havean associated difficulty in increasing the classifier results by adding new samples. Furthermore, thetechnique that adapted best to those kinds of datasets was the more classical one, SMOTE. As for thedatasets with more continuous features, the variations of Variational Autoencoder, principally the VAE withK-means and decay, as well as GAN, demonstrated an increased capability when augmenting those kinds ofdatasets. This dissertation demonstrated that more categorical datasets could achieve better performanceby including 25% synthetic samples, whereas continuous datasets could only do so by including minoritysamples.",
    "authors": [
      "Machado, Pedro Filipe Costa"
    ],
    "keywords": [
      "Data augmentation",
      "Imbalanced data",
      "Machine learning",
      "Dados desbalanceados",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84393",
    "title": "Hypatiamat: a funcionalidade de multijogador em jogos online",
    "abstract": "O desempenho escolar na área da Matemática é uma preocupação crescente juntoda comunidade educativa, tendo em conta o elevado insucesso escolar e correspondenteabandono escolar precoce. Os professores tentam perceber qual a melhor forma para captara atenção dos alunos ou incutir um maior interesse nestes relativamente a esta disciplina.Para além das técnicas utilizadas no ensino da matéria, os professores começam a aderir cadavez mais às novas tecnologias que auxiliam o ensino, uma vez que, estas permitem captaruma maior atenção por parte das crianças e jovens que, desde cedo, estão familiarizadascom as tecnologias atuais.O Hypatiamat é um projeto, orientado mais em específico para alunos do 1º ao 9º ano,cujo principal objetivo passa por despertar, junto dos alunos, o gosto pela Matemática econsequentemente aumentar o aproveitamento escolar nesta disciplina.Para este efeito, esta plataforma fornece diversos conteúdos hipermédia como explicações,resumos, aplicações, jogos, etc. Para além disso, esta plataforma realiza periodicamentediversos campeonatos com alguns dos seus jogos para milhares de alunos. No entanto, estespecam por não possuírem a opção de multijogador online, o que dificulta a sua realização eimpossibilita que os utilizadores possam jogar entre si os diversos jogos da plataforma deforma remota.Deste modo, esta dissertação centra-se na implementação deste modo multijogador onlinenum dos jogos do Hypatiamat, para futuramente servir como modelo de um guião a serimplementado nos restantes jogos que a plataforma Hypatiamat possui.",
    "authors": [
      "Costa, João da Cunha e"
    ],
    "keywords": [
      "Hypatiamat",
      "Matemática",
      "Jogos",
      "Multijogador online",
      "Math",
      "Games",
      "Online multiplayer",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79028",
    "title": "Software defined applications: a DevOps approach to monitoring",
    "abstract": "DevOps presents a mix of agile methodologies that allow an application’s release cycle to be shortened. Thistranslates into a faster delivery of value to the stakeholders.However, the value creation chain does not finish at the end of that cycle. It is necessary to monitor the artifactsproduced at a system level, and at the application level, in order to ensure the compliance of the functional andnon functional requirements.Today, there seems to be a clear separation between the monitoring process and the application developmentprocess. As the development and operations processes have merged in DevOps, this dissertation pretends toinvestigate how to integrate several aspects of monitoring into the regular lifecycle of an application’s development.The inclusion of external services further emphasizes the need to include an observability component into aninfrastructure.The main goal of this dissertation is to develop a solution for the deployment of an infrastructure using stateof-the-art technologies and frameworks, while also providing observability to the system and to the applicationsrunning on it.To do so, it required the investigation of the methodologies and concepts that are the base of the softwaredevelopment lifecycle, focusing on the latter stages of that process: the deployment, and monitoring phases.These methodologies and concepts were complemented with the study of state-of-the-art technologies andframeworks that aim to ease the burden of setting up an infrastructure quickly and with the necessary tools toevolve it after the initial setup and with each new software release. Furthermore, it also involved the research oftools that enable the collection of metrics from applications, as well as processing such data and displaying it inuseful ways for operators and stakeholders.In this context, this dissertation aims to provide a solution for the deployment of MobileID applications at INESCTEC, using the Mobile Driving Licence as the primary case study. The proposed design and implementationwith a container orchestration framework and CI/CD pipelines, enables faster development of different MobileIDapplications, while also providing continuous monitoring to the deployments.With this implementation, it was possible to assess how container orchestration frameworks provide greaterflexibility to applications, and how this observability can be augmented with the use of dedicated monitoringsystems.",
    "authors": [
      "Alves, Luís Miguel Andrade"
    ],
    "keywords": [
      "DevOps",
      "Software development",
      "Application deployment",
      "Monitoring",
      "Continuous integration",
      "Continuous delivery",
      "Virtualization",
      "Container orchestration",
      "Desenvolvimento de software",
      "Deployment de aplicações",
      "Monitorização",
      "Integração contínua",
      "Entrega contínua",
      "Virtualização",
      "Orquestradores de containers",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/70593",
    "title": "Privas: assuring the privacy in database exploring systems",
    "abstract": "Currently, given the technological evolution, data and information are increasingly valuable inthe most diverse areas for the most various purposes. Although the information and knowledgediscovered by the exploration and use of data can be very valuable in many applications,people have been increasingly concerned about the other side, that is, the privacy threats thatthese processes bring.This document follows an user-role approach within the data exploration process. Theseusers are: Data Provider (provides the data), Data Collector (collects and stores the dataprovided), Data Publisher (transforms data and publishes it to be explored) and Data Explorer(retrieves information from data). All of them have privacy concerns and can address themwith appropriate methods and techniques.In this Master thesis we built a system named Privas that aids the Data Publisher in itspublishing process. Currently he can assure the data privacy by adopting, manually choosingand then applying the privacy-preserving data publishing techniques (PPDP). Privas acceptsa repository with its description (written in a DSL) and creates a copy maintaining theinformation to be explored but assuring that involved individuals/organizations cannot beidentified by applying PPDP techniques. Privas automatically chooses the privacy models toapply according with the description, and applies the transformation. In the end of the processmetrics about the privacy loss are reported. The Domain Specific Language (DSL) – calledPrivasL – was developed to easily allow the original repository description, the identification ofthe data entities that one wants to explore and the definition of the privacy level to be assured.To visually help end-users to describe their repositories, a web platform was developed – whereafter describing the repository, the correspondent PrivasL description is generated.In the end, an analysis on different kind of repositories, with different information using thePrivas tool, was made – conclusions were drawn about transformations and in privacy loss.",
    "authors": [
      "Miguel, Joana Margarida"
    ],
    "keywords": [
      "Anonymization",
      "DSL",
      "PPDP",
      "Privacy",
      "Repositories",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47832",
    "title": "Classificação e monitorização escalável de serviços de vídeo",
    "abstract": "Face ao crescimento do volume de tráfego de vídeo na Internet, a monitorização eficiente de serviços de vídeo apresenta-se como um desafio de grande importância para a gestão das redes atuais e de próxima geração, onde múltiplos serviços, protocolos e tecnologias de acesso coexistem e competem por recursos. A monitorização eficiente de serviços envolvem a medição, ao precisa de parâmetros de interesse e o menor impacto possível na operação normal da rede. Neste sentido as técnicas de amostragem de tráfego procuram obter informações sobre todo o tráfego considerando apenas um subconjunto dos pacotes em trânsito na rede, apresentando-se como uma solução escalável para os desafios impostos pela monitorização de serviços de vídeo. Neste contexto, o presente trabalho de mestrado tem como principal objetivo analisar o desempenho da monitorização baseada em amostragem de tráfego na correta classificação e caracterização de serviços de vídeo.",
    "authors": [
      "Cunha, João Marcelo da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84628",
    "title": "Decoding human motion intentions from brain signals",
    "abstract": "Gait function can be affected by neurological disorders such as spinal cord injury (SCI), stroke, or traumaticbrain injury (TBI). These limitations have significant negative effects on the affected people’s independenceand quality of life. Brain-computer interfaces (BCIs) have the potencial to create solutions that may overcomeirreversible disabilities. Several studies in recent years have shown that electroencephalographic(EEG) signals can be used to develop BCIs for the rehabilitation of human limbs through lower-limbs roboticdevices and exoskeletons. Therefore, their effectiveness and safety depend on how successfully they candetect and react to movement.This dissertation aims at developing and validating an EEG-based motor intent decoding frameworkto accurately classify human intent regarding five daily performed locomotor tasks. This framework willcontribute on the developing of BCI to recover the mobility of neurologically impaired subjects. For this, aprovided multi-channel dataset will be used.The implementation of this solution was divided into two phases. The first is about how signals areprocessed to obtain the features that best characterize each of the locomotion modes under analysis.As a result, three distinct studies that differ in the number of channels used were created. Through theapplication of the ICA method, it has been determined that the more channels are used in a study, themore likely it is that these channels may be corrupted, affecting the ICA method’s effectiveness.The second section discusses the classification methodology. Three different Deep Learning algorithms,CNN, LSTM, and their combination, C-LSTM, were studied here. Additionally, three different features usedas the input for the models were compared for each of them and for each of the studies.The features that were selected showed a higher impact on the results than the actual classificationalgorithm, with ERPs being the features that produced the best results. On the other hand, across classifiers,all three provided high performance, demonstrating reduced differences between them. The study withhigher accuracy as the study 3 with the most reliable channel selection.",
    "authors": [
      "Gil, Ana Catarina Cardoso"
    ],
    "keywords": [
      "Brain-computer interface (BCI)",
      "Electroencephalogram (EEG)",
      "Motor intention decoding",
      "Signal processing",
      "Interface cérebro-computador (ICC)",
      "Eletroencefalografia (EEG)",
      "Decodificação de intenção motora",
      "Processamento do sinal",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59982",
    "title": "TOM Framework: uma ferramenta de testes baseados em modelos para interfaces gráficas web",
    "abstract": "As técnicas de teste baseados em modelos (do inglês, Model Based Testing (MBT)) comparamo comportamento do sistema sob teste com o comportamento do modelo do sistema(o oráculo). A aplicação de MBT às interfaces gráficas do utilizador (do inglês, GraphicalUser Interface (GUI)) permite uma avaliação mais exaustiva e contínua do sistema, atravésda simulação de ações do utilizador com a interface gráfica. Desta forma, é possível reduzirsignificativamente o custo de avaliação do sistema, e identificar, eventualmente, erros deimplementação através da GUI, sem o envolvimento de utilizadores externos. Este processodecorre através da execução dos casos de teste, gerados a partir do modelo do sistema, naaplicação sobre teste. São estes casos de teste que verificam se a implementação está deacordo com o modelo, assegurando assim uma melhoria da qualidade do sistema desenvolvido.Esta dissertação descreve uma ferramenta de MBT para aplicações web, a TOM Framework.Parte da framework (TOM Generator) aproveita trabalho anteriormente desenvolvido,a outra (TOM Editor) é aqui apresentada. Os objetivos principais da frameworkpassam por automatizar e facilitar a criação de modelos do sistema que, posteriormente,são utilizados para gerar automaticamente casos de teste executáveis na interface gráficasobre teste. A captura e interpretação da interação do utilizador com a aplicação web sobreteste foi um dos desafios ultrapassados no desenvolvimento desta dissertação. No final damesma, encontra-se uma aplicação da framework a um caso de estudo.",
    "authors": [
      "Pinto, Luís Miguel Carvalho"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56110",
    "title": "Seleção de genes diferencialmente expressos baseada em metodologia ROC (Receiver Operating Characteristic)",
    "abstract": "A análise da expressão genética é essencial para uma identificação da função dos genese para a identificação destes quando relacionados com doenças. Para a realização de umestudo em larga escala de mudanças na expressão genética é necessário encontrar ummétodo que o faça com precisão e exatidão. Desta forma, foi aqui incluída, uma análisepela tecnologia de microarrays, uma ferramenta importante no diagnóstico de doenças.A execução de um método que identificasse genes com regulação negativa e positiva egenes diferencialmente expressos simultaneamente, tornou-se, a principal motivação destetrabalho.De entre as diferentes técnicas estatísticas, a metodologia ROC (Receiver OperatingCharacteristic) foi a escolhida para o efeito.Quando se associa a metodologia ROC com a análise de dados de microarrays é possívelver que uma das principais aplicações é a identificação de grupos de genes associados aodesenvolvimento de qualquer patologia cancerígena. Para a análise deste último parâmetroé utilizado o arrow plot com a representação do OVL (Overlapping Coefficient) e da AUC(Area Under the Curve) para cada gene, numa experiência de microarays e comparar a suaeficácia com outros métodos existentes para o mesmo propósito.Através da análise de um conjunto de dados de pacientes afetados pelo adenocarcinomado pâncreas foi possível identificar os genes diferencialmente expressos, sendo este oprincipal objetivo do trabalho em questão.",
    "authors": [
      "Lemos, Catarina Isabel Ferreira Miranda"
    ],
    "keywords": [
      "Microarrays",
      "Genes",
      "Área abaixo da curva ROC",
      "Coeficiente de sobreposição",
      "Arrow plot",
      "Area under the ROC curve",
      "Overlapping coefficient",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92646",
    "title": "Imperative programs visualization with augmented reality",
    "abstract": "Este documento relata um Projeto de Mestrado, do segundo ano do Mestrado em EngenhariaInformática da Universidade do Minho, em Braga, Portugal. O projeto consiste no desenvolvimento de uma Aplicação Mobile de Realidade Aumentada a ser utilizada como Recurso Educacional para apoiar o ensino introdutório de Programação de Computadores, com o objetivo de aumentar a motivação e a perseverança dos estudantes, ajudando-os a superar certas dificuldades que muitos estudantes enfrentam na abstração e compreensão de diferentes tipos de estruturas de dados. Para atingir os objetivos do Projeto de Mestrado, foi necessário, no início, definir uma linguagem de programação imperativa com várias estruturas de dados e operações para as manipular e usar, para depois utilizar a Realidade Aumentada em telefones móveis para criar um sistema que ajude a visualizar e entender as referidas estruturas de dados. Nesta dissertação, a linguagem de programação referida é especificada e ilustrada com a ajuda de alguns exemplos. A arquitetura do sistema é proposto, e o seu desenvolvimento é descrito em detalhe, abordando os detalhes mais importantes. O sistema desenvolvido é apresentado, e os resultados obtidos a partir de um experimento envolvendo estudantes em uma sala de aula real são analisados.",
    "authors": [
      "Martins, Luis Carlos da Costa Salazar"
    ],
    "keywords": [
      "Pensamento computacional",
      "Recurso educacionais",
      "Realidade aumentada",
      "Programação de computadores",
      "Estruturas de dados",
      "Computational thinking",
      "Learning resources",
      "Augmented reality",
      "Computer programming",
      "Data structures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27870",
    "title": "Implementation and test of transactional primitives over Cassandra",
    "abstract": "NoSQL databases opt not to offer important abstractions traditionallyfound in relational databases in order to achieve high levels of scalability andavailability: transactional guarantees and strong data consistency. Theselimitations bring considerable complexity to the development of client applicationsand are therefore an obstacle to the broader adoption of the technology.In this work we propose a middleware layer over NoSQL databases thatoffers transactional guarantees with Snapshot Isolation. The proposed solutionis achieved in a non-intrusive manner, providing to the clients the sameinterface as a NoSQL database, simply adding the transactional context. Thetransactional context is the focus of our contribution and is modularly basedon a Non Persistent Version Store that holds several versions of elementsand interacts with an external transaction certifier.In this work, we present an implementation of our system over ApacheCassandra and by using two representative benchmarks, YCSB and TPC-C,we measure the cost of adding transactional support with ACID guarantees.",
    "authors": [
      "Coelho, Fábio André Castanheira Luís"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27888",
    "title": "Mining images of microbial communities for morphological characteristics in a support of clinical decision making",
    "abstract": "Um biofilme é uma comunidade de microrganismos envoltos por uma matriz extracelularproduzida pelos próprios, que lhes garante proteção. Os biofilmes representam um problema paraa saúde pública pois facilmente encontram-se em dispositivos médicos, podendo causarproblemas graves para os pacientes.Estudos prévios indicam algumas alterações observáveis do aspeto físico e bioquímico dascomunidades microbianas na resposta à resistência e à virulência. Assim a morfologia dacomunidade pode ser um indicativo da reação regulatória associada com fenómenos depatogenicidade microbiana.O objetivo deste trabalho é por um lado a criação de um novo sistema de classificação demorfologia de colonia com medidas extraídas de softwares de imagem por outro lado, o estudo daclassificação morfológica existente e do novo sistema de classificação, através de técnicas demineração de dados com o objetivo de ajudar nestas classificações. Apresentamos váriossoftwares como solução que vão desde a caraterização da estrutura dos biofilmes até acaraterização de morfologia de colonia",
    "authors": [
      "Domingues, Ana Catarina de Jesus"
    ],
    "keywords": [
      "Biofilmes",
      "Morfologia de colonia",
      "Processamento de imagens",
      "Mineração de dados",
      "Biofilms",
      "Colony morphology",
      "Image processing",
      "Data mining",
      "681.3:57",
      "57:681.3"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3:57",
      "57:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/66519",
    "title": "Melhorar a taxa de fermentação da xilose em etanol em Saccharomyces cerevisiae através da incorporação de informações estruturais de proteínas",
    "abstract": "O consumo crescente do petróleo constitui um grave problema ambiental e económico para a sociedade atual. Para solucionar este problema, o uso de biocombustíveis apresenta-se como uma viável alternativa. Através do aproveitamento da cana-de-açúcar e do milho, utilizando microrganismos, é possível obter etanol, o biocombustível mais utilizado atualmente. Porém, nem todos os açúcares provenientes destas matérias-primas são fermentados de forma economicamente eficiente, sendo que a D-xilose, o segundo açúcar mais abundante, continua a ter um aproveitamento ineficiente por parte de microrganismos como a Saccharomyce cerevisiae, apesar das alternativas já exploradas. A via de fermentação da xilose é constituída por diversas enzimas, sendo que o problema da fraca taxa de conversão de xilose em etanol se deva essencialmente às primeiras duas: a xilose redutase e a xilitol desidrogenase.Muitas das abordagens para solucionar o ineficiente consumo da D-xilose têm por base a engenharia metabólica, ou como tornar a xilose redutase ou a xilitol desidrogenase específica para o mesmo cofator. Este trabalho tem o objetivo de descobrir caraterísticas estruturais, na xilose redutase e na xilitol desidrogenase, que possam ter influência na afinidade da D-xilose e no seu modo de ligação. Para tal foram usadas e testadas abordagens in silico tendo em consideração a estrutura das proteínas e os açúcares que estas utilizam como substrato, deixando para segundo plano os cofatores e o metabolismo. Foram recolhidas diversas sequências de xilose redutases e xilitol desidrogenases de diversos organismos e as suas estruturas tridimensionais modeladas. Para esses modelos foi medido o volume dos seus centros ativos e realizado o docking molecular da D-xilose, xilitol e outros substratos. Os resultados do docking e dos volumes foram comparados com os KM dos diferentes substratos.Praticamente todos os volumes da xilose redutase foram corretamente medidos, o que não se verificou no caso da xilitol desidrogenase. Relativamente ao docking molecular, foram analisados os resíduos envolvidos nos processos catalíticos destas duas enzimas bem como os scores resultantes. A Candida tenuis e Candida boidiini foram as que apresentaram uma maior afinidade no docking da xilose com um valor de 5,1. No docking do xilitol nas xilitol desidrogenases a Rhizomucor pusillu foi a que teve melhor score com 5,1.Este trabalho evidenciou que os volumes do centro ativo e o KM aparentam não estar diretamente relacionados. Os resíduos N e H do centro catalítico da xilose redutase estão conservados em todas as xilose redutases e estão envolvidos nas interações polares com a xilose e outros açúcares.",
    "authors": [
      "Nóvoa, Cláudio Filipe Ferreira"
    ],
    "keywords": [
      "Xilose redutase",
      "Xilitol desidrogenase",
      "D-xilose",
      "Xilitol",
      "Docking",
      "Volume",
      "Xylitol",
      "Xylose reductase",
      "Xylitol dehydrogenase",
      "D-xylose",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84494",
    "title": "Automatic detection of daily living activities in people with Parkinson’s disease using kinematic-driven data",
    "abstract": "Parkinson’s Disease (PD) is a neurodegenerative disorder of the central nervous system. Resting tremor, akinesia, and bradykinesia (slow movements), rigidity, shuffling walking, and postural instabilityare some of the symptoms that not only negatively impacts patients’ life, but also the life of people aroundthem.Current approaches for monitoring patients’ motor autonomy are limited to the observer and self reported methods. The observer-based examinations, patients perform a set of standard PD examinations.The self-reported method relies on patients’ daily activities diaries. These approaches are commonly used,but are limited to a few sessions per year, they do not address common motor daily tasks, and their results are object of subjective interpretation by the clinical expert.By combining kinematic-driven data from wearable sensor with AI, the main goal of this dissertation is to develop an automatic software for recognition of human activities (e.g., walking, standing, turning, sitting, and lying) in PD to assist the clinical experts with objective and concrete data.A data collection protocol was developed and captured, resulting in a database comprised of data collected from eighteen PD patients who performed three trials of six different daily activities: walk; 180º turning; sit on chair; get up from chair; lay on bed and get up from bed.A Deep Learning (DL) framework based on Convolutional Neural Network capable of recognizing daily activities was developed and attained a performance of F1 Score equal to 0.90892.As a complementary goal an automatic software for human walk initial contact (IC) and final contact (FC) recognition using kinematic data was also developed. IC and FC are tremendously important to provide patient on-demand motor assistance and estimation of walking-associated metrics.A Deep Learning framework based on Bidirectional Long Short-Term Memory Neural Networkcapable of walking IC/FC events detection was developed and attained a performance of MCC Score equal to 0.538386.Promising results were attained for both DL frameworks, however, this dissertation suggests that there is still room for further improvements. Enriching the dataset with more data from different patient, data balancing and feature extraction techniques, experimenting new models’ architectures should be considered in future works.",
    "authors": [
      "Abreu, Luís Filipe Simões de"
    ],
    "keywords": [
      "Parkinson’s Disease",
      "Deep learning",
      "Human activity recognition",
      "Doença de Parkinson’s",
      "Reconhecimento de atividade humana",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/38679",
    "title": "Desenvolvimento de sistema de alarmes para incontinentes",
    "abstract": "A incontinência urinária, é um problema de saúde com múltiplas repercussões, que interferem negativamente na qualidade de vida dos doentes. Aliado a este facto, está a dificuldade em realizar diagnósticos corretos acerca do grau de incontinência. Sem um diagnóstico correto o tratamento, que já por si é complicado, incorre em dificuldades adicionais. Assim, tem-se assistido ao desenvolvimento de vários produtos para incontinência, cujo objetivo é melhorar a qualidade de vida dos pacientes, face aos danos causados pela doença. Tratam-se de sistemas concebidos para aumentar a autoestima das pessoas na sua presença em público, face ao receio de algum evento de incontinência. Nesse sentido, o principal objetivo deste trabalho, é a criação de um sistema de alarme para incontinentes, que é constituído por uma componente de deteção de líquido, incorporada em roupa interior específica para incontinentes e uma componente de geração de alarme, que corresponde a um dispositivo android . A comunicação entre os dois dispositivos é estabelecida, através do protocolo Bluetooth. A componente de deteção de líquido, inclui a implementação de um sensor de líquido baseado em materiais fibrosos, integrado no substrato têxtil específico para incontinentes. É também responsável por, processar a informação proveniente do sensor e por comunicar com o dispositivo android . Os resultados provenientes do sensor consistem na resistência elétrica produzida, relacionada com a quantidade de líquido libertado. Por sua vez, a componente de geração de alarme, é uma aplicação móvel, concretizada com o propósito de emitir alarmes, quando recebe sinais provenientes do dispositivo. Além disso, fornece uma interface de configuração do dispositivo de deteção, para o nível de alarme desejado. Permite ainda registar eventos de incontinência numa base de dados local. O trabalho desenvolvido tornou possível a conceção e implementação de um sistema de alarme para incontinentes, sendo que o nível de alarme pode ser configurado em função da quantidade de líquidos. A existência duma base de dados com registo de eventos permite a monitorização em modo offline por períodos mais longos.",
    "authors": [
      "Soares, Roberto da Silva"
    ],
    "keywords": [
      "Incontinence",
      "Alarm",
      "Bluetooth",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82115",
    "title": "Micas, a web platform to support teachers of computing at school",
    "abstract": "This document presents the motivation, development and results of a Masters Thesis workin Informatics focused on Computational Thinking Education, that was accomplished atUniversidade do Minho in Braga, Portugal. This thesis is based on a big ontology thatdescribes in detail the concepts ’Computational Thinking’ and ’Programming’, which mapsthose concepts to different education levels, starting with the first year. The main goal isthe development of a Web Platform that, on one hand, helps on collecting in a repositoryand classifying any kind of resources to be used by teachers on computing classes and, onthe other hand, helps on the retrieval from that repository of the most adequate resourcesto teach a specific subject to a specific level. The classification and the intelligent searchmechanism will follow the knowledge description defined by the ontology.",
    "authors": [
      "Azevedo, Ana Cristina Branco"
    ],
    "keywords": [
      "Education",
      "Computational thinking",
      "Web platform",
      "Programming",
      "Ontology",
      "Educação",
      "Pensamento computacional",
      "Plataforma web",
      "Programação",
      "Ontologia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92636",
    "title": "Monitorização do estado do pavimento e da congestão das vias em cidades inteligentes",
    "abstract": "Nos últimos anos, cada vez mais pessoas que anteriormente viviam em zonas rurais migram para centros urbanos à procura de novas oportunidades. Face a este movimento, vários problemas e adversidades foram-se agravando, nomeadamente, o aumento do fluxo rodoviário, que cria problemas de trânsito, o aumento dos níveis de poluição, o acesso à saúde, entre outros. Desta forma, torna-se imperativo gerir de forma eficaz e sustentável os recursos, com a finalidade de melhorar a qualidade de vida dos habitantes destas cidades.Neste contexto, juntamente com os avanços tecnológicos que se tem observado, surge o conceito de Cidades Inteligentes, que recorrendo a redes de sensores recolhem todos os dados necessários para ”virtualizar” as cidades. Desse modo, a informação coletada está centralizada, para que assim seja possível gerir os recursos disponíveis de forma informada, responsável e eficiente, para que seja possível responder às necessidades da população. Com este trabalho, pretende-se estudar dois problemas concretos no âmbito das Cidades Inteligentes, nomeadamente na área do Transporte Inteligente, recorrendo à simulação de redes de sensores, constituídas por sensores de aceleração instalados na rede de transporte públicos da cidade, a partir da qual vão ser recolhidos dados. O primeiro problema que se tenciona solucionar está relacionado com a monitorização do estado do pavimento. Com os dados provenientes dos acelerómetros, espera-se ser possível estimar o estado de conservação das vias rodoviárias e, desta forma, as entidades responsáveispassam a ser capazes de realizar decisões informadas e apropriadas face ao estado de determinada estrada, procedendo assim à sua restauração caso necessário. Uma segunda vertente que se pretende explorar foca a monitorização da congestão das vias rodoviárias em que, com base na mesma rede de transportes, se projeta ser possível determinar os níveis de fluxo rodoviário. Por fim, é ainda expectável que beneficiando dos transportes públicos dos quais já se está a tirar proveito, seja plausível medir os níveis de poluição aérea.",
    "authors": [
      "Oliveira, Carolina Castro de"
    ],
    "keywords": [
      "Cidades inteligentes",
      "Transporte inteligente",
      "Monitorização do tráfego",
      "Estado do pavimento",
      "Acelerómetros",
      "Internet das coisas",
      "Sensores",
      "Smart Cities",
      "Smart transportation",
      "Traffic monitoring",
      "Pavement condition",
      "Accelerometers",
      "Internet of things",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92828",
    "title": "MitoProfiles: cancer mitochondrial profiles in high metabolic rate organs",
    "abstract": "Metabolic reprogramming is recognized as a critical hallmark of cancer, influencing cancer initiation and progression. Emerging evidence suggests that the metabolism of non-cancer cells within the tumor microenvironment plays a pivotal role in modulating tumor development, underscoring the importance of metabolic variables for better understanding cancer. The main goal of this study is to identify genes exhibiting differential expression in cancer, with a specific emphasis on distinguishing between organs with high metabolic rates (brain, liver, and kidneys) and organs with low metabolic rates (bladder, colon, and skin), particularly focusing on genes encodingmitochondrial proteins. For this, we used two databases containing RNA-seq samples from normal and cancer tissues, obtained from the Genotype-Tissue Expression (GTEx) and The Cancer Genome Atlas (TCGA) projects, respectively. General Linear Models (GLMs) were applied for differential expression analysis, and hierarchical clustering e soft fuzzy clustering to identify distinct gene expression profiles. Our research showed that many of the differentially expressed mitochondrial genes, such as ACSM1and ACSM5, and PRODH, represent potential adaptations of cancer cells to metabolic and micro environmental stress. Additionally, FDX2, a crucial player in iron-sulfur protein biogenesis, and ACSM2B, responsible for catalyzing the activation of free fatty acids (FFAs) to CoA, showed substantial expression differences, highlighting the importance of these two pathways for the oncogenic process. The most sub stantial genetic expression differences were observed between normal and cancer tissues, rather than between high and low metabolic rate organs, suggesting that the signal from the metabolic rate could be masked by the pronounced changes that cancer induces in cells. Despite the unequal sample sizes and the usage of two different data sources, our findings provide valuable insights into the complex interplay between metabolism and gene expression in cancer.",
    "authors": [
      "Ferreira, Catarina Gomes"
    ],
    "keywords": [
      "Cancer",
      "Metabolic rate",
      "Mitochondrial proteins",
      "Differential gene expression",
      "Clustering",
      "Cancro",
      "Taxa metabólica",
      "Proteínas mitocondriais",
      "Expressão genética diferencial",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83637",
    "title": "Infrastructure as code: analysis of misconfiguration and non-compliance problems",
    "abstract": "Infrastructure as Code (IaC) is an innovative DevOps approach to infrastructure configuration and management.Instead of using traditional interactive tools — such as command line — or cloud provider webinterfaces, it automates several tasks through extensive use of scripting languages and tools.Being a relatively new field, with a fast-paced developing set of tools, it is of crucial importance to assistits users and its developers to tackle security concerns that might affect the environments these tools aremeant to manage. Some of those security concerns must always be handled within an actual live, runningenvironment. This is the case, for example, of checking for service availability. Issues like this are alreadybeing addressed by existing dynamic analysis tools. Others should be handled using a static analysisapproach, which, in turn, should prevent those security concerns from ever becoming a live security issue.In this dissertation, we focus on trying to bridge the gap between the set of security checks currentlybeing addressed by tools that follow these approaches. We identify 150 security checks currently beingperformed only by dynamic analysis tools, and we implement 23% of them in KICS, a Checkmarx-backed,open source, static code analysis tool for IaC solutions.The new checks we contribute to KICS address misconfiguration and non-compliance problems that canbe prevented using static analysis, mainly focusing on access control, but also on network security. Overall,this dissertation addresses 34 security checks, effectively bridging the gap between static and dynamicanalysis for IaC in the KICS context.Although not always possible, we strive to make available each security check to Ansible, CloudFormation,and Terraform. These new security checks and the necessary changes to KICS were submitted to the GitHubproject’s repository, were approved by the KICS team, and are now into its master branch. This means thatnew KICS releases will make available these security checks to its current users and to a broader audience,and, hopefully, will foster the development of community-based extensions and enhancements, such assupport for other IaC platforms and security domains that we were unable to tackle due to time constraints.",
    "authors": [
      "Silva, Rafaela Maria Soares da"
    ],
    "keywords": [
      "DevOps",
      "Dynamic analysis",
      "Infrastructure as code",
      "KICS",
      "Static analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47696",
    "title": "Creating intelligible metrics road traffic analysis",
    "abstract": "The increasing pervasiveness and lower cost of electronic devices equipped with sensorsis leading to a greater and cheaper availability of localized information. The advent ofthe internet has brought phenomena such as crowd-sourced maps and related data. Thecombination of the availability of mobile information, community built maps, with theadded convenience of retrieving information over the internet creates the opportunity tocontextualize data in new ways.This work takes that opportunity and attempts to generalize the detection of drivingevents which are deemed problematic as a function of contextual factors, such as neighbouringbuildings, areas, amenities, the weather, and the time of day, week or month.In order to research the problem at hand, the issue is first contextualized properly, providingan overview of important factors, namely Smart Cities, Data Fusion, and MachineLearning.That is followed by a chapter concerning the state of the art, that showcases relatedprojects and how the various facets of road traffic expression are being approached.The focus is then turned to creating a solution. At first this consists in aggregating dataso as to create a richer context than would be present otherwise, this includes the retrievalfrom different services, as well as the composition of a unique view of the same drivingsituation with new dimensions added to it. And then Models were created using differentMachine Learning methods, and a comparison of results according to selected and justifiedevaluation metrics was made. The compared Methods are Decision Tree, Naive Bayes, andSupport Vector Machine.The different types of information were evaluated on their own as potential classifiers andthen were evaluated together, leading to the conclusion that the various types combinedallow for the creation of better models capable of finding problems with more confidencein such results.According to the tests performed the chosen approach can improve the performanceover a baseline approach and point out problematic situations with a precision of over 90%.As expected by not using factors concerning the driver state or acceleration the scope ofproblems which are detected is limited in domain.",
    "authors": [
      "Quintas, Artur Filipe Freitas"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80970",
    "title": "Detection of anonymized traffic: Tor as case study",
    "abstract": "This master thesis studies Tor, an anonymous overlay network used to browse the Internet.It is an open-source project that has gain popularity mainly because it does not hide itsimplementation. In this way, researchers and security experts can examine and confirm itssecurity requirements.Its ease of use has attracted all kinds of people, including ordinary citizens who wantto avoid being profiled for targeted advertisements or circumvent censorship, corporationswho do not want to reveal information to their competitors, and government intelligenceagencies who need to do operations on the Internet without being noticed. In opposite, ananonymous system like this represents a good testbed for attackers, because their actionsare naturally untraceable.Traffic characteristics are studied in detail, which can be used to detect Tor. Further,a detection mechanism was developed to prevent users from reaching the Tor network.Finally, some changes are proposed so that Tor can better disguise its traffic with traditionalweb browsing traffic to overcome any intention of blocking it.",
    "authors": [
      "Dantas, Bruno Rafael Lamas Corredoura"
    ],
    "keywords": [
      "Anonymity",
      "Privacy",
      "Security",
      "Tor",
      "Traffic classification",
      "Anonimato",
      "Classificação de tráfego",
      "Privacidade",
      "Segurança",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28354",
    "title": "Estimativa de funções de probabilidade cumulativa em redes de larga escala",
    "abstract": "A capacidade de agregar dados é uma característica fundamental na conceção de sistemas de informação escaláveis, que permite a determinação de propriedades globais importantes de forma descentralizada, para a coordenação de aplicações distribuídas, ou para fins de monitorização.Agregados simples como mínimos/ máximos, contagens, somas e médias foram já extensivamente estudados no passado. No entanto, este tipo de agregados pode não ser suficiente para caracterizar distribuições de dados enviesadas e na presença de valores atípicos (outliers), tornando-se então relevante a determinação de uma estimativa dos valores na rede (e.g. histograma, função de distribuição cumulativa), dado que métricas como médias ou desvio padrão escondem em muitos casos alterações na propriedade monitorizada que são relevantes para decisão de controlo.São ainda relativamente escassos os trabalhos que se focam sobre a agregação de métricas mais expressivas. Uma proposta recente nesse domínio [SNSP10] refere atingir uma precisão nas estimativas superior à atingida em abordagens anteriores. Trata-se de um algoritmo para a determinação de funções cumulativas de distribuições.Apesar do contributo, essa proposta mostra limitações na tolerância a faltas e no suporte à monitorização contínua de propriedades, dado que para acompanhar alterações dos valores amostrados, a estratégia usada exige que o protocolo seja reiniciado periodicamente. Para além disso, os pressupostos dessa abordagem não admitem a perda de mensagens nem a sua duplicação.Assim, e tomando como ponto de partida o actual estado da arte, é apresentado nesta tese um algoritmo distribuído para a determinação de funções cumulativas de probabilidade em redes de larga escala. As suas principais vantagens são a imunidade à perda de mensagens, a velocidade de convergência e a precisão que se obtém na aproximação à distribuição original. É simultaneamente adaptável a alterações no valor amostrado e resiliente a dinamismo no número de nodos na rede. Usa também um mecanismo de quiesciência dos nodos assim que a variação local da estimativa é inferior a um determinado limiar. Nessa circunstância, o nodo deixa de transmitir. Isto leva à diminuição do número de mensagens trocadas entre nodos.As distribuições determinadas em todos os nodos permitem a tomada de decisões que tirem partido do facto de se estar a agregar uma função probabilística. Assim o nodo pode excluir outliers ou observar determinados quantis da propriedade. Para além disso, cada nodo da rede possui uma estimativa global sobre o estado geral da propriedade distribuída, o que lhe permite também a tomada de decisões com base em conhecimento local.São apresentados nesta tese resultados de simulação que confirmam a validade da abordagem seguida. É também apresentada uma revisão da literatura relacionada cujo âmbito incluiu as técnicas mais representativas da agregação de dados para métricas escalares e as técnicas de agregação de dados para métricas complexas.",
    "authors": [
      "Silva, Miguel Ângelo Borges da"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82753",
    "title": "Parallelization of the ADI method exploring vector computing in GPUs",
    "abstract": "The 2D convection-diffusion is a well-known problem in scientific simulation that often usesa direct method to solve a system of N linear equations, which requires N3 operations.This problem can be solved using a more efficient computational method, known as thealternating direction implicit (ADI). It solves a system of N linear equations in 2N times withN operations each, implemented in two steps, one to solve row by row, the other column bycolumn. Each N operation is fully independent in each step, which opens an opportunity toan embarrassingly parallel solution. This method also explores the way matrices are stored incomputer memory, either in row-major or column-major, by splitting each iteration in two.The major bottleneck of this method is solving the system of linear equations. Thesesystems of linear equations can be described as tridiagonal matrices since the elements arealways stored on the three main diagonals of the matrices. Algorithms tailored for tridiagonalmatrices, can significantly improve the performance. These can be sequential (i.e. the Thomasalgorithm) or parallel (i.e. the cyclic reduction CR, and the parallel cyclic reduction PCR).Current vector extensions in conventional scalar processing units, such as x86-64 andARM devices, require the vector elements to be in contiguous memory locations to avoidperformance penalties. To overcome these limitations in dot products several approachesare proposed and evaluated in this work, both in general-purpose processing units and inspecific accelerators, namely NVidia GPUs.Profiling the code execution on a server based on x86-64 devices showed that the ADImethod needs a combination of CPU computation power and memory transfer speed. Thisis best showed on a server based on the Intel manycore device, KNL, where the algorithmscales until the memory bandwidth is no longer enough to feed all 64 computing cores. Adual-socket server based on 16-core Xeon Skylakes, with AVX-512 vector support, proved tobe a better choice: the algorithm executes in less time and scales better.The introduction of GPU computing to further improve the execution performance (andalso using other optimisation techniques, namely a different thread scheme and sharedmemory to speed up the process) showed better results for larger grid sizes (above 32Ki x32Ki). The CUDA development environment also showed a better performance than usingOpenCL, in most cases. The largest difference was using a hybrid CR-PCR, where the OpenCLcode displayed a major performance improvement when compared to CUDA. But even withthis speedup, the better average time for the ADI method on all tested configurations on aNVidia GPU was using CUDA on an available updated GPU (with a Pascal architecture) andthe CR as the auxiliary method.",
    "authors": [
      "Silva, Filipe Pereira da"
    ],
    "keywords": [
      "Master thesis",
      "GPU Computing",
      "Physics",
      "HPC",
      "Mathematics",
      "Dissertação de mestrado",
      "Computação em GPU",
      "Física",
      "Matemática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86807",
    "title": "Implementation of practical and secure methods for storage of cryptographic keys in applications",
    "abstract": "Encryption has been essential to protect modern systems and services. It became the security foundation ofdatabases, payment systems, cloud services, and others. Cryptography enabled the creation and validation ofdigital signatures, where the protection of the private key is very important to prevent false signatures. Cryptocur rencies rely on this mechanism.Crypto wallets hold private keys used to sign transactions and prove ownership of a digital asset. These haveto keep the private key secure, but accessible to its owner, as it may be needed frequently. With the increasingnumber of decentralized web applications that interact with a blockchain, this subject has become more prevalent,as they usually require frequent signatures from the user.The mass adoption of cryptocurrencies by non-technical users urged the creation of crypto wallets that aresecure but prioritize usability. Some of these are hosted services that store the private keys in their serversand others are non-hosted, where the user is responsible for storing it. When implemented as a browser plugin,these wallets allow the user to seamlessly interact with a web application. The rise of cloud technology broughtforth multi-signature on the cloud, by combining different cloud services owned by the user. These give the usercontrol of his private key and are less vulnerable to cyber attacks.In this work, it is presented a comprehensive analysis of existing crypto wallet approaches in usability andsecurity to understand the existing problems. The next step was to propose multiple possible solutions to thoseproblems and produce their implementations. These take advantage of previously studied multi-cloud technologyand are used to attempt to improve usability and security. To evaluate the proposed solutions and to comparethem to the existing ones, we have developed a framework that consisted of various objective tests based onprevious work, which have the goal of evaluating security and usability.Finally, the proposed and existing solutions were compared using the proposed framework.",
    "authors": [
      "Lopes, João Nuno Alves"
    ],
    "keywords": [
      "Data encryption",
      "Multi-cloud",
      "Crypto wallet",
      "Encriptação de dados",
      "Multi-nuvem",
      "Carteira digital",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80740",
    "title": "Conversão de esboços de páginas Web para HTML usando aprendizagem automática",
    "abstract": "In the last decades, there has been an exponential development in the area of computing,which includes Artificial Intelligence (AI). The development of AI translates into the emergence of programs that replicate the ability to make decisions, perceive and solve problemsin a similar way to humans. Today, artificial intelligence is already part of various areas ofsociety, such as security, health, or virtual assistants.This dissertation aimed to develop a Web application that converts graphical interfacesketches, elaborated with the Balsamiq Mockups application, into HTML, CSS and Bootstrap code. Converting a Web page sketch into code is a task that developers typicallyperform. Due to the time consuming of this task, it becomes impossible to devote moretime to the application logic. On the other hand, it is a repetitive and tedious task.Two deep neural network models were built, divided into two distinct approaches. Thefirst approach, presenting poor results, uses a convolutional network and two recurringnetworks, according to an encoder-decoder architecture, similar to image captioning. It alsouses a DSL language and a compiler that transforms DSL into code. The second approachis completely different and it is more focused on the spatial component of the addressedtask. It uses YOLO model and a layout algorithm that converts the output of YOLO intocode.In the same test set, the first approach achieves 71.30% accuracy, while in the secondapproach it yields 88.28% accuracy.The Web application, which allows the user to upload images and automatically generateHTML, CSS and Bootstrap code, is supported by the YOLO based model as it gives betterresults.",
    "authors": [
      "Bouças, Tiago André Alves"
    ],
    "keywords": [
      "Artificial intelligence",
      "Neural network",
      "Deep",
      "Convolutional network",
      "Recurring network",
      "YOLO",
      "Web application",
      "Inteligência artificial",
      "Rede neuronal",
      "Profunda",
      "Convolucional",
      "Recorrentes",
      "Aplicação web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28221",
    "title": "Geração automática de código para padrões de conceção",
    "abstract": "O recurso a ferramentas de geração automática de código permite economizar tempo quando se desenvolvem soluções de software, factor importante em questões de produtividade.Existe um conjunto de padrões de conceção [Gamma et al., 1995] que representam soluções genéricas para problemas relativos ao desenvolvimento de aplicações de software, numa perspetiva orientada aos objetos. Para cada um deles pode ser vista a sua estrutura de classes, métodos e relacionamentos, bem como as situações mais adequadas para a sua utilização. Bastará consultar o catálogo de padrões de conceção [Gamma et al., 1995] e utilizar aquele que mais se adequar à resolução de determinado problema que surja no desenvolvimento de um novo programa.A existência de uma aplicação de software capaz de fazer a geração automática do código associado aos padrões de conceção, agiliza o desenvolvimento de novas aplicações, porque fornece de imediato o respetivo código.O que se propõe com o desenvolvimento desta dissertação é uma solução de software, capaz de efetuar a geração automática de código para os padrões de conceção catalogados em [Gamma et al., 1995]. Juntamente com o programa desenvolvido, é também apresentado um levantamento do estado da arte sobre os padrões de conceção, considerando também situações atuaisda sua aplicabilidade. Em seguida, é descrita a especificação da aplicação elaborada, bem como o seu processo de desenvolvimento, acompanhado de um exemplo de utilização. Por fim, encontram-se dois casos de estudo, servindo para provar que o programa elaborado pode ser utilizado em contextos reais.",
    "authors": [
      "Neto, Jaime Emanuel Vieira dos Santos Moura"
    ],
    "keywords": [
      "Geração de código",
      "Padrões de concepção",
      "Code generation",
      "Design patterns",
      "681.3.06",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.06"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/95647",
    "title": "Improving the performance of liquid surfaces modelling in multicore devices",
    "abstract": "When assembling bottom terminated components in printed circuit boards, connectivity isextended through metallized terminals. To minimize thermal fatigue failure of the welds, softwaretools have been developed to model liquid surfaces shaped by various forces and constraints.Surface Evolver (SE) is the software tool used by Bosch in their media entertainment products tomodel liquid surfaces through the analysis of discrete parts of that surface. However, dependingon the level of detail, this process may have long execution times, which is not consistent withthe demand of industry and mainly in an interactive software where users expect the results tobe obtained quickly.This dissertation aims to improve the efficiency of SE, through the optimization of the totalenergy computation, taking advantage of vectorization, parallel computing and other highperformance techniques.The analysis and profile of the current SE version were crucial to support the decisions takento improve the computational performance of the software. Scalability tests, taking into accountthe Amdahl’s law, call graphs and other profiling analysis helped to identify bottlenecks, wherean effort should be invested to improve the software. One of the heaviest computations identifiedin SE is the computation of the total energy of the configuration.SE was identified to be a memory-bounded software, mainly due to its current mesh datastructure, implemented with linked lists, which limits the use of the vectorization features oncurrent CPU cores and also does not support data parallelization techniques and data locality.A new data structure was proposed to overcome these performance constraints, which led to afaster execution of SE.The results showed an improvement on the total energy computation, an increase of vectorizableoperations, software prefetching techniques and scheduling optimizations which, alongsidethe alternative data structure, increased the performance of the SE.",
    "authors": [
      "Ribeiro, José Ricardo Cunha da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92821",
    "title": "Estratégias de deployment em arquitetura de microsserviços",
    "abstract": "Com a chegada das metodologias Agile, passou a ser possível a entrega e mudanças mais rápidas do software (1), aumentando o ritmo de desenvolvimento de toda a indústria de software, que trouxe como resultado inevitável o aumento do número de deployment de software desenvolvido de forma a satisfazer a necessidade de entregas ao cliente. Estas metodologias provocaram também a mudança de paradigma de sistemas monolíticos para a utilização de microsserviços, pois o crescente ritmo de desenvolvimento tornou a gestão de sistemas monolíticos insustentável, sendo vantajosa a utilização de microsserviços pela sua manutenção, reusabilidade, escalabilidade e disponibilidade ser facilitada (2). No entanto, o potencial dos microsserviços é maximizado aquando da utilização de um sistema de orquestração que permita a simplificação e gestão dos deployment’s, especialmente em sistemas de alta complexidade (3) que necessitem de manter a alta disponibilidade, gerir a sua escalabilidade e reagir rapidamente a falhas (4). Assim, esta dissertação pretende explorar diversas estratégias de deployment na tecnologia Kubernetes (5), com o objetivo de verificar quais os seus impactos nos casos de estudo utilizados no que dizrespeito à qualidade de software e prevenção de erros do software entregue, sejam eles, erros de disponibilidade durante e/ou após o deployment ou erros reportados pela monitorização posterior dos containers (6).",
    "authors": [
      "Pinto, Francisco Felícia Correia"
    ],
    "keywords": [
      "Kubernetes",
      "Microsserviços",
      "Sistemas monolíticos",
      "Deployment",
      "Agile",
      "Estratégias de deployment",
      "Estratégias de deployment em Kubernetes",
      "Microservices",
      "Monolithic",
      "Deployment strategies",
      "Kubernetes deployment strategies",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27843",
    "title": "Plataforma integradora de serviços em tempo real baseada em tecnologia OSGi",
    "abstract": "Este projeto tem o objetivo principal de melhorar a qualidade de vida de pessoas idosas, comdeficiências físicas ou psicológicas. Quando ficam em casa sozinhas são privadas de uma vidaautónoma e ativa o que leva à necessidade de uma monitorização continua. Tendo em consideraçãoa crescente disponibilidade de dispositivos interativos num ambiente doméstico abre-seuma porta para necessidade de sistemas de integração e fusão de sensores. Uma plataformainteligente de monitorização capaz de comunicar com os vários dispositivos prova a sua utilidadeao ser capaz de tornar as pessoas mais autónomas proporcionando aos familiares e amigosmecanismos de monitorização ajustados conforme o perfil do utilizador em caso de ausência.Pretende-se desenvolver um sistema que possa ser implementado num dispositivo central queestabeleça a comunicação entre os diferentes dispositivos e sistemas electrónicos, seja capaz deintegrar serviços em tempo real e registe o seu estado em determinado momento. Tem o intuitode aproveitar todos os aparelhos e serviços que as pessoas já possuem, evitando assim um gastomonetário exagerado tendo em conta o estado económico existente e a capacidade monetáriados utilizadores. Estes sistemas electrónicos ou dispositivos podem ser, por exemplo, um sistemade ar condicionado, capaz de adequar a temperatura à preferível pelo utilizador, ou um sistemade iluminação com capacidade de regular a intensidade da luz e assim reduzir nas despesas.Estes sistemas implicam uma melhoria em termos de qualidade de vida do utilizador ao providenciarautomatismos inteligentes no seu dia-a-dia.",
    "authors": [
      "Brandão, João Pedro Almeida"
    ],
    "keywords": [
      "681.3.02",
      "681.586"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.02",
      "681.586"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92631",
    "title": "Estudo da aplicabilidade de ML no âmbito de smart homes",
    "abstract": "As técnicas de aprendizagem automática são amplamente empregadas em Smart Homes, em combinação com dispositivos IoT. A combinação entre ambas as tecnologias permite a coleta e análise de dadosde sensores e outros dispositivos para a aprendizagem e automatização de tarefas domésticas, como regulação de temperatura, controle de iluminação e segurança. Algumas técnicas comuns de ML aplicadasincluem aprendizagem por reforço, aprendizagem supervisionada e aprendizagem não supervisionada.Essas tecnologias permitem que os dispositivos IoT aprendam com o comportamento do utilizador e melhorem as suas capacidades de automatizar tarefas, tornando a vida dos utilizadores mais conveniente eeficiente.A presente dissertação tem como objetivo principal a análise das principais técnicas de ML utilizadasno âmbito das Smart Homes. Neste sentido, é feito um levantamento sobre a automação residencialantes da popularização das técnicas de ML. Analisam-se as técnicas do período anterior ao atual, quandonão havia um poder computacional que permitisse a implementação de técnicas de ML em ambientesresidenciais. Além disso, é abordada a relação entre as técnicas de ML e a criação de ambientes domésticos mais sustentáveis, onde a racionalização de recursos se torna uma realidade e a sua implementaçãonão causa alterações significativas na qualidade de vida dos utilizadores deste tipo de residência. Porfim, é apresentada a implementação de uma prova de conceito relacionada com o reconhecimento deatividades humanas em ambiente doméstico.",
    "authors": [
      "Rosa, Alexandre da Silva"
    ],
    "keywords": [
      "Smart home",
      "Machine Learning",
      "Automação residencial",
      "Deep Learning",
      "IoT",
      "HAR",
      "Home automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91562",
    "title": "Semantic segmentation of medical images with deep learning",
    "abstract": "The use of deep learning techniques in medical image analysis has been a subject of growinginterest in recent years. One of the most important applications of these techniques is thedetection and segmentation of tumors in histological images. This dissertation focused oninvestigating the use of deep learning models to segment tumors, with the aim of providingmedical specialists with a tool that can help them make more precise diagnoses.Tumor growth patterns are an important histological characteristic that can provideinformation about the aggressiveness and degree of malignancy of a tumor. Specifically,the epithelial-mesenchymal transition on the tumor front is a pattern that has been shownto confer high aggressiveness and a great capacity to invade tissues and cause metastases,leading to a poor prognosis regarding the evolution of the tumor. Therefore, detectingand segmenting tumors in histological images can be a critical step in the diagnosis andtreatment of tumors.The research process involved several steps, including preprocessing the images to preparethem for deep learning models. This step involved developing methods to enhance thequality of the images and make them suitable for training deep learning models. Two typesof deep learning architectures, the U-Net and Tiramisu, were trained in a supervised way,and different types of loss functions were experimented with to measure their efficiency incontrolling the training process. Additionally, different types of hyperparameters were tried,and the best value was chosen for each hyperparameter.Finally, the effectiveness of the models was evaluated and compared both qualitativelyand quantitatively based on their performance in image segmentation. The results obtainedshow that deep learning models surpassed the initially predicted values and reached a valueabove 94% based on the training data. for the Interception over the Union metric. This resultdemonstrates the potential of deep learning techniques to detect and segment tumors inhistological images and reinforces the importance of continuing to investigate this topic. Thebest results of the present work were achieved with total loss, as explained on page 89.",
    "authors": [
      "Tabrizi, Mohammad Reza"
    ],
    "keywords": [
      "Medical image segmentation",
      "Brain tumor",
      "Deep learning",
      "U-Net",
      "Tiramisu",
      "Loss function",
      "Segmentação de imagens médicas",
      "Tumor cerebral",
      "Aprendizagem profunda",
      "Função de perda",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59897",
    "title": "Analysis and visualization of dynamic social networks",
    "abstract": "This document represents the study developed under the master’s thesis Analysis of Visualizationof Social Networks, that overlaps two main scientific fields, sociology (moreconcisely social networks) and computer science, aiming at the design and implementationof a system for social network analysis.Nowadays we face an age of massive Internet usage, with Online Social Networks wepractically live this parallel reality where everything we do and everyone we met is exposedand shared through these online ”worlds”. Today, being able to study and understandhow information flows and how relationships are built within these online networksis of paramount importance for various reasons, these can be social, educational, politicalor economical. This master work studied sociology, social network analysis, and computerscience to employ the researched material aiming at building a tool that allows users toexplore their social structure in order to derive sophisticated conclusions, that wouldn’tnormally come up when they are browsing through their online feeds, because we provideto the end user a personalized, macroscopic and objective perspective of their socialnetwork.",
    "authors": [
      "Caldas, Jorge"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47841",
    "title": "Estudo de alternativas open source para soluções IMDG",
    "abstract": "Conseguir satisfazer os clientes em mercados altamente competitivos depende diretamente daqualidade e desempenho das aplicações que lhes são direcionadas. Alguns segundos de atrasopodem fazer a diferença entre o sucesso e o fracasso de uma empresa. A incapacidade deprocessar, aceder, analisar e integrar dados rapidamente num dado sistema é bastanteproblemática para organizações que têm de processar uma grande quantidade e variedade dedados. Os sistemas In Memory Data Grids (IMDG) operam essencialmente com os seus dados emmemória, podendo, porém, ser suportados por vários servidores incorporados num sistemadistribuído. Estes sistemas são recomendados para aplicações que exijam a manipulação degrandes volumes de dados, uma vez que são facilmente escaláveis e de fácil implementação. Alémdisso, em termos técnicos, os sistemas IMDG são claramente vantajosos em processos querequeiram rápidas tomadas de decisão, exijam elevados níveis de produtividade e solicitem umatendimento de alta qualidade aos seus sistemas e utilizadores clientes. Neste trabalho dedissertação foram estudas, de forma detalhada, várias alternativas IMDG open source existentesna atualidade, tendo como base de trabalho um conjunto de condições funcionais e estruturaisdefinidas por uma empresa de telecomunicações, com o objetivo de viabilizar a utilização de umasolução IMDG open source em substituição de uma solução dita comercial. Adicionalmente,idealizou-se um pequeno conjunto de casos de estudo que foram utilizados como base para oprocesso de criação de duas aplicações práticas reais utilizando duas soluções IMDG open sourcedistintas, nomeadamente, o Hazelcast e o Infinispan. No processo de elaboração destes casos deestudo tomou-se em consideração alguns cenários de aplicação bastante típicos em sistemas detelecomunicações, bem como, nas fases de implementação das aplicações, as funcionalidades maisrelevantes que se podem encontrar em sistemas distribuídos deste género, em particular aexecução local de dados em ambiente distribuído, a afinidade de dados em casos departicionamento, a capacidade de replicação de cache em cenários topológicos com mais de umcluster e, por fim, a integração de Java Persistence API (JPA) e Java Transaction API (JTA) comomecanismos para controlo e gestão de persistência e das transações distribuídas.",
    "authors": [
      "Gomes, Hugo André Esteves"
    ],
    "keywords": [
      "Sistemas in memory data grid",
      "Software open-source",
      "Sistemas de transações distribuídas",
      "Avaliação de soluções IMDG",
      "In memory data grid systems",
      "Open-source software",
      "Distributed transactions systems",
      "IMDG solutions evaluation",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86572",
    "title": "Development of an e-portfolio social network using emerging web technologies",
    "abstract": "Digital portfolios (also known as e-Portfolios) can be described as digital collections of artifacts, being both aproduct (a digital collection of artifacts) and a process (reflecting on those artifacts and what they represent). Itis an extension of the traditional Curriculum Vitae, which tells the educational and professional milestones ofsomeone, while the portfolio proves and qualifies them (e.g.: annually thousands of students finish a Masterdegree on Informatics, but only one has built Vue, Twitter or Facebook – the Portfolio goes beyond the CVmilestones by specifying the person’s output throughout life and distinguishing them). e-Portfolios augment thisby introducing new digital representations and workflows, exposed to a community, being both a product anda process. This approach can be useful for individual self-reflection, education or even job markets, wherecompanies seek talented individuals, because it expands the traditional CV concept and empowers individualmerit. There have been many studies, theories, and methodologies related with e-Portfolios, but transpositionsto web applications have been unsuccessful, untuitive and too complex (in opposition to the CV format, whichhad success in various applications, for example LinkedIn).This project aims to study new approaches and develop an exploratory web/mobile application of this method ology, by exploring the potential of social networks to promote them, augmented by emergent web technologies.Its main output is the prototype of a new product (a social network of e-Portfolio) and its design decisions, withnew theoretical approaches applied to web development. By the end of this project, we will have idealized a webinfrastructure for interacting with networks of users, their skills, and communities seeking them.The approach to the development of this platform will be to integrate emerging technologies like WebAssemblyand Rust in its development cycle and document our findings. At the end of this project, in addition to theprototype of a new product, we hope to have contributed to the State of the Art of Web Engineering and to beable to answer questions regarding new emerging web development ecosystems.",
    "authors": [
      "Martins, Paulo Jorge Pereira"
    ],
    "keywords": [
      "e-Portfolios",
      "Microservices",
      "WebAssembly",
      "Rust",
      "Web engineering",
      "Microsserviços",
      "Engenharia Web",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/60032",
    "title": "Real-time MIDI device data analysis and context aware music generation",
    "abstract": "Computer music generation is an active research field encompassing a wide range of approaches.As we involve more and more technology in our creative endeavors, it becomesvital that we provide our systems with the capability to understand and represent art conceptsinternally, as well as understand and predict artistic intent and emotion. In respect tomusic, being able to work with this information opens up fantastic possibilities for artistmachinesynergetic collaborations as well as machine creativity endeavors. In this spirit,this dissertation explores a system capable of analyzing in real-time a performance pieceplayed on a MIDI (Musical Instrument Digital Interface) capable instrument and produce amusically coherent piece of accompaniment music. This system comprises of two major subsystems:one responsible for analyzing and extracting features from the live performanceand one that takes these features and generates MIDI tracks to be played in conjunctionwith the live audio, in a way that blends in with the performance.",
    "authors": [
      "Ribeiro, Miguel Angelo Ferreira Dias"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/65208",
    "title": "Sistema de recomendação de rotas em cidades inteligentes",
    "abstract": "Hoje em dia as zonas urbanas sofrem de sobrepopulação, por isso, é essencial que os seusrecursos sejam geridos da melhor forma com o intuito de proporcionar uma melhor qualidadede vida aos seus habitantes bem como, um desenvolvimento sustentável. Nestecontexto, surge o conceito de Cidades Inteligentes que é focado no uso de novas tecnologiasquer ao nível dos transportes, da saúde ou mesmo relacionadas com o meio ambiente,dando a oportunidade de melhorar a qualidade de vida dos habitantes das zonas urbanas.Com este trabalho, pretende-se desenvolver um sistema de recomendação de rotas paraauxiliar cidadãos com base numa função objetivo específica, como a definição de um percursode atividade desportiva. Um protótipo do sistema foi construído e testado usandouma ferramenta de simulação chamada CupCarbon. Assim, como caso de estudo, consideraseuma zona urbana onde é definida uma rede de sensores sem fios para monitorizar parâmetrosambientais como luminosidade ou cobertura Wi-Fi, com o intuito de auxiliar oplaneamento de rotas de acordo com os parâmetros pretendidos pelo utilizador.",
    "authors": [
      "Fernandes, Stéphane Alexandre Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/36808",
    "title": "Gestão de serviços de tecnologias de informação num ERP: caso prático SAP",
    "abstract": "Este trabalho pretende analisar, em simultâneo, um projeto de implementação de um softwarede gestão de serviços de tecnologias de informação desde o seu início á sua conclusão numaempresa, assim como a própria aplicabilidade do software de gestão escolhido SAP ITSM (ITService Management).Pretende deixar um contributo à forma e conteúdo da aplicação SAP ITSM e lançar as bases naempresa, em que o estudo se insere, para aproximação dos serviços prestados às boas práticasITIL, com a certificação ISO /IEC 20000 no horizonte.Para cumprir esse objetivo, exploram-se os conceitos e a literatura adjacente ao tema,identificando os principais problemas e preocupações a ter em consideração na aplicação dasboas práticas ITIL.Metodologicamente segue-se o estudo de caso, beneficiando da presença do autor no seio dotrabalho da equipa de implementação, levando a cabo, para além da observação, a que se juntadocumentação para análise.Por fim apresentam-se os resultados, discutindo o seu significado e concluindo quanto aoproblema de investigação identificado.",
    "authors": [
      "Lisboa, Nuno Jorge Rebelo Teixeira"
    ],
    "keywords": [
      "SAP",
      "ITIL",
      "ERP",
      "IT",
      "681.3:658.0",
      "658.0:681.3",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93038",
    "title": "Caderno de antepassados em suporte sistema de ficheiros",
    "abstract": "O Ancestors Notebook é uma ferramenta de apoio à gestão e organização de documentos e informaçõessobre a história e herança familiar. Tem como intuito oferecer diferentes potencialidades que facilitem todoo processo de registo e construção de um legado relativamente a uma ou mais genealogias específicas.O Ancestors Notebook funciona sobre o file-system Linux utilizando um conjunto de convenções,comandos e Domain Specific Languages (DSLs) para nomear e organizar diretorias e com um focoespecial em documentos com um formato específico - DGU - criados especialmente este toolkit. O seupropósito é trazer um controlo organizacional personalizado ao utilizador, contribuindo assim para umfluxo coerente de ideias sempre correlacionado com o aglomerar de dados de cariz genealógico.A organização dos dados passa pela definição de entidades representativas de vários elementos, paraaglutinar distintos formatos num só, de maneira a ter um maior segmento organizacional no sistema deficheiros. Definiu-se, também, a geração de templates genéricos para uma visualização mais agradávele familiar, exportável em formato PDF, denominados Caderno de Antepassados. Estes consideram umafuncionalidade de agregação e organização de documentos por entidades, contribuindo assim para ummaior leque de alternativas na definição dinâmica de opções de visualização.O Ancestors Notebook toolkit dispõe de um sistema de controlo de versões, cujo funcionamento estádependente de um sistema de representação de conhecimento sob forma de ontologia e um projectioneditor que permite visualizar e manipular a estrutura genealógica como está representada no sistema deficheiros.O toolkit é definido na linguagem de programação Python com definição de comandos disponíveis nosistema de ficheiros. Utilização de diferentes módulos Python para a definição de views para o utilizador.A criação de templates é feita usando o motor de geração de templatesJinja2. O toolkit é definido comopackage instalável através do pip.",
    "authors": [
      "Oliveira, Duarte Manuel Vilar de"
    ],
    "keywords": [
      "Ontologia",
      "Genealogia",
      "Preservação",
      "Sistema de ficheiros",
      "DSL",
      "Python",
      "Linux",
      "Jinja2",
      "Ontology",
      "Genealogy",
      "Preservation",
      "File-system",
      "Domain specific languages",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82793",
    "title": "Classification of dementias based on brain radiomics features",
    "abstract": "Neurodegenerative diseases impair the functioning of the brain and are characterized by alterations in the morphology of specific brain regions. Some of the main disorders include Alzheimer's, Parkinson's, and Huntington's diseases, and the number of cases increases exponentially since ageing is one of the main risk factors. Trying to identify the areas in which this type of disease appears is something that can have a very positive impact in this area of Medicine and can guarantee a more appropriate treatment or allow the improvement of the quality of life of patients. With the current technological advances, computer tools are capable of performing a structural or functional analysis of neuroimaging data from Magnetic Resonance Images(MRI). Therefore, Medical Informatics uses these techniques to create and manage medical neuroimaging data to improve the diagnosis and management of these patients. MRI is the image type used in the analysis of the brain area and points to a promising and reliable diagnostic tool since it allows high-quality images in various planes or strategies and MRI methods are fundamental diagnostic tools in clinical practice, allowing the diagnosis of pathologic processes such as stroke or brain tumours. However, structural MRI has limitations for the diagnosis of neurodegenerative disorders since it mainly identifies atrophy of brain regions.Currently, there is increased interest in informatics applications capable of monitoring and quantifying human brain imaging alterations, with potential for neurodegenerative disorders diagnosis and monitoring. One of these applications is Radiomics, which corresponds to a methodolog ythat allows the extraction of features from images of a given region of the brain. Specific quantitative metrics from MRI are acquired by this tool, and they correspond to a set of features, including texture, shape, among others. To standardize Radiomics application, specific libraries have been proposed to be used by the bioinformatics and biomedical communities, such as PyRadiomics, which corresponds to an open source Python package for extracting Radiomics of MRIs.Therefore, this dissertation was developed based on magnetic resonance images and the study of DeepLearning (DL) techniques to assist researchers and neuroradiologists in the diagnosis and prediction of neurodegenerative disease development. Two different main tasks were made: first, a segmentation, using FreeSurfer, of different regions of the brain and then, a model was build from radiomic features extracted from each part of the brain and interpreted for knowledge extraction.",
    "authors": [
      "Carvalho, Sofia Manuela Gomes de"
    ],
    "keywords": [
      "Machine Learning",
      "Dementia classification",
      "Magnetic resonance imaging",
      "Radiomics",
      "Brain morphological features",
      "Neurodegenerative diseases",
      "ADNI",
      "Classificação de demências",
      "Imagens ressonância magnética",
      "Radiomics",
      "Features morfológicas cerebrais",
      "Doenças neurodegenerativas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47437",
    "title": "Estimating recombination frequency throughout the human genome using a phylogenetic-based method",
    "abstract": "Recombination rate is an essential parameter for most studies on human variation. Linkagedisequilibrium (LD) measures the association between two variants in the same chromosome.When a new variant arises by mutation in a germinal line, that variant will be incomplete linkage with the variants in the chromosomic background where it arises. Recombinationthrough time (occurring during meiosis) will decrease the association decreasingthe LD. Understanding how recombination occurs throughout the genome is the basis tointerpret various association studies (search from causal variants for a given disease) andcharacterization of selective events. In this project the aim is to establish a novel methodologyto estimate rate of recombination along a chromosome using a phylogenetic method.For this to be done, each chromosome will be divided into small overlapping windows ofvariation containing 20/30 variants. For each of these windows a phylogenetic network willbe calculated using the reduced-median algorithm. Highly recombining regions will showa higher rate of cycles or reticulations in the network. A linkage map will be constructed foreach chromosome using this novel methodology, compare the results with methods alreadyavailable, locate region of low recombination of possible use for phylogenetic analysis andalso explore some properties of the method for evaluation of selection.",
    "authors": [
      "Silva, Raquel dos Santos"
    ],
    "keywords": [
      "Linkage disequilibrium",
      "Single-nucleotide polymorphism",
      "Recombination",
      "Phylogenetics",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84087",
    "title": "LiDAR based 3D object tracking for autonomous driving",
    "abstract": "Technology has become essential for society’s every-day-life and with the recent increase in artificial intelligence’s interest, this area has gained more and more relevance for both people (e.g., due to the increasing number of users of personal assistants, such as Siri, Alexa and Google Assistant) and service providers (e.g., Google search engine and social networks’ recommendation algorithms to keep users busy and active on their platforms - Facebook, Youtube, TikTok, etc.). Nevertheless, artificial intelligence has been applied to many other areas, such as targeted advertising to specific users, cybersecurity, medicine, and the automobile industry. Although artificial intelligence has not been the perfect solution in the aforementioned applications, it has been responsible for several significant improvements in the last decade. For example, in the automobile industry, there are more and more companies offering solutions for autonomous vehicles, being Tesla the most notorious. This evolution was driven by several factors, including need and interest in improving road safety, growing traffic problems that exist due to the increase of vehicles circulating, more reliable sensors, and recent advances in various areas of artificial intelligence, such as object detection, semantic segmentation, and object tracking. These three areas are interconnected. However, they have different purposes - the first two (detection and segmentation) more related to static frame analysis (e.g., image based analysis), while object tracking is usually applied in dynamic environments (e.g., sequence of frames, such as a video) where its input is processed in order to track objects over time, allowing an intelligent system to be “aware” of its environment. That said, this dissertation aims to study and explore the applicability and feasibility, as well as to develop and implement an object tracker in the context of autonomous driving. Furthermore, it is also intended to make a benchmark with state-of-the-art approaches and identify their main limitations. The input data will be focused on Light Detection and Ranging (LiDAR) based 3D point cloud, as there are several datasets available, in particular KITTI [1], which, in addition to being widely used in the state-of-the-art, has also achieved positive results, even in real-time execution situations. However, these solutions usually require a lot of computational resources and, which can be a hurdle for its application in real-life settings.",
    "authors": [
      "Figueiredo, André Sousa"
    ],
    "keywords": [
      "Object tracking",
      "Self-driving vehicles",
      "LiDAR",
      "Deep learning",
      "Tracking de objetos 3D",
      "Condução autonóma"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27793",
    "title": "Comparação de análises estruturais (Volumetria, Espessura Cortical, e Voxel based Morphometry) em Neuroimagem",
    "abstract": "Os estudos relacionados com a neuroimagem têm assumido nos últimos anos grandeimportância por parte da comunidade médica e científica. O aumento da esperançamédia de vida faz com que se registem cada vez mais doenças do cérebro, das quaisas demências e as doenças degenerativas têm assumido especial importância. Natentativa de perceber quais as alterações anatómicas registadas no cérebro com aidade e aquando do surgimento destas patologias começaram a ser realizados estudosestruturais a este onde a Ressonância Magnética tem-se demonstrado como principalferramenta para este estudo.Atualmente existem diversas técnicas que possibilitam o estudo estrutural eanatómico do cérebro todavia ainda não existe nenhuma técnica que possibilite oestudo integral de todas as características estruturais do cérebro; no entanto amedição da espessura cortical, volumetria e morfometria baseada em vóxel têmassumido especial preponderância no estudo destas características.O objetivo principal do presente trabalho consistiu em efetuar uma análise por regiõese por vóxeis de forma a perceber quais as regiões cerebrais que eram mais afetadascom a idade no estudo da volumetria, espessura cortical e da área, para isto foramutilizados um método convencional, e o GLMfit para a análise por regiões e o QDECe o SPM8 para o estudo por vóxeis.Para se poder efetuar os estudos referidos anteriormente foi necessário pré-processartodos os dados em estudo através da utilização da aplicação Freesurfer quepossibilitou a correção de pequenos erros originados durante a aquisição das imagens.Com esta dissertação conclui-se que os métodos utilizados para a deteção docomportamento das variáveis em estudo nas análises por regiões se demonstramcoerentes entre si e entre os dados bibliográficos consultados, todavia na análise porvóxeis as conclusões não foram tão lineares sendo mesmo impossível efetuar umacomparação entre esses métodos, pois os resultados obtidos foram totalmente distintos.",
    "authors": [
      "Magalhães, André Nogueira"
    ],
    "keywords": [
      "616-079",
      "616.8",
      "61:681.3",
      "681.3:61"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616-079",
      "616.8",
      "61:681.3",
      "681.3:61"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64275",
    "title": "GreenSource: repository tailored for green software analysis",
    "abstract": "Both energy consumption analysis and energy-aware development have gained the attentionof both developers and researchers over the past years. The interest is more notoriousdue to the proliferation of mobile devices, where energy is a key concern.There is a gap identified in terms of tools and information to detect and identify anomalousenergy consumption in Android applications. A large part of the existing tools arebased on external hardware (costly solutions in terms of setup-time), through predictivemodels (requiring previous hardware calibration) or static code analysis methods. We couldnot identify so far a tool capable of monitor all relevant system resources and componentsthat an application uses and appoint its energy consumption, while being easily integratedwith the application and/or with its development environment. Due to the lack of a toolcapable of gathering all this information, a natural consequence is the lack of informationabout the energy consumption of applications and factors that can influence it.This dissertation aims to carry out a study on the energy consumption of applications andmobile devices in the Android platform, having developed in this scope the GreenSourceinfrastructure, a repository containing the source code, representative metadata and metricsrelatively to a large number of applications (and respective execution in physical devices).In order to gather the results, an auxiliary tool has been developed to automatize theprocess of testing and collect the respective results for each one of the applications. This toolis a software-based solution, allowing to obtain results in terms of consumption throughexecutions made directly on a physical device running the Android platform.The developed framework, the AnaDroid, has the capability to perform static and dynamicanalysis of an application, being able to monitor power consumption and usage ofresources for each application through tests execution. This is done following a whiteboxtesting approach, in order to test applications at source code level. It invokes calls tothe TrepnLib library at strategic locations of the application code (through instrumentationtechniques) to gain control over relevant portions of the source code, like methods and unittests. In this way the programmer can have results about the use, state and consumption ofresources such as energy, CPU, GPU, memory, sensor usage and complexity of developedtest cases.The information gathered through the use of the AnaDroid over a large set of applicationswas stored in GreenSource backend. With the collected results, we expect to be able tocharacterize and classify applications, as well the tests developed for it. It is intended thatthis will be made publicly available and serve as a reference for future works and studies.",
    "authors": [
      "Rua, Rui António Ramada"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79866",
    "title": "Desenvolvimento de ferramentas para análise de sentimentos em redes sociais: a rede social Twitter",
    "abstract": "Social networks have allowed, over the past few years, the appearance of new ways to shareopinions and ideas in texts, providing a basis for studying opinions on a large scale. The tools for the retrieval and analysis of sentiments, contained in this information, are still underdevelopment, limited by access restrictions and technical difficulties in the development ofnew methods, which involve natural language processing and text mining. This work aimsto develop tools to recover and analyze sentiments present in social networking texts.A case study using Twitter will be used for validation. During this process, data obtained from this social network was stored in a document oriented database, Elasticsearch,organized by topics helping its use in the following phases. Then, the data went througha set of pre-processing steps, to maximize the value of their content, seeking to improvethe chances of obtaining a correct classification. Finally, the processed textual data weresubmitted to the algorithm chosen for classification, Naive Bayes. The results obtained overtwo different datasets show that the pre-treatment of data is very important regarding tothe classification of sentiments in texts.Overall, a computational architecture has been developed that can foster sentiment analysis applications over social network data from Twitter",
    "authors": [
      "Brito, Luís"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92824",
    "title": "Optimization of the inventory routing problem with Artificial Intelligence",
    "abstract": "Nowadays the success of a business is dependent on the ability to effectively integrate in an intricatenetwork of entities that are connected by material and information flows, inventory management beingone of the main concerns. These flows are characterized by decision-making processes that will varydepending on the environment, entities and business models in the network. So, these networks needa decision-making system capable of providing solutions that dictate the optimal way the network and itsentities provide and collect inventory in order to reduce costs and maximize profit. In the context of thisdissertation, the problem arises when there is a stock disruption in the network and outside entities can nolonger answer the stores’ supply requests and these stores become the entities responsible for requesting and delivering products to each other. This problem is modeled as an Inventory Routing problem,since it encompasses inventory management and routing decisions. The main goal of the system can bedescribed as maximizing the collection of products per travel distance, without causing stock-outs at anysupplier, for the entire network. The problem at hand is an optimization problem.In order to solve this optimization problem, first, the structural characteristics and key aspects were identified and studied, followed by the mathematical conceptualization, which involved the definition of theobjective function and the corresponding set of constraints. The mathematical formulation allows theproblem to be translated into a specific and precise mathematical language, making it possible to evaluatesolutions, by means of a fitness function, and apply optimization algorithms to solve the problem. Theseoptimization algorithms can be approximate or exact methods and their suitability to the problem dependson many factors such as the size, structure and complexity of the problem. So, the choice of the optimization algorithms must be preceded by a careful analysis of the problem at hand and its characteristics.After this, in the implementation phase, two adaptations of the genetic algorithm, two adaptions of thesimulated annealing algorithm, two adaptations of the tabu search algorithm were developed. Additionaly,another algorithm responsible for generating reference solutions was also developed (Dynamic2). In order to test and compare the developed optimization algorithms, three different sized scenarios were generated.Each of these scenarios has a different amount of data associated with it, whether it be in the number ofstores, types of products or number of requests. As to compare the results of the different instances ofthe algorithms fairly in each scenario, these were made to generate roughly the same number of solutions.In scenarios 1 and 3, all the optimization algorithms developed were successful in finding solutions withhigher fitness values than the baseline Dynamic2 solution. In scenario 2, due to time constraints andcomputational complexities, only the Genetic algorithm and the Genetic algorithm with Elitism using aninitial population consisting of solutions generated by the Dynamic2 algorithm, managed to find solutionswith higher fitness value than the baseline solution. In this scenario, the developed optimization algorithmswere also tested using feasible solutions generated through random mechanisms as initial solutions. Theseinstances also achieve solutions with improved fitness values when compared to their respective initial solutions or populations.Furthermore, in scenarios 1 and 3, the Genetic algorithm with an initial population consisting of feasiblesolutions generated through random mechanisms was the optimization algorithm that found the best solutions, with these solutions having fitness values 56.14% and 92.07% greater than the baseline Dynamic2solution’s, respectively. In scenario 2, the Genetic algorithm with Elitism, utilizing an initial population consisting of solutions generated by the mentioned Dynamic2 algorithm, found the solution with the highestfitness value, being approximately 1.00% higher than the baseline solution.",
    "authors": [
      "Silva, João Miguel Pimenta da"
    ],
    "keywords": [
      "Optimization",
      "Inventory routing problem",
      "Approximate methods",
      "Exacts methods",
      "Metaheuristics",
      "Otimização",
      "Métodos aproximados",
      "Métodos exatos",
      "Metaheurísticas",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79368",
    "title": "Sensor fusion for detection and classification of vehicle impacts",
    "abstract": "This thesis was developed as part of a curricular internship at Bosch Car Multimédia SA, in collaboration with the University of Minho, More specifically, an exploratory research thesis aligned with an R&D project that is being developed internally and whose objective is to detect impacts on vehicles that cause damage based on data obtained through sensors, The usefulness of the work developed in this thesis and the project in which it is inserted, in a real context, would be to help vehicle rental companies and car-sharing services to better monitor the conditions of vehicles in their fleets, This would be achieved by placing a device in vehicles that continuously monitored their status, reducing the need for validation and human interaction after use, The main focus of this thesis was to explore how the fusion of information from different sensors could improve the decision-rnaking capabilities of a system whose purpose is to determine whether impacts on the exterior of a vehicle, captured with a set of sensors, resulted in damage, This conjugation of sensory information is known as sensor fusion. ft is a process of combining information from different homogeneous and heterogeneous sensors to obtain a better representation of what is being observed, The approach chosen to achieve this goal consisted of training a set of Machine Learning (ML) algorithms with two distinct datasets, one based only on one data source and the other multiple sources combined. Each pair of models was further evaluated on unseen data, and their performances were compared based on the va lues obtained, Based on the results obtained, it can be said that the application of sensor fusion allowed for better learning by the models, which led to greater robustness in data never seen before. Of the four chosen algorithms, XGBoost, Random Forest (RF), Support Vector Machine (SVM), and Artificial Neural Network (ANN), all had at least one of the evaluation metrics, the Matthews Correlation Coefficient (MCC) and number of False Positive (FP)s in the test set, superior in model-based fused data. Of these, XGBoost and ANN stand out where the results were significantly better in both metrics,",
    "authors": [
      "Santos, João Gabriel Lopes dos"
    ],
    "keywords": [
      "Sensor fusion",
      "Impact detection",
      "Machine learning",
      "Signal processing",
      "Fusão sensorial",
      "Deteção de impactos",
      "Processamento de sinal",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81102",
    "title": "A secure IoT Communication system for smart contracts",
    "abstract": "The need to ensure the confidentiality and integrity of data generated in industrial systemsand applications has been increasingly highlighted over the years, due to the clear andurgent requirements of not disclosing sensible proprietary information and ensuring thatdata is kept immutable since it is generated until it is permanently stored.It is from these two main ideas that this dissertation is created, framed in a project that isbeing developed at the Digital Transformation CoLab with Bilanciai and Cachapuz. These are theindustrial partners and key stakeholders of this project, having identified the requirementsfor the weight measurement process that occurs in the weighing stations that are placed intheir customers. This dissertation essentially consists on the definition of a secure Internet ofThings (IoT) communication system between the devices that operate on the weighing stationsof the customers and on top of that, develop a smart contract application using blockchaintechnology capable of: i) automating the process of verifying the correct application ofweighing guidelines; and ii) registering and storing ”receipts” of weighings that take placein the customers’ weighing stations.In this dissertation, a revision of the state of the art is made with the goal to perceivethe most secure and current technologies capable of providing the required functionalities,which are the fuel for the identification of the problems and challenges that such a projectmight face, ultimately leading to the design of a solution that can both: i) mitigate theaforementioned problems and challenges; and ii) comply with the goals defined for thedissertation. Additionally, in this document, the development of such a solution is alsoexplored by providing clear insights into the decisions that were made and the reasoningbehind them and by implementing components that are able to provide registration and richquerying of weighing tickets (receipts), weighing ticket building and secure communicationas well as the enforcing of a blockchain network structure that fosters data confidentiality.Ultimately, results are shown, collected from a proof of concept, which essentially provideevidence on the functional correctness of the system that was built, i.e., its ability to grantthe retainment of weighing ticket characteristics and the capabilities of the communicationsystem, which demonstrates to be able to securely build and transmit weighing tickets, withfault tolerance.The outcomes of this project can be integrated into existing systems of the industrialpartners to increase efficiency, security and business innovation.",
    "authors": [
      "Leite, Nuno André Lopes"
    ],
    "keywords": [
      "Blockchain",
      "Smart contracts",
      "Iot",
      "Security",
      "Compliance",
      "Contratos inteligentes",
      "Segurança"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27883",
    "title": "Social simulations of human behavior in virtual agents for sustainability management platforms",
    "abstract": "impacto negativo que o ser humano tem provocado no meio ambiente precisa de ser contrariado,assim, são necessárias soluções para atuar nesse sentido, soluções estas que precisam de assegurar asustentabilidade entre questões ambientais, económicas, e sociais. Esta dissertação aborda este tópico nocontexto de sistemas de informação, mais concretamente, na interacção social de uma pessoa com o meioem que está inserida. A ideia surgiu para colmatar uma lacuna existente nas soluções computacionais paragestão de sustentabilidade, pois estas centravam o problema nas questões externas ao utilizador, descartandoo seu grau de conforto. Para isso, foi implementado um sistema de simulação de emoções humanas,baseado nas características psicológicas dos utilizadores, com o propósito de integrar este sistema numaplataforma informática de apoio à sustentabilidade. Esta simulação pediu emprestado os conceitos deComputação Afectiva, área da informática que tem ganho importância nos últimos anos, e é responsávelpor fundir os trabalhos em psicologia e computação. Assim, o trabalho presente nesta tese apresentadados sobre a relação psicologia, sustentabilidade, computação afectiva e como estes campos se podeminterligar. Foi desenvolvida uma plataforma utilizando tecnologia de agentes virtuais que recolhessem informaçõessobre o ambiente e procedessem ao cálculo de um estado emocional. Os resultados obtidoscom o trabalho desenvolvido foram animadores, e revelam que este tipo de simulação poderá ser umavantagem, para que um sistema de suporte à sustentabilidade, baseado em inteligência ambiente, possatomar decisões e atuar sobre um meio, tendo informação prévia ou uma hipótese do estado emocional dosseus utilizadores.",
    "authors": [
      "Felgueiras, Gilberto Martins"
    ],
    "keywords": [
      "Computação afetiva",
      "Sustentabilidade",
      "Inteligência ambiente",
      "Psicologia",
      "Affective computing",
      "Sustainability",
      "Ambient intelligence",
      "Psychology",
      "681.3",
      "159.942",
      "504.06"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3",
      "159.942",
      "504.06"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92673",
    "title": "Remote work adoption among software development teams in Portugal after the COVID-19 pandemic: an empirical analysis",
    "abstract": "Remote working is not a new concept, having become a more viable option with the advent of personalcomputers and high-speed Internet connections. Even so, it is safe to say that the percentage of profession als remotely working reached unprecedented proportions during the COVID-19 pandemic. Consequently,for many, this peculiar virus containment period meant their first contact with teleworking.However, the obligation to work from home eventually came to an end, meaning that employers andemployees regained the autonomy to decide together whether or not to invest in teleworking. Now, with anotable difference: both, with a few exceptions, are already familiarized with teleworking, its advantagesand challenges, and the team dynamics adapted to allow for virtual communication.It is within this post-pandemic context that this dissertation provides a comprehensive view of theadoption of remote working among Portuguese software development teams. Therefore, it intends tostudy the current prevalence of teleworking, the challenges posed by the coexistence of remote and in office work, and how to make this symbiosis more effective and productive.To attain this, 175 valid testimonials were collected through a questionnaire distributed between Marchand June of 2023. Analyzing the responses, it is possible to observe a significant migration from face-to face to remote work between the period before and after the pandemic. Avoiding daily commuting andhaving more time for family and leisure activities were some of the primary motivations for this migration.It can be asserted that the coexistence of remote and face-to-face professionals induces a slight negativeimpact on team dynamics. Lastly, and with the intention of optimizing the dynamics of teams that acceptremote work, a set of recommendations is presented based on the participants’ testimonies.",
    "authors": [
      "Costa, Filipe Barbosa Soares da"
    ],
    "keywords": [
      "Post-pandemic",
      "Remote work",
      "Software development",
      "Questionnaire",
      "Portugal",
      "Pos-pandemia",
      "Trabalho remoto",
      "Desenvolvimento de software",
      "Questionário",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56112",
    "title": "Reconstructing the metabolic network of Lactobacillus helveticus on a genome-wide scale",
    "abstract": "The constant growth of high-throughput data generation and omics approaches requireinformatics support and (semi) automated processes to be developed. With increasing numberof sequenced genomes available, metabolic engineering processes will allow a rational alterationof the genetic architecture to achieve specific phenotypes. These alterations will allowto generate and optimize features of some organisms with economic and health interest.Lactobacillus helveticus is an important industrial lactic-acid bacterium being used inthe production of several types of cheese. The metabolic activities of the bacterium contributeto the cheese flavour and reduce bitterness. Lb. helveticus is a growing body of literature onthe health-promoting properties of its various strains and generally accepted as probiotic forits anti-mutagenic, immunomodulatory and anti-diarrheal effects.The aim of this project was to reconstruct a genome-scale metabolic network of Lb. helveticusCNRZ32, based on its genome sequence annotation as well as known biochemical andphysiological characteristics. The generated model contained 790 reactions, 894 metabolitesand 1687 genes. The growth rate predicted by the model on sugar was comparable to thereported in literature.This model provides the basis for a constraint-based mathematical model capable ofsimulating the phenotype of the organism under different growth conditions and guiding indepthphysiological studies and hypothesis generation.",
    "authors": [
      "Dias, José Miguel Gonçalves"
    ],
    "keywords": [
      "Metabolic network",
      "Lactobacillus helveticus",
      "Metabolic Models Reconstruction using Genome-Scale Information (merlin)",
      "Computational biology",
      "Enzymes",
      "Transporters",
      "TRIAGE",
      "COBRA",
      "OptFlux",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80109",
    "title": "Formalization of deep learning techniques with the Why3 proof platform",
    "abstract": "Machine Learning como um campo, parte integrante da área de Inteligência Artificial, tem crescidoexponencialmente, principalmente nesta última década, onde passou de quase desconhecido pelo públicoem geral para a existência de carros autónomos e até robôs humanóides como o robô Sophia da ArábiaSaudita. A maioria de nós agora lida com Inteligência Artificial todos os dias, em anúncios direcionadospor exemplo, o que é agora a norma.Deep Learning, um ramo específico de Machine Learning de onde originaram as Redes Neuronais,é vastamente utilizado no desenvolvimento de sistemas autónomos de alta complexidade. Alguns destessistemas em particular podem ser classificados como sistemas críticos, o que traz a necessidade defornecer alguma forma de garantia de que estes sistemas vão sempre funcionar como é suposto, uma vezque qualquer falha em sistemas desta categoria pode ter consequências graves. Isto naturalmente levantapreocupações relativas à segurança, levando a comunidade a procurar uma forma de obter tais garantias,eventualmente levando-os aos métodos de Verificação Formal para atingir os níveis de confiabilidadenecessários para a adoção pública de tais sistemas.Tem havido um interesse crescente quanto a este assunto, uma vez que as aplicações de RedesNeuronais estão em constante expansão, e muitas ferramentas de software já resultaram deste trabalho,sendo algumas dessas ferramentas analisadas nesta dissertação. Este estudo vem contribuir para esseesforço, e tem como objetivo principal a avaliação do Why3, a fim de compreender se esta ferramentapossui as características necessárias que lhe permitam juntar-se a estas ferramentas já existentes comoum novo meio de verificação da correção de Redes Neuronais. Para atingir este objetivo, primeiramentecriamos um proof-of-concept a fim de analisar se o Why3 fornece o suporte necessário para esta tarefa.Em seguida, damos um passo em frente e formalizamos uma Rede Neuronal à escala de uma aplicaçãoreal no Why3, de onde tiraremos as nossas conclusões.Durante o trabalho sobre a formalização de Redes Neuronais, pretendemos também compilar umguia abrangente sobre Why3, desde as funcionalidades que oferece, até exemplos de como pode seraplicado explicados passo a passo, com o objetivo de oferecer uma base de conhecimento compreensivapara qualquer pessoa interessada em explorar o Why3, contribuindo ao mesmo tempo para a escassadocumentação existente sobre o Why3.",
    "authors": [
      "Sousa, Márcio Alexandre Mota"
    ],
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Rede neuronal",
      "Verificação formal",
      "Why3",
      "Formal verification",
      "Neural network"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84146",
    "title": "Gestão de permissões e acesso a dados para Hyperledger Fabric",
    "abstract": "A gestão de acesso e permissões afeta várias áreas como a da saúde, mais concretamente os registos desaúde eletrónicos, que se espera que nos próximo anos cresça exponencialmente e alcancem um valorno mercado de $39.7 biliões no ano 2022. A utilização de blockchain aparece como uma solução paraestes cenários onde existem diversos domínios em que os dados são potencialmente sensíveis, quer comodados pessoais, quer como dados que podem revelar segredos de negócio.Algumas tecnologias, como o HyperLedger Fabric, já prometem resolver estes problemas, mas sempre com uma granularidade baixa, com bastantes limitações ao nível da definição de políticas de acessoe de transformações dos dados. No contexto de HyperLedger Fabric vamos implementar um mecanismode gestão de permissões flexível, que além de diferenciar o acesso com base na identidade de quemfaz o pedido permite considerar um conjunto de atributos configurável. Adicionalmente, além de decisões binárias sobre o acesso, o sistema que implementamos permite implementar políticas que definemtransformações a ser aplicadas aos dados acedidos.",
    "authors": [
      "Parente, João Pedro Araújo"
    ],
    "keywords": [
      "Gestão de permissões",
      "Blockchain",
      "Hyperledger Fabric",
      "Privacidade",
      "Confidencialidade",
      "Permission management",
      "Privacy",
      "Confidentiality",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23525",
    "title": "Serviços + perto : prestação de serviços ao cidadão sobre dispositivos móveis",
    "abstract": "Numa era de grandes evoluções tecnológicas e de fácil acesso a todo o tipo de informaçãoa nível global, gerou-se uma oportunidade há muito idealizada para que fossemlibertados dados adquiridos pelos governos para uso livre, pela e para a comunidadeem geral. No entanto, o acesso aos dados por parte de um cidadão por si só podenão ter grande impacto social. Posto isto, a utilização livre dos dados transformandoosem informação, seja de forma aberta ou comercial, por parte de indivíduos ou equipasde desenvolvimento especializadas poderá fornecer ao cidadão todo um leque de serviços até então inexistentes. Assim, o principal objetivo deste trabalho passa por disponibilizar uma aplicaçãomóvel que se torne uma mais-valia para os cidadãos, auxiliando-os na resposta a necessidadesinerentes aos serviços públicos e privados mais procurados.De forma a concretizar o objetivo proposto, foi efetuado um levantamento do funcionamentoreal da estrutura a representar. Posto isto, verificou-se a necessidade decriação de um back-office para gestão e centralização da informação associada ao modelooriginado.Como resultado, foi criada uma aplicação para Android que servirá como primeiraferramenta de medida dos benefícios que a solução pretendida poderá levar à vida doscidadãos. Espera-se ainda que a aplicação sirva de incentivo à criação de soluções domesmo âmbito.Os objetivos foram alcançados e brevemente os cidadãos poderão usar a aplicaçãocriada, assim como terão acesso a todo o código fonte para melhorias que possam seracrescentadas ao sistema.",
    "authors": [
      "Silva, António Pedro Cunha Martins da"
    ],
    "keywords": [
      "35:681.324",
      "681.324:35"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "35:681.324",
      "681.324:35"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83521",
    "title": "Desenvolvimento da plataforma “Portugal Athletics FPA”",
    "abstract": "Nesta dissertação propõe-se a realização de uma plataforma e ferramentas capaz de elevar informaticamente a modalidade em questão (atletismo). O objetivo desta dissertação é criar um ponto comum, capazde coordenar diversas ferramentas existentes, com a inovação de aprimoramento de algumas destas.Este projeto encontra-se com o apoio da FPA (Federação Portuguesa de Atletismo), utilizando a AAB(Associação de Atletismo de Braga), como os primeiros utilizadores na fase de testes, extendendo gradualmente o número de intervenientes, até o sistema ser implementado a nível nacional.Serão desenvolvidas 2 ferramentas distintas:• Plataforma web• Aplicação para Concursos/Provas Fora da PistaA primeira ferramenta será uma plataforma web, que terá a função de ser o ponto de controlo, teráacesso à base dados que contêm os atletas federados na FPA. Esta deverá ser capaz de criar e tolerartodas as necessidades para a gestão de competição, desde a possibilidade para os clubes inscreveremos seus atletas nas competições até a geração dos comunicados de resultados e estatísticas das competições. Numa fase final do projeto também deverá efetuar rankings dos melhores atletas por prova, assimpossibilitando os atletas de visualizar o seu perfil, visualizando assim os seus resultados e melhores marcas.A segunda será uma aplicação desktop, com o objetivo de gerir os diversos concursos a ocorrer numaprova de pista ( Lançamentos, Saltos Verticais e Saltos Horizontais), assim como provas de Corta-mato eEstrada fora da pista. Esta aplicação também deverá ter funcionalidades específicas para que seja possívela utilização simultânea de placares electrónicos no local da prova e esteja integrada para possibilitar liveresults com a primeira ferramenta.",
    "authors": [
      "Pacheco, Alexandre de Freitas Ferreira"
    ],
    "keywords": [
      "FPA",
      "AAB",
      "Atletismo",
      "Plataforma",
      "Live-results",
      "Gestão de competição",
      "Rankings",
      "Perfil de atleta",
      "Confirmações online",
      "Inscrições",
      "Athletics",
      "Platform",
      "Competition management",
      "Athlete profile",
      "Online confirmations",
      "Registration",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/34165",
    "title": "Plataforma de Business Intelligence para o estudo de infeção nosocomial",
    "abstract": "O controlo e a prevenção de infeções nosocomiais são essenciais para aredução de custos, bem como para a melhoria dos cuidados prestados numainstituição de saúde. Por outro lado, o tratamento de dados que permitamcompreender, caracterizar e monitorizar as infeções possibilita um controloe uma prevenção mais eficaz das mesmas. Sendo um método automatizadoe eficiente para o tratamento de dados, a tecnologia de Business Intelligencepermite a extração de informação importante para gerar conhecimento quepode auxiliar o processo de tomada de decisão dos profissionais de saúde.O principal objetivo deste trabalho é o desenvolvimento e implementaçãode uma plataforma de Business Intelligence que permita o estudo da incidênciade infeção nosocomial nas Unidades de Medicina do Centro Hospitalardo Porto. Este estudo é feito através da apresentação de um conjunto deindicadores clínicos (informações importantes extraídas dos dados referentesa infeções nosocomiais) que ajudam a analisar e caracterizar estas infeções.Por conseguinte, depois de identificados os indicadores relevantes, torna-sepertinente desenvolver um sistema que permita tratar os dados, extrair osindicadores destes e apresentá-los, de forma atrativa, na plataforma. Porsua vez, a plataforma facilita a análise das informações que disponibiliza,apoiando a tomada de decisões, nomeadamente através da identificação dosprincipais fatores de risco. Assim, o sistema atua como um Sistema de Apoioà Decisão Clínica, podendo auxiliar no controlo e prevenção destas infeções.Pretende-se ainda estudar a aplicabilidade da tecnologia de Data Miningna criação de modelos de classificação capazes de prever a ocorrência deinfeções nosocomiais, na presença de determinados fatores de risco.O conhecimento obtido com a análise dos indicadores e as previsões efetuadaspode possibilitar a diminuição da incidência de infeção nosocomiale, consequentemente, a redução dos custos associados à sua ocorrência, bemcomo o aumento da segurança e do bem-estar dos doentes, ao permitir a tomadade decisões mais fundamentadas. A aplicação de Business Intelligencena área da saúde contribui para melhorar não só o fluxo de trabalho diárionas unidades de saúde, como também a qualidade dos cuidados prestados.",
    "authors": [
      "Silva, Eva"
    ],
    "keywords": [
      "681.3:614",
      "614:681.3",
      "613.63",
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2014",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:614",
      "614:681.3",
      "613.63"
    ],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/11386",
    "title": "Desenvolvimento de ferramentas computacionais para a optimização de processos de fermentação em Biotecnologia",
    "abstract": "Actualmente, uma larga variedade de produtos tais como antibióticos,proteínas, vacinas e outros compostos químicos são produzidos através de processosfermentativos. Devido à subida dos preços do petróleo e aos fortes incentivos porparte das instituições para substituir os produtos derivados de petróleo por “produtosverdes”, muitos dos processos tradicionais têm vindo a ser substituídos porbioprocessos. Consequentemente, tem existido um esforço para melhorar aprodutividade dos processos biológicos. A optimização destes processos pode serrealizada em duas etapas: primeiramente, faz-se uma selecção e uma melhoriagenética do microrganismo e num segundo passo são identificadas as melhorescondições para realizar o processo fermentativo. Nesta etapa, normalmente sãorealizados estudos experimentais através de tentativa-erro para obter as condiçõesambientais que propiciem o melhor crescimento e produtividade do microrganismo,manipulando as concentrações iniciais dos nutrientes, os perfis de alimentação desubstrato ao reactor, os modos de operação, bem como a temperatura e o pH.Nos últimos anos, têm sido desenvolvidas várias ferramentas informáticas parasimulação e optimização de bioprocessos. Porém, a maioria destas ferramentas estádireccionada para estudar as vias metabólicas de um microrganismo de modo aoptimizar a produtividade de determinado produto. Numa fase posterior, é efectuadauma optimização genética do microrganismo. Apesar de existir uma grande variedadede ferramentas informáticas verifica-se que nenhuma delas está desenhadaespecificamente para a optimização e simulação de processos fermentativos. Assim, oobjectivo deste trabalho foi desenvolver de raiz uma ferramenta direccionada parasimulação, optimização e estimação de parâmetros de processos fermentativos.A aplicação OptFerm foi desenvolvida sobre uma plataforma denominadaAIBench, tendo-se utilizado a linguagem Java como linguagem de programação. OOptFerm foi então desenvolvido de modo a ser uma ferramenta de fácil uso, extensívele que pudesse funcionar em qualquer sistema operativo, estando disponível comosoftware livre em http://darwin.di.uminho.pt/optferm/. A aplicação foi desenhada demodo a que o utilizador pudesse realizar várias tarefas de simulação, optimização eestimação de parâmetros com diferentes condições no que se refere a variáveis deestado, parâmetros, perfis de alimentação, etc.. As tarefas de optimização foramfocadas na determinação do melhor perfil de alimentação de uma corrente desubstrato a alimentar ao reactor, dos melhores valores das variáveis de estado parainiciar uma fermentação e do tempo óptimo de duração para uma fermentação.Foram realizados alguns estudos de optimização e estimação de parâmetroscom o objectivo de verificar se a aplicação era suficientemente robusta. Os estudosforam baseados na repetição das experiências por 30 vezes para obter significânciaestatística. Após os estudos, verificou-se que as operações foram realizadas levando aresultados coerentes, não tendo sido detectados erros relevantes.",
    "authors": [
      "Rocha, Orlando"
    ],
    "keywords": [
      "663.15",
      "681.323"
    ],
    "date": "2009",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "663.15",
      "681.323"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46708",
    "title": "A survival prediction model for colorectal cancer patients",
    "abstract": "The importance of making predictions in health is mainly linked to the decision-makingprocess. Make survival predictions accurately is a very difficult task for healthcare professionalsand a major concern for patients. On the one hand, it can help physicians decidebetween palliative care or other medical practice for a patient. On the other hand, the notionof remaining lifetime could help patients in the realization of dreams. However, theprediction of survivability is directly related to the experience of health professionals andtheir ability to memorize.Most decisions are made based on probability and statistics, but these are based on largegroups of people and may not be suitable to predict what will happen in particular cases.Consequently, the use of machine learning techniques have been explored in healthcare. Theirability to help solve diagnostic and prognosis problems has been increasingly exploited.The main contribution of this work is a prediction tool of survival of patients with cancerof the colon and/or rectum, after treatment and a few years after treatment. The characteristicsthat distinguishes it is the balance between the number of required inputs and theirperformance in terms of prediction. The tool is compatible with mobile devices, includesa online learning component that allows for automatic recalculation and flexibly of theprediction models, by adding new cases.The tool aims to facilitate the access of healthcare professionals for instruments thatenrich their practice and improve their results. This increases the productivity of healthcareprofessionals, enabling them to make decisions faster and with a lower error rate.",
    "authors": [
      "Silva, Ana Paula Pinto da"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59722",
    "title": "Desenvolvimento e prospeção de ferramentas colaborativas nos cuidados de neonatologia e pediatria",
    "abstract": "É cada vez mais importante que os Sistemas de Informação Hospitalar garantam umamelhoria na segurança e na qualidade dos cuidados médicos. Os pacientes neonatais epediátricos são mais vulneráveis que os pacientes adultos tornando essencial orientar asTecnologias de Informação para as suas necessidades.Erros na administração de medicamentos são os erros mais comuns e potencialmentemais nocivos nas instalações hospitalares, sendo a sua taxa de incidência maior na populaçãopediátrica. Neste sentido, torna-se essencial melhorar a segurança do paciente. Para alémdisto, é essencial ao profissional de saúde uma interligação da informação do paciente pelosdiferentes Sistemas de Informação que ele possui ao seu dispor.Esta dissertação tem como principal objetivo a finalização e implementação de uma plataformade apoio à decisão médica através do desenvolvimento de diversas ferramentas, queauxiliem os médicos nas suas atividades diárias e que contribuam para a diminuição dataxa de ocorrência de erro médico. Este desenvolvimento foi acompanhado por um médicopediatra do Centro Hospitalar do Porto.O sistema desenvolvido permite colmatar falhas existentes nos sistemas utilizados atualmenteem algumas unidades hospitalares e, deste modo, obter um sistema que permitisseuma troca de informação e uma comunicação entre os serviços de pediatria e neonatologiae os serviços de farmácia. Assim, é possível facilitar o trabalho diários dos profissionaisde saúde e, ainda, provocar uma diminuição nos efeitos adversos causados por erros demedicação.Sendo um processo acompanhado, a plataforma foi testada e melhorada ao longo dotempo de forma a obter um sistema final satisfatório. Por fim, é lançada uma avaliação aostestes realizados na aplicação.",
    "authors": [
      "Martins, Bia Soraia da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92816",
    "title": "Estudo empírico sobre parâmetros de qualidade na adoção de microsserviços",
    "abstract": "O desenvolvimento de aplicações tem sido alvo de recentes alterações, procurando cada vez mais rapidamente entregar as aplicações aos clientes. Aliado a isto, a procura de uma integração mais eficaz com as várias equipas de desenvolvimento leva à procura por alternativas ao que era feito anteriormente. Indo um pouco ao encontro do que é pretendido, acaba por surgir a arquitetura utilizando microserviços, que apresenta várias vantagens, sendo que muitas vezes é apresentada como a alternativa perfeita à arquitetura monolítica. Por estas razões, em conjunção com a adoção desta arquitetura por grandes empresas, regista um grande crescimento e aceitação nos últimos anos, tanto no mercado, como em contextos académicos. A verdade é que com pesquisas profundas em vários artigos é possível verificar que esta arquitetura também apresenta vários inconvenientes, principalmente relacionados com a sua natureza distribuída, que muitas das vezes acabam por passar despercebidos devido às suas prometidas vantagens e por serem vagamente referidos na maioria dos trabalhos na comunidade cientifica. Este paradigma distribuído acaba por levantar todo um novo conjunto de desafios e associado com o facto de ser uma arquitetura recente, muitas das equipas de desenvolvimento não estão preparadas para fazer a sua implementação corretamente. Como resultado, as aplicações, tem dificuldades em cumprir os objetivos pretendidos. Para mitigar estas questões, começaram a ser desenvolvidos vários padrões para problemas bastante comuns, para a grande maioria das aplicações. Atualmente existem vários padrões para esta arquitetura já desenvolvidos, sendo que para cada problema, podem existir vários padrões desenvolvidos,como o problema de leitura de dados distribuídos em vários serviços. Cada um destes padrões tem as sociados vários compromissos e por esse motivo é importante identificar cada um, de modo a escolher o padrão que melhor se adeque à aplicação que se aspira desenvolver. Para a realização deste estudo empírico, foi definida uma aplicação referência que servirá como base. A seguir, são definidos vários casos de estudo onde são desenvolvidas as diversas aplicações com os padrões em questão. No final, foi feita uma comparação e uma análise dos compromissos relacionados com vários atributos de qualidade.",
    "authors": [
      "Fernandes, Luís Filipe Silva"
    ],
    "keywords": [
      "Microserviços",
      "Estudo empírico",
      "Padrões arquiteturais",
      "Arquitetura de software",
      "Microservices",
      "Empirical study",
      "Architectural patterns",
      "Software architectures",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84065",
    "title": "Development of DNA sequence classifiers based on deep learning",
    "abstract": "Deoxyribonucleic acid (DNA) is a biological macromolecule whose primary function is to store an individual’sgenetic information. Because of breakthroughs in sequencing technology, the number of DNA sequencesis now growing at an exponential rate. The assignment of a function to these sequences is a great obstaclein Bioinformatics, and current methods rely on homologies, a solution that is slow and less accurate.Machine learning (ML) has been widely employed as it is a relevant tool for processing huge amounts ofdata by learning on its own without explicit programming. Using ML, it is now possible to speed up andautomatically classify DNA sequences into existing categories with the objective of learning their functions.However, building a machine learning classifier of biological sequences is a tough challenge due to thelack of numerical properties in the sequence that the model requires. Therefore, it is still necessary to applysome pre-processing techniques so that the sequences are properly represented for the model. Thesetechniques include feature extraction and feature selection, and they are the most difficult componentsbecause sequences lack explicit features. Deep learning models have recently been developed that notonly extract features from input automatically, but also improve the prediction and classification of DNAsequences.The main goal of this project is to create a tool that can automatically classify DNA sequences usingmachine and deep learning models and algorithms, followed by its integration into ProPythia, a Pythonpackage developed by the host group. Automated ML classifiers will also be developed to integrate inOmniumAI software platforms. Transcription factor annotation and essential gene determination will beused as case studies for the platform validation. With this study, it is intended to encourage the use ofsuch technologies to develop new tools that can manage vast volumes of biological data, thus boostingDNA prediction understanding.",
    "authors": [
      "Abreu, João Nuno Cardoso Gonçalves de"
    ],
    "keywords": [
      "DNA",
      "DNA sequence classification",
      "Machine Learning",
      "Deep Learning",
      "Classificação de sequências de ADN"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81342",
    "title": "Regulamento Geral de Protecção de Dados: uma plataforma de apoio à certificação",
    "abstract": "The number of personal data circulating through computer applications and web today isquite large, leading the European Parliament to propose and approve a regulation aimed atprotecting this data.The General Data Protection Regulation (GDPR) is a new European legal framework thatcame into force on May 25, 2018 that focuses on the protection, collection and managementof personal data, i.e. data about individuals. This regulation applies to all companies andorganizations in the European UnionThis Master’s thesis in Computer Engineering focused on creating a solution that has asits main objective to facilitate the work of the people who are responsible for monitoringand ensuring that the regulation is being complied with.This solution emerged in a work context from a sharing of ideas between me and theIdealMais entity, which proposed my integration in the architecture and development teamof the solution.The developed backend has as main features the management of measures, customersand compliance reports, i.e. reports intended to indicate the measures, their status and theactions that should be taken, serving as support for certification.",
    "authors": [
      "Baptista, Sandra Clemente"
    ],
    "keywords": [
      "Certification",
      "General Data Protection Regulation",
      "Software",
      "Programming",
      "Data protection officer",
      "Certificação",
      "Regulamento Geral de Proteção de Dados",
      "Programação",
      "Responsável pela proteção de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28604",
    "title": "Ambient intelligence for monitoring weight and physical activity",
    "abstract": "We have an increasingly sedentary population without the concern about a healthy diet.Therefore, it becomes necessary to give the population the opportunity, despite living avery busy and tiring life, to have control over important aspects of their health. This workaims to analyze and evaluate the impact of an ambient intelligence system on weight controland physical activity in active individuals. To accomplish this objective we have developeda mobile application that allows users to monitor their weight over a period of time,identify the amount of food they consume and the amount of exercise they practice. Universitystudents will be invited and selected, in a first stage, to participate in this study. Allof the students must be considered “active students”, according to our selection criteria.Students with physical disabilities will be excluded from the study. This mobile applicationgives information to the users about dietary and physical activity guidelines in order to improvetheir lifestyles. It is expected that students improve their lifestyles.",
    "authors": [
      "Ferreira, João Manuel Rodrigues"
    ],
    "keywords": [
      "681.324",
      "613"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.324",
      "613"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47842",
    "title": "Ruby on Rails versus Java WEB: estudo comparativo arquitetural e metodológico",
    "abstract": "O desenvolvimento metodológico de aplicações Web multi-camada, satisfazendo o modeloMVC e contendo uma camada de persistência de dados suportada por bases de dadosrelacionais, tem sido suportada fundamentalmente por duas grandes plataformas de desenvolvimento:Java Web e .NET. O aparecimento do open source Framework Ruby onRails, num momento em que a criação de aplicações Web é uma área fundamental daEngenharia de Software, obriga a uma análise detalhada das características desta terceiraplataforma. Este trabalho tem por objetivo fundamental estudar de forma comparativa asarquiteturas de software e tecnologias propostas por Ruby on Rails versus as tecnologiasJava Web (i.e Apache Struts2) e, ainda, comparar as metodologias e tecnologias de suporteao desenvolvimento subjacentes a ambos os ambientes.",
    "authors": [
      "Maia, Paulo Jorge Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84496",
    "title": "Opportunistic Wi-Fi network selection in heterogeneous vehicular wireless networks for detecting VRUs through edge computing",
    "abstract": "The future of vehicles is for them to become smarter: able to perceive the its surroundings, detect dangeroussituations, and act accordingly. To realize this vision, vehicles must collect information and share it with others,allowing them to have shared knowledge of an event that their sensors could not yet detect. However, somevehicles may not have enough computational resources to process the information and comply with low-delayrequirements, and may need to offload the data to edge computing platforms to extend their own processingcapabilities.While offloading, a vehicle may have multiple access networks at their disposal and by choosing the bestnetwork it may maximize the amount of data it can offload. These access networks include the mobile 5Gnetwork and Wi-Fi access points, which the operator can integrate within the 5G system to take advantage ofunlicensed spectrum in the 2.4, 5 and 60 GHz ranges. In the current work, we focused on leveraging Wi-Fi andallowing vehicles to decide which access network to use among three Wi-Fi 802.11n/ac/ad networks.We developed a Wi-Fi performance monitoring and decision-making system (WiPerf) that can: (i) collectthroughput measurements and channel state information for multiple Wi-Fi networks; (ii) estimate throughputusing passive measurements; (iii) predict the next 40 seconds of throughput; (iv) decide which network to use,based on the throughput forecasts and on the time it takes to switch to another network. The system wasimplemented in a real-world setup with two vehicles and two TP-Link Talon AD7200 access points. We performedan initial set of experiments to collect a dataset to develop estimation and forecasting models, and a second setof experiments to validate our decision-making system.For throughput estimation, we developed the UKF-SR model, a novel approach that combines Symbolic Re gression with a non-linear recursive Bayes filter. Results showed that our solution was superior to NN, DT, andRF models, with these having higher RMSE values by at least 4.94 %, 38.09 %, and 9.59 % for 802.11n/ac/ad,respectively. Considering forecasting, we adapted previously developed spatial-clustering models, that forecastthroughput based on a set of similar historic samples, and compared them with a time-series approach, us ing ARIMA and VAR models. VAR showed the best results among the time-series models, but they were stilloutclassed by spatial-clustering, considering both MAE and MASE values.We integrated the estimation and forecasting models with a decision-making algorithm. The algorithm sched ules which networks the vehicle should use considering the time if takes to switch networks, to maximize theamount of data it can offload. Compared with the optimal solution, based on the real throughput measurementsand without forecasting (perfect prediction), the results show that our approach has near-optimal performancewith an average throughput of only 4.43 % less than the optimal one.",
    "authors": [
      "Teixeira, Daniel Filipe da Rocha"
    ],
    "keywords": [
      "Vehicular networks",
      "5G system",
      "Wi-Fi",
      "Machine learning",
      "Throughput prediction",
      "Redes veiculares",
      "Sistemas de transporte inteligente",
      "Sistema 5G",
      "Aprendizagem automática",
      "Estimação de taxa de transmissão",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27887",
    "title": "iLeisure : an intelligent free time scheduler based on user profiling",
    "abstract": "O envelhecimento populacional é uma realidade dos nossos tempos. O envelhecimento provoca muitas vezes o problema do sedentarismo, que é associado à obesidade e doenças cardiovasculares. O sedentarismo é por isso um problema preocupante que para além de afetar as idades avançadas afeta todas as outras faixas etárias. É nos tempos livres que as pessoas são mais sedentárias.A prática de atividades físicas combate o sedentarismo e é benéfica em termos psicológicos, sociais e económicos. É necessário incentivar as pessoas a praticá-la e uma das formas de o conseguir é através de um sistema que recomende tais atividades. A recomendação deverá contudo ter em consideração as preferências das pessoas pelas atividades, para que estas tendam a aceitar as atividades que são recomendadas. As preferências de uma pessoa são associadas ao conceito de perfil e é através deste, que um sistema de recomendação infere sobre que atividades recomendar. É por isso imperativo que o perfil esteja constantemente atualizado e consistente com a pessoa que representa. Garantir estas propriedades num perfil não é computacionalmente claro, devido às propriedades naturais de um perfil e devido à capacidade das pessoas em evoluir e variar nas suas preferências.Este trabalho tem como objetivo desenvolver um sistema que recomende às pessoas atividades que não sejam sedentárias e que incentivem o convívio social. Para que o sistema seja preciso nas atividades que recomenda, é também objetivo deste trabalho desenvolver um mecanismo para obtenção, representação e modulação de perfis que garanta a sua dinamicidade, que garanta que os perfis sejam eficientemente adaptáveis e atualizáveis.Realizaram-se observações acerca da taxa de aceitação de utilizadores, em relação às atividades recomendadas pelo sistema desenvolvido. Estas observações assistiram na avaliação da precisão do sistema em recomendar e a sua capacidade em acompanhar os utilizadores e aprender acerca das suas preferências. Consultando periodicamente a quantidade de atividades recomendadas aceites ou rejeitadas, verificou-se que o número de atividades aceites tende a aumentar e as rejeitadas a diminuir. Estes resultados demonstram que o sistema possui a capacidade de aprender e aperfeiçoar o perfil dos seus utilizadores, tornando-se mais preciso nas atividades que recomenda. Acredita-se desta forma que o mecanismo desenvolvido para a obtenção e modulação de perfis dinâmicos é eficiente e robusto.",
    "authors": [
      "Marques, Vítor"
    ],
    "keywords": [
      "681.3",
      "613.98"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3",
      "613.98"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/62428",
    "title": "Desenvolvimento e optimização de back-end de compilador criptográfico para plataformas ARM",
    "abstract": "Devido à elevada proliferação tecnológica, existe software criptográfico implementado numa miríade deplataformas. Plataformas essas que podem ter características de computação bastante diferentes. Noentanto, o software criptográfico deverá ser imperceptível ao utilizador final, uma vez que deve funcionarcomo uma camada de protecção às tarefas que o utilizador possa estar a realizar e não aumentar a suapegada computacional. Essa exigência leva muitas vezes a que o software criptográfico tenha que serre-implementado de acordo com as capacidades computacionais da plataforma/dispositivo-alvo (sendo atécomum recorrer-se a assembly para atingir tal objectivo). O desenvolvimento de software criptográficotambém exige que o programador seja versado em diversas áreas da ciência, devendo assim ser realizadopor criptógrafos especializados.O CAO é uma DSL imperativa para a área da criptografia. Munido de um compilador, interpretador euma ferramenta de verificação formal, o CAO permite a passagem de conhecimento criptográfico para umprogramador com menos experiência na área da criptografia através da automatização da validação formaldas implementações. O compilador CAO possui um back-end altamente configurável que permite geraçãode executáveis dedicados às plataformas destino.Com a grande utilização actual de processadores ARM, torna-se interessante estudar formas de exploraras características destes processadores e desenvolver um back-end que as implemente com o objectivo deobter melhor desempenho do software criptográfico. Nesta dissertação explorou-se a utilização dos tiposde dados vectoriais e instruções do co-processador NEON de forma a obter paralelismo ao nível dos dados.Também foi estudada a possibilidade de inclusão de paralelismo nativo ao nível das tarefas no CAO, deforma a tirar partido das arquitecturas multi-core.",
    "authors": [
      "Boas, Rui Pedro Araújo Vilas"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/62544",
    "title": "Combined MRI with non-image clinical data for brain tumor classification: a CNN/DL approach",
    "abstract": "Prognosis and patient stratification for brain tumors is an important and clinically relevant task and a precise treatment outcome prediction would allow to choose an adequate therapy strategy and schedule the most appropriate follow-up examinations. Magnetic Resonance Imaging (MRI) is an already know imaging technique to assess these tumors. Next to medical imaging, other clinical information is important for patient management, e.g. genetic markers like O6-Methyl-Guanine-Methyl-Transferase (MGMT) methylation is a well-known prognostic marker in Glioblastoma (GBM) tumors.Therefore, the main goal of this thesis was to study Deep Learning (DL) approaches to combine MRI with non-image clinical data in two different classification scenarios: brain tumor segmentation and patient outcome prediction. There are studies that combine these two types of data, however, in two steps: extracting MRI features and then combining them with relevant non-image data. Here, end-to-end DL architectures with two input layers are presented, as well as an infrastructure that allows the easy development of future Machine Learning (ML) /DL models that consumes these two types of data in a clinical context. In this way, the classification in both scenarios is done in a single step, where Convolution Layers perform the feature extraction in MRI input.In brain tumor segmentation, the model with combined data achieved a slightly better Dice Similarity Coefficient (DSC) (0.894 ± 0.025) over image only model (0.882 ± 0.025). As for patient outcome prediction, when trying to predict the Progression-Free Survival (PFS) class (“bad”,” medium” and “good” outcomes), the combined model didn't improve when compared with the model where only MRI was used. Both models, however, outperformed models where only non-image data was used.The segmentation results point to a positive influence when adding the clinical information to MRI. Nevertheless, there is a lot more to investigate in this field, not only in the model architecture, but also in selecting relevant clinical information. In same way, more tests should be run for patient outcome prediction, especially using Overall Survival (OS) information.",
    "authors": [
      "Espanha, Raphael Alves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92797",
    "title": "Arquitetura integradora com SNMP para gestão de edifícios",
    "abstract": "Ao longo das últimas duas décadas a utilização de sistemas domóticos, num mundo cada vez maisconectado e tecnológico, tem vindo a revelar-se cada vez mais atrativo e com maior aceitação do públicoem geral. Novos produtos suportados por novos protocolos e tecnologias estão constantemente a serintroduzidos no mercado. No entanto, este desenvolvimento foi quase sempre efetuado sem grandepreocupação em definir regras e normas para que fosse possível a interoperacionalidade entre produtosde diferentes fabricantes, originando soluções pouco modulares, de elevado custo e forçando os clientes aescolher um ecossistema de um mesmo fabricante sem ser possível de forma rápida e facilitada integrartecnologias de vários fabricantes num mesmo sistema.O principal objetivo desta dissertação foi a definição de uma arquitetura integrada para sistemasdomóticos baseada no protocolo de gestão SNMP e que permitisse ultrapassar algumas das mais importantes limitações das soluções atuais para este tipo de sistema. Nesse sentido foi criada uma novaMIB domótica para implementação num agente SNMP integrador. Além disso, foi desenvolvido umnovo protocolo de gestão para dispositivos domóticos, mais simples que o SNMP e mais adequado paragestão de pequenos equipamentos sensores ou atuadores utilizados em sistemas domóticos. Este protocolo, designado por Light SNMP (L-SNMP), será utilizado na comunicação entre o agente SNMPe os dispositivos domóticos que implementam uma Light MIB (L-MIB) de domótica. No decorrer doprojeto foi criado um sistema protótipo com dispositivos domóticos implementando a L-MIB de domótica,um agente SNMPv2 implementando a MIB domótica e uma aplicação gestora SNMP que contém umsimples interface com o utilizador. As experiências realizadas com este protótipo permitiram confirmar acorreção funcional da solução e a sua viabilidade como alternativa tecnológica válida, potencialmente debaixo custo e com elevados níveis de interoperabilidade.",
    "authors": [
      "Nogueira, Gonçalo Pinto"
    ],
    "keywords": [
      "Domótica",
      "SNMP",
      "SNMPv2",
      "Agente SNMP",
      "MIB",
      "Light SNMP",
      "Light MIB",
      "Dispositivos domóticos",
      "Home automation",
      "SNMP Agent",
      "Home automation devices",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/46400",
    "title": "Desempenho de uma aplicação web para câmaras ONVIF e notificação de eventos",
    "abstract": "O recurso a sistemas de videovigilância tem-se tornado cada vez mais popular. No entanto,cada fabricante deste tipo de equipamentos desenvolvia os seus próprios protocolos de comunicação,não existindo compatibilidade entre diversos sistemas de videovigilância. Estecenário era economicamente prejudicial para os consumidores, e dificultava o desenvolvimentode sistemas que integrem equipamentos diferentes ou de diferentes fabricantes. Foientão necessário criar um protocolo comum a todos eles.Nesse contexto surgiu o Open Network Video Interface Forum (ONVIF), uma organiza-ção sem fins lucrativos composta pelas principais companhias deste ramo que tem comoobjetivo desenvolver normas para estes dispositivos. A norma ONVIF baseia-se em serviçosweb Simple Object Access Control (SOAP) e também em protocolos que já estão padronizadoscomo o HyperText Transfer Protocol (HTTP) ou Real-time Transfer Protocol (RTP). Osdispositivos ONVIF são divididos em Network Video Transmitter (NVT), Network VideoDisplay (NVD), Network Video Storage (NVS) e Network Video Analytics (NVA).O HyperText Markup Language (HTML) era, inicialmente, utilizado para definir a estruturade documentos. No entanto, devido à sua baixa complexidade de utilização tornou-serapidamente a linguagem de marcação mais utilizada para a construção de páginas web.Hoje em dia, a mesma está na quinta versão a qual permite maior flexibilidade na utiliza-ção de conteúdo multimédia. Estas páginas juntamente com plugins ou com a linguagem deprogramação JavaScript são capazes de constituir as Rich Internet Application (RIA), aplicaçõesque são executadas em ambiente web. Devido à falta de segurança e instabilidadecausados pelos plugins, hoje em dia começa a ser utilizado apenas o JavaScript.Desta forma, foi desenvolvida uma aplicação web que consiste num cliente que faz acomunicação com um Web Service (WS) Representational State Transfer (REST). Este porsua vez, encontra-se alojado num servidor HyperText Transfer Protocol (HTTP) Apachee está implementado como um Fast Common Gateway Interface (FastCGI). Este FastCGIutiliza a biblioteca UMOC para transferir dados com dispositivos NVT (câmaras InternetProtocol (IP)).O objetivo deste projeto é aumentar o desempenho desta aplicação existente, tanto noservidor como no cliente e ainda implementar novas funcionalidades do ONVIF.Foram desenvolvidas soluções para o lado do cliente que permitem que a aplicação sejaexecutada com maior velocidade e com menor consumo de recursos e foram também implementadasnovas funcionalidades. De entre as contribuições técnicas destacam-se a utiliza-ção da Web Storage em vez da Indexed DB, a transformação da Application Programming Interface (API) de comunicação com as câmaras mais percetível e mais eficaz e a apresentaçãodos dados de forma dinâmica. Em termos de funcionalidades, foi adicionadoo suporte à receção dos eventos da câmara utilizando Server Sent Events (SSE). No quetoca ao lado do servidor, foi realizado o estudo experimental dos servidores HTTP maisconhecidos pela sua eficiência e implementação do WS-Notification através da ferramentagSOAP.",
    "authors": [
      "Varela, José Luís Cerqueira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81324",
    "title": "Dynamic genome-scale modelling of the Saccharomyces non-cerevisiae yeasts metabolism in wine fermentation",
    "abstract": "The wine industry is facing challenging times due, mostly, to climate change and changingconsumer demands. The urge to innovate stimulates R&D of new fermentation processesusing non-conventional yeast species (e.g. non-cerevisiae Saccharomyces species).While recent research approached the physiology of diverse non-conventional yeastspecies, little is known about their metabolism in different environmental conditions.In this work, a previously developed dynamic genome-scale model was adapted tostudy the metabolism of Saccharomyces kudriavzevii in wine fermentation at two temperatures,25ºC and 12ºC. Adjustments included the addition of metabolic pathways anddynamic constraints. Goodness-of-fit of the model to measurements of the extracellularcompounds was satisfactory, i.e. the median values of R2 are 0.95 and 0.87 for 25ºC and12ºC, respectively.The model was then used to explore the differences in the dynamics of metabolismbetween temperatures. The most significant differences appeared in the stationaryphase: 1) the strain produces more mevalonate and succinate at 25ºC, probably dueto a late response to stress and the maintenance of redox balance via the GABA shunt,respectively, 2) erythritol flux is higher at 12ºC, probably due to the conditions of formationlasting longer and 3) the production of higher alcohols, mostly de novo, is higher at 12ºC,due to the longer viability of the cells.The proposed model provided a comprehensive picture of the main steps occurringinside the cell during wine fermentation. Model predictions are consistent with experimentaldata and previous findings, but it also brought novel results, such as the role ofthe GABA shunt or the production of mevalonate in the metabolism of S. kudriavzevii,worth being explored further.",
    "authors": [
      "Santos, David Miguel Ferreira dos"
    ],
    "keywords": [
      "Fermentation",
      "Metabolism",
      "Modelling",
      "S. kudriavzevii",
      "Fermentação",
      "Metabolismo",
      "Modelação",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81529",
    "title": "Quantum random walks: simulations and physical realizations",
    "abstract": "Quantum computing is an emergent field that brings together Quantum Mechanics,Computer Science and Information Theory, which promises improvements to classicalalgorithms such as simulation of quantum systems, cryptography, data base searching andmany others. Among these algorithms, quantum walks may provide a quadratic speed upwhen compared to their classical counterparts, allowing improvements to applications suchas element distinctness, searching problems, matrix product verification and hitting timesin graphs. The present work offers a general theoretical overview, simulation and circuitimplementation of the coined, staggered and continuous-time quantum walk models. Thefirst two chapters of this thesis are dedicated to the definition of the theoretical framework,simulation in Python and comparison of the aforementioned quantum walk models for thesimple case of the dynamics in a line graph and for the search algorithm in a completegraph. This is then used as a benchmark for the final chapter, devoted to building andtesting the circuits corresponding to models mentioned above in IBM’s Qiskit. A maincontribution of this dissertation concerns the circulant graph approach to diagonal operatorsfor continuous-time quantum walks.",
    "authors": [
      "Santos, Jaime Pereira"
    ],
    "keywords": [
      "Quantum computing",
      "Quantum walks",
      "Python",
      "Qiskit",
      "Computação quântica",
      "Caminhadas quânticas",
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81344",
    "title": "Development of a system compliant with the Application-Layer Traffic Optimization Protocol",
    "abstract": "With the ever-increasing Internet usage that is following the start of the new decade,the need to optimize this world-scale network of computers becomes a big priorityin the technological sphere that has the number of users rising, as are the Quality ofService (QoS) demands by applications in domains such as media streaming or virtualreality.In the face of rising traffic and stricter application demands, a better understand ing of how Internet Service Providers (ISPs) should manage their assets is needed. Animportant concern regards to how applications utilize the underlying network infras tructure over which they reside. Most of these applications act with little regard forISP preferences, as exemplified by their lack of care in achieving traffic locality duringtheir operation, which would be a preferable feature for network administrators, andthat could also improve application performance. However, even a best-effort attemptby applications to cooperate will hardly succeed if ISP policies aren’t clearly commu nicated to them. Therefore, a system to bridge layer interests has much potential inhelping achieve a mutually beneficial scenario.The main focus of this thesis is the Application-Layer Traffic Optimization (ALTO) work ing group, which was formed by the Internet Engineering Task Force (IETF) to explorestandardizations for network information retrieval. This group specified a request response protocol where authoritative entities provide resources containing networkstatus information and administrative preferences. Sharing of infrastructural insightis done with the intent of enabling a cooperative environment, between the networkoverlay and underlay, during application operations, to obtain better infrastructural re sourcefulness and the consequential minimization of the associated operational costs.This work gives an overview of the historical network tussle between applicationsand service providers, presents the ALTO working group’s project as a solution, im plements an extended system built upon their ideas, and finally verifies the developedsystem’s efficiency, in a simulation, when compared to classical alternatives.",
    "authors": [
      "Caldas, Paulo Edgar Mendes"
    ],
    "keywords": [
      "Application-layer Traffic Optimization",
      "Content distribution networks",
      "Network optimization",
      "Peer-to-peer",
      "Traffic engineering",
      "Engenharia de tráfego",
      "Otimização de rede"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27900",
    "title": "Modularidade em Java : o impacto do projeto Jigsaw",
    "abstract": "A modularidade é um conceito importante na implementação de sistemassuportados por software. A linguagem Java é uma das linguagens utilizadas paraimplementar este tipo de sistemas.Esta dissertação apresenta um estudo sobre os conceitos de modularidade que oprojeto Jigsaw propõe para a linguagem Java, demonstrando como se comparamcom o estado de arte de modularidade em ambientes de desenvolvimento Java,as melhorias para a linguagem Java e para os sistemas de softwaredesenvolvidos em Java, nomeadamente sistemas baseados em servidoresaplicacionais.O projeto, através do conceito de modularidade proposto, introduz alteraçõesimportantes na linguagem e plataforma Java, na forma de desenvolvimento edistribuição de aplicações e esta dissertação pretende, através de análise edemonstração, mostrar a importância da metodologia apresentada e de queforma pode melhorar e substituir as várias metodologias de modularidade emJava atualmente existentes.No âmbito desta dissertação, é apresentada uma aplicação informática, na formade prova de conceito, desenvolvida utilizando a linguagem Java, que procuraautomatizar processos associados à aplicação da metodologia Jigsaw nodesenvolvimento de aplicações.As conclusões deste estudo permitem perceber que o Jigsaw apresenta melhoriassignificativas que devem ser incorporadas no Java mas, permitem tambémperceber a existência de limitações que devem ser corrigidas por forma a tornar oconceito mais abrangente para ser utilizado nos mais variados cenários,nomeadamente na implementação de aplicações complexas, como é o caso deservidores aplicacionais. A plataforma Java encontra-se numa fase de evolução sensível, onde decisões que estão a ser tomadas pelas várias entidades quedeterminam o futuro da plataforma podem implicar o sucesso ou fracasso daplataforma, sendo o Jigsaw um ponto em aberto nesses processos de decisão.",
    "authors": [
      "Santos, Luís Fernando Rodrigues Loureiro dos"
    ],
    "keywords": [
      "Modularidade em java",
      "Metodologias de desenvolvimento",
      "Arquiteturas modulares",
      "Programação estruturada",
      "Jigsaw",
      "Modularity in java",
      "Development methodologies",
      "Modular architectures",
      "Structured programming",
      "681.3.062"
    ],
    "date": "2011",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3.062"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81106",
    "title": "Especificação e implementação de um workflow numa plataforma para classificação e avaliação documental",
    "abstract": "A substituição do papel pelo formato digital nas instituições e empresas tornou-se umaprática comum, sendo que algumas já extinguiram a utilização de formatos analógicos.As políticas europeias incentivam a que sejam adotadas medidas para a redução de papel.Desta forma, a administração pública tem abandonado a utilização de suportes analógicos,substituindo-os pelo formato digital, tendo as entidades públicas passado a prestar os seusserviços e a disponibilizar a documentação de forma eletrónica.A Plataforma “CLAV - Classificação e avaliação da informação pública” desenvolvida pelaDGLAB pretende disponibilizar instrumentos, tal como a Lista Consolidada; a mediação des materializada da produção de tabelas de seleção; e a prestação de um serviço automatizadode controlo da eliminação da informação pública. A adoção de esquemas de metainformaçãopara a interoperabilidade permite a disponibilização de uma linguagem comum aos váriosorganismos da Administração existente na Lista Consolidada, através da plataforma CLAV,permitindo ainda a integração com sistemas de informação organizacionais e a troca deinformação entre entidades.Em concordância com os objetivos delineados para o desenvolvimento do projeto, realizou se um estudo sobre abordagens idênticas ao CLAV noutros países, onde foram identificadasas formas de classificação desenvolvidas e as tecnologias utilizadas para essa implemen tação, fazendo uma comparação com a abordagem de Portugal. O plano de trabalho doprojeto dividiu-se então em três fases. Inicialmente realizou-se um estudo teórico acerca daabordagem de Andaluzia (Espanha) e a de Portugal. Seguiu-se com o desenvolvimento deum modelo gráfico acompanhando as especificações do Business Process Model and Notation e,por fim, a implementação e o desenvolvimento desse modelo na plataforma CLAV.Verificou-se que, após a implementação do workflow para a gestão dos pedidos, este tornoutodo o processo mais simples e rápido para os responsáveis pelo tratamento destes, umavez que todo o processo é realizado através da plataforma, não havendo a necessidade detransitar documentos físicos.",
    "authors": [
      "Araújo, Rui Filipe Ferreira"
    ],
    "keywords": [
      "Administração Pública",
      "CLAV",
      "Web Semântica",
      "Workflow",
      "Public Administration",
      "Semantic Web",
      "Workflow"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/23499",
    "title": "Sistema de geo-localização referencial para pessoas com perdas cognitivas",
    "abstract": "A tecnologia tem revelado avanços significativos ao longo dos anos. Porém os desenvolvimentosnão se focaram apenas na área da computação, expandido-se para outras áreas como a saúde,permitindo o desenvolvimento de novas técnicas de diagnóstico ou o aperfeiçoamento das existentes.Desta forma a qualidade de vida e os cuidados de saúde disponibilizados pelas instituiçõesàs pessoas foram melhorados. No entanto, estes progressos não permitem a total supressão depatologias, existindo algumas sem uma cura efetiva como é o caso das perdas cognitivas.O diagnóstico de perdas cognitivas pode potenciar algumas modificações na vida do pacientecomo, por exemplo, a presença de uma pessoa prestadora de cuidados, provocando a perda deindependência do doente. De forma a diminuir a invasão de privacidade, vários investigadoresdesenvolveram projetos para permitir a orientação destas pessoas, porém exigiam algum esforçomental que pontualmente se poderia tornar demasiado complexo.O projeto desenvolvido pretende orientar as pessoas com perdas cognitivas, permitindo queestas tenham uma vida mais independente. Por outro lado, a pessoa prestadora de cuidadospode desenvolver outro tipo de atividade sem descurar o tipo de serviço que presta através deaplicações que lhe permitem conhecer a posição atual da pessoa a seu cargo.Com este projeto pretende-se que a pessoa com perdas cognitivas tenha uma maior independênciatanto em trajetos comuns como em percursos de lazer. Este grau de independência épossível através da utilização de uma aplicação informática para dispositivos móveis que permitea seleção de diversos pontos de interesse e posterior orientação até estes. Este projeto utilizaconceitos de realidade aumentada e sinal GPS para aquisição da localização atual, indicando ocaminho através de símbolos simples.Numa outra aplicação para dispositivos móveis o percurso pode ser visualizado em temporeal possibilitando uma constante monitorização da pessoa com perdas cognitivas. No caso deocorrer alguma eventualidade é possível consultar os últimos pontos frequentados por esta.",
    "authors": [
      "Ramos, João Ricardo Martins"
    ],
    "keywords": [
      "616.8:681.324",
      "681.324:616.8"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "616.8:681.324",
      "681.324:616.8"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91569",
    "title": "ACE grader automatic grading of programming exercises",
    "abstract": "Despite their rising usage in classrooms, most automatic grading tools for programmingexercises are quite simple, using only output comparison or unit tests to evaluate a solution,in contrast with manual grading methods used by teachers, which also look at the codeitself, even if it doesn’t produce a correct solution. Static analysis methods for code havebeen around for a while, but largely ignored in assessment software.The Master’s project here reported proposes an automatic grading method for pro gramming exercises that, in addition to dynamic analysis, uses static analysis to evaluatesubmissions. This method benefits both teachers and students, since, by scoring solutionsthat produce the wrong output, it provides a more comprehensive evaluation of student submitted programs while also making it easier to see exactly what needs to be improved.Moreover, it makes evaluation more rigorous, by requiring more than just a program thatsolely produces the correct result. A prototype application called ACE Grader was createdto demonstrate the efficacy of this grading strategy.This dissertation describes a bibliographic review of existing automatic grading tools,proposes and introduces ACE Grader through an overview of its architecture and itsdevelopment process. As an initial version of the application was deployed in the middleof the second semester of university classes, experiments with students in a real classroomsetting are also presented and discussed.",
    "authors": [
      "Santos, Sofia Guilherme Rodrigues dos"
    ],
    "keywords": [
      "Automatic assessment",
      "Assessment software",
      "Programming exercises",
      "Dynamic analysis",
      "Static analysis",
      "Education technology",
      "Avaliação automática",
      "Programa de avaliação",
      "Exercícios de programação",
      "Análise dinâmica",
      "Análise estática",
      "Tecnologia educacional",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47831",
    "title": "Televisão interativa: recomendação de conteúdos multimédia",
    "abstract": "A televisão de hoje em dia disponibiliza um enorme lote de conteúdos. Quando existe umleque de conteúdos de grande dimensão é difícil optar pela solução que mais nos agrada ou quemelhor satisfaz as exigências dos espetadores. O tempo consumido na seleção pode mesmo esgotaro tempo que o espetador dispõe para visualizar o conteúdo selecionado. Este facto influenciao nível de stresse nas pessoas. Para tentar responder a este problema, recorreu-se a técnicas deinteligência artificial para a criação de um sistema inteligente que seja capaz de proporcionar aoutilizador uma melhor experiência televisiva, contribuindo para o seu bem estar e diminuição dosseus níveis de stress, reduzindo o tempo gasto.",
    "authors": [
      "Machado, Luís Duarte Dias"
    ],
    "keywords": [
      "Stress",
      "Conteúdos multimédia",
      "Sistemas de recomendação",
      "Televisão interativa",
      "Multimedia content",
      "Recommender systems",
      "Interactive television",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82556",
    "title": "Decisões automatizadas e scoring no crédito ao consumo",
    "abstract": "Em plena revolução digital, seria inconcebível que o crescendo na procura de crédito ao consumo não fosse acompanhado pela maior preponderância que mecanismos como o scoring baseado em téc nicas de Inteligência Artificial representa para uma mais e melhor avaliação da solvabilidade em sede de decisões automatizadas. Vislumbra-se o começo de um mundo novo em que a aceitação ou rejeição no pedido de empréstimo já não é tão-só conatural a decisões de puro julgamento dos analistas. E se, por um lado, a possível integração de megadados de crédito em modelos analíticos pode, até certo ponto, minorar o esforço das análises preditivas, em contramão, maiores serão os riscos de preterição do rango de tutela do bem jurídico proteção de dados, tal-qual prescrito no (ou pelo) Direito dos Livros. De facto, tópicas como esta exibem uma complexidade tal que é impossível olvidar debruçar-se sobre as mesmas tendo por base uma única lente jurígena, sem se ampliar o pendor multidisciplinar que a uma investiga ção desta índole deve subjazer. Como tal, principiara-se o presente ensaio com o discorrer da temática em contexto histórico e sociotécnico. E porque a concessão de crédito ao consumo é, no atual estado de arte scoring, não só comandada pelos modelos de exploração e aprendizagem automática, mas antes, e sobretudo, pela qualidade dos conjuntos de dados que os alimentam, ainda em contexto prolegómeno, epitomaram--se os elementos-chave que concorrerão para um scoring de crédito mais exato e, espera- -se, cada vez mais transparente. Foi, portanto, a partir destas premissas técnico-científicas que se abor dou a figura jurídica dos contratos de crédito ao consumo, com especial enfoque na contratação rápida em linha e nas diligências pré-contratuais concernentes à injuntividade da avaliação de solvabilidade. Em última instância, porquanto o objeto de estudo suscita uma energética (ou antes, idiossincrática) tutela sobre a proteção de dados pessoais, foi à luz do emaranhado axiológico-normativo do regime das deci sões individuais e exclusivamente automatizadas, incluindo a definição de perfis, bem como das quimé ricas salvaguardas adequadas (ou antes, ilusórias) consagradas e sugeridas pelo legislador europeu,tanto no Regulamento Geral sobre Proteção de Dados Pessoais, como na Proposta de Diretiva, relativa aos créditos aos consumidores, de 30 de junho de 2021, que se conclui a necessária adoção de políticas setoriais que primem por uma lhana infoliteracia financeira dos (ciber)consumidores.",
    "authors": [
      "Rebelo, Diogo José Morgado"
    ],
    "keywords": [
      "Avaliação de solvabilidade",
      "Crédito ao consumo",
      "Dados",
      "Decisões automatizadas",
      "Inteligência Artificial",
      "Métodos preditivos",
      "Scoring",
      "Artificial Intelligence",
      "Automated decision-making",
      "Consumer credit",
      "Creditworthiness",
      "Data",
      "Predictive methods",
      "Scoring",
      "Ciências Sociais::Direito"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Sociais::Direito"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79990",
    "title": "Urban evolution of Fafe in the XIX and XX centuries",
    "abstract": "The movement of people from dispersed living to concentration in urban environmentsis a large change both for human civilization and for the environment. Urbanization isthe process of changing from natural habitats to dense grey space made up primarily ofbuildings, roads, and accessory infrastructure accompanied by dense human populations.While many cities are well established, humans continue to build new cities or expand citiesoutward in a network of suburban environments. And urbanization is not simply about atransition from green to grey space, other abiotic changes such as changes in light regimensdue to artificial lighting, increased pollution, and increased impervious surfaces leadingto runoff are found in urban areas. The study of Urban Evolution of Fafe in the XIX andXX Centuries is an interesting theme not only because of the lack of works in this area butalso because of the possibility of understanding the organization of the current city. Themain problem that we faced it was that as the years go, the mapping and buildings of citieschange. And the information of these changes is stored in texts, records, maps, etc. Thisfact made the study of urban evolution difficult because the information is widespread andhard to gather. So, in order to study Fafe urban evolution we needed to recover and gatherinformation of the changes and new buildings in the city during the XIX and XX centuries.Given the inexistence of an exhaustive investigation of an urban history we had to seek tointerpret from the present formation the successive processes of urbanization and respectiveextensions, juxtapositions and overlaps. More important is the diverse set of sources thatallowed to characterize the urbanism of the city of Fafe. With that said it was importantto create an integrated repository in digital format to enable its analysis and search ofinformation, and visual exploration through a map. For that purpose, it was necessaryto create ontologies related to urban evolution that allowed us to develop web-supportedtools derived from these ontologies for the acquisition of the state and the location of thebuildings and in order to analyze the changes as the years go by. The web-supported toolsare available in http://www4.di.uminho.pt/∼gepl/UEF/.",
    "authors": [
      "Lameiras, João Filipe Campos"
    ],
    "keywords": [
      "Urban evolution",
      "Urban research",
      "Urban morphology",
      "Ontology",
      "XML",
      "Evolução urbana",
      "Investigação urbana",
      "Morfologia urbana",
      "Ontologia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28394",
    "title": "Safety critical interactive computing systems' modelling",
    "abstract": "Typically, testing an interactive system involves manually testing their possible interactions. Since this is a manual process, it becomes very costly to check all possible interactions. In safety critical interactive systems this task is essential. One way to overcome this problem is to use tools for systematic analysis. IVY Workbench is one of these tools. We plan to apply it to perform verification of Safety Critical Interactive Systems. The objectives for this dissertation are: development of a set of models of safety critical interactive systems; verification of relevant properties of the models; critical assessment of the modelling process and suggestion of improvements to the tool and language.",
    "authors": [
      "Sousa, Manuel António Freitas de"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/86513",
    "title": "Green communications: an environment to support energy-aware networks developments",
    "abstract": "The continuous joint growth of the communication networks is causing a simultaneous increase inthe energy consumption of these infrastructures. To help fight the consequent environmental impact isrequired that new traffic engineering techniques are developed to help mitigate this energy consumption.This work proposes the Flexcomm Simulator 1, a simulation environment with the intent of creating aplatform that helps to develop new routing algorithms, that combine Software Defined Networking (SDN)and Energy Flexibility techniques to optimize the energy consumption of large scale wired communicationnetworks. The environment allows simulating real infrastructures conditions, so policies can be developedclose to real scenarios, facilitating their deployment in real environments.The tool’s abilities have been demonstrated, by generating data that allows the evaluation of differentrouting strategies. Moreover, the Flexcomm Simulator has made possible the developments on early workof new algorithms that demonstrate the ability to achieve a more balanced energy consumption. Thesenew techniques are capable of adapting to changes in energy availability and relocating network flowsacross different regions of a network, respecting flexibility imposed by electrical grids while maintaining aminimum Quality of Service (QoS).",
    "authors": [
      "Monteiro, Rui Pedro da Cunha"
    ],
    "keywords": [
      "Network simulators",
      "Software defined networking",
      "Energy models",
      "OpenFlow",
      "Simuladores de rede",
      "Modelos de energia",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/55844",
    "title": "Development of web-based tools for spectral data analysis and mining",
    "abstract": "The recent advances in different analytical techniques able to produce spectral data, includingRaman, Infrared (IR) or Ultraviolet-Visible (UV-vis) spectroscopies, have provided novelapproaches for many research issues in the biological and chemical fields. Indeed, they haveallowed to address tasks in functional genomics, sample characterization and classification,or drug discovery. To take full advantage of these data, advanced bioinformatics methodsare required for data analysis and mining.A number of methods and tools for spectral data analysis have been put forward recently,being one of the major limitations still faced the lack of integrated frameworks for extractingrelevant knowledge from these data and being able to integrate these data with previousbiochemical knowledge. Also, the lack of reproducibility in many data analysis or datamining processes is a strong obstacle for biological discovery, being common the lack ofdata and data analysis pipelines in the published work.In recent work from the host group, specmine, a metabolomics and spectral data analysis/mining framework, in the form of a package for the R system, has been developed toaddress some of these issues. In this thesis, the main aim was to design and develop anintegrated web-based platform for spectral data analysis and mining, based on the specminepackage, providing an easier and more user friendly interface, but also addressing some ofthe package’s current limitations.The developed platform contains features that cover the main steps of the metabolomicsdata analysis workflow, with modules for data reading and dataset creation, data preprocessingand a variety of analysis types. It includes an authentication system, allowingthe user to have his own personal workspace where projects can be stored and accessedlater, with the option to share projects with other users. The different modules were validatedusing real data from previously published studies in the host group, related to theanalysis of the characteristics and potential of natural products, addressing as well theexploration and integration of data from distinct experimental techniques, attesting theplatform’s robustness and utility.",
    "authors": [
      "Afonso, Telma Adriana Pereira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84081",
    "title": "Two-level fingerprint-based indoor positioning using advanced Machine Learning",
    "abstract": "These days, positioning systems are Global Navigation Satellite System (GNSS) based – such as Global Positioning System (GPS) or the European Galileo – have been deployed worldwide, due to their efficiency, reliability, and need. Today using GPS for navigation or localization is quite common, this technology shaped our world and it is now part of our life.However, these satellite-based positioning systems fail to provide good results inside infrastructures. If someone is inside a building, walls and other objects inside will attenuate the signals, making them unreliable for obtaining a position. For example, an Indoor Positioning System (IPS) offering localization services inside a hospital could bring a lot of benefits, namely patient orientation, locating doctors and nurses for emergency responses, or immediately locating critical instrumentation, among others. In the case of a warehouse, it can be used for better logistics, optimization of resources and autonomous vehicle driving. Other related contexts can be found in airports, museums and shopping malls, where IPSs can be used to support indoor navigation.There is a large number of solutions created for this challenge, using technologies such as Bluetooth LowEnergy (BLE), Wi-Fi, Ultra-Wideband (UWB), Light Detection and Ranging (LIDAR), and Infrared, among many others. These are usually associated with techniques such as proximity, trilateration, triangulation, and fingerprinting. This work will focus on using Wi-Fi technology using the fingerprinting technique, i.e., Wi-Fi Fingerprinting for large indoor enviroments.",
    "authors": [
      "Ramires, Moisés Manuel Borba Roriz"
    ],
    "keywords": [
      "Fingerprinting",
      "Clustering",
      "RSSI Averaged Positioning Error (APE)",
      "RSSI APE"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84361",
    "title": "Formalizing ROS2 security configuration with Alloy",
    "abstract": "Industrial manufacturing is becoming highly reliant on automation developments, as they bring moreefficient and accurate processes, with lower associated costs. Consequently, robots are increasingly beingdeployed in a wide range of scenarios, especially where safety is demanded. In such cases, it is criticalto employ appropriate procedures to verify both the system’s quality and safety.Following the current growth of cyber-physical systems, as well as their usage in various technologydomains, the development of software applications is becoming more demanding due to the complexitybehind the integration of complementing services, beyond those provided by the operating system.One of the most popular open-source software platforms for building robotic systems is the Robot Operating System (ROS) [53] middleware, where highly configurable robots are usually built by composingthird-party modules. Robot Operating System 2 (ROS2) is implemented using the Data DistributionService (DDS) [49] communication protocol. ROS2 implicitly makes use of the DDS-Security artefactsthrough the Secure Robot Operating System 2 (SROS2) security toolset.The present study focus on detecting security problems in ROS2 networks, in which it is intended toverify, through formal techniques, security properties. However, security is a very broad subject, so thisstudy focuses on a particular security property to show the viability of the proposed technique, namelyObservational Determinism (OD).This dissertation introduces a software tool, named Security Verification in ROS (svROS), thatprovides multiple functionalities to support this type of security analysis using Alloy [32], a formal specification language and analysis tool.",
    "authors": [
      "Ribeiro, Luís Mário Macedo"
    ],
    "keywords": [
      "Robotics",
      "ROS2",
      "SROS2",
      "Security properties",
      "Observational determinism",
      "Software verification",
      "Alloy",
      "Robótica",
      "Propriedades de segurança",
      "Determinismo observacional",
      "Verificação de software",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79817",
    "title": "Adequacy analysis of learning resources in adult education",
    "abstract": "The present document identifies and details the research and development held under the scopeof a MSc Thesis pertaining to the scientific area of pedagogic tools for teaching support, ontolo gies and learning resources. This masters thesis in Informatics Engineering was developed in theUniversity of Minho, Braga.The purpose of the project is to study the learning process of adults and how it connects to Learn ing Resources (LRs) in order to understand if a learning resource used to teach ComputationalThinking (CT) to children, is suitable for adult learners. This approach ought to take into accountadult learning theory to set its requirements, as well as CT principles and learning resourcesclassification.To this end, an approach to the Adequacy of Learning Resources in Adult Education was createdwhich comprises the ontology OntoAL that describes in detail the domain of Adult Learning(AL) including the theory of AL and a classification of both the adult learner and the learningresources. This ontology was developed in OntoDL and Prolog. In addition, we analyze theexperiment conducted as part of the validation of this approach and the OntoAL ontology.Therefore, in this document, it is presented the state of the art pertaining to this field, exploringthe concepts of learning resources, computational thinking, ontologies and adult learning andeducation. Furthermore, it is rendered an introduction of the subject and the project, detailingthe context of the problem, the objectives to be accomplished and the research hypothesis ofsaid thesis. Next, it is presented the state of the art regarding Computational Thinking, AdultLearning and Education, Ontologies and Learning Resources. Thereafter, it is put forward thework proposal. Then it is introduced the OntoAL ontology in both OntoDL and Prolog (detailingthe process of its development and the choices made), the questionnaires that were created aswell as the analysis of the responses that we obtained. Lastly, there are listed the conclusions andthe future work.",
    "authors": [
      "Barbosa, Diana Ribeiro"
    ],
    "keywords": [
      "Computational thinking",
      "Learning resources",
      "Ontology",
      "Adult learning",
      "Gamebased learning",
      "OntoDL",
      "Prolog",
      "Pensamento computacional",
      "Recursos educativos",
      "Ontologias",
      "Ensino de adultos",
      "Ensino baseado em jogos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/77568",
    "title": "Caracterização de tráfego de serviços de streaming em dispositivos móveis",
    "abstract": "No contexto atual, o contínuo desenvolvimento tecnológico permite um acesso fácil e rápido a variadíssimosserviços e plataformas, por meio de dispositivos móveis. Por este motivo, o volume e diversidade de tráfego temcrescido de forma exponencial também.O conhecimento do tráfego que circula nas redes atuais torna-se indispensável, seja para ajudar a melhorara gestão e configuração dos elementos e serviços de rede, seja para os utilizadores terem a oportunidade degerir melhor os recursos na utilização dos seus dispositivos móveis e aplicações.Assim, este trabalho pretende aprofundar o estudo das características do tráfego gerado por dispositivos móveis,no acesso a determinadas plataformas/serviços. Também se espera incluir uma componente de MachineLearning para previsão da experiência do utilizador. Além disso, pretende-se obter base de comparação entreas versões web e aplicacional de um mesmo serviço.",
    "authors": [
      "Martins, Gabriela Sá"
    ],
    "keywords": [
      "Redes de computadores",
      "Análise de tráfego",
      "Machine mearning",
      "Computer networks",
      "Traffic analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47367",
    "title": "Distributed databases synchronization in named data delay tolerant networks",
    "abstract": "Delay Tolerant Network (DTN) is a small regional network designed to provide better communications when the end-to-end connection is not always possible. DTN is well known for intermittent connections and long delays. Nodes store data packets in the buffers and forward later when the connection is restored. Recently, Named Data Networking (NDN) has been drawing wide attention as a Future Internet architecture. This architectureshifts the emphasis from host to content and pays little attention to where is the content. Routing in NDN is based on the name of the content.Named Data-Delay Tolerant (ND-DT) network is an integration of DTN and NDN. It takes the advantages of both architectures by applying named data approach in DTN scenarios. In ND-DT network, distributed databases are maintained by a group of fixed or moving nodes. Data inconsistency always exists because of the intermittent connections and long delays.However, data synchronization solutions can minimize this inconsistency, helping to reduce the data access delay. ChronoSync is a well-known NDN state synchronization protocol. Data synchronization in ND-DT networks are challenging because of the intermittent connections and the nodes’ mobility. Moreover, the connection between nodes is not assured, which may make synchronization to fail. In this work, it is assumed that there is at least one path between each pair of database nodes. The aim of this work is to improve the recovery process of ChronoSync in order to enhance its adaptability to ND-DT network scenarios. For thisgoal, ChronoSync and our improved solution were implemented and tested on an ND-DT network simulator.The results show that our improved ChronoSync is more adaptable to ND-DT networks. The improved ChronoSync consumes less time to finish synchronization tasks in all the scenarios. What’s more, in three database scenarios, IChronoSync decreasing about 83% of the synchronization time while Chronosync decreases 62% when changed from sparse network to dense network. What’s more, improved ChronoSync generates 27% fewer data packets, which can increase the probability of other network nodes getting connected.",
    "authors": [
      "Liu, Chong"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27823",
    "title": "Materialização à medida de vistas multidimensionais de dados",
    "abstract": "Com o emergir da era da informação foram muitas as empresas que recorreram a data warehouses para armazenar a crescente quantidade de dados que dispõem sobre os seus negócios. Com essa evolução dos volumes de dados surge também a necessidade da sua melhor exploração para que sejam úteis de alguma forma nas avaliações e decisões sobre o negócio. Os sistemas de processamento analítico (ou OLAP – On-Line Analytical Processing) vêm dar resposta a essas necessidades de auxiliar o analista de negócio na exploração e avaliação dos dados, dotando-o de autonomia de exploração, disponibilizando-lhe uma estrutura multiperspetiva e de rápida resposta. Contudo para que o acesso a essa informação seja rápido existe a necessidade de fazer a materialização de estruturas multidimensionais com esses dados já pré-calculados, reduzindo o tempo de interrogação ao tempo de leitura da resposta e evitando o tempo de processamento de cada query. A materialização completa dos dados necessários torna-se na prática impraticável dada a volumetria de dados a que os sistemas estão sujeitos e ao tempo de processamento necessário para calcular todas as combinações possíveis. Dado que o analista do negócio é o elemento diferenciador na utilização efetiva das estruturas, ou pelo menos aquele que seleciona os dados que são consultados nessas estruturas, este trabalho propõe um conjunto de técnicas que estudam o comportamento do utilizador, de forma a perceber o seu comportamento sazonal e as vistas alvo das suas explorações, para que seja possível fazer a definição de novas estruturas contendo as vistas mais apropriadas à materialização e assim melhor satisfaçam as necessidades de exploração dos seus utilizadores.Nesta dissertação são definidas estruturas que acolhem os registos de consultas dos utilizadores e com esses dados são aplicadas técnicas de identificação de perfis de utilização e padrões de utilização, nomeadamente a definição de sessões OLAP, a aplicação de cadeias de Markov e a determinação de classes de equivalência de atributos consultados. No final deste estudo propomos a definição de uma assinatura OLAP capaz de definir o comportamento OLAP do utilizador com os elementos identificados nas técnicas estudadas e, assim, possibilitar ao administrador de sistema uma definição de reestruturação das estruturas multidimensionais “à medida” da utilização feita pelos analistas.",
    "authors": [
      "Duarte, Ana Sofia da Silva"
    ],
    "keywords": [
      "Sistemas de data warehousing",
      "Processamento analítico de dados",
      "OLAP",
      "Sessões OLAP",
      "Cadeias de markov",
      "Classes de equivalência",
      "Assinaturas OLAP",
      "Data warehousing systems",
      "On-line analytical processing",
      "OLAP Sessions",
      "Markov chains",
      "Equivalence classes",
      "OLAP signatures",
      "681.3:658.0",
      "658.0:681.3"
    ],
    "date": "2012",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "681.3:658.0",
      "658.0:681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80045",
    "title": "Development of a marketing automation platform to integrate online e-commerce services",
    "abstract": "E-commerce is continuously expanding which increases market competitiveness. With the increase of plat-forms arrives a need to stand out from the competition, thus creating the necessity to improve marketing strate-gies. Marketing strategies, such as creating personalized birthday emails or registration welcome-emails cannot be done in the traditional way. This idea of creating custom services like sending user-specific emails creates the need for a marketing automation solution. Following this need, its proposed the development of a marketing automation platform with integration with a machine learning engine. This system will be hosted on a cloud and will automate marketing campaigns and provide dents with results from machine learning models.",
    "authors": [
      "Gonçalves, Diogo Alexandre Domingues"
    ],
    "keywords": [
      "Marketing Automation",
      "Software architecture",
      "Web services",
      "Representational state transfer (REST)",
      "e-commerce",
      "Cloud computing",
      "Marketing Automático",
      "Arquitetura de software",
      "Serviços web",
      "e-comércio",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/49418",
    "title": "Exploração de paralelismo massivo em algoritmos evolucionários",
    "abstract": "Esta dissertação está centrada na paralelização massiva da biblioteca Java Evolutionary Cornputation Library), JECoLi, que se foca no desenvolvimento de meta-heurísticas de otimização(e.g. Algoritmos Evolucionários (AEs)) na linguagem Java. Os AEs são um paradigma da Computação Evolucionária (CE) utilizados para resolver problemas complexos através de um método iterativo que evolui um conjunto de soluções (população) tendo em conta os princípios da teoria de evolução por seleção natural apresentada por Charles Darwin. Estes algoritmos estão divididos em duas categorias, AEs não estruturados e AEs estruturados. Os AEs não estruturados são caracterizados por uma população centralizada onde existe apenas um conjunto de soluções ao qual é aplicado o processo evolutivo. Por outro lado, os AEs estruturados contêm várias populações onde os processos evolutivos são conduzidos de forma independente, embora existindo troca de informação. Os algoritmos de ambas as categorias podem ser paralelizados de diferentes maneiras. Nesta dissertação, foram implementadas quatro versões paralelas da plataforma JECoLi de forma o menos invasiva possível, tendo em conta modelos paralelos já formulados: um modelo de paralelismo global; um modelo de ilhas em ambiente de memória partilhada; um modelo de ilhas em ambiente de memória distribuída; e um modelo híbrido. Estas implementações paralelas foram executadas no cluster Services and Advanced Research Cornputing with HTC/HPC clusters (SeARCH) utilizando o máximo de recursos computacionais possíveis de modo a realizar uma posterior análise dos resultados obtidos. Foram utilizados dois casos de estudo reais para validar as implementações paralelas, um problema de otimização de um bioprocesso de fermentação fed-batch e outro de otimização dos pesos de um protocolo de encaminhamento (OSP F). Cada uma das implementações paralelas foi testada nos dois casos de estudo, aplicando o máximo de paralelismo possível tendo em conta as limitações de cada caso de estudo, dos modelos paralelos e dos recursos disponíveis. Com estes testes concluí-se uma boa escalabilidade destes algoritmos, onde se destacam as implementações relativas ao modelo de ilhas em memória distribuída e ao modelo híbrido. Contudo, algumas configurações que originam maiores ganhos foram descartadas pois não produzem valores de aptidão aceitáveis.",
    "authors": [
      "Martins, Tiago Augusto Simões"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/28945",
    "title": "Translating alloy specification to the point-free style",
    "abstract": "Every program starts from a model, an abstraction, which is iteratively re ned until we reach the nal result, the implementation. However, at the end, one must ask: does the nal program resemblein anyway the original model? Was the original idea correct to begin with? Formal methodsguarantee that those questions are answered positively, resorting to mathematical techniques. Inparticular, in this thesis we are interested on the second factor: veri cation of formal models.A trend of formal methods defends that they should be lightweight, resulting in a reducedcomplexity of the speci cation, and automated analysis. Alloy was proposed as a solution for thisproblem. In Alloy, the structures are described using a simple mathematical notation: relationallogic. A tool for model checking, automatic veri cation within a given scope, is also provided.However, sometimes model checking is not enough and the need arises to perform unboundedveri cations. The only way to do this is to mathematically prove that the speci cations are correct.As such, there is the need to nd a mathematical logic expressive enough to be able to representthe speci cations, while still being su ciently understandable.We see the point-free style, a style where there are no variables or quanti cations, as a kindof Laplace transform, where complex problems are made simple. Being Alloy completely relational,we believe that a point-free relational logic is the natural framework to reason about Alloyspeci cations.Our goal is to present a translation from Alloy speci cations to a point-free relational calculus,which can then be mathematically proven, either resorting to proof assistants or to manual proving.Since our motivation for the use of point-free is simplicity, we will focus on obtaining expressionsthat are simple enough for manipulation and proofs about them.",
    "authors": [
      "Macedo, Nuno"
    ],
    "keywords": [
      "681.3"
    ],
    "date": "2010",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [
      "681.3"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/81128",
    "title": "Network and cross-platform SaaS performance: a case study",
    "abstract": "The traditional role of a personal computer is dramatically changing with the shift towards cloud-based services. Cloud computing and storage provides end-users with universal access to their data across various devices, coherent application and service experience, and substantially decreases hardware requirements for end-user clients (personal computers). However, this cloud-oriented paradigm requires the redesign of applications and services, as well as a serious analysis of the current situation and proper scaling of access networks since this new paradigm changes everyday habits of end-users. This work is focused on the impact of cloud-based applications (using the paradigm of Software as a Service (SaaS)) on access networks, analyzes the network and the application behavior, while also addressing application usability and Quality of Experience (QoE) in different scenarios. A detailed study of the impact on access networks imposed by such cloud-based services (and vice versa) is currently missing, especially in the case of bandwidth-constrained, high-latency mobile access networks.Furthermore, this work involves analysis of various cloud-based applications, namely office tasks (text, presentation, and spreadsheet editing), in different combinations and executed on different hardware and software platforms with different levels of integration with cloud-based services. Network traffic analysis will be executed, including collecting Wireshark traces of the generated and received traffic, correlated with specific executed tasks. The impact of network congestion and latency is also examined in the QoE focused section. The work discussion is broken down into individual hypotheses, reflecting expectationsregarding behavior of SaaS applications, data volume of the network, and QoE of the end user. Different end-user experience metrics are used in combination with network-based monitoring (including peak and average bandwidth measurements, latency, packet loss, etc.).",
    "authors": [
      "Carreira, Joana Lourenço"
    ],
    "keywords": [
      "Cloud computing",
      "Google Docs suite",
      "Microsoft Online suite",
      "Network behavior",
      "Quality of experience",
      "QUIC",
      "SaaS performance",
      "Traffic profile",
      "Comportamento da rede",
      "Computação em nuvem",
      "Desempenho SaaS",
      "Perfil de tráfego",
      "Qualidade de experiência",
      "Suite da Google Docs",
      "Suite da Microsoft Online"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/78249",
    "title": "Automatic speech recognition for European Portuguese",
    "abstract": "The process of Automatic Speech Recognition (ASR) opens doors to a vast amount of possibleimprovements in customer experience. The use of this type of technology has increasedsignificantly in recent years, this change being the result of the recent evolution in ASRsystems. The opportunities to use ASR are vast, covering several areas, such as medical,industrial, business, among others. We must emphasize the use of these voice recognitionsystems in telecommunications companies, namely, in the automation of consumer assistanceoperators, allowing the service to be routed to specialized operators automatically throughthe detection of matters to be dealt with through recognition of the spoken utterances. Inrecent years, we have seen big technological breakthrough in ASR, achieving unprecedentedaccuracy results that are comparable to humans. We are also seeing a move from whatis known as the Traditional approach of ASR systems, based on Hidden Markov Models(HMM), to the newer End-to-End ASR systems that obtain benefits from the use of deepneural networks (DNNs), large amounts of data and process parallelization.The literature review showed us that the focus of this previous work was almost exclusivelyfor the English and Chinese languages, with little effort being made in the development ofother languages, as it is the case with Portuguese. In the research carried out, we did notfind a model for the European Portuguese (EP) dialect that is freely available for generaluse. Focused on this problem, this work describes the development of a End-to-End ASRsystem for EP. To achieve this goal, a set of procedures was followed that allowed us topresent the concepts, characteristics and all the steps inherent to the construction of thesetypes of systems. Furthermore, since the transcribed speech needed to accomplish our goalis very limited for EP, we also describe the process of collecting and formatting data from avariety of different sources, most of them freely available to the public. To further try andimprove our results, a variety of different data augmentation techniques were implementedand tested. The obtained models are based on a PyTorch implementation of the Deep Speech2 model.Our best model achieved an Word Error Rate (WER) of 40.5%, in our main test corpus,achieving slightly better results to those obtained by commercial systems on the same data.Around 150 hours of transcribed EP was collected, so that it can be used to train other ASRsystems or models in different areas of investigation. We gathered a series of interestingresults on the use of different batch size values as well as the improvements provided bythe use of a large variety of data augmentation techniques. Nevertheless, the ASR theme is vast and there is still a variety of different methods and interesting concepts that we couldresearch in order to seek an improvement of the achieved results.",
    "authors": [
      "Campinho, Adriano Vaz de Carvalho"
    ],
    "keywords": [
      "Automatic speech recognition",
      "European Portuguese",
      "End-to-end learning",
      "Data collection",
      "Reconhecimento automático de fala",
      "Português Europeu",
      "Aprendizagem ponta a ponta",
      "Recolha de dados",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47746",
    "title": "Parallel verification of dynamic systems with rich configurations",
    "abstract": "Model checking is a technique used to automatically verify a model which represents the specificationof some system. To ensure the correctness of the system the verification of both static and dynamicproperties is often needed.The specification of a system is made through modeling languages, while the respective verificationis made by its model-checker. Most modeling frameworks are not ready to verify models rich in bothkind of properties thereby limiting the verification of dynamic systems with rich configurations.Electrum is a modeling language which mixes the best of the Alloy and TLA specification languages,with the capability of handling the problem mentioned above. This language is supported bytwo model-checking techniques – one bounded and one unbounded.Nonetheless, the Electrum’s bounded model-checker has limitations, thus, this dissertation aims toovercome them with the purpose of improving the analysis procedure of Electrum models, in particular,the definition of a new Electrum’s semantics through a translation into Kodkod as well as thecreation of a novel procedure of verifying Electrum models in parallel. Hence, in order to achievethese goals, a temporal extension to the Kodkod constraint solver was implemented.",
    "authors": [
      "Pessoa, Eduardo José Dias"
    ],
    "keywords": [
      "Alloy",
      "Electrum",
      "TLA",
      "Formal specification language",
      "Dynamic systems",
      "Rich configurations",
      "Model-Checking",
      "Parallel verification",
      "Linguagem de especificação formal",
      "Sistemas dinâmicos",
      "Configurações ricas",
      "Verificação de modelos",
      "Verificação paralela",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91608",
    "title": "Anomaly-based intrusion detection systems for industrial networks",
    "abstract": "The combination of operational technology of industrial networks with information technologies has en abled the increase of attacks on industrial networks, causing professionals and researchers linked to thesecurity area to study tools, mechanisms and techniques capable of detecting and blocking maliciousactivities in industrial network environments.This dissertation sets out to comprehensively explore and analyze Anomaly-based Intrusion Detection Sys tems (IDS) as a central focal point. The primary ambition is to meticulously investigate the efficacy ofsuch systems in identifying, monitoring, and recording abnormal behaviors, detecting malicious activities,and pinpointing potential attacks that exploit remote services and orchestrate denial of service incidents.Through an in-depth examination and critical evaluation, this study aims to contribute to the existing bodyof knowledge in the realm of cybersecurity, advancing our understanding of IDS capabilities and their sig nificance in safeguarding digital and industrial environments against emerging threats.In this way, it is intended to create a model relating the attack techniques, their indicators, and the fields,logs, and events generated by the IDS, in order to help detect such attacks, in a way, to evaluate theefficiency of the IDS in detecting malicious activities.",
    "authors": [
      "Costa, Afonso Fernando da"
    ],
    "keywords": [
      "Intrusion detection system",
      "Security in industrial networks",
      "Denial of service",
      "Brute force",
      "Sistema de detecção de intrusão",
      "Segurança em redes industriais",
      "Negação de serviço",
      "Força bruta",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92809",
    "title": "Suporte wearable para Mobile Documents (mDoc) ISO/IEC 18013-5",
    "abstract": "The popularization of mobile devices and the characteristic pervasiveness of internet access have led toincreased interest in developing potentially relevant services for citizens. The various solutions for identifying peopleusing digital devices are an example of this. Among these solutions is the ISO/IEC DIS 18013-5 standard. Thisstandard published by the International Organization of Standardization (ISO) defines the technical requirementsnecessary for secure transmission, i.e. with guarantees of data integrity and authenticity, of identification attributesrelated to a driver’s license. The interest to implement the standard ISO/IEC DIS 18013-5 has risen due to theincrease in development and public interest in wearable devices, and the recent effort by the European Unionto encourage the use of digital identification documents. Implementation of this standard is made easier by theexistence of cross-development tools, e.g., Xamarin, Flutter, that allow code sharing between different wearableplatforms. These tools serve as a catalyst for implementation while maintaining the functionality of the code.",
    "authors": [
      "Matias, Hugo Fernandes"
    ],
    "keywords": [
      "Smartwatch",
      "Digital",
      "ISO",
      "Identity",
      "Service",
      "Autenticidade",
      "Identidade",
      "Serviço",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80753",
    "title": "Genome-scale metabolic modelling of the pathogen Xylella fastidiosa",
    "abstract": "Xylella fastidiosa is a phytopathogenic bacteria that causes disease in hundreds of differ ent plant species. Increased reports of plants infected by these xylem-limited bacteria arealarming as this pathogen continues to attack crops of relevant economic power such ascitrus, grapes, olives and almonds, with considerable economical losses for the producers.The current employed strategy to contain this epidemic is radical in action as it destroysthe infected plant and surrounding area. For this reason, it became urgent to develop newways to eliminate these bacteria with therapeutics that are more pathogen oriented.Genome-Scale Metabolic (GSM) models contain genomic and metabolic information of agiven organism and can be used to discover new potential drug targets. Thus, a GSMmodel of X. fastidiosa may unveil new ways to control these bacteria.In this work, we developed a high-quality GSM model for X. fastidiosa subsp. pauca DeDonno, using the user-friendly software Metabolic Models Reconstruction Using Genome Scale Information (merlin). This strain was chosen for its importance in the national econ omy, as it causes Olive Quick Decline Syndrome. The reconstructed model of X. fastidiosacomprises a set of 1280 reactions and 524 genes.The genome of X. fastidiosa subsp. pauca De Donno was functionally annotated in orderto identify the metabolic potential of the phytopathogenic organism. Metabolic functionsidentified in the genome were used to assemble the initial draft metabolic network. Manualcuration procedures were made in order to correctly represent organism’s capabilities andbiomass related reactions, based on literature and experimental data, were added to model.The reconstructed model was then validated using experimental data, regarding the aerobicmetabolism, carbon flux pattern, carbon usage, amino acid auxotrophies and growth inseveral media developed for X. fastidiosa.In silico simulations revealed interesting metabolic properties of X. fastidiosa. The usageof carbon through the Entner-Doudoroff pathway seems to be a way of generating redoxpotential for defensive mechanism against the host plant. An absence of auxotrophies andthe presence of catabolic routes for amino acids shows the metabolic and adaptive potentialof the organism, as expected since it grows on nutrient-limited environments.This reconstructed model can be used to explore the metabolism and provide informationfor potential drug targets for the agricultural-destructive phytopathogen X. fastidiosa.",
    "authors": [
      "Silva, Miguel Ângelo Fernandes da"
    ],
    "keywords": [
      "Xylella fastidiosa subsp. pauca De Donno",
      "Phytopathogen",
      "Systems biology",
      "Genome-scale metabolic model",
      "Ciências Agrárias::Biotecnologia Agrária e Alimentar"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Agrárias::Biotecnologia Agrária e Alimentar"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80859",
    "title": "Children dyscalculia: a web system for diagnosis and treatment",
    "abstract": "A discalculia é uma desordem neurológica que dificulta a aprendizagem de diversos conceitos matemáticos. Sendo uma desordem inerente à pessoa, pois nasce com ela, é crucial dedicar-lhe atenção desde cedo. Uma forma de atenuar os seus efeitos é o uso de tecnologia moderna como aplicações móveis, nomeadamente jogos com foco na prática e ensino de diversos conceitos relacionados com matemática, nos primeiros anos de vida escolar em pessoas que sofrem de discalculia. Este projeto tem como foco a criação de uma plataforma cujo objetivo é complementar estes jogos registando diversos parâmetros relacionados com o desempenho dos alunos em diversas tarefas e retirando conclusões baseadas nestes parâmetros de forma a detetar a discalculia e os seus sintomas, assim como acompanhar o progresso do aluno no que toca ao tratamento desta desordem.A criação desta plataforma permitiria a existência de uma nova e útil ferramenta no que toca à discalculia e ao tratamento dos seus sintomas o mais cedo possível na vida de uma pessoa.",
    "authors": [
      "Cachulo, César Augusto Lourenço"
    ],
    "keywords": [
      "Discalculia",
      "Crianças",
      "Aplicação móvel",
      "Jogo",
      "Plataforma web",
      "Base de dados",
      "Interface de programação de aplicações",
      "Aplicação web",
      "Análise de dados",
      "Modelação",
      "Dyscalculia",
      "Children",
      "Mobile application",
      "Game",
      "Web platform",
      "Database",
      "Application programming interface",
      "Web application",
      "Data analytics",
      "Modeling",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47388",
    "title": "Instalação e manutenção de redes de acesso e distribuição em operadores 3play",
    "abstract": "No âmbito da Dissertação, incluída no plano curricular do Mestrado em Engenharia de Redes e Serviços Telemáticos, foi proposta a realização de um período de estudo a um prestador de serviços de um ISP (Internet Service PrQv/del) que fornece tecnologias de acesso de voz, Internet e televisão (TV) em cobre e fibra. Este prestador de serviços é responsável por toda a construção e implementação das infraestruturas de redes, tanto de cliente como de acesso público.Todo este trabalho em backgroundé o que suporta e dá vida às redes de nova geração. Sem ele, não seria possível construir esta gigante rede a que se chama \"Internet\", que permite o acesso facilitado à comunicação e troca de informações.Com este documento pretende apresentar-se a metodologia para a criação de infraestruturas de rede cio operador, de modo a fazer chegar os serviços 3play (voz, TV e Internet) à casa do cliente. O caso de estudo apresentado visa analisar a implementação de uma rede de fibra ótica na Ribeira Grande, ilha de São Miguel, Açores, tendo em consideração as especificidades insulares, como é o caso da distância ao continente e os perigos geológicos associados. São exemplos destes perigos os sismos e os movimentos de vertentes.",
    "authors": [
      "Ferreira, Miguel Carreiro Gomes"
    ],
    "keywords": [
      "Serviços 3play",
      "Infraestruturas de comunicação",
      "Cablagem de rede",
      "Cobre",
      "Fibra",
      "3play services",
      "Communications infrastructure",
      "Network cabling",
      "Copper",
      "Fiber",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/80081",
    "title": "Improving the response time of emergency vehicles: using V2X communications",
    "abstract": "Emergency vehicles are bound to lose unnecessary time on their response. Furthermore,research shows that emergency vehicles are prone to fatal crashes while on an emergencycall. Emergency vehicle’s response time is commonly affected by factors unbeknownst to it,such as traffic, traffic lights, road conditions, and accidents.The overall goal of this dissertation project was to improve the response time of an emergency vehicle, while improving its safety along the route. Thus, V2X communications wasused to obtain knowledge from the environment, so that the emergency vehicle can choosethe best decisions to reduce the response time. The emergency vehicle issued alerts to allof the entities of the road system so that these can adapt their behaviour collaboratively. Inthis context, V2X communications may occur between vehicles and every other entity of thesystem, such as infrastructures, pedestrians, vehicles, and the network.The present document describes the research and development work that aimed to establish a comparison between two scenarios: with and without support on a vehicular networksimulator, namely Veins, incorporating the American standard WAVE. The V2X scenariobenefited from use cases as emergency avoidance, dynamic routing, intersection and trafficlight’s logic. Alternatively, the non-V2X communications setup did not have access to communications, simulating a current context for urban road environments. The evaluation ofthe V2X benefits was made by analysing the most relevant criteria like response time, stoptime, distance covered by the emergency vehicle.The performance of the system prototype based on these metrics showed a clear improvement when using V2X communications. In particular, the response time was reduced by41% using V2X, guaranteeing that the use of V2X is a step forward in this context. Sincethe results from the tests using a full simulated setup were positive, a pair of additionalexperiments integrating real Bosch V2X communication boards were planned: one with theintegration of these boards into the already simulated environment and another with the integration of these boards on real Bosch prototype vehicles. While this later experiment wasnot possible to realize during this dissertation work time, the former was conducted withsuccesses proving that the it is already possible to develop RD projects that use real V2Xcommunications boards, which is a step closer to test these new technologies on real environments or, at least, on controlled environments that better emulate real environments.",
    "authors": [
      "Pereira, Bruno Martins"
    ],
    "keywords": [],
    "date": "2020",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79950",
    "title": "AIDA-MCDT: nova abordagem à visualização de meios complementares de diagnóstico e terapia",
    "abstract": "Over the last years, the implementation and evolution of computer resources has been improving both the financial and temporal efficiency of clinical processes, as well as the security in the transmission and maintenance of their data, also ensuring the reduction of clinical risk. Currently, the importance of all the information flowing in healthinstitutions is unquestionable. In this way, it is essential that institutions, more specifi cally hospital institutions, have a good Hospital Information System (HIS) in order to collect and analyze information, also helping to support decision making. The most common application of these type of systems is the Electronic Health Record (EHR),which, despite bringing many benefits, is still associated with a low level of usability.However, the different systems present in hospitals are distributed and heterogeneous. Since the interaction between these systems is crucial these days, there is the Agency for Integration, Diffusion and Archive (AIDA) implemented in some Portuguese hospitals. AIDA is a platform developed to enable the dissemination and inte gration of information generated in a health environment by different systems, inclu ding for example information on Complementary Diagnostic and Therapeutic Means (MCDT).Previous research has shown that health professionals often do not analyze and actaccurately and appropriately on test results. Preventing errors during the access toMCDT is essential as this is a crucial step in the diagnostic process, thus avoidingnegative consequences for the patient.In this sense, a new MCDT visualization platform (AIDA-MCDT) was implementedin this project, specifically in the Hospital Center of Porto (CHP), with several newfunctionalities in order to make this process faster, intuitive and efficient, always guaranteeing the confidentiality and protection of patients’ personal data and significantlyimproving the usability of the system, leading to a better delivery of health care.",
    "authors": [
      "Neto, Cristiana Marisa Pereira"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Médica"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94079",
    "title": "Ukraine War: an online corpus to analyze the impact of the war in Ukraine",
    "abstract": "This document reports a Master’s work, the final project of the 5th year of the IntegratedMaster’s in Informatics Engineering, that was accomplished at Universidade do Minho inBraga, Portugal.On February 24, 2022, a conflict between two countries, Ukraine and Russia, began. Thewar between two countries is devastating and affects many people, both residents of thecountries directly involved and neighboring countries. As a highly significant event, itgathers coverage from many sources globally, including traditional print newspapers, onlinenews platforms, social networks, blogs, television programs, and more. However, all of thisinformation is scattered across different websites and social networks. If researchers (in theareas of Linguistics, History, Humanities, etc.) and curious people want to analyze this data,their work will be very difficult. Therefore, it is essential to gather the information on asingle platform.This work aims to create an online corpus in the Portuguese language regarding theUkraine War, based on Portuguese online newspapers’ news as well as comments on socialmedia.To fulfill the goal of this work, initially, a variety of news sources were considered, andthe Portuguese online newspapers “Público” and “Jornal de Negócios” were selected, aswell as the platform “Reddit”. To extract the required information, the technique of WebScraping was used. Therefore, for each source, an extractor was developed that extractedthe necessary information and saved it in a JSON file. Following that, Natural LanguageProcessing Techniques were used to process the gathered information. Afterward, theextracted information was stored in a non-relational database, MongoDB. Finally, a websitecalled GUCO was designed and implemented, providing users with the capability to navigateand explore the created corpus.The GUCO website is available at the address: https://guco.epl.di.uminho.pt/.",
    "authors": [
      "Rosendo, Ana Rita Miranda"
    ],
    "keywords": [
      "Online corpus",
      "Ukraine war",
      "Rebuild the war through the news",
      "Social network analysis",
      "Web scraping",
      "Natural language processing",
      "Corpus online",
      "Guerra da Ucrânia",
      "Reconstrução da guerra através das notícias",
      "Análise de redes sociais",
      "Web scraping",
      "Processamento de linguagem natural",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79995",
    "title": "Building an imaging-based research platform for experiments with brain connectivity data",
    "abstract": "Within the past decade, not only societies in general but also medicine and healthcare, in particular, have changed tremendously. In large part because of the rapid dissemination of computers and digital communications which lead to the appearance of new medical disciplines, such as Medical Informatics. Nowadays, one of the most prominent field in Medical Informatics is Medical Imaging, as it is implied, it is a collection of methodologies and techniques used in order to visually and spatially represent parts of the brain for diagnostic and research purposes.In the research ecosystem, Neuroimaging is an increasing popular field, with applications in neurology and psychiatry. However, due to the difficulties to handle Neuroimaging data, since data has its own specificities, researchers have encountered problems to correctly handling this data. This can be a crucial issue specially with large volumes of Neuroimaging data and all the research materials associated. This work aims to architect and build a research platform to correctly archive Medical Imaging data and all the associated research materials, where researchers can exchange imaging data and collaborate in Neuroimaging research projects. The platform offers a correct way to collect and store all imaging data, archiving all of patient exams with the correspondent information, making available the correspondent information to researchers in a confidential, secure and efficient way.The two main outcomes of this work are an architecture of a platform that manages all imaging data and associated research materials, plus an open-source Python package to easily interact with that platform.",
    "authors": [
      "Moreira, Rogério Gomes Lopes"
    ],
    "keywords": [
      "Data sharing",
      "Medical imaging",
      "Neuroinformatics",
      "Neuroimaging",
      "XNAT",
      "Partilha de dados",
      "Imagiologia médica",
      "Neuro-informática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2019",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56078",
    "title": "Computação segura sobre sistemas de dados em ambientes cloud",
    "abstract": "O desenvolvimento dos serviços cloud gerou uma migração massiva de dados para os mesmos.O facto da cloud fornecer um serviço acessível a partir de qualquer dispositivo (computadorpessoal, telemóvel, tablet, etc.) através do acesso à Internet fez com que a suautilização aumentasse. Estes serviços trazem bastantes vantagens para os utilizadores. Afacilidade em aceder e partilhar os dados pelos diferentes utilizadores combinada coma maleabilidade e escalabilidade oferecidas pela cloud, faz com que este serviço se tornenuma plataforma bastante pretendida pelas empresas para armazenar os seus projetos remotamentee, desta forma reduzir custos associados aos servidores locais.No entanto, armazenar dados remotamente provoca o aparecimento de questões acercade segurança e privacidade. De forma a garantir confiabilidade no serviço, o fornecedorcloud tem de garantir que os dados estejam seguros em termos de privacidade, confidencialidadee integridade. Ataques recentes a fornecedores de serviços cloud (Greene, 2015)levantaram questões em termos de segurança da informação, provocando um decréscimona confiança depositada nestes serviços.Esta dissertação aborda um desafio de segurança que a cloud está a enfrentar, a computaçãosobre dados. Esta computação tem de ser feita sobre dados cifrados para que o fornecedorde cloud não tenha acesso ao valor real da informação nem ao resultado da computação.",
    "authors": [
      "Ferreira, João Carlos Carvalho"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84073",
    "title": "Development of tools for sentiment analysis in the portuguese language",
    "abstract": "A Análise de Sentimentos é uma das áreas mais importantes na ciência da computação, nomeadamente no Processamento da Linguagem Natural. As suas aplicações vão desde a análise de produtos até à contenção do cyberbullying. A importância da análise dos sentimentos é inigualável, mas quando se trata de línguas menos faladas, o campo parece ficar para trás. Neste contexto, Omnium AI propôs uma dissertação onde exploramos a Análise de Sentimentos para a Língua Portuguesa, com a intenção de criar uma nova ferramenta computacional. Esta dissertação vai examinar o campo da análise de sentimentos e o desenvolvimento do package Omnia. Este package é composto por ferramentas para a leitura de dados, o seu processamento e a criação de modelos Machine Learning (ML) e Deep Learning (DL) a partir dos dados lidos. Em específico, vamos concentrarnos no desenvolvimento do package Omnia Text Mining, com objectivo de criar ferramentas de pré-processamento e modelos de ML e DL para a análise de sentimentos para a língua portuguesa.Esta dissertação vai criar uma abordagem para lidar com problemas de análise de sentimentos composta por um processo de recolha de dados, seguido de um passo de pré-processamento e acabandocom o desenvolvimento de modelos de ML e DL. Esta abordagem será aplicada ao tópico do Covid-19.Após serem criados os modelos para os datasets relativos ao Covid, avaliamos os resultados para asdiferentes combinações de métodos de pré-processamento e modelos onde apuramos que as Long ShortTerm Memory (LSTM)s e o HFAutoModel com o embedding Bert foram os melhores modelos. No geral,os modelos de DL e Autogluon obtiveram melhores resultados que os modelos de ML. Nos métodos depré-processamento visualizamos que não existe uma Pipeline geral que possa ser utilizada para todos oscasos.No final, iremos discutir as conclusões que podemos retirar desta dissertação juntamente com umasecção de trabalho futuro, onde exploraremos os próximos passos possíveis para este projecto.",
    "authors": [
      "Gonçalves, Jorge Miguel da Silva Brandão"
    ],
    "keywords": [
      "Deep learning",
      "Machine learning",
      "Text mining",
      "Sentiment analysis"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79755",
    "title": "Otimização na alocação de recursos de cloud computing num serviço de autenticação de produtos",
    "abstract": "A UN1Qnx, S.A., soluções de autenticidade ciber-físicas, é uma empresa sediada em Braga,que desenvolve e comercializa sistemas físicos, eletrónicos e cibernéticos de validação eautenticação de produtos, sendo o objetivo a proteção da marca e o combate à contrafação.Neste momento, a empresa possui um serviço de autenticação de produtos localizado numamáquina virtual na cloud, mais especificamente na Microsoft Azure. Contudo, a utilizaçãodeste serviço é intermitente e passa por períodos de inatividade. Porém, quando utilizado,cada execução do serviço é computacionalmente custosa, o que obriga à utilização de umamáquina virtual que tem em conta o caso de máxima utilização. Assim, nos intervalos entreutilizações os custos acumulam-se sem aproveitar os recursos alocados. Deste modo, estatese passa por otimizar a utilização dos recursos na cloud, tendo em vista tirar proveito daescalabilidade e elasticidade das tecnologias de computação na nuvem, bem como melhorara latência dos pedidos.A otimização dos recursos passa por comparar diferentes serviços de diferentes forne cedores e selecionar o que se apresenta como a melhor opção. A fim de realizar estascomparações, fez-se antes uma investigação baseada na metodologia Design Science Research.Primeiramente, explorou-se o ambiente da solução (computação na nuvem) e o ambientedo problema, isto é, qual a situação atual da empresa no que diz respeito ao funcionamentodo serviço de validação e dos recursos afetos ao mesmo.Em segundo lugar, fez-se uma averiguação sobre o estado da arte das tecnologias usadas,das tecnologias que poderiam vir a ser usadas e de outras empresas da mesma área, sobrequais os seus produtos e o seu modo de funcionamento. Por último, investigaram-se métodosde seleção e comparação entre várias opções.Em terceiro lugar, realizou-se a parte mais trabalhosa e demorada: o desenvolvimentoprático. Nesta fase realizaram-se testes de performance, a colocação do serviço num dockercontainer e a utilização de kubernetes. Ainda nesta última parte, houve vária experimentaçãocom diversas arquiteturas. Por fim, o sistema estabilizou numa arquitetura assíncrona, quefez reduzir os custos e, permitiu com que o serviço se adequasse melhor à quantidade detrabalho a processar.",
    "authors": [
      "Braga, Luís Tiago Machado"
    ],
    "keywords": [
      "Computação na nuvem",
      "Cloud",
      "Otimização",
      "Design science research",
      "Escalabilidade",
      "Elasticidade",
      "Cloud computing",
      "Optimization",
      "Service selection",
      "Scalability",
      "Elasticity",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83499",
    "title": "Automation of machine learning models benchmarking",
    "abstract": "Na área de ciência de dados, o machine learning está-se a revelar uma ferramenta essencial para resolver problemas complexos. As empresas estão a investir em equipas de ciência de dados e Machine Learning para desenvolver modelos que apresentem valor para os clientes. No entanto, estes modelos são uma pequena percentagem de uma pipeline de projetos de Machine Learning (ML) e, para entregar um produto de ML completo, é necessário um número maior de componentes. DevOps é uma mentalidade de engenharia e um conjunto de práticas que visa unificar o processo de desenvolvimento e o processo de operações em um software, MLOps é um conceito similar a DevOps mas aplicado ao desenvolvimento e entrega de soluções de ML. O nível de automatização das etapas em uma pipeline de ML define a maturidade do processo de ML, que reflete a velocidade de treino de novos modelos com novos dados ou de treino de novos modelos com diferentes implementações. Um sistema de ML é um sistema de software, desenvolvimento e atualizações contínuas são necessárias para garantir um sistema que escale conforme as necessidades. O principal objetivo desta tese é apoiar a criação de um sistema integrado de ML com uma arquitetura que proporcione a capacidade de ser continuamente operada em um ambiente de produção. Um conceito para avaliação de desempenho de algoritmos deve ser elaborado e implementado. O principal obetivo e melhorar e ace'erar o cicio de desenvolvimento de modelos de ML na empresa. Para atingir este objetivo surge a necessidade de definir uma arquitetura com especificações e a implementação de processos automatizadas num pipeline de ML existente, este processo têm como objetivo alcançar uma ferramenta de benchmark de modelos, com capacidade de analisar o desempenho do modelo, um motor de inferência e um banco de dados para armazenar todas as métricas computadas. Um sistema baseado em IA em desenvolvimento fornece o caso de estudo para desenvolver e validar a arquitetura. Os avanços atuais na área da condução semiautomática introduz a necessidade de sistemas de monitoramento que podem localizar e detectar eventos especificas no veículo. Os conjuntos de sensores são instalados dentro da cabine para alimentar sistemas inteligentes que visam analisar e sinalizar certos comportamentos que podem impactar a segurança e o conforto dos passageiros..",
    "authors": [
      "Sá, João Pedro Barros"
    ],
    "keywords": [
      "Engenharia Software",
      "Aprendizagem máquina",
      "Ciência dados",
      "DevOps",
      "MlOps",
      "Machine Learning",
      "Software",
      "Data Science",
      "Pipelines",
      "Automation",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93211",
    "title": "Identify gene expression profiles in freshwater mussels under thermal stress",
    "abstract": "Due to global climate change, the temperatures of streams and rivers are increasing, negatively affecting aquatic life, including bivalve species. Freshwater mussels are vital components of rivers, streams, and lake ecosystems, participating in essential ecological roles such as nutrient cycling, and increasing water quality. Furthermore, they serve as essential ecosystem engineers, providing habitat to other organisms and supporting intricate food webs. Besides their biological importance, freshwater mussels are poorly studied in terms of genomics. In the present work, the Iberian dolphin freshwater mussel Unio delphinus Spengler, 1793 (Bivalvia: Unionoida) was used as a model species to investigate the effects of climate change in freshwater mussels. The primary objective of this thesis was to determine the gene expression patterns in a model species of freshwater mussels under the effects of thermal stress exacerbated by climate change, with an overall goal of understanding the potential consequences for freshwater mussel populations. Two different ecological experiments were performed: chronic and acute. The chronic experiments where temperatures were gradually increased to simulate a scenario of progressive increasing temperatures. The acute experiments where temperatures were rapidly increased to replicate the effects of a briefer extreme climatic event. To achieve this main goal, a comprehensive bioinformatic pipeline focused on transcriptomics analysis was developed using the R Bioconductor package to generate the differential gene expression profiles of these individuals under thermal stress. The bioinformatic methodology of this work differs from the past studies, by developing an R code compilation of three methods, EdgeR, limma, and DESeq2 for differential gene expression analysis in these organisms. The output of the present work provides a comprehensive overview of gene expression profile responses of U. delphinus under climate change scenarios. Additionally, the results revealed a wide range of pathways and the corresponding genes that are impacted by thermal stress, with a particular emphasis on the up-regulation of the genes ATP6V1A, ATP6V0A1, ATP6V0A, and ATP6V1. In the chronic experiments, and high temperatures, mussels expressed these genes and, interestingly, all the pathways that these genes included appeared up-regulated. The discovered genes and pathways provide vital insights into these organisms’ adaptation tactics and identify prospective targets for monitoring and conservation efforts.",
    "authors": [
      "Silva, Beatriz Ferreira da"
    ],
    "keywords": [
      "Freshwater mussels",
      "Transcriptomics",
      "Bioinformatics",
      "Gene expression",
      "Climate change",
      "Mexilhões de água doce",
      "Transcriptómica",
      "Bioinformática",
      "Expressão genética",
      "Alterações climática",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83620",
    "title": "Plataformas móveis adaptativas para sistemas de avaliação",
    "abstract": "As Plataformas Móveis estão cada mais enraizadas no nosso quotidiano. O conforto de escolher ondeusar e a facilidade de as utilizar em qualquer lugar criam formas de informação e comunicação para asmais diversas áreas, facilitando assim num grande conjunto de tarefas. Para algumas destas áreas, adisponibilização de uma Plataforma Móvel pode ser a chave para o sucesso dos objetivos pretendidos, oferecendoao utilizador uma experiência completamente inovadora face aos métodos mais convencionais.A área do ensino encaixa perfeitamente nestes moldes, tendo em conta que o uso destas plataformaspode aumentar o envolvimento dos alunos nos seus deveres, através da projeção simples e intuitivados vários exercícios e ferramentas adequadas que fomentem os seus conhecimentos e aprendizagem,ajudando-os a alcançar melhores resultados.Tendo isso em consideração, neste trabalho de dissertação, desenvolveu-se, numa primeira etapa,uma fundamentação teórica relativamente ao uso de Plataformas Móveis e a sua evolução ao longo dosanos, abordando-se com isso uma perspetiva totalmente voltada para a área do ensino. Numa segundafase, foram detalhados e analisados os diferentes tipos de Plataformas Móveis que atualmente imperamno mercado dos smartphones, bem como alguns exemplos de plataformas especialmente criadas paraservirem como Sistemas de Aprendizagem. Numa última etapa, com a escolha do tipo de plataformaa desenvolver para a dissertação, idealizou-se e implementou-se um sistema de interação adaptativopara suporte a processos de aferição do conhecimento de estudantes ao longo do tempo, em domíniosespecíficos.",
    "authors": [
      "Nogueira, Diogo Emanuel da Silva"
    ],
    "keywords": [
      "Plataformas móveis",
      "Sistemas de avaliação de conhecimento",
      "Mobile learning",
      "Sistemas de interação",
      "Mineração de dados",
      "Dashboards",
      "Profiling",
      "Mobile plataforms",
      "Knowledge assessment systems",
      "Interaction systems",
      "Data mining",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92662",
    "title": "Ethics in IST: challenges and directions for practice and research",
    "abstract": "Ethics, as a set of moral principles that guide an individual's behavior, assisting them in doingwhat is right, must be present in any decision taken by technology and information systems professionals, even if it is an unconscious process intrinsic to the individual.Given its importance, ethics in technologies and information systems has been a subject under study for several decades; however, little research has been carried out focused on the ethical challenges of emerging technologies and, in particular, how codes of ethics are concerned with answering them. In order to fill this gap, a literature review was conducted to identify the ethical challenges of emerging technologies and to understand the main topics investigated in the literature on codes of ethics. The current versions of codes of ethics were analysed, followed by a proposal for an updated Unified Structure of Codes of Ethics.Overall, this research identified forty-five ethical challenges associated with ten emerging technologies, of which eighteen are represented in the Unified Structure, indicating that, although there is some coverage, there is still room for improvement. This thesis ends with a set of recommendations for future research, encouraging an in-depth study of ethical challenges, combined with an update of the codes of ethics to reflect the results obtained and concluding with the recommendation to study the awareness of ethical challenges by technologies and information systems’ actors.",
    "authors": [
      "Rebelo, Carla Maria Leite"
    ],
    "keywords": [
      "Codes of ethics",
      "Emerging technologies",
      "Ethical challenges",
      "Information and system technologies’ ethics",
      "Códigos de ética",
      "Desafios éticos",
      "Ética das tecnologias e sistemas de informação",
      "Tecnologias emergentes",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64742",
    "title": "Aplicações Web com requisitos de armazenamento e processamento privados",
    "abstract": "O desenvolvimento de aplicações e serviços baseados em web está a crescer todos os diascada vez mais. As facilidades que nos oferecem, entre elas a alta-disponibilidade e acessibilidade,levou a que as grandes empresas de tecnologia investissem neste tipo de tecnologias,surgindo assim aplicações como o Evernote, o Google Photos, o Dropbox, o Slack, entre outras.Associadas à utilização constante destas aplicações e serviços pelos seus clientes estãoas enormes quantidade de dados criados, bem como os dados gerados a partir destes. Coma necessidade de armazenar e processar esses de forma rápida e eficiente, estes serviçostem vindo a optar pela utilização de serviços de computação em nuvem de terceiros.Existem vantagens claras associadas à migração de dados para estas plataformas, desdea redução de custos associados armazenamento, manutenção e compra de infraestruturas,até às conveniências oferecidas pela disponibilização ferramentas de monitorização econfiguração avançadas, entre muitas outras. Associado também à utilização desta plataformasde cloud computing estão também os problemas com a privacidade dos dados por elasarmazenadas. Apesar dos esforços, por parte dos fornecedores destes serviços, em negaro acesso a entidades não autorizadas, existem ameaças fora do seu controlo e temos vistomuitas vezes que o acesso a dados sensíveis por terceiros tem um risco elevado associado.Com vista a combater este aspeto existem hoje em dia soluções capazes de garantir a confidencialidadedos dados em bases de dados relacionais e não relacionais, através de técnicascriptográficas. Estas soluções estão usualmente associadas a arquiteturas específicas deforma a precaverem sempre esta questão de segurança dos dados em todos os momentos.Estas arquiteturas implicam um maior esforço computacional do lado do cliente, pois édesse lado que se encontra toda a lóogica da aplicacional e mecanismos de segurança.Esta dissertação oferece uma nova arquitetura web onde maior parte do trabalho aplicacionalé delegado para as infraestruturas de nuvem maximizando assim o desempenhoda aplicação, tirando para isso partido da arquitetura browser servidor característica destessistemas.",
    "authors": [
      "Couto, Diogo José Linhares"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/83938",
    "title": "Extração automática de ontologias em textos de culinária não estruturados",
    "abstract": "A resolução de problemas no âmbito de um domínio específico pode adotar técnicas e ideologias distintas. Para tal, é vital e imperativo elaborar uma análise contextual a todos os elementos pertencentes à teia de relações entre conceitos. Nesse sentido, o uso de uma ontologia permite construir uma rede semântica, no qual a mais importante premissa é a correta identificação dos conceitos e respetivos atributos. A automatização do processo de extração de ontologias permite construir ontologias mais escaláveis e uniformes, extraindo conhecimento assente nas mesmas premissas e padrões. No plano geral, uma extração automática facilita a análise e a leitura de informação de um problema apresentado numa linguagem própria. O trabalho desta dissertação focou-se na extração de conhecimento em textos não estruturados, mais concretamente, textos de culinária, com o intuito de disponibilizar uma ontologia que espelhasse o conhecimento interligado entre receitas. O verdadeiro desafio passa pela correta identificação de termos relevantes, com base em análise sintática, semântica, e linguística em geral, e pela formalização de relações entre os mesmos. A utilização de mecanismos de controlo e de automatização permitiu a extração do conhecimento presente nos textos não estruturados. Estes mecanismos foram aplicados conforme as características linguísticas inerentes aos documentos e restrições de domínio. A ontologia gerada pode ser consultada através de uma plataforma web, na qual o utilizador pode pesquisar os documentos importados no sistema e analisar a interligação entre receitas através da pesquisa por termos e por hiperligações que se encontram nos detalhes de cada registo de culinária.",
    "authors": [
      "Silva, Bruno Vilas Boas da"
    ],
    "keywords": [
      "Ontologias",
      "Textos não estruturados",
      "Processamento de linguagem natural",
      "Text Mining",
      "Extração automática",
      "Análise de textos",
      "Ontology",
      "Unstructured texts",
      "Natural language processing",
      "Domain",
      "Automatic extraction",
      "Text analysis",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/91577",
    "title": "Optimized video retrieval for interior vehicle monitoring",
    "abstract": "With the rapid growth in the amount of video data, an increasing need for efficient video retrievalsystems has become an important problem in the multimedia management topic. Despite having a longpast, the increase in file size of video collections, caused mostly by the increase of video resolution andquantity of videos, originated a big push for applying Machine Learning on the video retrieval subject. Intoday’s world, when dealing with Big Data, it’s unfeasible to still rely on video metadata and manuallyannotated videos to provide an accurate video retrieval engine, seeing as the sheer quantity of videosoverwhelms an inept search and browse system, unable to provide the video the user wants. Therefore, byrelying on machine algorithms to accurately mass tag the video collection we achieve great improvements.The process of allocating the video information to the video retrieval framework is severely less time consuming and the viewer has at his disposal more precise and semantically accurate filters. This in turn,drastically reduces the quantity of redundant videos that are pulled from the user’s queries. Another wayto also ease the time it takes to analyze an immense quantity of videos, is by summarizing the contentthat is present on them. Condensing dozens of hours, pulled from one or more video streams, into a moreaccessible source of information that displays the most relevant data, is considerably a more efficientviewing experience for the user as it unburdens him of the task of surveying a grotesque amount of mediacontent. The main focus of this thesis is to implement a video summarization method for recappingfootage from the interior of a vehicle, that will be integrated on a video retrieval platform that is also beingdeveloped in parallel.",
    "authors": [
      "Dias, Diogo Barroso"
    ],
    "keywords": [
      "Video retrieval",
      "Video summarization",
      "Histogram",
      "Disparity minimization",
      "Greedy algorithm",
      "Machine learning",
      "Recuperação de vídeo",
      "Sumarização de vídeo",
      "Histograma",
      "Minimização de disparidades",
      "Algorítmo ganancioso",
      "Aprendizagem automática",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82112",
    "title": "Continuous monitoring of door-to-door postal service",
    "abstract": "Logistics services, including express mail delivery areas, have been growing significantlyby the increase in the volume of e-commerce activity worldwide. It is expected that therise in the level of digital competencies of companies and citizens will not only promoteconsiderable growth in this sector over the next few years, but also demand higher levels ofefficiency, quality, and modernization of digital platforms for interaction with customers.In terms of continuous monitoring, new technologies offer potential, namely the use ofGPS devices to collating coordinates. With system integration, the collected coordinates canbe temporarily saved and then sent to a remote server.Door-to-door service requires exact locations, so there are certain technologies, whichallow us to collect that information accurately without the minimum margin of error. In thecontext of door-to-door distribution, most companies have simple technology that providesa piece of insufficient information regarding the status of their order, they only presentinformation that the postal service may be delivered, refused, or the addressee may not befound.Regarding door-to-door distribution, technologies can be implemented to improve thecurrent industry solutions, providing more detailed information about the order status.Thus, a solution was developed based on international standards, that allow live trackingapplication ensuring also data security through blockchain technologies.",
    "authors": [
      "Costa, Carlos Daniel Martins da"
    ],
    "keywords": [
      "Logistics",
      "E-commerce",
      "System integration",
      "Live tracking",
      "Blockchain",
      "Logística",
      "Integração de sistemas",
      "Localização em tempo real",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/47439",
    "title": "Genomic and transcriptomic analyses in cancers related with viral infection",
    "abstract": "In the past 30 years, accumulated evidence has been supporting viral infection as one factorresponsible for 15-20% of human malignancies worldwide (W. S. Liang et al. 2014;McLaughlin-Drubin and Munger 2008). Studies on oncogenic viruses have proved their importanceon cellular malfunction along the carcinogenic process, and showed that their associationwith cancer can amount from 15% to 100% (McLaughlin-Drubin and Munger 2008),depending on the type of tumour. With the large amount of genomic and metagenomic informationavailable on public international consortia, such as TCGA database, it is nowadayspossible to indirectly infer viral infections from the human centred omics studies, as a portionof the reads will align in viruses and bacteria.Taking as starting point the research made by Tang et al. 2013, we focused on cervical(CESC), hepatocellular (LIHC) and head and neck squamous cell (HNSC) carcinomas, whichare known to show a high proportion of viral-positive cases (Tang et al. 2013). We downloadedRNAseq data from 309, 424 and 566 samples, respectively, and run the unmapped reads againsta reference database of viruses (downloaded from NCBI) by using the tools Batch,SAMTOOLS, Bowtie and PRINTSEQ. Quantification of each virus was performed using partsper million reads (ppm) and only viruses with ppm above 10 were considered as positivelyinfecting the sample. We confirmed that around 94% of CESC samples were infected, mostlyby HPV (Human papillomavirus) and specifically by the HPV16 strain. Nearly 32% of LIHCwere infected by HBV (hepatitis B virus). Almost 17% of HNSC samples were infected, andthe HPV16 was the most common present virus.The evaluation of differential enrichment of metabolic pathways between infected and noninfectedgroups, for each cancer type, was performed in GSEA. Signs of enrichment for infectionand immune related pathways were evident in CESC infected group, while in LIHC andHNSC infected groups the enrichment was mostly related with DNA replication and repair.This seems to indicate that infection is especially active in CESC, contradicting previous claimsthat tumorigenesis in cervix was not directly linked with infection. For the three cancer types,the viruses integrate their genome in the host genome, affecting DNA replication, maintenanceand repair. In our investigation of integration of HPV16 genome in one HNSC tumor sample,we confirmed integration in the human RAD51B gene that codes a protein involved in DNArepair by homologous recombination. We thus confirmed that HPV16 can act both as indirectand direct carcinogen. The infection, most probably through the integration of the viral genome in the host genome,increased the amount of somatic mutations in the infected group in LIHC, but not in HNSCwhere tobacco consumption is also an important carcinogen. The low number of non-infectedsamples in CESC did not allow a reliable evaluation of changes in the amount of somatic mutations.Even so, in both LIHC and HNSC infected groups, some somatic mutations occurredin the context of immune-related pathways, showing that they can contribute to render theseindividuals susceptible to infection.Also, when checking expression of HPV16 genes in five samples each from CESC andHNSC, we confirmed that E6 and E7 genes are amongst the ones more expressed in manysamples, while E2 is not expressed. E6 and E7 have been said to be preferentially integrated inthe host genome, while E2, which controls their expression, is not integrated or it is disrupted.It is believed that the overexpression of E6 and E7 initiates carcinogenesis.The viral infection rates inferred here from mining the omics databases are very similar tothe ones evaluated by standard methods (Tang et al. 2013), showing that public internationalconsortia can indirectly provide interesting insights into the involvement of viral infection intumorigenesis. The high number of samples per tumor, the wide geographic origin of the samples,and the high-throughput characterisation for different omics platforms allows multilayercomparisons and evaluations, in a scale not affordable before.",
    "authors": [
      "Ferreira, Joana Catarina da Rocha"
    ],
    "keywords": [
      "Cancer",
      "Viral infection",
      "RNAseq",
      "Cervical carcinoma",
      "Hepatocellular carcinoma",
      "Head and neck squamous cell carcinoma",
      "Cancro",
      "Infecção viral",
      "Carcinoma do colo do útero",
      "Carcinoma hepatocelular",
      "Carcinoma da cabeça e pescoço",
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2016",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92701",
    "title": "Desenvolvimento de um modelo de estimativa de pose usando Deep Learning",
    "abstract": "A estimação de pose procura de permitir aos computadores calcular a pose do corpo humano atravésda utilização de sensores Unidades de medição inercial (IMU). Por esta razão, apresenta diversasutilizações na indústria, onde pode ser usado na melhoria da colaboração entre humanos e robôs, na qualpermite aos robôs monitorizar os movimentos humanos. A estimação de pose também tem aplicaçõesno setor médico, mais precisamente na avaliação de riscos ergonómicos, reabilitação e desportos.Esta dissertação visa o desenvolvimento de um modelo de estimação de pose utilizando Deep Le arning, através da fusão de dados provenientes de sensores acelerómetro, giroscópio e magnetómetro,existentes nos IMU.Para alcançar este objetivo foi implementada uma framework em Pytorch, onde estão elaboradosos modelos para a estimativa de pose, na qual estão incluídos modelos que usam redes neuronais emodelos que usam redes híbridas, que juntam redes neuronais e filtros. Nesta framework, tambémfoi implementado um modelo de calibração de dados (CalibNet), que usa redes neuronais e procuraa diminuição do erro de calibração dos dados provenientes dos sensores, para posteriormente seremusados na estimativa de pose.Na avaliação dos métodos implementados foram usados dois datasets o Ergowear e o MTw Awinda,onde se obtiveram como melhores resultados de 18,78º e de 7,556º, respetivamente, na estimativa depose.",
    "authors": [
      "Araújo, André Filipe Oliveira"
    ],
    "keywords": [
      "Estimação de pose",
      "Unidades de medição inercial (IMU)",
      "Redes neuronais",
      "Pose estimation",
      "Neural networks",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/79400",
    "title": "Extracting knowledge from documents related with invasive fungal infections in iron overload context",
    "abstract": "Invasive fungal infections caused by Candida are associated with high mortality and morbidity rates in hospitalized patients. Iron plays a major role in these infections, as they are exacerbated underiron overload conditions. In this context, it is important to understand the association between iron levels and invasive fungal infections, as it can serve as an indicator of the severity of the disease, andeventually it can help establish measures to improve treatment efficacy.Nowadays, manually inferring these associations from biomedical documents is a time consuming task, due to the high amount of available scientific text data. As such, these tasks naturally benefit from the Biomedical Text Mining field, which includes a wide variety of methods for automatic extraction of high-quality information from biomedical text documents.In this work, relevant documents related to iron overload and fungal infections were retrieved from PubMed to build a corpus. Then, both Named Entity Recognition and Relation Extractionprocesses were executed using the @Note text mining tool. Finally, relevant sentences were manually extracted and a curated dataset with documents containing those sentences was created.Since the number of publications obtained about Candida and iron overload was very low, the analysis was made taking into account all fungi. A total of 15 publications were considered relevant and 168 relevant associations were extracted.Although associations of iron levels with both severity of infection and treatment efficacy were not extracted, it was possible to conclude that, in many cases, iron overload is a predictor for fungal infections, and patients’ iron levels highly affect treatment efficacy.The Biomedical Text Mining process described in the present thesis enabled the creation of a dataset of relevant biomedical publications containing interesting associations between fungal infections, drugs and associated diseases in a clinical context of iron overload, although in the future this process could be improved, especially regarding dictionaries, in order to obtain a higher number of relevant publications.",
    "authors": [
      "Rodrigues, Andreia Dóris Pedras"
    ],
    "keywords": [
      "Biomedical text mining",
      "Invasive fungal infections;",
      "Iron overload",
      "Mineração de textos biomédicos",
      "Infeções fúngicas invasivas",
      "Excesso de ferro",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/82803",
    "title": "Monitorizações na camada Middleware e os seus desafios",
    "abstract": "The MiddleWare layer is an abstraction method that acts as an intermediary in a softwareinfastructure implementing interoperability between existing applications, operating systems,networks and the hardware of a distributed system.It is considered a cross-platform tool capable of providing an essential programmingabstraction to this type of systems, allowing for easier management of the inherent heterogeneity in these. A Middleware solution application allows a responsible user to orchestratemessage flows, to prepare their contents so that they always reach their destination in theformat they need. It also provides the users with the possibility to obtain information inreal time regarding the performance of the systems it encompasses, allowing evaluation andconsequent action to improve efficiency, in order to achieve the requirements of the systems’operation.The project to be developed, aims to take advantage of Internet of Things and Middlewaretechnologies and concepts, applying them to the creation of a service monitoring tool essential to a more efficient performance of a distributed system. The services and architecturesto focus with greater attention are Micro-Services, SOA Architecture, ETL processes andEvents, of which one will be chosen to be the focus in the development of the project.",
    "authors": [
      "Ramos, José Lopes"
    ],
    "keywords": [
      "MiddleWare",
      "Micro-Services",
      "Soa architecture",
      "Events",
      "Arquitetura SOA",
      "Eventos",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2021",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/27589",
    "title": "Caracterização de tráfego de rede : Cloud Storage na Universidade do Minho",
    "abstract": "Monitoring of Internet services reveals that there is a growing trend in the use of Cloud Services.Considering the strong growth on the access to these services in a relatively short period of time,it is estimated that shortly they will be responsible for a significant amount of Internet flows.In this context, this project aims to identify and quantify the use of Cloud Services at theUniversity of Minho (UM). Achieving this goal involves identifying appropriate techniques fortraffic classification and the definition of a model for processing the collected traces. Attendingto the available set of Cloud Services, this study focuses on characterizing Cloud Storageservices, identifying the most accessed Cloud Storage Providers and the characteristics of thecorresponding traffic.Cloud Storage services present several characteristics that turn the current classification methodsinsufficient or too complex to apply, namely the use of dynamic communication ports and securityprotocols encrypting the traffic. This motivates the use of a new classification approach basedon Tstat tool, which uses the technique of extracting signatures of servers during the handshakeof Secure Sockets Layer (SSL) protocol.The obtained results provide global statistics regarding the most used services at UM, focusingsubsequently on Cloud Storage services. For these, the top Cloud Storage Providers withinUM users preferences are identified and the characteristics of the traffic associated to each oneare discussed.",
    "authors": [
      "Oliveira, Daniela Catarina Ferreira de"
    ],
    "keywords": [
      "621.39",
      "681.324"
    ],
    "date": "2013",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [
      "621.39",
      "681.324"
    ],
    "subjects_fos": [],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59695",
    "title": "Aplicações Java em arquiteturas paralelas de acesso não uniforme à memória",
    "abstract": "Durante várias décadas, o aumento do desempenho dos processadores era conseguidomaioritariamente através do aumento da frequência de relógio. Contudo, aumentar o desempenhoatravés do aumento da frequência tornou-se cada vez mais difícil conduzindoa problemas de consumo energético e dissipação de calor. Para resolver estes problemas,as arquiteturas mais recentes evoluíram num sentido de aumentar o número de processadores,e mais tarde, o número de núcleos. As arquiteturas de memória inicialmenteadotadas, designadas arquiteturas de acesso uniforme à memória (UMA), apresentaramalgumas limitações. Entre as arquiteturas UMA existentes, existe a arquitetura de multiprocessamentosimétrico (SMP). Esta arquitetura possui múltiplos processadores queacedem à memória principal utilizando um barramento partilhado pelos processadoresque conduziu a problemas de contenção no acesso à memória principal.As arquiteturas de acesso não uniforme à memória (NUMA) surgiram durante os anos90 com múltiplos bancos de memória. No entanto, os programadores que queiram tirartotal partido das vantagens desta arquitetura, terão que lidar com novos desafios ao nívelda programação, ao nível da afinidade dos fios de execução e das alocações de memória.Esta dissertação mostra que a forma como os algoritmos memory-bound acedem adados de memória principal numa arquitetura NUMA, pode ter impacto no seu desempenho.Um conjunto de testes foi utilizado para demonstrar em que medida várias técnicasde afinidade podem contribuir para aumentar o desempenho de aplicações Javae C em arquiteturas NUMA. Os testes realizados com recurso a diferentes abordagenstais como ferramentas do sistema operativo, opções da JVM, técnicas de programaçãoe até variáveis de afinidade para os compiladores, foram usados para aumentar a desempenhode dois casos de estudo em arquiteturas NUMA. A utilização destas abordagenspermitiram aumentar o desempenho com um ganho adicional de até 1.8 vezes para asimplementações em Java, e de até 2.16 vezes para as versões em C das mesmas aplicações.",
    "authors": [
      "Sá, Carlos Diogo da Silva"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Outras Engenharias e Tecnologias"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64274",
    "title": "An SNMP-based audio distribution service architecture",
    "abstract": "The constant growth of integration and popularity of “Internet of Things”devices is affecting home automation systems, where new technologies wereintroduced, in the recent years for this particular sector. These automationsystems integrate devices that can be anywhere in the house, connectedto a home network, either through a wire or wireless connection. A homeautomation system can be used to control air conditioning, lighting, poolcontrol systems, home-entertainment systems and much more.Within the field of home-entertainment systems, the best known technologiesare the Digital Living Network Alliance and the Digital Audio AccessProtocol, which provide interoperability to allow sharing of digital mediacontent between devices across a home network. However, these technologieshave the disadvantage of being proprietary, maintaining restrict documentationaccess, complex architectures and concepts and not optimal to specificpurposes, like audio distribution.The main goal of this project was to prove that is possible to use standardizedprotocols, such as the Simple Network Manager Protocol and opensource tools in order to develop a music distribution service that allows theimplementation of similar features than the ones already existing proprietarytechnologies. As such, the implementation prototype system allows a userto manage and play audio from a music collection that is stored in a singlehome audio server. The system architecture enables audio streaming betweenthe server and the various devices in the same local network. Further more,the music collection, can integrate virtual audio files that are available fromexternal music sources, like iTunes, etc.",
    "authors": [
      "Coelho, Vasco Miguel Gonçalves"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/59690",
    "title": "Utilização de process mining no estabelecimento de ações de web marketing",
    "abstract": "A mineração de processos - process mining - define-se como uma técnica de extração de informação em que, de uma forma automática, se extrai a partir de um registo de eventos informação relevante acerca do desempenho de uma dada organização ou sistema numa dada área de negócio. As técnicas de mineração de processos podem ser aplicadas em diversos contextos aplicacionais, como a informática, a medicina ou o marketing. Relativamente a esta última área, a mineração de processos pode ser utilizada para estabelecer um conjunto de ações que permitam lançar uma dada campanha na Web, tendo como base pontos de maior intensidade de ações Web. Deste modo, nesta dissertação realizou-se um estudo pormenorizado acerca de como estabelecer perfis e preferências de exploração Web a partir da informação obtida através dos processos de navegação e exploração Web dos utilizadores, e definir um conjunto de ações específicas para suporte a uma dada campanha de Web Marketing de forma (semi)automática.",
    "authors": [
      "Pinto, Filipe de Passos"
    ],
    "keywords": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/84353",
    "title": "Detetor de conteúdo multimédia falso gerado através de algoritmos Deep Fake",
    "abstract": "sociedade encontra-se a gerar um volume de dados sem precedentes na história. Apesar disso, adesinformação tem vindo a crescer, provocando preocupações no meio jornalístico e democrático. Esteproblema se tornou ainda mais evidente com os avanços tecnológicos, como a capacidade de manipularo significado semântico de imagens, vídeos ou áudios. Os resultados destas manipulações impõemuma dificuldade em distinguir entre conteúdo falso e original. Desta forma, os conteúdos multimédiamanipulados são chamados de deepfake. Esta nomenclatura é resultado da combinação dos termosDeep Learning (DL) e fake.Para solucionar tal problemática trabalhos no estado da arte apresentam diferentes algoritmos paraclassificação de vídeos deepfake, dos quais utilizam-se de arquiteturas neuronais baseadas em ConvolutionalNeural Network (CNN), Long Short-Term Memory (LSTM) ou redes Transformers. Há trabalhos dosquais apresentam resultados positivos ao classificar dados deepfake criados a partir de uma determinadatécnica para geração deepfake. No entanto, estes em sua maioria não apresentam resultados quandoseus modelos são confrontados com dados gerados a partir de outra técnica deepfake distinta dos dadosutilizados no treino do modelo. Consequentemente, há um lacuna nos trabalhos para uma classificaçãomais generalista independente do método utilizado para criação do conteúdo manipulado.Desta forma, as experiências desenvolvidas nesse trabalho utilizaram redes neuronais em diferentesestratégias. Resultando na proposta de uma solução para tal lacuna encontrada no estado da arte, acapacidade de um modelo ao classificar um vídeo falso independente da técnica de criação. Após asexperiências escolheu-se o algoritmo de multi-classificação para a utilização no detetor. Para uma maiorprecisão na análise dos vídeos, o detetor possui níveis de confiança (Não Confiável, Nada Confiável, MuitoConfiável) para cada classificação realizada. Ao validar o detetor, com vídeos das quatro técnicas deepfakee vídeos originais, foi possível saber que o nível de confiança ”Muito Confiável”a precisão média é de 91%quando a label era deepfake, independente da técnica (DeepFakes, Face2Face, FaceSwap e NeuralTextures)utilizada para criá-lo, enquanto que ao rotular dados como reais com o mesmo nível de confiançaobtém um precisão de 60%.",
    "authors": [
      "Silva, Leonardo de Jesus"
    ],
    "keywords": [
      "Conteúdo multimédia",
      "Deepfake",
      "Conteúdo sintético",
      "Deep learning",
      "Detetor",
      "Multimedia content",
      "Synthetic content",
      "Detector",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2022",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/92640",
    "title": "Estratégias de monitorização de tráfego em cidades inteligentes",
    "abstract": "Atualmente, mais da metade da população mundial vive em centros urbanos e as estatísticas indicam queem 2050 essa percentagem rondará os 70%. A forte concentração da população em urbes, apresentagrandes desafios, principalmente devido à densidade populacional, habitação, circulação, ou a escalabi lidade de serviços. Gerir esta realidade, garantindo as condições indispensáveis para uma alta qualidadede vida, é um desafio que as tecnologias inteligentes poderão ajudar a conseguir. A União Europeia de fine cidades inteligentes, ou smart cities, como um conjunto de sistemas e de pessoas que interagem deforma inteligente utilizando energia, materiais, serviços e recursos de forma sustentável. Assim, estima-seque o valor das tecnologias relacionadas com o controle e a monitorização do tráfego em smart cities éproporcional à redução dos acidentes de trânsito, congestionamentos urbanos, e outros impactos sociais.São exemplos, a necessidade de comunicação ou controle de tráfego a partir de ferramentas inteligentesque na atualidade é difícil de manipular já que possuem grande impacto económico e social. Para esseefeito, é necessário a implementação de técnicas ou estratégias (amostragem, agregação e filtragem) quevão permitir monitorar fluxos de dados, a fim de garantir eficiência no tratamento de grandes volumes dedados nos múltiplos contextos das cidades. O objetivo desta dissertação é efetuar uma análise critica so bre estratégias de monitorização veicular, seu impacto e suas limitações frentes aos grandes volumes detráfego gerados pelas smart cities. Avaliam-se ainda técnicas contextuais que serviram para a construçãode soluções frente aos desafios da mobilidade e transportabilidade no contexto urbano.",
    "authors": [
      "Eurico, Isaías Chico Nambissi"
    ],
    "keywords": [
      "Monitorização de tráfico veicular",
      "Estratégias de monitorização",
      "Cidade inteligente",
      "Técnicas de monitorização veicular",
      "Vehicle traffic monitoring",
      "Monitoring strategies",
      "Smart city",
      "Vehicle monitoring techniques",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/56113",
    "title": "Development of an automated pipeline for meta-omics data analysis",
    "abstract": "Knowing what lies around us has been a goal for many decades now, and the new advances in sequencing technologies and in meta-omics approaches have permitted to start answering some of the main questions of microbiology - what is there, and what is it doing?The exponential growth of omics studies has been answered by the development ofsome bioinformatic tools capable of handling Metagenomics (MG) analysis, with a scarcefew integrating such analysis with Metatranscriptomics (MT) or Metaproteomics (MP) studies.Furthermore, the existing tools for meta-omics analysis are usually not user friendly,usually limited to command-line usage.Because of the variety in meta-omics approaches, a standard workflow is not possible,but some routines exist, which may be implemented in a single tool, thereby facilitatingthe work of laboratory professionals. In the framework of this master thesis, a pipeline forintegrative MG and MT data analysis was developed. This pipeline aims to retrieve comprehensivecomparative gene/transcript expression results obtained from different biologicalsamples. The user can access the data at the end of each step and summaries containing severalparameters of evaluation of the previous step, and final graphical representations, likeKrona plots and Differential Expression (DE) heatmaps. Several quality reports are alsogenerated. The pipeline was constructed with tools tested and validated for meta-omicsdata analysis. Selected tools include FastQC, Trimmomatic and SortMeRNA for preprocessing,MetaSPAdes and Megahit for assembly, MetaQUAST and Bowtie2 for reporting onthe quality of the assembly, FragGeneScan and DIAMOND for annotation and DeSEQ2 forDE analysis.Firstly, the tools were tested separately and then integrated in several python wrappers toconstruct the software Meta-Omics Software for Community Analysis (MOSCA). MOSCAperforms preprocessing of MG and MT reads, assembly of the reads, annotation of theassembled contigs, and a final data analysis.Real datasets were used to test the capabilities of the tool. Since different types of filescan be obtained along the workflow, it is possible to perform further analyses to obtainadditional information and/or additional data representations, such as metabolic pathwaymapping.",
    "authors": [
      "Costa, João Carlos Sequeira"
    ],
    "keywords": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "date": "2017",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Ciências Naturais::Ciências da Computação e da Informação"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/38681",
    "title": "Avaliação do desempenho de indicadores com base na metodologia ROC (Receiver Operating Characteristic)",
    "abstract": "O objetivo deste estudo é avaliar e comparar o desempenho de dois indicadores de previsão de risco de mortalidadeneonatal para recém-nascidos de muito baixo peso (<1500g), o CRIB (Clinical Risk Index for Babies) e o SNAPPEII (Score for Neonatal Acute Physiology-Perinatal Extension II), com recurso à metodologia ROC (Receiver OperatingCharacteristic).A execução prática deste estudo foi suportada com auxílio a programas estatísticos próprios para a análise dametodologia ROC, como o SPSS, ROCNPA, Comp2ROC, ROCR e caTools.Os dados que contemplam o presente estudo foram recolhidos pelas unidades de cuidados intensivos neonataisdo território português entre 2010 e 2012, e enviados para o Registo Nacional de Recém-Nascidos de Muito BaixoPeso (RNMBP), que é a entidade responsável pelo armazenamento desta informação.Será aferida também a comparação e avaliação de variáveis de elevada expressão na previsão da mortalidade,que compõem os indicadores de mortalidade em estudo, sendo elas, o Peso à Nascença e a Idade Gestacional.A amostra em estudo é composta por 789 recém-nascidos de muito baixo peso, dos quais 51,3% são do géneromasculino. Em média os recém-nascidos em questão apresentam um peso médio ao nascimento de 1214 g ±343,1e 29,8 ±2,5 semanas de gestação e, dos integrantes na amostra 11,3% foram declarados óbitos hospitalares.A exatidão dos indicadores de mortalidade e das variáveis foi obtida através do cálculo da AUC, área abaixo dacurvaROC,queparaoCRIBfoide0,876±0,025,paraoSNAPPE-IIde0,867±0,026,seguindo-sedasvariáveisidadegestacional e o peso ao nascimento com 0,785 ±0,032 e 0,782 ±0,028, respetivamente.Com base nos resultados obtidos durante a elaboração do presente estudo, o CRIB provou ser melhor em predizer a mortalidade para recém-nascidos de muito baixo peso, e tem a seu favor um menor número de variáveiscomparativamente ao SNAPPE-II.",
    "authors": [
      "Araújo, Joana Margarida Rodrigues Barros de"
    ],
    "keywords": [
      "Recém-nascidos de muito baixo peso",
      "CRIB",
      "SNAPPE-II",
      "Metodologia ROC",
      "Very low birth weight infants",
      "ROC methodology",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2015",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/88579",
    "title": "Desenvolvimento de um sistema de ensino para crianças com Perturbação do Espectro do Autismo com recurso a um robô humanoide",
    "abstract": "Progressivamente, os robôs tendem a ser mais cooperativos e inteligentes, para que possam interagir de forma natural com os seres humanos. Uma tipologia de robôs frequentemente utilizada na interação com os seres humanos são os robôs humanoides, uma vez que se assemelham fisicamente à aparência do corpo humano e são capazes de prestar auxílio na realização de diversas tarefas do quotidiano. Ao longo da última década, investigadores têm estudado a viabilidade do uso de robôs humanoides no estímulo da interação social em crianças com Perturbação do Espectro do Autismo (PEA). O autismo é um transtorno do neurodesenvolvimento que se manifesta precocemente, onde o comportamento dos indivíduos é caracterizado por padrões repetitivos, atividades ou interesses restritos e pela dificuldade na comunicação/ interação social.Neste contexto, a presente dissertação tem como objetivo o desenvolvimento de um sistema de ensino interativo, capaz de promover o desenvolvimento sócio emocional em crianças com PEAutilizando, como mediador, um robô humanoide. O sistema desenvolvido possui uma arquitetura capaz de incorporar diferentes atividades de ensino e utiliza o algoritmo You Only Look Once(YOLO) para deteção de objetos em imagens. Com a finalidade de testar o sistema desenvolvido, foram desenvolvidas duas atividades de ensino, nomeadamente, o ensino de figuras geométricas e cores. Finalmente, o sistema desenvolvido foi testado de duas formas distintas: a) sem o robô humanoide, sendo este substituído pela voz do computador; b) com robô humanoide, sendo este responsável pela interação robô-humano. Os resultados revelam que o sistema de ensino desenvolvido é capaz de detetar, com elevada percentagem de precisão, as figuras geométricas e as cores pré-definidas na lista de atividades, podendo assim ser uma ferramenta promissora no ensino de crianças com dificuldades de aprendizagem.",
    "authors": [
      "Alves, David Miguel Duarte Rodrigues"
    ],
    "keywords": [
      "Inteligência Artificial",
      "Reconhecimento de objetos",
      "Robô ZECA",
      "Perturbação do Espectro do Autismo (PEA)",
      "You Only Look Once (YOLO)",
      "Artificial intelligence",
      "Image recognition",
      "ZECA robot",
      "Autism Spectrum Disorder (ASD)",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2023",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/64259",
    "title": "Apreensão e discretização de ambientes tangíveis em sistemas de realidade aumentada",
    "abstract": "A Realidade Aumentada (RA) caracteriza-se pela mistura de elementos virtuais no mundo real de forma interativa e em tempo real. O conceito de RA levanta uma ampla variedade de questões quanto à coerência visual entre os objetos reais e virtuais num ambiente. De forma a melhorar o processo de inclusão destes elementos no meio físico foram criadas várias técnicas e algoritmos de visão por computador que através do mapeamento de espaços físicos, extração de características e marcadores fiduciais de objetos, verificação, deteção, identificação, classificação, entre outros, permitem analisar e estruturar o conteúdo de uma cena.O maior desafio que se coloca com a realização desta proposta de dissertação encontra-se associado à forma como é extraída e processada a informação que conseguimos obter a partir dos sensores que complementam os dispositivos de RA hoje em dia, a fim de representar e compreender, da melhor forma possível, os ambientes que nos rodeiam e preparar um espaço apto para a introdução e apresentação de conteúdo virtual com a maior harmonia.Neste documento é possível encontrar o estado da arte relativo aos temas previamente citados a fim de explorar, melhorar e desenvolver novas técnicas e paradigmas para, a partir da informação dos sensores mais genéricos encontrados em muitas das tecnologias móveis e óculos de realidade aumentada mais atuais, extrair várias características do cenário e objetos envolventes em tempo real. O processamento e tratamento desta informação tem como objetivo final realizar o reconhecimento e compreensão da cena e objetos que se encontram no espaço que rodeia estes sensores.Em paralelo à realização desta proposta de dissertação, foi desenvolvida uma framework denominada “Tangible Environments in Augmented Reality Systems (TEARS)” com o objetivo de demonstrar tudo o que é discutido neste documento não só como algo para fins de investigação científica, mas também para utilização e apoio num projeto e protótipo realizado no âmbito da unidade curricular do 5ºAno do Mestrado Integrado em Engenharia Informática (MIEI) de Projeto em Engenharia Informática (PEI) e que apresenta o título: “Assistência Remota com Realidade Mista (ARRM)”.",
    "authors": [
      "Silva, André Filipe Proença e"
    ],
    "keywords": [
      "Visão por computador",
      "Realidade aumentada",
      "Análise de cena",
      "Contexto",
      "Objetos",
      "Reconhecimento",
      "Computer vision",
      "Augmented reality",
      "Scene understanding",
      "Context",
      "Objects",
      "Recognition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2018",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/93039",
    "title": "Sistema de apoio a pessoas idosas baseado em aprendizagem automática",
    "abstract": "No presente trabalho foi desenvolvida uma solução de apoio a pessoas idosas, doentesou com limitações físicas, baseada em técnicas de aprendizagem automática e visão porcomputador. Os modelos de aprendizagem profunda utilizados resolvem problemas declassificação de imagens. Para desenvolver a solução proposta foi utilizada a linguagemPython e as bibliotecas TensorFlow, MediaPipe e OpenCV. Antes de treinar os modelos deaprendizagem automática, foram aplicadas técnicas de pré processamento às imagens,para as preparar para os modelos classificadores. A solução final desenvolvida combinaduas tarefas de classificação diferentes: a estimação da pose humana e o reconhecimentode expressões faciais. Para estimar a pose humana, primeiro utilizou-se um algoritmoque identifica a posição das articulações do corpo, sendo estas posições posteriormenteclassificadas com uma rede neuronal convolucional em três classes, queda, sentado e empé/a andar. Para efetuar o reconhecimento de expressões faciais utilizaram-se dois tiposde dados, os atributos de cor dos pixeis das imagens e os pontos de referência das facespreviamente identificadas. Estes dados foram depois classificados por uma rede neuronalhíbrida, que inclui uma rede completamente ligada a uma rede convolucional. A soluçãofinal proposta combina estes dois modelos, o que permite a partir de uma imagem de umapessoa, gerar um aviso se o modelo de estimação da pose detetar uma queda ou quando omodelo de reconhecimento de expressões faciais identificar uma expressão de dor. O modelode estimação da pose identificou a classe queda com uma precisão de 97%, um recall de 98%e uma acurácia de 97%. A expressão de dor foi identificada pelo modelo de reconhecimentode expressões faciais com uma precisão de 82%, um recall de 86% e uma acurácia de 92%.O maior desafio no reconhecimento da expressão foi a deteção da face nas imagens.",
    "authors": [
      "Fontes, Rui Miguel Carvalho da Silva"
    ],
    "keywords": [
      "Aprendizagem automática",
      "Aprendizagem profunda",
      "Estimativa da pose humana",
      "Reconhecimento de expressões faciais",
      "TensorFlow",
      "OpenCV",
      "MediaPipe",
      "Machine learning",
      "Deep learning",
      "Human pose estimation",
      "Facial expression recognition",
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "por",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Engenharia Eletrotécnica, Eletrónica e Informática"
    ],
    "collections": []
  },
  {
    "id": "oai:repositorium.sdum.uminho.pt:1822/94063",
    "title": "Functional graphene-based coatings for contact lenses and glucose sensing",
    "abstract": "Diabetes mellitus, uma doença caracterizada por níveis elevados de açúcar no sangue, é frequentemente monitorizada através de protocolos invasivos e dolorosos, o que resulta em redundância entre os doentes. Esta monitorização é geralmente efetuada através do rastreio dos níveis de açúcar no sangue. Devido à sua grande intimidade com o sangue, as lágrimas estão a ser consideradas como um outro potencial bio fluído de diagnóstico. As lágrimas contêm múltiplos biomarcadores relacionados com várias doenças sistémicas e, como tal, podem ser utilizadas para monitorizar, diagnosticar e tratar estas doenças utilizando protocolos não invasivos. No entanto, as concentrações dos analitos são muito mais baixas nas lágrimas do que no sangue. Estão a ser desenvolvidos dispositivos específicos para detetar essas baixas concentrações. As lentes de contacto, em particular, são utilizadas por milhões de pessoas com problemas de visão e têm potencial para serem transformadas em dispositivos portáteis funcionais. As lentes de contacto necessitam de materiais de elétrodos biocompatíveis e estáveis para serem integrados em plataformas de biossensores. O grafeno, uma camada atomicamente fina de átomos de carbono, apresenta propriedades favoráveis, como a biocompatibilidade, a elevada condutividade elétrica, a fácil funcionalização e a flexibilidade. Devido à sua espessura atómica e excelente mobilidade de portadores, este material pode ser utilizado para fabricar transístores de efeito de campo de grafeno para bio deteção, com potencial para atingir uma sensibilidade ultraelevada.Este trabalho explora tecnologias baseadas em grafeno para permitir uma plataforma inovadora para a deteção de glucose, potencialmente integrada em lentes de contacto. Para tal, foram testadas e otimizadas as condições para o fabrico de uma lente de contacto à base de grafeno. Em primeiro lugar, foi abordada a transferência do grafeno para a superfície altamente complexa das lentes de contacto. Em segundo lugar, foi estudada uma funcionalização do grafeno, concebida para induzir as reações químicas envolvidas na deteção da glucose, nas várias fases intermédias, para compreender as interações fundamentais que ocorrem. Por último, foram utilizados transístores de efeito de campo de grafeno com gate eletrolítica, fabricados com o grafeno funcionalizado, para detetar a glucose numa vasta gama de concentrações.",
    "authors": [
      "Lopes, Vicente Silva"
    ],
    "keywords": [
      "Diabetes mellitus",
      "Lentes de contacto",
      "Grafeno",
      "Transístores de efeito de campo de grafeno com gate eletrolítica",
      "Contact lenses",
      "Graphene",
      "Electrolyte-gated graphene field-effect transistors",
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "date": "2024",
    "type": "info:eu-repo/semantics/masterThesis",
    "language": "eng",
    "subjects_udc": [],
    "subjects_fos": [
      "Engenharia e Tecnologia::Nanotecnologia"
    ],
    "collections": []
  }
]